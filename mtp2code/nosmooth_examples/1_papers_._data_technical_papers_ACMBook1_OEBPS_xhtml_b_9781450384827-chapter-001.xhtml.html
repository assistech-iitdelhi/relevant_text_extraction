 <!DOCTYPE html>
                    <html>
                    <body style="background-color:black;">
<img src="../data/technical_papers/ACMBook1/OEBPS/images/b_9781450384827-001_fig_001.jpg" style="background-color:white;">
<h2 style="color:white;">Fig caption sentence =  figure 1.1causal hierarchy. questions at each level can be answered only if information from the lower level is available [pearl 2019].. this figure contains level ( symbol ) 1. association plylx ), typical activity seeing, 2. intervention plyldo ( x ) , z ), doing intervening, typical questions examples what is ? how would what does a symptom seeing x change my tell me about a disease ? belief in y ? what does a survey tell us about the election results ? what if ? what if i do x ? what if i take aspirin , will my headache be cured ? what if we ban cigarettes ? why ? was it x that was it the aspirin that caused y ? what if i had stopped my headache ? acted differently ? would kennedy be alive had oswald not shot him ? what if i had not been smoking the past two years ?, 3. counterfactuals ply , lx , y ), imagining , retrospection. </h2>
<img src="scatter_plots/1_papers_._data_technical_papers_ACMBook1_OEBPS_xhtml_b_9781450384827-chapter-001.xhtml.png" style="background-color:white;" width="auto" height="400">
<p>----------------- START ---------------------------</p>
<p style="color:darkturquoise;"> Chapter 01 </p>
<p style="color:grey;"> 1Introduction </p>
<p style="color:darkturquoise;"> The quest for human knowledge is driven by the desire to manage future events </p>
<p style="color:darkturquoise;"> From the ancient art of astrology to the harder, more formal, modern data-based sciences, the main purpose of a study is to predict a future state and take corrective action </p>
<p style="color:darkturquoise;"> If we understand the natural world, we can predict and try to manage the outcomes </p>
<p style="color:grey;"> This is valid in many applications ranging from major storms to developing and employing technology to create medicines as needed. </p>
<p style="color:grey;"> Advances in sensing coupled with progress in data processing and storage have resulted in a rapid rise in the ability to collect and use large volumes of data about the real world </p>
<p style="color:grey;"> The so-called big data has in turn spawned popular new disciplines, such as data science, machine learning, and data mining </p>
<p style="color:grey;"> The popularity of machine learning in particular is primarily due to its application in building models of systems that were not previously possible due to data sparsity </p>
<p style="color:grey;"> Today, machine learning and artificial intelligence (AI) are used in almost any application that we can imagine </p>
<p style="color:grey;"> Of course, this is only possible if we collect sufficient data to model the event using emerging advanced techniques </p>
<p style="color:darkturquoise;"> Prediction in dynamic situations requires modeling relationships among events instead of just static objects </p>
<p style="color:darkturquoise;"> Although correlations can sometimes be used, causality is required to precisely model spatio-temporal relationships among events </p>
<p style="color:grey;"> Current machine learning techniques are powerful in capturing correlations, but they are mostly black boxes that are unable to explain the reasons behind their predictions </p>
<p style="color:grey;"> In most applications that deal with future events, dynamic data streams that include a combination of events in time and space are essential </p>
<p style="color:darkturquoise;"> Instead of object recognition through static data objects, predictive systems must create event recognition models based on causal relationships among a set of events in related data streams </p>
<p style="color:grey;"> This book is about advancing the emerging techniques in AI and machine learning to detect events in data streams and then build models by discovering relationships among events to recognize specific points of interest </p>
<p style="color:grey;"> We call this explanatory process event mining </p>
<p style="color:darkturquoise;"> It has two distinct phases: hypothesis formation and hypothesis testing </p>
<p style="color:darkturquoise;"> Hypothesis formation requires the analysis of large amounts of event data to find strong candidate hypotheses </p>
<p style="color:limegreen;"> Based on expert knowledge and judgment, one or more hypotheses are then formed for further testing via relevant data analysis </p>
<p style="color:darkturquoise;"> Thus, event mining relies on contextual data analysis that is guided by the knowledge of the domain expert </p>
<p style="color:darkturquoise;"> The result of the event mining process is an explainable model that explains causal relationships in the form of structural and temporal local patterns in the data </p>
<p style="color:darkturquoise;"> 1.1Correlation is the Mother of Causality </p>
<p style="color:darkturquoise;"> Because future events are unknown, prediction is often used as a guide for making plans and taking action </p>
<p style="color:grey;"> Predictive analytics uses models based on historical data to predict future events </p>
<p style="color:grey;"> Typically, historical data are used to build a mathematical model that captures important trends </p>
<p style="color:darkturquoise;"> That predictive model is then used on current data to predict what will happen next, or to suggest actions to take for optimal outcomes </p>
<p style="color:grey;"> Models are essential for prediction, which in turn are essential for shaping the future via control systems and processes </p>
<p style="color:grey;"> Scientific approaches are based on a well-established principle: objective reality is governed by natural laws that can be discovered by systematic observation and experimentation </p>
<p style="color:grey;"> Objectivity is important and is accomplished via data collection and analytical processes </p>
<p style="color:grey;"> Scientific disciplines that collect enough data can advance over time, but they often need to wait for technology to catch up to record key observations </p>
<p style="color:grey;"> With the availability of objective data streams in some application domains, machine learning has been proven to be very successful in processing the data and making actionable predictions </p>
<p style="color:grey;"> For example, fraud detection in finance and banking, predictive maintenance in industrial and other applications, product recommendation in retail, and drug discovery and personalized medicine in healthcare are now commonly used </p>
<p style="color:grey;"> With this success, there are increasing expectations for autonomous systems to exhibit human-level intelligence </p>
<p style="color:grey;"> However, there are some fundamental obstacles in the widespread utilization of machine learning as an adoptable and robust solution in many domains </p>
<p style="color:limegreen;"> One of the main challenges is understanding cause effect connections. </p>
<p style="color:darkturquoise;">## 37:  Judea Pearl [2019] argues that understanding cause effect connections is the hallmark of human cognition and is the necessary ingredient for achieving human-level intelligence </p>
<p style="color:limegreen;">## 38:  He outlined a three-level modeling hierarchy with the questions that can be answered at each level (Figure 1.1) </p>
<p style="color:limegreen;">## 39:  These levels are association, intervention, and counterfactual </p>
<p style="color:darkturquoise;">## 40:  Association is purely based on statistical relationships </p>
<p style="color:grey;">## 41:  Machine learning algorithms are very advanced in finding common patterns and formulating these associations </p>
<p style="color:darkturquoise;">## 42:  For instance, a person who drinks more than one cup of coffee might experience disturbed sleep later that night; such associations can be observed from data alone </p>
<p style="color:grey;">## 43:  Predictive modeling is the main focus in this layer </p>
<p style="color:limegreen;">## 44:  The second level, intervention, ranks higher than association because it involves not just observing the association relationship but changing the behavior through intervention </p>
<p style="color:darkturquoise;">## 45:  For example,  What will happen if the person drinks tea instead of coffee </p>
<p style="color:limegreen;">## 46:   Finally, the top level supports counterfactuals, and subsumes interventional and associational levels </p>
<p style="color:limegreen;">## 47:  Counterfactuals is a mode of reasoning that goes back to the philosophers David Hume and John Stuart Mill and has been given computer-friendly semantics in the past two decades through the efforts of Pearl [2000] </p>
<p style="color:limegreen;">## 48:  In this level, a typical question necessitates retrospective reasoning </p>
<p style="color:darkturquoise;">## 49:  For example,  Was it coffee that disturbed the person s sleep </p>
<p style="color:darkturquoise;">## 50:   The second and third levels are the bases of explanatory modeling. </p>
<p style="color:limegreen;">## 51:  Figure 1.1Causal hierarchy </p>
<p style="color:limegreen;">## 52:  Questions at each level can be answered only if information from the lower level is available [Pearl 2019]. </p>
<p style="color:darkturquoise;">## 53:  In the modeling hierarchy, there is a direction, with the top level being the most powerful one and the association level being the most fundamental one </p>
<p style="color:darkturquoise;">## 54:  Correlation is not causality, as we cannot conclude cause effect connections from association </p>
<p style="color:limegreen;">## 55:  However, without any association to begin with, we will not have the incentive to study causation </p>
<p style="color:darkturquoise;">## 56:  Hence, correlation is the mother of causality </p>
<p style="color:limegreen;">## 57:  Although designed controlled experiments or specialized causal inference methods for testing causality exists, in practice association-based models are commonly applied to observational data for causality testing. </p>
<p style="color:darkturquoise;"> 1.2Explanatory Modeling versus Predictive Modeling </p>
<p style="color:darkturquoise;"> A model is a high-level abstract representation of the data with a collection of parameters that can be estimated from the given data </p>
<p style="color:grey;"> Throughout this book, we use the term modeling to highlight the entire process involved, from goal definition and data collection to model building/training </p>
<p style="color:grey;"> Depending on the modeling goals, models can be further classified as predictive and explanatory </p>
<p style="color:grey;"> There is a fundamental difference between modeling for prediction and modeling for explanation </p>
<p style="color:grey;"> Because of this difference not only the types of learning algorithms being used are different but also result interpretation and evaluations are different </p>
<p style="color:grey;"> Given a set of inputs X=(X1,X2, ,Xi) and an output variable Y, in the general form the relationship between X and Y can be written as Y=f(X) </p>
<p style="color:darkturquoise;"> Depending on the goals of modeling, estimating an outcome is possible in two ways: explanation and prediction </p>
<p style="color:darkturquoise;"> Explanation is considered when we are interested in understanding the way that Y is affected as one or multiple predefined key predictors change </p>
<p style="color:limegreen;"> Therefore, explanation studies the causal relationships between an outcome and predictors while adjusting for confounders </p>
<p style="color:grey;"> Prediction is considered when a set of inputs X are readily available but the output Y cannot be easily obtained </p>
<p style="color:grey;"> In this case, we predict Y using f^, which is an estimate of f derived from the input </p>
<p style="color:grey;"> The f^ is often treated as a black box in the sense that one is not typically concerned with the exact form of f, provided that it yields accurate predictions for Y. </p>
<p style="color:darkturquoise;"> For instance, in a public health setting, one may seek to relate asthma attacks in an asthmatic patient to inputs such as pollution, temperature, season, air quality, physical activity, and so forth </p>
<p style="color:darkturquoise;"> In this case, one might be interested in understanding how the individual input variables affect the asthma attack such as, Does exercise or high physical activity cause asthma attacks in the patient </p>
<p style="color:grey;"> If not, are asthma attacks caused by exercising outdoors on a hot day when air quality was bad </p>
<p style="color:grey;"> This is an explanation problem </p>
<p style="color:darkturquoise;"> In explanatory modeling the goal is to understand the relationships in the data to establish the effects of various predictors on a response </p>
<p style="color:grey;"> Since controlled experiments are very costly and in some cases not even feasible, in practice, explanatory modeling is oftentimes performed on observational data </p>
<p style="color:darkturquoise;"> Alternatively, one may simply be interested in predicting the asthma attack for a patient given all factors: will the patient experience an asthma attack or not </p>
<p style="color:grey;"> This is a prediction problem </p>
<p style="color:grey;"> In predictive modeling, on the other hand, the goal is to predict the next unseen data as accurately as possible </p>
<p style="color:grey;"> In addition, explanatory analysis requires interpretable models whereas having a black box predictive model is rather the norm as long as it can generalize unseen data well. </p>
<p style="color:darkturquoise;"> Leo Breiman a professor at the Department of Statistics at the University of California, Berkeley summarized years of experience in consulting projects and in academia in an interesting article about two cultures that apply statistical modeling to reach conclusions based on the data [Breiman 2001] </p>
<p style="color:darkturquoise;"> Data modeling culture, which according to Breiman accounts for 97% of statisticians, assumes that the data are generated by a given stochastic data model and the statistical community has been committed to almost exclusive use of data models to extract some information about how input variables are associated with the response variables </p>
<p style="color:darkturquoise;"> This adherence towards data models  has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems </p>
<p style="color:grey;">  On the other hand, algorithmic modeling culture with only 3% of statisticians committed to it, tries to find a function f(x) an algorithm that operates on x to predict y. </p>
<p style="color:grey;"> The increasingly widespread collection of big data has led scientists to envision a future in which predictive models will eventually rival the traditional explanatory models that have dominated statistical research areas for at least the past century </p>
<p style="color:grey;"> Although with big data it is possible to find some interesting correlations, the chances of finding spurious correlations increase as well </p>
<p style="color:grey;"> Cristian Calude and Giuseppe Longo authored an interesting article about the rise of spurious correlations in big data analysis [Calude and Longo 2017] </p>
<p style="color:grey;"> They proved that very large databases have to contain arbitrary correlations and these correlations appear only due to the size and not the nature of data </p>
<p style="color:grey;"> Moreover, the black box nature of machine learning models has raised suspicions on potential biases from the algorithms employed and data sets used </p>
<p style="color:grey;"> At the institutional level, the US Department of Defense is currently working on what is defined as  Explainable Artificial Intelligence  (XAI), which is a project that aims to create a suite of machine learning techniques that (a) produce more explainable models while maintaining a high level of prediction accuracy and (b) enable human users to understand, appropriately trust, and effectively manage AI outputs [Gunning and Aha 2019] </p>
<p style="color:darkturquoise;"> The field of interpretability and explainability in machine learning has exploded since 2015 and there are now dozens of techniques on this subject [Lundberg and Lee 2017; Ribeiro et al </p>
<p style="color:grey;"> 2018] </p>
<p style="color:darkturquoise;"> XAI is the ability to provide an explanation on why a machine decision has been reached </p>
<p style="color:grey;"> These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected </p>
<p style="color:darkturquoise;"> However, explanations produced by these systems are leaning toward finding important features for each prediction value rather than focusing on bringing in the experience, expertise, knowledge, and intuition of an expert into the modeling. </p>
<p style="color:darkturquoise;"> Our ultimate goal of using data is to solve interesting problems, and our main argument throughout this book is that we need to move away from exclusive dependence on modeling techniques and to adopt a more diverse set of tools </p>
<p style="color:grey;"> Our emphasis is on combining these two modeling approaches under event mining to get the best of both worlds </p>
<p style="color:darkturquoise;"> We define explanatory modeling as the use of event mining for finding, formulating, and testing causal explanations </p>
<p style="color:darkturquoise;"> We assume data are produced by a system whose inside is complex, mysterious, and at least partly unknowable </p>
<p style="color:darkturquoise;"> Initial findings of interesting patterns in the data are purely data driven </p>
<p style="color:limegreen;"> Once some of those patterns are observed, a comprehensive event language shall be used to iteratively formulate and test a hypothesis based on those findings </p>
<p style="color:darkturquoise;"> 1.3Logs are the Source of Knowledge </p>
<p style="color:grey;"> Logs are the basis of scientific research and discovery, and scientists rely on them for detailed descriptions of observations both in real life and in laboratories </p>
<p style="color:grey;"> Until a few decades ago, these logs were recorded by hand in organized notebooks </p>
<p style="color:grey;"> Advances in technology over the last few decades have resulted in an increasing trend to record frequent observations via sensors </p>
<p style="color:darkturquoise;"> Personal observations can also be included, but they are usually explicitly marked as personal reports </p>
<p style="color:grey;"> With sensors, intricate details can now be automatically and frequently captured and recorded </p>
<p style="color:grey;"> Even physical attributes at a spatial and temporal granularity well beyond normal human capabilities can be captured </p>
<p style="color:grey;"> Moreover, these logs are recorded by collecting multimodal data from multiple disparate sensors </p>
<p style="color:grey;"> As sensors continuously log data streams related to events in a chronicle, key information about the events can be extracted using continuous computational engines in the background. </p>
<p style="color:grey;"> With advances in data capturing and recording techniques, the interest in collecting such logs about events happening in, say, a meeting, sporting event, wedding, or major public space have become increasingly important outside the scientific realm </p>
<p style="color:grey;"> Many companies collect logs of activities on their websites, such as customer keystrokes, selections, inputs, and other actions </p>
<p style="color:grey;"> These logs are then used to create a user profile for future advertising </p>
<p style="color:grey;"> The idea of collecting data related to a person s life activities has been around forever, but it has only recently been performed invisibly, via wearable and environmental sensors </p>
<p style="color:darkturquoise;"> Initially, such logs were considered as data collection that could be manipulated by using appropriate visualization and analytic techniques </p>
<p style="color:grey;"> More powerful tools are starting to make much more use of these logs </p>
<p style="color:grey;"> 1.4From Logs to Chronicles to Models </p>
<p style="color:grey;"> Progress in sensing, storage, and communication has made it easier and cheaper to keep all the data related to an entity, including a person, at almost every moment of its existence </p>
<p style="color:grey;"> By mining this time-ordered data, it is possible to get valuable information for understanding the entity as well as for predicting its behavior </p>
<p style="color:darkturquoise;"> The first step in this process is to convert a log into a chronicle, which is a time sequence of events </p>
<p style="color:grey;"> Chronicles have played an important role in human civilization, even though earlier manual chronicles were sparsely populated and had a lot of subjective data </p>
<p style="color:grey;"> Modern, sensor-based chronicles bring a tremendous amount of objective data with them, making them a powerful source of knowledge </p>
<p style="color:darkturquoise;"> A chronicle contains events </p>
<p style="color:darkturquoise;"> Since many different data streams are collected via diverse sensors, and one can detect events in each data stream, a chronicle is actually a collection of event streams </p>
<p style="color:darkturquoise;"> These event streams capture the nature and relationships among different aspects of a system or organization, often among different types and combinations of events, and with different temporal and logical relationships among them </p>
<p style="color:darkturquoise;"> Discovery of these relationships is an essential part of model building. </p>
<p style="color:grey;"> Analyzing these event streams and understanding their relationships is an essential part of event mining, which aims to understand a system and thereby predict and control its future behavior </p>
<p style="color:darkturquoise;"> However, there are many event streams and, among them, too many combinations of events and temporal relationships exist </p>
<p style="color:grey;"> So, how do we start the process </p>
<p style="color:darkturquoise;"> What works in the scientific discovery process also applies here; based on some explicit or implicit correlations, we need to form a hypothesis, which is then analyzed by an expert and modified to form a new hypothesis that can be tested </p>
<p style="color:darkturquoise;"> A strong event mining platform should thus have both a hypothesis formation component and a hypothesis testing component </p>
<p style="color:darkturquoise;"> These two components are related and are complementary, but they are also mediated by an observer who brings in external knowledge sources </p>
<p style="color:darkturquoise;"> 1.5The Importance of an Event Language for Explanatory Modeling </p>
<p style="color:darkturquoise;"> Explanatory modeling involves hypothesis formation and hypothesis testing and both must use the correct events in the correct combinations </p>
<p style="color:darkturquoise;"> With multiple event streams, the hypothesis formation step tests different combinations of events to find strong support for a handful of those combinations that need to be tested </p>
<p style="color:limegreen;"> These potential hypotheses may be refined using other knowledge sources </p>
<p style="color:limegreen;"> A precise language is required to express spatial, temporal, relational, and logical relationships among the events that form a hypothesis </p>
<p style="color:darkturquoise;"> This language must be sufficiently expressive to represent the desired relationships as well as precise enough to capture the subtle nuances of different events and their attributes. </p>
<p style="color:darkturquoise;"> Currently, concepts related to complex event processing (CEP) [Luckham and Frasca 1998] are popular in analyzing streams of information </p>
<p style="color:darkturquoise;"> CEP provides a simple language to express relationships among events, but it is only suitable for simple scenarios that deal with a limited number of event streams, and a few pre-defined temporal and logical relationships among them </p>
<p style="color:darkturquoise;"> We believe the development of a language that formulates a hypothesis at the right level of complexity is essential for successful event mining </p>
<p style="color:darkturquoise;"> This book discusses issues related to event mining with the goal of explanatory modeling, and providing not only inference through statistical models or data mining algorithms but also explanation through interactive human-centered modeling perspectives </p>
<p style="color:darkturquoise;"> We envision that event mining will be the main modeling framework to provide explanations through a natural language (i.e., event query language), an iterative process, and interactive visualizations, letting the expert decide when and how to question the explanation and steer her discovery by herself </p>
<p style="color:grey;"> There are many challenging problems where a human expert can understand the context and bring in experience, expertise, knowledge and intuition in the model building process </p>
<p style="color:limegreen;"> We argue that an event language is one of the most intuitive ways for a human expert to interact with the system, formulate questions, and build explainable models </p>
<p>----------------- END ---------------------------</p>
</body>
                    </html>
