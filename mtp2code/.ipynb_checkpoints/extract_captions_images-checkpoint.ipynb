{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4e6ab7-a4d2-46e3-b0cd-8535ddf6b440",
   "metadata": {},
   "source": [
    "# Libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e2c3a-644e-4314-a81f-9393916830db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from os import error, walk\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from IPython.display import Image as Image_display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from urllib.parse import unquote\n",
    "\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import json\n",
    "import io\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "# Mention the installed location of Tesseract-OCR in your system\n",
    "pytesseract.pytesseract.tesseract_cmd = \"C:/Program Files/Tesseract-OCR/tesseract.exe\"\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207feec-503e-411c-a7e2-c6e7c95ac0cf",
   "metadata": {},
   "source": [
    "# Intializing Sentence Trasnformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791ef12-af3c-4ee5-937c-7e363db9643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# sentence_transformer_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "sentence_transformer_model_nondebug_dict={}\n",
    "sentence_transformer_names_nondebug_list = ['sentence-transformers/all-mpnet-base-v2','sentence-transformers/multi-qa-MiniLM-L6-cos-v1','sentence-transformers/all-MiniLM-L6-v2','sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2','sentence-transformers/bert-base-nli-mean-tokens']\n",
    "for transformer in sentence_transformer_names_nondebug_list:\n",
    "    sentence_transformer_model_nondebug_dict[transformer] = SentenceTransformer(transformer)\n",
    "\n",
    "sentence_transformer_model_debug_dict = {}\n",
    "sentence_transformer_names_debug_list = ['sentence-transformers/all-MiniLM-L6-v2']\n",
    "sentence_transformer_model_debug_dict['sentence-transformers/all-MiniLM-L6-v2'] = sentence_transformer_model_nondebug_dict['sentence-transformers/all-MiniLM-L6-v2']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850a85a-f037-4343-b49f-451f7c285d23",
   "metadata": {},
   "source": [
    "# summarizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd34b36e-c665-40f1-85f2-1b665d6cfb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline_summarizer = pipeline(\"summarization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fdede-2d2c-429f-a37c-b041b8fce416",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b0d20d-09db-439c-afd3-94148a16330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in text])\n",
    "\n",
    "def clean_string(txt):\n",
    "    return remove_non_ascii(txt).replace(\"Fig. \", \"Figure \").replace(\"fig. \",\"Figure \")\n",
    "\n",
    "def extract_lastfig(txt):\n",
    "    txt = remove_non_ascii(txt).strip().replace(\"\\n\",\" \")\n",
    "    last_index = [_.start() for _ in re.finditer(r\"Fig\", txt)][-1]\n",
    "    return txt[last_index:]\n",
    "\n",
    "def get_image_path(html_path,img_src): # get image path from (html path) and (image path in html file) @textbooks\n",
    "    oebps_path = html_path.split(\"/Text/\")[0]\n",
    "    img_src = img_src[2:]\n",
    "    return unquote(oebps_path+img_src)\n",
    "\n",
    "def get_image_path_papers(html_path, img_src):# get image path from (html path) and (image path in html file) @papers\n",
    "    acmbooks_path = html_path.split(\"/OEBPS/\")[0]\n",
    "    img_src = img_src[3:]\n",
    "    return unquote(acmbooks_path + \"/OEBPS/\" + img_src)\n",
    "\n",
    "def convert_trans_to_white_bg(trans_image_path): # change background from transparent to white background\n",
    "    im_test = Image.open(trans_image_path)\n",
    "\n",
    "    fill_color = (255,255,255)  # white background color\n",
    "\n",
    "    im_test = im_test.convert(\"RGBA\")   # it had mode P after DL it from OP\n",
    "    if im_test.mode in ('RGBA', 'LA'):\n",
    "        background = Image.new(im_test.mode[:-1], im_test.size, fill_color)\n",
    "        background.paste(im_test, im_test.split()[-1]) # omit transparency\n",
    "        im_test = background\n",
    "    white_image_path = trans_image_path[:-4] + \"_white.png\"\n",
    "    im_test.convert(\"RGB\").save(white_image_path)\n",
    "    return white_image_path\n",
    "\n",
    "\n",
    "def remove_multi_newlines(text):\n",
    "    text_list = text.split(\"\\n\")\n",
    "    clean_text_list  = []\n",
    "    for sent in text_list:\n",
    "        if len(sent) != 0:\n",
    "            clean_text_list.append(sent)\n",
    "    return '\\n'.join(clean_text_list)\n",
    "    \n",
    "def split_text_to_sent(text):\n",
    "    text_clean = clean_string(text)\n",
    "    all_sent_list = re.split('\\. |\\? |\\?|\\n',text_clean)\n",
    "    clean_all_sent_list = []\n",
    "\n",
    "    for sent in all_sent_list:\n",
    "        if len(remove_non_ascii(sent).replace(\" \",\"\")) > 3:\n",
    "            clean_all_sent_list.append(sent)\n",
    "\n",
    "    all_sent_list = clean_all_sent_list\n",
    "    return all_sent_list\n",
    "\n",
    "def moving_average(x, w):\n",
    "    if w == 100:\n",
    "        pad_x = x.tolist() + [0,0,0,0,0,0,0,0]\n",
    "        return np.array([max(pad_x[i:i+9]) for i in range(0,len(x))])                    \n",
    "    elif w == 101:\n",
    "        pad_x = x.tolist() + [0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        return np.array([max(pad_x[i:i+13]) for i in range(0,len(x))])                    \n",
    "    elif w > 0:\n",
    "        pad_len = int(w/2)\n",
    "        pad_x = np.pad(np.array(x), (pad_len, pad_len), 'mean')\n",
    "        return np.convolve(pad_x, np.ones(w), 'valid') / w\n",
    "    elif w < 0:\n",
    "        w = -1*w\n",
    "        pad_len = int((3*w)/2)\n",
    "        pad_x = np.pad(np.array(x), (pad_len, pad_len), 'mean')\n",
    "        sigma=w/2.0\n",
    "        w = 3*w\n",
    "        fil  = np.arange(-(w-1)/2,w/2,1)\n",
    "        assert(len(fil)==w)\n",
    "        fil = np.exp(-fil**2/(2*sigma**2))\n",
    "        fil = fil/np.sum(fil)\n",
    "        # print(\"len of filter = \",str(w))\n",
    "        return np.convolve(pad_x, fil, 'valid')\n",
    "\n",
    "def extract_caption(fig, min_caption_words, max_caption_words):\n",
    "    caption_parent_text = extract_lastfig(fig.parent.parent.get_text())\n",
    "    caption_grandparent_text = extract_lastfig(fig.parent.parent.parent.get_text())\n",
    "    fig_text = extract_lastfig(fig)\n",
    "    fig_caption = \"\"\n",
    "    if len(fig_text.split()) >= max_caption_words:\n",
    "        return fig_caption\n",
    "    elif len(fig_text.split()) >= min_caption_words:\n",
    "        # print(f\"fig\\nlength = {len(fig.split())}\")\n",
    "        print(\"fig\")\n",
    "        fig_caption = fig_text\n",
    "    elif len(caption_parent_text.split()) >= max_caption_words:\n",
    "        # print(f\"fig\\nlength = {len(fig.split())}\")\n",
    "        print(\"fig\")\n",
    "        fig_caption = fig_text\n",
    "    elif len(caption_grandparent_text.split()) >= max_caption_words:\n",
    "        # print(f\" fig + parent\\n length = {len(caption_parent_text.split())}\")\n",
    "        print(\"fig + parent\")\n",
    "        fig_caption = caption_parent_text\n",
    "    else:\n",
    "        # print( f\"fig + grandparent \\nlength = {len(caption_grandparent_text.split())}\")\n",
    "        print(\"fig + grandparent\")\n",
    "        fig_caption = caption_grandparent_text\n",
    "    return fig_caption\n",
    "\n",
    "def extract_imagepath(fig, filename, type_html):\n",
    "    same_el_img = fig.parent.find_all(\"img\")\n",
    "    prev_img = fig.parent.find_previous(\"img\")\n",
    "    image_path = \"\"\n",
    "    if same_el_img == []:\n",
    "        if prev_img == None:\n",
    "            print(\"No image found\")\n",
    "        else:\n",
    "            if type_html == \"textbooks\":\n",
    "                image_path = get_image_path(filename,prev_img[\"src\"])\n",
    "            if type_html == \"papers\":\n",
    "                image_path = get_image_path_papers(filename,prev_img[\"src\"])\n",
    "            # image_path = convert_trans_to_white_bg(image_path)\n",
    "            print(\"previous image = \"+ image_path)\n",
    "    else:\n",
    "        if type_html == \"textbooks\":\n",
    "            image_path = get_image_path(filename,same_el_img[-1][\"src\"])\n",
    "        if type_html == \"papers\":\n",
    "            image_path = get_image_path_papers(filename,same_el_img[-1][\"src\"])\n",
    "        # image_path = convert_trans_to_white_bg(image_path)\n",
    "        print(\"same tag image = \"+ image_path)\n",
    "    return image_path\n",
    "\n",
    "def smooth_label(smooth_no):\n",
    "    if smooth_no==100:\n",
    "        return \"9_max_smooth\"\n",
    "    elif smooth_no==101:\n",
    "        return \"13_max_smooth\"\n",
    "    elif smooth_no >0:\n",
    "        return str(smooth_no)+\"_uniform_smooth\"\n",
    "    elif smooth_no < 0:\n",
    "        return str(-1*smooth_no)+\"_gaussian_smooth\"\n",
    "    else:\n",
    "        return \"no_smooth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1643e0fe-bee6-4b81-8dc9-ad2e8a6b6291",
   "metadata": {},
   "source": [
    "# Extracting Epub files to folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3d745-d37f-4b4f-bfa4-94751adc7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Epub books to folders containing HTML\n",
    "\n",
    "# folder path of epub books you want to extract images from\n",
    "epub_folder = \"./data/e_pathshala_epub_debug/\"\n",
    "epub_folder_list = [\"./data/e_pathshala_epub_debug/\", \"./data/technical_papers\"]\n",
    "\n",
    "for epub_folder in epub_folder_list:\n",
    "    for root, dirs, files in os.walk(epub_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".epub\"):\n",
    "                folder_name = root+\"/\"+file[:-5]\n",
    "                file_name = root+\"/\"+file\n",
    "                # print(folder_name)\n",
    "                # print(file_name)\n",
    "                if os.path.isdir(folder_name):\n",
    "                    print(\"  Already extracted : \"+folder_name)\n",
    "                else:\n",
    "                    print(\"  Extracting to : \"+folder_name)\n",
    "                    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(folder_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e1e01-d302-47bf-a910-198017298bcd",
   "metadata": {},
   "source": [
    "# Loading all textbooks path as html file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96bd80-b6b6-4ad6-aaac-0bb25f958e8a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "classes_folders = []\n",
    "classes_folders_path = \"./data/e_pathshala_epub_debug\"\n",
    "extensions_list = []\n",
    "\n",
    "for file in os.listdir(classes_folders_path):\n",
    "    d = os.path.join(classes_folders_path, file).replace(\"\\\\\",\"/\")\n",
    "    if os.path.isdir(d):\n",
    "        classes_folders.append(d)\n",
    "\n",
    "print(f\"number of classes_folders = {len(classes_folders)}\")\n",
    "for classes_folder in classes_folders:\n",
    "    print(classes_folder)\n",
    "\n",
    "textbooks_html_files = []\n",
    "for classes_folder in classes_folders:\n",
    "    for file in os.listdir(classes_folder):\n",
    "        d = os.path.join(classes_folder, file).replace(\"\\\\\",\"/\")\n",
    "        if os.path.isdir(d):\n",
    "            html_files = os.listdir(d+\"/OEBPS/Text/\")\n",
    "            html_files_path = []\n",
    "            for html in html_files:\n",
    "                html_files_path.append(d+\"/OEBPS/Text/\"+html)\n",
    "            largest_html_file = max(html_files_path, key=os.path.getsize)\n",
    "            textbooks_html_files.append(largest_html_file)\n",
    "            extension = largest_html_file.split(\".\")[-1]\n",
    "            if extension not in extensions_list:\n",
    "                extensions_list.append(extension)\n",
    "\n",
    "print(f\"number of textbooks_folders = {len(textbooks_html_files)}\")\n",
    "for html in textbooks_html_files:\n",
    "    print(html)\n",
    "\n",
    "\n",
    "# papers_html_files = []\n",
    "# papers_folders = []\n",
    "# papers_folders_path = \"./data/technical_papers\"\n",
    "\n",
    "# for file in os.listdir(papers_folders_path):\n",
    "#     d = os.path.join(papers_folders_path, file).replace(\"\\\\\",\"/\")\n",
    "#     if os.path.isdir(d):\n",
    "#         papers_folders.append(d)\n",
    "\n",
    "# print(f\"number of papers_folders = {len(papers_folders)}\")\n",
    "# for papers_folder in papers_folders:\n",
    "#     print(papers_folder)\n",
    "\n",
    "# for papers_folder in papers_folders:\n",
    "#     html_files_path = papers_folder.replace(\"\\\\\",\"/\") + \"/OEBPS/xhtml\"\n",
    "#     html_files = os.listdir(html_files_path)\n",
    "#     for html in html_files:\n",
    "#         html_file = html_files_path+\"/\"+html\n",
    "#         papers_html_files.append(html_file)\n",
    "#         extension = largest_html_file.split(\".\")[-1]\n",
    "#         if extension not in extensions_list:\n",
    "#             extensions_list.append(extension)\n",
    "\n",
    "# print(\"Number of technical papers = \"+str(len(papers_html_files)))\n",
    "# print(papers_html_files)\n",
    "\n",
    "papers_html_files = ['./data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-003.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-004.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-005.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-006.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-007.xhtml', './data/technical_papers/ACMBook2/OEBPS/xhtml/09_Chapter01.xhtml','./data/technical_papers/ACMBook2/OEBPS/xhtml/11_Chapter02.xhtml', './data/technical_papers/ACMBook2/OEBPS/xhtml/12_Chapter03.xhtml', './data/technical_papers/ACMBook2/OEBPS/xhtml/13_Chapter04.xhtml', './data/technical_papers/ACMBook2/OEBPS/xhtml/14_Chapter05.xhtml', './data/technical_papers/ACMBook2/OEBPS/xhtml/15_Chapter06.xhtml', './data/technical_papers/ACMBook2/OEBPS/xhtml/17_Chapter07.xhtml', './data/technical_papers/ACMBook2/OEBPS/xhtml/18_Chapter08.xhtml', './data/technical_papers/ACMBook2/OEBPS/xhtml/19_Chapter09.xhtml', './data/technical_papers/ACMBook2/OEBPS/xhtml/20_Chapter10.xhtml', './data/technical_papers/ACMBook2/OEBPS/xhtml/21_Chapter11.xhtml', './data/technical_papers/ACMBook2/OEBPS/xhtml/22_Chapter12.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/12_Chapter01.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/13_Chapter02.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/14_Chapter03.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/15_Chapter04.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/17_Chapter05.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/18_Chapter06.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/19_Chapter07.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/20_Chapter08.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/21_Chapter09.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/23_Chapter10.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/24_Chapter11.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/25_Chapter12.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/26_Chapter13.xhtml', './data/technical_papers/ACMBook4/OEBPS/xhtml/28_Chapter14.xhtml', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter01.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter02.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter03.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter04.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter05.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter06.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter07.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter08.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter09.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter10.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter11.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter12.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter13.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter14.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter15.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter16.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter17.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter18.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter19.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter20.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter21.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter22.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter23.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter24.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter25.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter26.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter27.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter28.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter29.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter30.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter31.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter32.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter33.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter34.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter35.html', './data/technical_papers/ACMBook5/OEBPS/xhtml/chapter36.html']\n",
    "\n",
    "print(\"Number of technical papers = \"+str(len(papers_html_files)))\n",
    "for html_file in papers_html_files:\n",
    "    print(html_file)\n",
    "\n",
    "extension_file = open(\"extension_list.txt\", \"w\")\n",
    "extension_file.write(str(extensions_list))\n",
    "extension_file.close()\n",
    "\n",
    "# print(f\"number of textbooks_folders = {len(textbooks_html_files)}\")\n",
    "# for html in textbooks_html_files:\n",
    "#     print(html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbd0e7-8c78-491a-ab6c-758ed123a33c",
   "metadata": {},
   "source": [
    "# Google vision api to detect text from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa05b4-0bae-416f-aa7c-93feb02c2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(path):# corse text in a image\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    from google.cloud import vision\n",
    "    \n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"./googleapi_key/micro-answer-336515-152e5dd6b508.json\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    \n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    \n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    \n",
    "    ocr_full_txt_path = path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "    ocr_full_txt_file = open(ocr_full_txt_path, \"w\")\n",
    "    print(ocr_full_txt_path)\n",
    "    ocr_full_txt_file.write(str(response))\n",
    "    ocr_full_txt_file.close()\n",
    "\n",
    "    ocr_dict = {\"description\" : [], \"bounding_poly\" : []}\n",
    "    for text in texts:\n",
    "        ocr_dict[\"description\"].append(text.description)\n",
    "        vertices_list = []\n",
    "        for vertex in text.bounding_poly.vertices:\n",
    "            vertices_list.append((vertex.x,vertex.y))\n",
    "        ocr_dict[\"bounding_poly\"].append(vertices_list)\n",
    "    \n",
    "    ocr_json = json.dumps(ocr_dict)\n",
    "    ocr_json_path = path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "    ocr_json_file = open(ocr_json_path, \"w\")\n",
    "    print(ocr_json_path)\n",
    "    ocr_json_file.write(ocr_json)\n",
    "    ocr_json_file.close()\n",
    "       \n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))\n",
    "\n",
    "    return ocr_dict\n",
    "\n",
    "\n",
    "def detect_document(path, type_html): #dense text in a image\n",
    "    \"\"\"Detects document features in an image.\"\"\"\n",
    "    from google.cloud import vision\n",
    "\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"./googleapi_key/micro-answer-336515-152e5dd6b508.json\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    response = client.document_text_detection(image=image)\n",
    "    ocr_dict = {\"block\": [],\"bounding_poly\": []}\n",
    "    for page in response.full_text_annotation.pages:\n",
    "        for block in page.blocks:\n",
    "            # print('\\nBlock confidence: {}\\n'.format(block.confidence))\n",
    "            block_vertices = []\n",
    "            for vertex in block.bounding_box.vertices:\n",
    "                block_vertices.append((vertex.x,vertex.y))\n",
    "                \n",
    "            # print('\\nBlock bounding box: {}\\n'.format(block_vertices))\n",
    "            \n",
    "            block_dict = {\"text\":\"\",\"paragraph\": [],\"bounding_poly\": []}\n",
    "            for paragraph in block.paragraphs:\n",
    "                # print('Paragraph confidence: {}'.format(paragraph.confidence))\n",
    "                paragraph_vertices = []\n",
    "                for vertex in paragraph.bounding_box.vertices:\n",
    "                    paragraph_vertices.append((vertex.x,vertex.y))\n",
    "                # print('\\nParagraph bounding box: {}\\n'.format(paragraph_vertices))\n",
    "                \n",
    "                para_dict = {\"text\":\"\", \"word\": [], \"bounding_poly\": []}\n",
    "                for word in paragraph.words:\n",
    "                    word_text = ''.join([\n",
    "                        symbol.text for symbol in word.symbols\n",
    "                    ])\n",
    "                    # print('Word text: {} (confidence: {})'.format(word_text, word.confidence))\n",
    "                   \n",
    "\n",
    "                    word_vertices = []\n",
    "                    for vertex in word.bounding_box.vertices:\n",
    "                        word_vertices.append((vertex.x,vertex.y))\n",
    "\n",
    "                    if para_dict[\"text\"] == \"\":\n",
    "                        para_dict[\"text\"] = word_text\n",
    "                    else:\n",
    "                        para_dict[\"text\"] = para_dict[\"text\"] +\" \"+ word_text\n",
    "                        \n",
    "                    para_dict[\"word\"].append(word_text)\n",
    "                    para_dict[\"bounding_poly\"].append(word_vertices)\n",
    "                if block_dict[\"text\"] == \"\":\n",
    "                    block_dict[\"text\"] =  para_dict[\"text\"]\n",
    "                else:\n",
    "                    block_dict[\"text\"] = block_dict[\"text\"] +\" \"+ para_dict[\"text\"]\n",
    "                block_dict[\"paragraph\"].append(para_dict)\n",
    "                block_dict[\"bounding_poly\"].append(paragraph_vertices)\n",
    "            ocr_dict[\"block\"].append(block_dict)\n",
    "            ocr_dict[\"bounding_poly\"].append(block_vertices)\n",
    "            \n",
    "    if type_html == \"textbooks\":\n",
    "        ocr_full_txt_path = path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "    if type_html == \"papers\":\n",
    "        ocr_full_txt_path = path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "    ocr_full_txt_file = open(ocr_full_txt_path, \"w\")\n",
    "    print(ocr_full_txt_path)\n",
    "    ocr_full_txt_file.write(str(response))\n",
    "    ocr_full_txt_file.close()\n",
    "\n",
    "    ocr_json = json.dumps(ocr_dict)\n",
    "    if type_html == \"textbooks\":\n",
    "        ocr_json_path = path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "    if type_html == \"papers\":\n",
    "        ocr_json_path = path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\".json\"\n",
    "    ocr_json_file = open(ocr_json_path, \"w\")\n",
    "    print(ocr_json_path)\n",
    "    ocr_json_file.write(ocr_json)\n",
    "    ocr_json_file.close()\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))\n",
    "    return ocr_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418542bb-694e-4cde-9ecd-b0faf65211ef",
   "metadata": {},
   "source": [
    "# Extracting captions and corresponding image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a33cf2-a090-49a4-afcc-f3840e3652c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# types_html_files_list = [\"papers\"]\n",
    "\n",
    "for type_html in types_html_files_list:\n",
    "    filenames_list = []\n",
    "    if type_html == \"textbooks\":\n",
    "        filenames_list = textbooks_html_files\n",
    "    if type_html == \"papers\":\n",
    "        filenames_list = papers_html_files\n",
    "\n",
    "    for filename in filenames_list:\n",
    "        try:\n",
    "            fp = open(filename, encoding=\"utf8\")\n",
    "            print(\"++++++++++++++++++++++++++++   START   +++++++++++++++++++++\")\n",
    "            print(filename)\n",
    "        except:\n",
    "            print(\"***************  Error while opening file   ***************\")\n",
    "            print(filename)\n",
    "            print(\"************************************************************\")\n",
    "        extension = filename.split(\".\")[-1]\n",
    "        if extension == \"xhtml\":\n",
    "            try:\n",
    "                print(\"@@@@@@@@@@@@   XML parser @@@@@@@@@@@@\")\n",
    "                soup = BeautifulSoup(fp, 'lxml-xml')\n",
    "            except Exception as e:\n",
    "                print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                print(e)\n",
    "                print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "        elif extension == \"html\":\n",
    "            try:\n",
    "                print(\"@@@@@@@@@@@@   HTML parser @@@@@@@@@@@@\")\n",
    "                soup = BeautifulSoup(fp, 'lxml')\n",
    "            except Exception as e:\n",
    "                print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                print(e)\n",
    "                print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "        all_fig = soup.find_all(string=re.compile(\"^Fig\"))\n",
    "        fig_no = 1\n",
    "        max_caption_words = 60\n",
    "        min_caption_words = 3\n",
    "\n",
    "        for fig in all_fig:\n",
    "            print(\"------- fig  \"+str(fig_no)+\"   --------\")\n",
    "\n",
    "            caption_parent_text = extract_lastfig(fig.parent.parent.get_text())\n",
    "            caption_grandparent_text = extract_lastfig(fig.parent.parent.parent.get_text())\n",
    "            fig_text = extract_lastfig(fig)\n",
    "            fig_caption = \"\"\n",
    "            if len(fig_text.split()) >= max_caption_words:\n",
    "                continue\n",
    "            elif len(fig_text.split()) >= min_caption_words:\n",
    "                print(f\"fig\\nlength = {len(fig.split())}\")\n",
    "                print(fig_text)\n",
    "                fig_caption = fig_text\n",
    "            elif len(caption_parent_text.split()) >= max_caption_words:\n",
    "                print(f\"fig\\nlength = {len(fig.split())}\")\n",
    "                print(fig_text)\n",
    "                fig_caption = fig_text\n",
    "            elif len(caption_grandparent_text.split()) >= max_caption_words:\n",
    "                print(f\" fig + parent\\n length = {len(caption_parent_text.split())}\")\n",
    "                print(caption_parent_text)\n",
    "                fig_caption = caption_parent_text\n",
    "            else:\n",
    "                print( f\"fig + grandparent \\nlength = {len(caption_grandparent_text.split())}\")\n",
    "                print(caption_grandparent_text)\n",
    "                fig_caption = caption_grandparent_text\n",
    "\n",
    "            same_el_img = fig.parent.find_all(\"img\")\n",
    "            prev_img = fig.parent.find_previous(\"img\")\n",
    "\n",
    "\n",
    "            if same_el_img == []:\n",
    "                if prev_img == None:\n",
    "                    print(\"No image found\")\n",
    "                else:\n",
    "                    if type_html == \"textbooks\":\n",
    "                        image_path = get_image_path(filename,prev_img[\"src\"])\n",
    "                    if type_html == \"papers\":\n",
    "                        image_path = get_image_path_papers(filename,prev_img[\"src\"])\n",
    "                    # image_path = convert_trans_to_white_bg(image_path)\n",
    "                    print(\"previous image = \"+ image_path)\n",
    "            else:\n",
    "                if type_html == \"textbooks\":\n",
    "                    image_path = get_image_path(filename,same_el_img[-1][\"src\"])\n",
    "                if type_html == \"papers\":\n",
    "                    image_path = get_image_path_papers(filename,same_el_img[-1][\"src\"])\n",
    "                # image_path = convert_trans_to_white_bg(image_path)\n",
    "                print(\"same tag image = \"+ image_path)\n",
    "\n",
    "            # ocr_full_txt_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "            if type_html == \"textbooks\":\n",
    "                ocr_json_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "            if type_html == \"papers\":\n",
    "                ocr_json_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\".json\"\n",
    "            plt.imshow(mpimg.imread(image_path))\n",
    "            plt.show()\n",
    "            if os.path.isfile(ocr_json_path):\n",
    "                print(\"    Json already exist :  \"+ocr_json_path)\n",
    "                json_file = open(ocr_json_path)\n",
    "                ocr_dict = json.load(json_file) \n",
    "                #structure of ocr_dict ={\"block\":[ {\"text\":\"\",\"\",\"paragraph\":[{\"text\":\"\", \"word\":[\"w1\",\"w2\",..], \"bounding_poly\": [[],[],..]}, { .. }, ...],\"bounding_poly\":[[(x1,y1),..(x4,y4)],[]...]}, { ... }, ..],\"bounding_poly\":[[],[]...]}\n",
    "            else:\n",
    "                print(\"  $$$$  Calling google vision API $$$$ \")\n",
    "                ocr_dict = detect_document(image_path, type_html)\n",
    "            labels_list = []\n",
    "            print(\"~~~~~~~ Labels  ~~~~~~~~\")\n",
    "            if len(ocr_dict[\"block\"]) > 0:\n",
    "                for i in range(0,len(ocr_dict[\"block\"])):\n",
    "                    block_dict = ocr_dict[\"block\"][i]\n",
    "                    print(block_dict[\"text\"])\n",
    "                    labels_list.append(block_dict[\"text\"])\n",
    "                    # print(block_dict[\"text\"], \"  \",ocr_dict[\"bounding_poly\"][i])\n",
    "                    for i in range(0,len(block_dict[\"paragraph\"])):\n",
    "                        para_dict = block_dict[\"paragraph\"][i]\n",
    "                        # print(\"      -\",para_dict[\"text\"])\n",
    "                        # print(\"      \",para_dict[\"text\"],\"   \",block_dict[\"bounding_poly\"][i])\n",
    "            else:\n",
    "                print(\"  NO Labels  \")\n",
    "            print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "            fig_no = fig_no + 1\n",
    "            # print(\"~~~~~~~~~~~~~~~~~  Labels  ~~~~~~~~~~~~~~~\")\n",
    "            # print(pytesseract.image_to_string(Image.open(image_path)).strip())\n",
    "            # # detect_text(image_path)\n",
    "            # print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "        print(\"++++++++++++++++++++++++++++   END   +++++++++++++++++++++\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840558c5-604d-4fe0-904d-b8ddc2fe9667",
   "metadata": {},
   "source": [
    "# test: generate example output in html for a model and gaussian smoothing -7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19143267-5d18-4dc0-8c4e-859dbd53923e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "filename = \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\"\n",
    "textbooks_html_files = [ \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/3_5239_3/OEBPS/Text/Untitled-1.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/4_5239_4/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/5_5239_5/OEBPS/Text/Untitled-3.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/6_5239_6/OEBPS/Text/Untitled-4.xhtml\"]\n",
    "papers_html_files = ['./data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-004.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-005.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-006.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-007.xhtml']\n",
    "\n",
    "sentence_transformer_model = sentence_transformer_model_nondebug_dict['sentence-transformers/multi-qa-MiniLM-L6-cos-v1']\n",
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# types_html_files_list = [\"papers\"]\n",
    "smooth_true = 1\n",
    "no_bounding_sent = 30\n",
    "h2_optimal_cosine_thershold = 0.08\n",
    "h6_optimal_cosine_thershold = 0.36\n",
    "\n",
    "\n",
    "for type_html in types_html_files_list:\n",
    "    filenames_list = []\n",
    "    if type_html == \"textbooks\":\n",
    "        filenames_list = textbooks_html_files\n",
    "    if type_html == \"papers\":\n",
    "        filenames_list = papers_html_files\n",
    "\n",
    "    for filename in filenames_list:\n",
    "        try:\n",
    "            fp = open(filename, encoding=\"utf8\")\n",
    "            print(\"++++++++++++++++++++++++++++   START   +++++++++++++++++++++\")\n",
    "            print(filename)\n",
    "        except:\n",
    "            print(\"***************  Error while opening file   ***************\")\n",
    "            print(filename)\n",
    "            print(\"************************************************************\")\n",
    "        extension = filename.split(\".\")[-1]\n",
    "        if extension == \"xhtml\":\n",
    "            try:\n",
    "                print(\"@@@@@@@@@@@@   XML parser @@@@@@@@@@@@\")\n",
    "                soup = BeautifulSoup(fp, 'lxml-xml')\n",
    "            except Exception as e:\n",
    "                print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                print(e)\n",
    "                print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "        elif extension == \"html\":\n",
    "            try:\n",
    "                print(\"@@@@@@@@@@@@   HTML parser @@@@@@@@@@@@\")\n",
    "                soup = BeautifulSoup(fp, 'lxml')\n",
    "            except Exception as e:\n",
    "                print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                print(e)\n",
    "                print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "        fp.close()\n",
    "        all_fig = soup.find_all(string=re.compile(\"^Fig\"))\n",
    "        fig_no = 1\n",
    "        max_caption_words = 60\n",
    "        min_caption_words = 3\n",
    "\n",
    "        full_text = soup.get_text()\n",
    "        \n",
    "#         if filename == './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml':\n",
    "#             output_file = open(\"output_full_text.txt\",\"w\")\n",
    "#             output_file.write(remove_non_ascii(full_text))\n",
    "#             output_file.close()\n",
    "\n",
    "#             output_file = open(\"output_full_text_str.txt\",\"w\")\n",
    "#             output_file.write(str(full_text.encode('utf8')))\n",
    "#             output_file.close()\n",
    "\n",
    "        full_text = remove_multi_newlines(full_text)\n",
    "        all_sent_list = split_text_to_sent(full_text)\n",
    "        all_sentence_embeddings = sentence_transformer_model.encode(all_sent_list)\n",
    "\n",
    "        # for fig in all_fig:\n",
    "        #     fig_caption = extract_caption(fig, min_caption_words, max_caption_words)\n",
    "        #     print (fig_caption)\n",
    "        for fig in all_fig:\n",
    "            print(\"------- fig  \"+str(fig_no)+\"   --------\")\n",
    "\n",
    "            fig_caption = extract_caption(fig, min_caption_words, max_caption_words)\n",
    "            image_path = extract_imagepath(fig, filename, type_html)\n",
    "\n",
    "            # ocr_full_txt_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "            if type_html == \"textbooks\":\n",
    "                ocr_json_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "            if type_html == \"papers\":\n",
    "                ocr_json_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\".json\"\n",
    "            plt.imshow(mpimg.imread(image_path))\n",
    "            plt.show()\n",
    "            if os.path.isfile(ocr_json_path):\n",
    "                print(\"    Json already exist :  \"+ocr_json_path)\n",
    "                json_file = open(ocr_json_path)\n",
    "                ocr_dict = json.load(json_file) \n",
    "                #structure of ocr_dict ={\"block\":[ {\"text\":\"\",\"\",\"paragraph\":[{\"text\":\"\", \"word\":[\"w1\",\"w2\",..], \"bounding_poly\": [[],[],..]}, { .. }, ...],\"bounding_poly\":[[(x1,y1),..(x4,y4)],[]...]}, { ... }, ..],\"bounding_poly\":[[],[]...]}\n",
    "            else:\n",
    "                print(\"  $$$$  Calling google vision API $$$$ \")\n",
    "                ocr_dict = detect_document(image_path, type_html)\n",
    "            labels_list = []\n",
    "            # print(\"~~~~~~~ Labels  ~~~~~~~~\")\n",
    "            if len(ocr_dict[\"block\"]) > 0:\n",
    "                for i in range(0,len(ocr_dict[\"block\"])):\n",
    "                    block_dict = ocr_dict[\"block\"][i]\n",
    "                    # print(block_dict[\"text\"]) # uncomment for labels\n",
    "                    labels_list.append(block_dict[\"text\"].replace(\"-\",\"\"))\n",
    "                    # print(block_dict[\"text\"], \"  \",ocr_dict[\"bounding_poly\"][i])\n",
    "\n",
    "                    # for i in range(0,len(block_dict[\"paragraph\"])):\n",
    "                    #     para_dict = block_dict[\"paragraph\"][i]\n",
    "                    #     print(\"      -\",para_dict[\"text\"])\n",
    "                    #     print(\"      \",para_dict[\"text\"],\"   \",block_dict[\"bounding_poly\"][i])\n",
    "            else:\n",
    "                print(\" \")\n",
    "                # print(\"  NO Labels  \")\n",
    "            # print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "            if type_html == \"textbooks\":\n",
    "                above_header = fig.parent.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                if above_header is not None:\n",
    "                    above_header_text = above_header.get_text()\n",
    "                else:\n",
    "                    above_header_text = \"\"\n",
    "                while clean_string(above_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(above_header_text).replace(\" \",\"\"))<3:\n",
    "                    if above_header is not None:\n",
    "                        above_header = above_header.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if above_header is not None:\n",
    "                        above_header_text = above_header.get_text()\n",
    "                    else:\n",
    "                        above_header_text = \"\"\n",
    "                        break\n",
    "\n",
    "                below_header = fig.parent.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                if below_header is not None:\n",
    "                    below_header_text = below_header.get_text()\n",
    "                else:\n",
    "                    below_header_text = \"\"\n",
    "                while clean_string(below_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(below_header_text).replace(\" \",\"\"))<3:\n",
    "                    if below_header is not None:\n",
    "                        below_header = below_header.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if below_header is not None:\n",
    "                        below_header_text = below_header.get_text()\n",
    "                    else:\n",
    "                        below_header_text = \"\"\n",
    "                        break\n",
    "\n",
    "                print(\"\\nabove header = \",above_header_text)\n",
    "                print(\"fig caption = \",fig_caption)\n",
    "                print(\"below header = \",below_header_text)\n",
    "\n",
    "                section_text = above_header_text\n",
    "                if above_header is not None:\n",
    "                    next_header = above_header.find_next()\n",
    "                if below_header is not None:\n",
    "                    while next_header.get_text() != below_header.get_text():\n",
    "                        # section_text = section_text + next_header.text\n",
    "                        for child in next_header.contents:\n",
    "                            if isinstance(child, str):\n",
    "                                # if len(clean_string(child).replace(\" \",\"\")) > 1:\n",
    "                                section_text = section_text + child\n",
    "                                # print(\"---------------\\n\",clean_string(child),\"\\n\")\n",
    "                        next_header = next_header.find_next()\n",
    "                        section_text = section_text + \"\\n\"\n",
    "                else:\n",
    "                    while next_header is not None:\n",
    "                        # section_text = section_text + next_header.text\n",
    "                        for child in next_header.contents:\n",
    "                            if isinstance(child, str):\n",
    "                                # if len(clean_string(child).replace(\" \",\"\")) > 1:\n",
    "                                section_text = section_text + child\n",
    "                                # print(\"---------------\\n\",clean_string(child),\"\\n\")\n",
    "                        next_header = next_header.find_next()\n",
    "                        section_text = section_text + \"\\n\"\n",
    "\n",
    "                section_text = remove_multi_newlines(section_text)\n",
    "                sent_list = split_text_to_sent(section_text)\n",
    "                # print(\"---------\\n\",section_text,\"\\n--------------\")\n",
    "                # print(\"++++++\\n\",full_text.encode(\"utf-8\"),\"\\n++++++++\")\n",
    "            if type_html == \"papers\":\n",
    "                print(\"Figure caption = \",fig_caption)\n",
    "                relevant_txt_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\"_relevant.txt\"\n",
    "                relevant_file = open(relevant_txt_path,\"r\", encoding='utf-8')\n",
    "                print(relevant_txt_path + \"  read!\")\n",
    "                section_text = relevant_file.read()\n",
    "                # print(\"\\n!!! section text !!! \\n\", section_text, \"\\n!!!!!!!!\")\n",
    "                section_text = remove_multi_newlines(section_text)\n",
    "                sent_list = split_text_to_sent(section_text)\n",
    "                print (\"Length of labels list = \", len(labels_list))\n",
    "                print (\"Length of sent list = \", len(sent_list))\n",
    "                # for inx in range(0,len(sent_list)):\n",
    "                #     print( str(inx)+\": \",f\"{bcolors.OKBLUE}{sent_list[inx]}{bcolors.ENDC}\")\n",
    "                relevant_file.close()\n",
    "            \n",
    "            fig_caption_sent = \"\"\n",
    "            if len(labels_list) > 0:\n",
    "                str_labels_list = str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "                fig_caption_sent = fig_caption +\". This figure contains \"+ str_labels_list +\".\"\n",
    "                # fig_caption_sent = fig_caption + \" and this figure contains \"+ str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")+ \" as labels.\"\n",
    "            else:\n",
    "                fig_caption_sent = fig_caption\n",
    "            \n",
    "            fig_caption_sent = clean_string(fig_caption_sent).replace(\"\\t\", \" \").lower()\n",
    "\n",
    "            print(\"Fig caption sentence = \",fig_caption_sent)\n",
    "            print(\"s^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            fig_caption_embedding = sentence_transformer_model.encode(fig_caption_sent)\n",
    "            # print(\"t^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "            sentence_embeddings = sentence_transformer_model.encode(sent_list)\n",
    "            # print(\"u^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            cosine_list = cosine_similarity([fig_caption_embedding],sentence_embeddings)[0]\n",
    "            all_cosine_list = cosine_similarity([fig_caption_embedding],all_sentence_embeddings)[0]\n",
    "            # print(\"v^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            no_space_sent_list = []\n",
    "            for sent in sent_list:\n",
    "                no_space_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "            no_space_all_sent_list = []\n",
    "            for sent in all_sent_list:\n",
    "                no_space_all_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "            fig_caption_clean = remove_multi_newlines(fig_caption)\n",
    "            fig_caption_start = split_text_to_sent(fig_caption_clean)[0]\n",
    "            fig_caption_index = -1\n",
    "            if fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_all_sent_list:\n",
    "                fig_caption_index = no_space_all_sent_list.index(fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "            sect_index_list = []\n",
    "            sect_cosine_list = []\n",
    "            average_cosine_value = np.average(all_cosine_list)\n",
    "            check_no_space_sent_list = no_space_sent_list.copy()\n",
    "            for index in range(0,len(all_sent_list)):\n",
    "                if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "                    sect_index_list.append(index)\n",
    "                    sect_cosine_list.append(all_cosine_list[index])\n",
    "                    check_no_space_sent_list.remove(all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "            plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = \"non_smooth\")\n",
    "            scatter_x = []\n",
    "            scatter_y = []\n",
    "            for sect_index in sect_index_list:\n",
    "                scatter_x.append(sect_index)\n",
    "                scatter_y.append(0)\n",
    "            plt.scatter(scatter_x, scatter_y, label = \"Relevant text\", marker = '^', c = \"green\")\n",
    "\n",
    "            if smooth_true == 1:\n",
    "                smooth_no = -7 # gaussian smooth with window size 7\n",
    "                all_cosine_list = moving_average(np.array(all_cosine_list), smooth_no).tolist()\n",
    "                plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = smooth_label(smooth_no))\n",
    "                example_folder = \"h2_smooth_examples/\"\n",
    "            else:\n",
    "                example_folder = \"h2_nosmooth_examples/\"\n",
    "            prediction_index_list = []\n",
    "            for index in range(0,len(all_sent_list)):\n",
    "                if abs(index - fig_caption_index) <= no_bounding_sent and all_cosine_list[index] - average_cosine_value >= h2_optimal_cosine_thershold:\n",
    "                    prediction_index_list.append(index)\n",
    "            scatter_x = []\n",
    "            scatter_y = []\n",
    "            for pred_index in prediction_index_list:\n",
    "                scatter_x.append(pred_index)\n",
    "                scatter_y.append(0.95)\n",
    "            plt.scatter(scatter_x, scatter_y, label = \"Predicted text\", marker = 'v', c = \"blue\")\n",
    "\n",
    "            plt.xlabel('Sentence index')\n",
    "            plt.ylabel('Cosine value')\n",
    "            plt.title(\"cosine simlairity scores\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.ylim(-0.1, 1)\n",
    "            plt.legend(loc=\"upper right\",fontsize=\"x-small\")\n",
    "            scatter_plot_path = example_folder+\"scatter_plots/\"+str(fig_no)+'_'+type_html+'_'+filename.replace(\"/\",\"_\")+'.png'\n",
    "            example_html_path = example_folder+str(fig_no)+'_'+type_html+'_'+filename.replace(\"/\",\"_\")+'.html'\n",
    "            plt.savefig(scatter_plot_path,dpi=500)\n",
    "            plt.show()\n",
    "\n",
    "            # check_no_space_sent_list = no_space_sent_list.copy()\n",
    "            prediction_text = fig_caption\n",
    "            prediction_text_labels = fig_caption_sent\n",
    "            for pred_index in prediction_index_list:\n",
    "                prediction_text = prediction_text +\". \"+all_sent_list[pred_index]\n",
    "                prediction_text_labels = prediction_text_labels +\". \"+ all_sent_list[pred_index]\n",
    "            if len(prediction_text) > 1020:\n",
    "                prediction_text = prediction_text[:1020]\n",
    "            if len(prediction_text_labels) > 1020:\n",
    "                prediction_text_labels = prediction_text_labels[:1020]\n",
    "            summary_text = pipeline_summarizer(prediction_text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']         \n",
    "            summary_text_labels = pipeline_summarizer(prediction_text_labels, max_length=100, min_length=30, do_sample=False)[0]['summary_text']       \n",
    "            original_stdout = sys.stdout\n",
    "            out_file = open(example_html_path, 'w')\n",
    "            sys.stdout = out_file\n",
    "            print(\"\"\" <!DOCTYPE html>\n",
    "                    <html>\n",
    "                    <body style=\"background-color:black;\">\"\"\")\n",
    "            print('<img src=\".'+image_path+'\" style=\"background-color:white;\">')\n",
    "            print('<h2 style=\"color:white;\">Fig caption sentence = ',fig_caption_sent,'</h2>')\n",
    "            print('<img src=\"scatter_plots/'+scatter_plot_path.split(\"/\")[-1]+'\" style=\"background-color:white;\" width=\"auto\" height=\"400\">')\n",
    "            print('<p style=\"color:silver;\"><b>Summary without labels:</b>'+summary_text+'</p><br>')\n",
    "            print('<p style=\"color:silver;\"><b>Summary with labels:</b>'+summary_text_labels+'</p><br>')\n",
    "            print(\"<p>----------------- START ---------------------------</p>\")\n",
    "            print('<ul><li><h3 style=\"color:limegreen;\">True positive</h3></li>')\n",
    "            print('<li><h3 style=\"color:darkgreen;\">False negative</h3></li>')\n",
    "            print('<li><h3 style=\"color:brown;\">False positive</h3></li>')\n",
    "            print('<li><h3 style=\"color:grey;\">True negative</h3></li></ul>')\n",
    "            for index in range(0,len(all_sent_list)):\n",
    "                if index in prediction_index_list:\n",
    "                    # cosine_value = all_cosine_list[index]\n",
    "                    if index in sect_index_list:\n",
    "                        print( '<p style=\"color:limegreen;\">'+str(index)+\": \", all_sent_list[index], \"</p>\")\n",
    "                    else:\n",
    "                        print( '<p style=\"color:brown;\">'+str(index)+\": \", all_sent_list[index], \"</p>\")\n",
    "                else:\n",
    "                    if index in sect_index_list:\n",
    "                        print( '<p style=\"color:darkgreen;\">'+str(index)+\": \", all_sent_list[index], \"</p>\")\n",
    "                    else:\n",
    "                        print( '<p style=\"color:grey;\">'+str(index)+\": \", all_sent_list[index], \"</p>\")\n",
    "                        \n",
    "                # if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "                #     cosine_value = all_cosine_list[index]\n",
    "                #     if cosine_value > 0.35:\n",
    "                #         print( '<p style=\"color:limegreen;\">## '+str(index)+\": \", all_sent_list[index], \"</p>\")\n",
    "                #     elif cosine_value > 0.2:\n",
    "                #         print( '<p style=\"color:darkturquoise;\">## '+str(index)+\": \", all_sent_list[index], \"</p>\")\n",
    "                #     else:\n",
    "                #         print( '<p style=\"color:grey;\">## '+str(index)+\": \", all_sent_list[index], \"</p>\")\n",
    "                # else:\n",
    "                #     cosine_value = all_cosine_list[index]\n",
    "                #     if cosine_value > 0.35:\n",
    "                #         print( '<p style=\"color:limegreen;\">', all_sent_list[index], \"</p>\")\n",
    "                #     elif cosine_value > 0.2:\n",
    "                #         print( '<p style=\"color:darkturquoise;\">', all_sent_list[index], \"</p>\")\n",
    "                #     else:\n",
    "                #         print( '<p style=\"color:grey;\">', all_sent_list[index], \"</p>\")\n",
    "            print(\"<p>----------------- END ---------------------------</p>\")\n",
    "            print(\"\"\"</body>\n",
    "                    </html>\"\"\")\n",
    "\n",
    "            # for index in range(0,len(all_sent_list)):\n",
    "            #     if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "            #         cosine_value = all_cosine_list[index]\n",
    "            #         if cosine_value > 0.35:\n",
    "            #             print( \"## \"+str(index)+\": \", f\"{bcolors.OKGREEN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "            #         elif cosine_value > 0.2:\n",
    "            #             print( \"## \"+str(index)+\": \", f\"{bcolors.OKCYAN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "            #         elif cosine_value > 0.15:\n",
    "            #             print( \"## \"+str(index)+\": \", f\"{bcolors.WARNING}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "            #         else:\n",
    "            #             print( \"## \"+str(index)+\": \", all_sent_list[index])\n",
    "            #     else:\n",
    "            #         cosine_value = all_cosine_list[index]\n",
    "            #         if cosine_value > 0.35:\n",
    "            #             print( f\"{bcolors.OKGREEN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "            #         elif cosine_value > 0.2:\n",
    "            #             print( f\"{bcolors.OKCYAN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "            #         elif cosine_value > 0.15:\n",
    "            #             print( f\"{bcolors.WARNING}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "            #         else:\n",
    "            #             print( all_sent_list[index])\n",
    "\n",
    "            sys.stdout = original_stdout\n",
    "            out_file.close()\n",
    "            fig_no = fig_no + 1\n",
    "\n",
    "        print(\"++++++++++++++++++++++++++++   END   +++++++++++++++++++++\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a490b1f4-4fd1-461a-8a97-ec7f3b49d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\"\n",
    "textbooks_html_files = [ \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/3_5239_3/OEBPS/Text/Untitled-1.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/4_5239_4/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/5_5239_5/OEBPS/Text/Untitled-3.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/6_5239_6/OEBPS/Text/Untitled-4.xhtml\"]\n",
    "papers_html_files = ['./data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-004.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-005.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-006.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-007.xhtml']\n",
    "\n",
    "sentence_transformer_model = sentence_transformer_model_nondebug_dict['sentence-transformers/multi-qa-MiniLM-L6-cos-v1']\n",
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# types_html_files_list = [\"papers\"]\n",
    "smooth_true = 0\n",
    "\n",
    "for type_html in types_html_files_list:\n",
    "    filenames_list = []\n",
    "    if type_html == \"textbooks\":\n",
    "        filenames_list = textbooks_html_files\n",
    "    if type_html == \"papers\":\n",
    "        filenames_list = papers_html_files\n",
    "\n",
    "    for filename in filenames_list:\n",
    "        try:\n",
    "            fp = open(filename, encoding=\"utf8\")\n",
    "            print(\"++++++++++++++++++++++++++++   START   +++++++++++++++++++++\")\n",
    "            print(filename)\n",
    "        except:\n",
    "            print(\"***************  Error while opening file   ***************\")\n",
    "            print(filename)\n",
    "            print(\"************************************************************\")\n",
    "        extension = filename.split(\".\")[-1]\n",
    "        if extension == \"xhtml\":\n",
    "            try:\n",
    "                print(\"@@@@@@@@@@@@   XML parser @@@@@@@@@@@@\")\n",
    "                soup = BeautifulSoup(fp, 'lxml-xml')\n",
    "            except Exception as e:\n",
    "                print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                print(e)\n",
    "                print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "        elif extension == \"html\":\n",
    "            try:\n",
    "                print(\"@@@@@@@@@@@@   HTML parser @@@@@@@@@@@@\")\n",
    "                soup = BeautifulSoup(fp, 'lxml')\n",
    "            except Exception as e:\n",
    "                print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                print(e)\n",
    "                print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "        fp.close()\n",
    "        all_fig = soup.find_all(string=re.compile(\"^Fig\"))\n",
    "        fig_no = 1\n",
    "        max_caption_words = 60\n",
    "        min_caption_words = 3\n",
    "\n",
    "        full_text = soup.get_text()\n",
    "        \n",
    "#         if filename == './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml':\n",
    "#             output_file = open(\"output_full_text.txt\",\"w\")\n",
    "#             output_file.write(remove_non_ascii(full_text))\n",
    "#             output_file.close()\n",
    "\n",
    "#             output_file = open(\"output_full_text_str.txt\",\"w\")\n",
    "#             output_file.write(str(full_text.encode('utf8')))\n",
    "#             output_file.close()\n",
    "\n",
    "        full_text = remove_multi_newlines(full_text)\n",
    "        all_sent_list = split_text_to_sent(full_text)\n",
    "        all_sentence_embeddings = sentence_transformer_model.encode(all_sent_list)\n",
    "\n",
    "        # for fig in all_fig:\n",
    "        #     fig_caption = extract_caption(fig, min_caption_words, max_caption_words)\n",
    "        #     print (fig_caption)\n",
    "        for fig in all_fig:\n",
    "            print(\"------- fig  \"+str(fig_no)+\"   --------\")\n",
    "\n",
    "            fig_caption = extract_caption(fig, min_caption_words, max_caption_words)\n",
    "            image_path = extract_imagepath(fig, filename, type_html)\n",
    "\n",
    "            # ocr_full_txt_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "            if type_html == \"textbooks\":\n",
    "                ocr_json_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "            if type_html == \"papers\":\n",
    "                ocr_json_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\".json\"\n",
    "            plt.imshow(mpimg.imread(image_path))\n",
    "            plt.show()\n",
    "            if os.path.isfile(ocr_json_path):\n",
    "                print(\"    Json already exist :  \"+ocr_json_path)\n",
    "                json_file = open(ocr_json_path)\n",
    "                ocr_dict = json.load(json_file) \n",
    "                #structure of ocr_dict ={\"block\":[ {\"text\":\"\",\"\",\"paragraph\":[{\"text\":\"\", \"word\":[\"w1\",\"w2\",..], \"bounding_poly\": [[],[],..]}, { .. }, ...],\"bounding_poly\":[[(x1,y1),..(x4,y4)],[]...]}, { ... }, ..],\"bounding_poly\":[[],[]...]}\n",
    "            else:\n",
    "                print(\"  $$$$  Calling google vision API $$$$ \")\n",
    "                ocr_dict = detect_document(image_path, type_html)\n",
    "            labels_list = []\n",
    "            # print(\"~~~~~~~ Labels  ~~~~~~~~\")\n",
    "            if len(ocr_dict[\"block\"]) > 0:\n",
    "                for i in range(0,len(ocr_dict[\"block\"])):\n",
    "                    block_dict = ocr_dict[\"block\"][i]\n",
    "                    # print(block_dict[\"text\"]) # uncomment for labels\n",
    "                    labels_list.append(block_dict[\"text\"].replace(\"-\",\"\"))\n",
    "                    # print(block_dict[\"text\"], \"  \",ocr_dict[\"bounding_poly\"][i])\n",
    "\n",
    "                    # for i in range(0,len(block_dict[\"paragraph\"])):\n",
    "                    #     para_dict = block_dict[\"paragraph\"][i]\n",
    "                    #     print(\"      -\",para_dict[\"text\"])\n",
    "                    #     print(\"      \",para_dict[\"text\"],\"   \",block_dict[\"bounding_poly\"][i])\n",
    "            else:\n",
    "                print(\" \")\n",
    "                # print(\"  NO Labels  \")\n",
    "            # print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "            if type_html == \"textbooks\":\n",
    "                above_header = fig.parent.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                if above_header is not None:\n",
    "                    above_header_text = above_header.get_text()\n",
    "                else:\n",
    "                    above_header_text = \"\"\n",
    "                while clean_string(above_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(above_header_text).replace(\" \",\"\"))<3:\n",
    "                    if above_header is not None:\n",
    "                        above_header = above_header.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if above_header is not None:\n",
    "                        above_header_text = above_header.get_text()\n",
    "                    else:\n",
    "                        above_header_text = \"\"\n",
    "                        break\n",
    "\n",
    "                below_header = fig.parent.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                if below_header is not None:\n",
    "                    below_header_text = below_header.get_text()\n",
    "                else:\n",
    "                    below_header_text = \"\"\n",
    "                while clean_string(below_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(below_header_text).replace(\" \",\"\"))<3:\n",
    "                    if below_header is not None:\n",
    "                        below_header = below_header.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if below_header is not None:\n",
    "                        below_header_text = below_header.get_text()\n",
    "                    else:\n",
    "                        below_header_text = \"\"\n",
    "                        break\n",
    "\n",
    "                print(\"\\nabove header = \",above_header_text)\n",
    "                print(\"fig caption = \",fig_caption)\n",
    "                print(\"below header = \",below_header_text)\n",
    "\n",
    "                section_text = above_header_text\n",
    "                if above_header is not None:\n",
    "                    next_header = above_header.find_next()\n",
    "                if below_header is not None:\n",
    "                    while next_header.get_text() != below_header.get_text():\n",
    "                        # section_text = section_text + next_header.text\n",
    "                        for child in next_header.contents:\n",
    "                            if isinstance(child, str):\n",
    "                                # if len(clean_string(child).replace(\" \",\"\")) > 1:\n",
    "                                section_text = section_text + child\n",
    "                                # print(\"---------------\\n\",clean_string(child),\"\\n\")\n",
    "                        next_header = next_header.find_next()\n",
    "                        section_text = section_text + \"\\n\"\n",
    "                else:\n",
    "                    while next_header is not None:\n",
    "                        # section_text = section_text + next_header.text\n",
    "                        for child in next_header.contents:\n",
    "                            if isinstance(child, str):\n",
    "                                # if len(clean_string(child).replace(\" \",\"\")) > 1:\n",
    "                                section_text = section_text + child\n",
    "                                # print(\"---------------\\n\",clean_string(child),\"\\n\")\n",
    "                        next_header = next_header.find_next()\n",
    "                        section_text = section_text + \"\\n\"\n",
    "\n",
    "                section_text = remove_multi_newlines(section_text)\n",
    "                sent_list = split_text_to_sent(section_text)\n",
    "                # print(\"---------\\n\",section_text,\"\\n--------------\")\n",
    "                # print(\"++++++\\n\",full_text.encode(\"utf-8\"),\"\\n++++++++\")\n",
    "            if type_html == \"papers\":\n",
    "                print(\"Figure caption = \",fig_caption)\n",
    "                relevant_txt_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\"_relevant.txt\"\n",
    "                relevant_file = open(relevant_txt_path,\"r\", encoding='utf-8')\n",
    "                print(relevant_txt_path + \"  read!\")\n",
    "                section_text = relevant_file.read()\n",
    "                # print(\"\\n!!! section text !!! \\n\", section_text, \"\\n!!!!!!!!\")\n",
    "                section_text = remove_multi_newlines(section_text)\n",
    "                sent_list = split_text_to_sent(section_text)\n",
    "                print (\"Length of labels list = \", len(labels_list))\n",
    "                print (\"Length of sent list = \", len(sent_list))\n",
    "                # for inx in range(0,len(sent_list)):\n",
    "                #     print( str(inx)+\": \",f\"{bcolors.OKBLUE}{sent_list[inx]}{bcolors.ENDC}\")\n",
    "                relevant_file.close()\n",
    "            \n",
    "            fig_caption_sent = \"\"\n",
    "            if len(labels_list) > 0:\n",
    "                str_labels_list = str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "                fig_caption_sent = fig_caption +\". This figure contains \"+ str_labels_list +\".\"\n",
    "                # fig_caption_sent = fig_caption + \" and this figure contains \"+ str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")+ \" as labels.\"\n",
    "            else:\n",
    "                fig_caption_sent = fig_caption\n",
    "            \n",
    "            fig_caption_sent = clean_string(fig_caption_sent).replace(\"\\t\", \" \").lower()\n",
    "\n",
    "            print(\"Fig caption sentence = \",fig_caption_sent)\n",
    "            print(\"s^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            fig_caption_embedding = sentence_transformer_model.encode(fig_caption_sent)\n",
    "            # print(\"t^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "            sentence_embeddings = sentence_transformer_model.encode(sent_list)\n",
    "            # print(\"u^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            cosine_list = cosine_similarity([fig_caption_embedding],sentence_embeddings)[0]\n",
    "            all_cosine_list = cosine_similarity([fig_caption_embedding],all_sentence_embeddings)[0]\n",
    "            # print(\"v^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            no_space_sent_list = []\n",
    "            for sent in sent_list:\n",
    "                no_space_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "            sect_index_list = []\n",
    "            sect_cosine_list = []\n",
    "            check_no_space_sent_list = no_space_sent_list.copy()\n",
    "            for index in range(0,len(all_sent_list)):\n",
    "                if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "                    sect_index_list.append(index)\n",
    "                    sect_cosine_list.append(all_cosine_list[index])\n",
    "                    check_no_space_sent_list.remove(all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "            plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = \"non_smooth\")\n",
    "            scatter_x = []\n",
    "            scatter_y = []\n",
    "            for sect_index in sect_index_list:\n",
    "                scatter_x.append(sect_index)\n",
    "                scatter_y.append(0)\n",
    "            plt.scatter(scatter_x, scatter_y, label = \"Relevant text\", marker = 'o', c = \"green\")\n",
    "            if smooth_true == 1:\n",
    "                smooth_no = -7 # gaussian smooth with window size 7\n",
    "                all_cosine_list = moving_average(np.array(all_cosine_list), smooth_no).tolist()\n",
    "                plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = smooth_label(smooth_no))\n",
    "                example_folder = \"smooth_examples/\"\n",
    "            else:\n",
    "                example_folder = \"nosmooth_examples/\"\n",
    "            plt.xlabel('Sentence index')\n",
    "            plt.ylabel('Cosine value')\n",
    "            plt.title(\"cosine simlairity scores\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.ylim(-0.1, 1)\n",
    "            plt.legend(loc=\"upper right\",fontsize=\"x-small\")\n",
    "            scatter_plot_path = example_folder+\"scatter_plots/\"+str(fig_no)+'_'+type_html+'_'+filename.replace(\"/\",\"_\")+'.png'\n",
    "            example_html_path = example_folder+str(fig_no)+'_'+type_html+'_'+filename.replace(\"/\",\"_\")+'.html'\n",
    "            plt.savefig(scatter_plot_path,dpi=500)\n",
    "            plt.show()\n",
    "\n",
    "            check_no_space_sent_list = no_space_sent_list.copy()\n",
    "            original_stdout = sys.stdout\n",
    "            out_file = open(example_html_path, 'w')\n",
    "            sys.stdout = out_file\n",
    "            print(\"\"\" <!DOCTYPE html>\n",
    "                    <html>\n",
    "                    <body style=\"background-color:black;\">\"\"\")\n",
    "            print('<img src=\".'+image_path+'\" style=\"background-color:white;\">')\n",
    "            print('<h2 style=\"color:white;\">Fig caption sentence = ',fig_caption_sent,'</h2>')\n",
    "            print('<img src=\"scatter_plots/'+scatter_plot_path.split(\"/\")[-1]+'\" style=\"background-color:white;\" width=\"auto\" height=\"400\">')\n",
    "            print(\"<p>----------------- START ---------------------------</p>\")\n",
    "            for index in range(0,len(all_sent_list)):\n",
    "                if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "                    cosine_value = all_cosine_list[index]\n",
    "                    if cosine_value > 0.35:\n",
    "                        print( '<p style=\"color:limegreen;\">## '+str(index)+\": \", all_sent_list[index], \"</p>\")\n",
    "                    elif cosine_value > 0.2:\n",
    "                        print( '<p style=\"color:darkturquoise;\">## '+str(index)+\": \", all_sent_list[index], \"</p>\")\n",
    "                    else:\n",
    "                        print( '<p style=\"color:grey;\">## '+str(index)+\": \", all_sent_list[index], \"</p>\")\n",
    "                else:\n",
    "                    cosine_value = all_cosine_list[index]\n",
    "                    if cosine_value > 0.35:\n",
    "                        print( '<p style=\"color:limegreen;\">', all_sent_list[index], \"</p>\")\n",
    "                    elif cosine_value > 0.2:\n",
    "                        print( '<p style=\"color:darkturquoise;\">', all_sent_list[index], \"</p>\")\n",
    "                    else:\n",
    "                        print( '<p style=\"color:grey;\">', all_sent_list[index], \"</p>\")\n",
    "            print(\"<p>----------------- END ---------------------------</p>\")\n",
    "            print(\"\"\"</body>\n",
    "                    </html>\"\"\")\n",
    "\n",
    "            # for index in range(0,len(all_sent_list)):\n",
    "            #     if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "            #         cosine_value = all_cosine_list[index]\n",
    "            #         if cosine_value > 0.35:\n",
    "            #             print( \"## \"+str(index)+\": \", f\"{bcolors.OKGREEN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "            #         elif cosine_value > 0.2:\n",
    "            #             print( \"## \"+str(index)+\": \", f\"{bcolors.OKCYAN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "            #         elif cosine_value > 0.15:\n",
    "            #             print( \"## \"+str(index)+\": \", f\"{bcolors.WARNING}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "            #         else:\n",
    "            #             print( \"## \"+str(index)+\": \", all_sent_list[index])\n",
    "            #     else:\n",
    "            #         cosine_value = all_cosine_list[index]\n",
    "            #         if cosine_value > 0.35:\n",
    "            #             print( f\"{bcolors.OKGREEN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "            #         elif cosine_value > 0.2:\n",
    "            #             print( f\"{bcolors.OKCYAN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "            #         elif cosine_value > 0.15:\n",
    "            #             print( f\"{bcolors.WARNING}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "            #         else:\n",
    "            #             print( all_sent_list[index])\n",
    "\n",
    "            sys.stdout = original_stdout\n",
    "            out_file.close()\n",
    "            # for index in range(0,len(sent_list)):\n",
    "            #     cosine_value = smooth_cosine_list[index]\n",
    "            #     if cosine_value > 0.35:\n",
    "            #         print( str(index)+\": \", f\"{bcolors.OKGREEN}{sent_list[index]}{bcolors.ENDC}\")\n",
    "            #     elif cosine_value > 0.2:\n",
    "            #         print( str(index)+\": \", f\"{bcolors.OKCYAN}{sent_list[index]}{bcolors.ENDC}\")\n",
    "            #     elif cosine_value > 0.15:\n",
    "            #         print( str(index)+\": \", f\"{bcolors.WARNING}{sent_list[index]}{bcolors.ENDC}\")\n",
    "            #     else:\n",
    "            #         print( str(index)+\": \", sent_list[index])\n",
    "            # print(\"e^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "            fig_no = fig_no + 1\n",
    "\n",
    "        print(\"++++++++++++++++++++++++++++   END   +++++++++++++++++++++\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19e511-3970-4ca3-87b2-a75bb16839a9",
   "metadata": {},
   "source": [
    "# test: text-text similarity all models accuracy test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4c08f-a19b-48e0-9c26-940bd3cbd011",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------- Heuristic 0 -------------------------------\n",
    "# Consider few sentences above and below the fig as boundary.\n",
    "# Next, consider similar sentence with in the boundary as similar sentences.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# filename = \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\"\n",
    "textbooks_html_files = [ \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/3_5239_3/OEBPS/Text/Untitled-1.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/4_5239_4/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/5_5239_5/OEBPS/Text/Untitled-3.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/6_5239_6/OEBPS/Text/Untitled-4.xhtml\"]\n",
    "papers_html_files = ['./data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-004.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-005.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-006.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-007.xhtml']\n",
    "\n",
    "filenames_list_debug = [ './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml']\n",
    "\n",
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# types_html_files_list = [\"papers\"]\n",
    "\n",
    "no_bounding_sent = 30\n",
    "no_points_fpr_fnr_plot = 50\n",
    "\n",
    "debug = 0\n",
    "\n",
    "if debug == 1:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_debug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_debug_dict\n",
    "if debug == 0:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_nondebug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_nondebug_dict\n",
    "\n",
    "\n",
    "smooth_true = True\n",
    "# if smooth_true:\n",
    "# smooth_no_list = [-11,-9,-7,-5,-3,0,3,5,7,9,11,100,101]\n",
    "smooth_no_list = [0,-7]\n",
    "# smooth_no_list = [0,3,9,13]\n",
    "# smooth_no_list = [0,100,101]\n",
    "# smooth_no_list = [-13,0,13,101]\n",
    "# else:\n",
    "#     smooth_no_list = [0]\n",
    "\n",
    "typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict = {}\n",
    "for type_html in types_html_files_list:\n",
    "    typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html] = {}\n",
    "    for smooth_no in smooth_no_list:\n",
    "        typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no] = {}\n",
    "\n",
    "for type_html in types_html_files_list:\n",
    "    \n",
    "    for smooth_no in smooth_no_list:\n",
    "        if smooth_no==0:\n",
    "            smooth_true=False\n",
    "        else:\n",
    "            smooth_true=True\n",
    "        fpr_fnr_plot_dict = {}\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            fpr_fnr_plot_dict[transformer] = []\n",
    "\n",
    "        total_fig_no  = 0\n",
    "        file_no = 0\n",
    "        filenames_list = []\n",
    "        if type_html == \"textbooks\":\n",
    "            filenames_list = textbooks_html_files\n",
    "        if type_html == \"papers\":\n",
    "            filenames_list = papers_html_files\n",
    "        if debug ==  1:\n",
    "            filenames_list = filenames_list_debug\n",
    "            \n",
    "        for filename in filenames_list:\n",
    "            try:\n",
    "                fp = open(filename, encoding=\"utf8\")\n",
    "                print(\"++++++++++++++++++++++++++++   START   +++++++++++++++++++++\")\n",
    "                print(filename)\n",
    "            except:\n",
    "                print(\"***************  Error while opening file   ***************\")\n",
    "                print(filename)\n",
    "                print(\"************************************************************\")\n",
    "            extension = filename.split(\".\")[-1]\n",
    "            if extension == \"xhtml\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   XML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml-xml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            elif extension == \"html\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   HTML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "            fp.close()\n",
    "            fig_no = 0\n",
    "            max_caption_words = 60\n",
    "            min_caption_words = 3\n",
    "\n",
    "            full_text = soup.get_text()\n",
    "\n",
    "            # uncomment for debugging \n",
    "            # output_file = open(\"output_full_text.txt\",\"w\")\n",
    "            # output_file.write(remove_non_ascii(full_text))\n",
    "            # output_file.close()\n",
    "            # output_file = open(\"output_full_text_str.txt\",\"w\")\n",
    "            # output_file.write(str(full_text.encode('utf8')))\n",
    "            # output_file.close()\n",
    "\n",
    "            full_text = remove_multi_newlines(full_text)\n",
    "            all_sent_list = split_text_to_sent(full_text)\n",
    "\n",
    "            all_sentence_embeddings = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                all_sentence_embeddings[transformer] = sentence_transformer_model.encode(all_sent_list)\n",
    "\n",
    "            fpr_fnr_list_fig = []\n",
    "            fpr_fnr_dict_fig = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_dict_fig[transformer] = []\n",
    "            \n",
    "            all_fig = soup.find_all(string=re.compile(\"^Fig\"))\n",
    "\n",
    "            for fig in all_fig:\n",
    "                print(\"------- fig  \"+str(fig_no)+\"   --------\")\n",
    "                \n",
    "                fig_caption = extract_caption(fig, min_caption_words, max_caption_words)\n",
    "                image_path = extract_imagepath(fig, filename, type_html)\n",
    "\n",
    "                # ocr_full_txt_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "                if type_html == \"textbooks\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if type_html == \"papers\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if debug != 1:\n",
    "                    plt.imshow(mpimg.imread(image_path))\n",
    "                    plt.show()\n",
    "                if os.path.isfile(ocr_json_path):\n",
    "                    print(\"    Json already exist :  \"+ocr_json_path)\n",
    "                    json_file = open(ocr_json_path)\n",
    "                    ocr_dict = json.load(json_file) \n",
    "                    #structure of ocr_dict ={\"block\":[ {\"text\":\"\",\"\",\"paragraph\":[{\"text\":\"\", \"word\":[\"w1\",\"w2\",..], \"bounding_poly\": [[],[],..]}, { .. }, ...],\"bounding_poly\":[[(x1,y1),..(x4,y4)],[]...]}, { ... }, ..],\"bounding_poly\":[[],[]...]}\n",
    "                else:\n",
    "                    print(\"  $$$$  Calling google vision API $$$$ \")\n",
    "                    ocr_dict = detect_document(image_path, type_html)\n",
    "                labels_list = []\n",
    "                print(\"~~~~~~~ Labels  ~~~~~~~~\")\n",
    "                if len(ocr_dict[\"block\"]) > 0:\n",
    "                    for i in range(0,len(ocr_dict[\"block\"])):\n",
    "                        block_dict = ocr_dict[\"block\"][i]\n",
    "                        print(block_dict[\"text\"])\n",
    "                        labels_list.append(block_dict[\"text\"].replace(\"-\",\"\"))\n",
    "                        # print(block_dict[\"text\"], \"  \",ocr_dict[\"bounding_poly\"][i])\n",
    "\n",
    "                        # for i in range(0,len(block_dict[\"paragraph\"])):\n",
    "                        #     para_dict = block_dict[\"paragraph\"][i]\n",
    "                        #     print(\"      -\",para_dict[\"text\"])\n",
    "                        #     print(\"      \",para_dict[\"text\"],\"   \",block_dict[\"bounding_poly\"][i])\n",
    "                else:\n",
    "                    print(\"  NO Labels  \")\n",
    "                print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "                if type_html == \"textbooks\":\n",
    "                    above_header = fig.parent.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if above_header is not None:\n",
    "                        above_header_text = above_header.get_text()\n",
    "                    else:\n",
    "                        above_header_text = \"\"\n",
    "                    while clean_string(above_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(above_header_text).replace(\" \",\"\"))<3:\n",
    "                        if above_header is not None:\n",
    "                            above_header = above_header.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if above_header is not None:\n",
    "                            above_header_text = above_header.get_text()\n",
    "                        else:\n",
    "                            above_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    below_header = fig.parent.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if below_header is not None:\n",
    "                        below_header_text = below_header.get_text()\n",
    "                    else:\n",
    "                        below_header_text = \"\"\n",
    "                    while clean_string(below_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(below_header_text).replace(\" \",\"\"))<3:\n",
    "                        if below_header is not None:\n",
    "                            below_header = below_header.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if below_header is not None:\n",
    "                            below_header_text = below_header.get_text()\n",
    "                        else:\n",
    "                            below_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    print(\"\\nabove header = \",above_header_text)\n",
    "                    print(\"fig caption = \",fig_caption)\n",
    "                    print(\"below header = \",below_header_text)\n",
    "\n",
    "                    section_text = above_header_text\n",
    "                    next_header = above_header.find_next()\n",
    "\n",
    "                    while next_header.get_text() != below_header.get_text():\n",
    "                        # section_text = section_text + next_header.text\n",
    "                        for child in next_header.contents:\n",
    "                            if isinstance(child, str):\n",
    "                                # if len(clean_string(child).replace(\" \",\"\")) > 1:\n",
    "                                section_text = section_text + child\n",
    "                                # print(\"---------------\\n\",clean_string(child),\"\\n\")\n",
    "                        next_header = next_header.find_next()\n",
    "                        section_text = section_text + \"\\n\"\n",
    "\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    # print(\"---------\\n\",section_text,\"\\n--------------\")\n",
    "                    # print(\"++++++\\n\",full_text.encode(\"utf-8\"),\"\\n++++++++\")\n",
    "                if type_html == \"papers\":\n",
    "                    print(\"Figure caption = \",fig_caption)\n",
    "                    relevant_txt_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\"_relevant.txt\"\n",
    "                    relevant_file = open(relevant_txt_path,\"r\", encoding='utf-8')\n",
    "                    print(relevant_txt_path + \"  read!\")\n",
    "                    section_text = relevant_file.read()\n",
    "                    # print(\"\\n!!! section text !!! \\n\", section_text, \"\\n!!!!!!!!\")\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    print (\"Length of sent list = \", len(sent_list))\n",
    "                    # for inx in range(0,len(sent_list)):\n",
    "                    #     print( str(inx)+\": \",f\"{bcolors.OKBLUE}{sent_list[inx]}{bcolors.ENDC}\")\n",
    "                    relevant_file.close()\n",
    "                                        \n",
    "                str_labels_list = str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "\n",
    "                if len(labels_list) > 0:\n",
    "                    fig_caption_sent = fig_caption +\". This figure contains \"+ str_labels_list +\".\"\n",
    "                    # fig_caption_sent = fig_caption + \" and this figure contains \"+ str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")+ \" as labels.\"\n",
    "                else:\n",
    "                    fig_caption_sent = fig_caption\n",
    "\n",
    "\n",
    "                fig_caption_sent = clean_string(fig_caption_sent)\n",
    "\n",
    "                print(\"Fig caption sentence = \",fig_caption_sent)\n",
    "\n",
    "\n",
    "                moving_average_no = smooth_no                 #takes only odd numbers\n",
    "\n",
    "                no_space_sent_list = []\n",
    "                for sent in sent_list:\n",
    "                    no_space_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                no_space_all_sent_list = []\n",
    "                for sent in all_sent_list:\n",
    "                    no_space_all_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                fig_caption_clean = remove_multi_newlines(fig_caption)\n",
    "                fig_caption_start = split_text_to_sent(fig_caption_clean)[0]\n",
    "                fig_caption_index = -1\n",
    "                if fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_all_sent_list:\n",
    "                    fig_caption_index = no_space_all_sent_list.index(fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                for transformer in sentence_transformer_names_list:\n",
    "                    sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                    fig_caption_embedding = sentence_transformer_model.encode(fig_caption_sent)\n",
    "                    sentence_embeddings = sentence_transformer_model.encode(sent_list)\n",
    "\n",
    "\n",
    "                    cosine_list = cosine_similarity([fig_caption_embedding],sentence_embeddings)[0]\n",
    "                    all_cosine_list = cosine_similarity([fig_caption_embedding],all_sentence_embeddings[transformer])[0]\n",
    "\n",
    "                    sect_index_list = []\n",
    "                    sect_cosine_list = []\n",
    "                    check_no_space_sent_list = no_space_sent_list.copy()\n",
    "                    for index in range(0,len(all_sent_list)):\n",
    "                        if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "                            sect_index_list.append(index)\n",
    "                            sect_cosine_list.append(all_cosine_list[index])\n",
    "                            check_no_space_sent_list.remove(all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                    if len(sect_index_list) != len(sent_list):\n",
    "                        print(\"!!!!! ERROR !!!!\")\n",
    "                        print(\"start index of section = \",sect_index_list[0])\n",
    "                        print(\"end index of section = \",sect_index_list[-1])\n",
    "                        print(\"indices of section = \",sect_index_list)\n",
    "                        print(\"len of matched sent = \",len(sect_index_list))\n",
    "                        print(\"len of section sent = \",len(sent_list))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = \"non_smooth\")\n",
    "                        plt.vlines(x = sect_index_list[0], ymin = 0, ymax = 1, colors = 'green', label = 'section_start')\n",
    "                        plt.vlines(x = sect_index_list[-1], ymin = 0, ymax = 1, colors = 'red', label = 'section_end')\n",
    "\n",
    "                    if smooth_true:\n",
    "                        all_cosine_list = moving_average(np.array(all_cosine_list),moving_average_no).tolist()\n",
    "                        if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                            plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = smooth_label(smooth_no))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.title(\"cosine simlairity scores\")\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "\n",
    "                    cosine_threshold_list = np.arange(0.0, 1.0001 ,(1/no_points_fpr_fnr_plot))\n",
    "                    cosine_threshold_list = np.around(cosine_threshold_list, 4)\n",
    "\n",
    "                    fpr_fnr_list_threshold = []\n",
    "                    for cosine_threshold in cosine_threshold_list:\n",
    "                        no_sim_sent = 0\n",
    "                        no_sim_sent_in_sect = 0\n",
    "                        no_sent_in_sect = len(sect_index_list)\n",
    "                        no_sim_sent_not_in_sect = 0\n",
    "                        prediction_index_list=[]\n",
    "                        for index in range(0,len(all_sent_list)):\n",
    "                            cosine_value = all_cosine_list[index]\n",
    "                            # if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_sent_list:\n",
    "                            #     sect_index_list.append(index)\n",
    "                            if fig_caption_index!=-1:\n",
    "                                if cosine_value >= cosine_threshold and abs(index-fig_caption_index)<=no_bounding_sent:\n",
    "                                    prediction_index_list.append(index)\n",
    "                            else:\n",
    "                                if cosine_value >= cosine_threshold:\n",
    "                                    prediction_index_list.append(index)\n",
    "                        no_sim_sent = len(prediction_index_list)\n",
    "                        for index in prediction_index_list:\n",
    "                            if index in sect_index_list:\n",
    "                                no_sim_sent_in_sect = no_sim_sent_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "                            else:\n",
    "                                no_sim_sent_not_in_sect = no_sim_sent_not_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",all_sent_list[index])\n",
    "\n",
    "                        if no_sim_sent != 0:\n",
    "                            false_positive_rate = no_sim_sent_not_in_sect/no_sim_sent\n",
    "                        else:\n",
    "                            false_positive_rate = 0.0\n",
    "                        if no_sent_in_sect != 0:\n",
    "                            false_negative_rate = (no_sent_in_sect - no_sim_sent_in_sect)/no_sent_in_sect\n",
    "                        else:\n",
    "                            false_negative_rate = 0.0\n",
    "\n",
    "                        fpr_fnr_list_threshold.append((false_positive_rate,false_negative_rate))\n",
    "\n",
    "                        # print(\"no_sim_sent = \",no_sim_sent)\n",
    "                        # print(\"no_sent_in_sect = \",no_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_in_sect = \",no_sim_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_not_in_sect = \",no_sim_sent_not_in_sect)\n",
    "                        # print(\"false_positive_rate = \",false_positive_rate)\n",
    "                        # print(\"false_negative_rate = \",false_negative_rate)\n",
    "                        # print(\"cosine_threshold = \",cosine_threshold)\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "                        # for index in range(0,len(sent_list)):\n",
    "                        #     cosine_value = cosine_list[index]\n",
    "                        #     if cosine_value > cosine_threshold:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{sent_list[index]}{bcolors.ENDC}\")\n",
    "                        #     else:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",sent_list[index])\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "                    fpr_fnr_dict_fig[transformer].append(fpr_fnr_list_threshold)\n",
    "\n",
    "                section_text_caption_labels = section_text + \" \" +fig_caption_sent\n",
    "\n",
    "                fig_no = fig_no + 1\n",
    "\n",
    "            marker_dict = {'sentence-transformers/all-mpnet-base-v2':\"s\",'sentence-transformers/multi-qa-MiniLM-L6-cos-v1':\"o\",'sentence-transformers/all-MiniLM-L6-v2':\"P\",'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2':\"v\",'sentence-transformers/bert-base-nli-mean-tokens':\"X\"}\n",
    "\n",
    "            textbook_name = filename.split(\"/\")[4]+\"_\"+(filename.split(\"/\")[-1]).split(\".\")[0]\n",
    "            # class_name = filename.split(\"/\")[3]\n",
    "            class_name = smooth_label(smooth_no)+\"_\"+filename.split(\"/\")[3] # while smooth\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_list_fig = fpr_fnr_dict_fig[transformer]\n",
    "                np_fpr_fnr_list_fig = np.array(fpr_fnr_list_fig)\n",
    "                mean_fpr_fnr_list = (np.mean(np_fpr_fnr_list_fig, axis=0)).tolist()\n",
    "                fpr_fnr_plot_dict[transformer].append(mean_fpr_fnr_list)\n",
    "                plot_fpr_list = []\n",
    "                plot_fnr_list = []\n",
    "\n",
    "                for tup in mean_fpr_fnr_list:\n",
    "                    plot_fpr_list.append(tup[0])\n",
    "                    plot_fnr_list.append(tup[1])\n",
    "\n",
    "        #         plt.scatter(plot_fnr_list, plot_fpr_list, label = transformer.split(\"/\")[1], marker = marker_dict[transformer], c = cosine_threshold_list, cmap = \"jet\", vmin=0, vmax=1)\n",
    "        #         plt.xlabel('False Negative Rate (FNR)')\n",
    "        #         plt.ylabel('False Positve Rate (FPR)')\n",
    "        #         if moving_average_no == 0:\n",
    "        #             plt.title('FNR-FPR plot')\n",
    "        #         else:\n",
    "        #             plt.title(str(moving_average_no)+' smooth FNR-FPR plot')\n",
    "        #         plt.colorbar(label=\"Cosine treshold\", orientation=\"vertical\")\n",
    "        #         plt.grid(True)\n",
    "        #         plt.legend(loc=\"lower left\",fontsize=\"small\")\n",
    "\n",
    "        #         plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_\"+transformer.replace(\"/\",\"_\")+\"_scatter.png\",dpi=1000)\n",
    "        #         plt.show()\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                mean_plot_fpr_list = []\n",
    "                mean_plot_fnr_list = []\n",
    "                for tup in fpr_fnr_plot_dict[transformer][file_no]:\n",
    "                    mean_plot_fpr_list.append(tup[0])\n",
    "                    mean_plot_fnr_list.append(tup[1])\n",
    "                plt.plot(mean_plot_fnr_list,mean_plot_fpr_list, label = transformer.split(\"/\")[1])    \n",
    "\n",
    "            plt.xlabel('False Negative Rate (FNR)')\n",
    "            plt.ylabel('False Positve Rate (FPR)')\n",
    "            if smooth_true == False:\n",
    "                plt.title('H0 FNR-FPR plot of multiple models')\n",
    "            else:\n",
    "                plt.title(smooth_label(smooth_no)+' H0 FNR-FPR plot of multiple models')\n",
    "            plt.grid(True)\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.axis('square')\n",
    "            plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "            # plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_allmodels.png\",dpi=1000)\n",
    "            plt.show()\n",
    "\n",
    "            file_no = file_no + 1\n",
    "            total_fig_no = total_fig_no + fig_no\n",
    "            print(\"++++++++++++++++++++++++++++   END   +++++++++++++++++++++\\n\")\n",
    "\n",
    "        original_stdout = sys.stdout\n",
    "        if smooth_true:\n",
    "            out_file = open(\"plots/h0_\"+smooth_label(smooth_no)+'_'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        else:\n",
    "            out_file = open('plots/h0_'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        sys.stdout = out_file\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            mean_allfiles_fpr_fnr_list = (np.mean(np.array(fpr_fnr_plot_dict[transformer]), axis=0)).tolist()\n",
    "            typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer] = mean_allfiles_fpr_fnr_list \n",
    "            allfile_fpr_plot_list = []\n",
    "            allfile_fnr_plot_list = []\n",
    "            for tup in mean_allfiles_fpr_fnr_list:\n",
    "                allfile_fpr_plot_list.append(tup[0])\n",
    "                allfile_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allfile_fnr_plot_list, allfile_fpr_plot_list, label = transformer.split(\"/\")[1])\n",
    "            min_dist_index = -1\n",
    "            min_dist = 100\n",
    "            for i in range(0,len(cosine_threshold_list)):\n",
    "                dist = math.sqrt(allfile_fnr_plot_list[i]*allfile_fnr_plot_list[i] + allfile_fpr_plot_list[i]*allfile_fpr_plot_list[i])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_dist_index = i\n",
    "            print(\"-------  \", transformer, \"  ------------------\")\n",
    "            print(\" least distance cosine threshold = \", cosine_threshold_list[min_dist_index])\n",
    "            print(\" least distance = \", min_dist)\n",
    "            print(\" least distnant FNR = \", plot_fnr_list[min_dist_index])\n",
    "            print(\" least distnant FPR = \", plot_fpr_list[min_dist_index])\n",
    "            print(\" number of total figures = \", total_fig_no)\n",
    "\n",
    "        sys.stdout = original_stdout\n",
    "        out_file.close()\n",
    "\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        if smooth_true == False:\n",
    "            plt.title('H0 FNR-FPR plot of multiple models')\n",
    "        else:\n",
    "            plt.title(smooth_label(smooth_no)+' H0 FNR-FPR plot of multiple models')\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        if smooth_true == False:\n",
    "            plt.savefig(\"plots/h0_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        else:\n",
    "            plt.savefig(\"plots/h0_\"+smooth_label(smooth_no)+\"_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        plt.show()\n",
    "\n",
    "    for transformer in sentence_transformer_names_list:    \n",
    "        for smooth_no in smooth_no_list:\n",
    "            allsmooth_fpr_plot_list = []\n",
    "            allsmooth_fnr_plot_list = []\n",
    "            for tup in typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer]:\n",
    "                allsmooth_fpr_plot_list.append(tup[0])\n",
    "                allsmooth_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = smooth_label(smooth_no))\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        plt.title('H0 FNR-FPR plot of multiple smoothing of '+transformer.split(\"/\")[1])\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        # if smooth_true:\n",
    "        plt.savefig(\"plots/h0_\"+transformer.split(\"/\")[1]+\" \"+type_html+\"_all_smooth_average_allchapters_allmodels.png\",dpi=1000)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "for type_html in types_html_files_list: \n",
    "    for smooth_no in smooth_no_list:\n",
    "        pickle.dump( typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no], open( \"pickles/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7f750-75f1-400a-b7f6-bacfffe84ff0",
   "metadata": {},
   "source": [
    "# test: all models accuracy test with heuristic 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be2036-bd3f-4f88-92d9-47bb5cf1a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Heuristic 1 -------------------------------\n",
    "# Consider few sentences above and below the fig as boundary.\n",
    "# Next, consider all text between the first similar sentence and last similar sentence with in the boundary as similar sentences.\n",
    "# -------------------------------------------------------------------\n",
    "# filename = \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\"\n",
    "textbooks_html_files = [ \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/3_5239_3/OEBPS/Text/Untitled-1.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/4_5239_4/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/5_5239_5/OEBPS/Text/Untitled-3.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/6_5239_6/OEBPS/Text/Untitled-4.xhtml\"]\n",
    "papers_html_files = ['./data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-004.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-005.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-006.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-007.xhtml']\n",
    "\n",
    "filenames_list_debug = [ './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml']\n",
    "\n",
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# types_html_files_list = [\"papers\"]\n",
    "\n",
    "debug = 0\n",
    "\n",
    "if debug == 1:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_debug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_debug_dict\n",
    "if debug == 0:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_nondebug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_nondebug_dict\n",
    "\n",
    "\n",
    "smooth_true = True\n",
    "\n",
    "# smooth_no_list = [-11,-9,-7,-5,-3,0,3,5,7,9,11,100,101]\n",
    "smooth_no_list = [0,-7]\n",
    "# smooth_no_list = [0,3,9,13]\n",
    "# smooth_no_list = [0,100,101]\n",
    "# smooth_no_list = [-13,0,13,101]\n",
    "\n",
    "def smooth_label(smooth_no):\n",
    "    if smooth_no==100:\n",
    "        return \"9_max_smooth\"\n",
    "    elif smooth_no==101:\n",
    "        return \"13_max_smooth\"\n",
    "    elif smooth_no >0:\n",
    "        return str(smooth_no)+\"_uniform_smooth\"\n",
    "    elif smooth_no < 0:\n",
    "        return str(-1*smooth_no)+\"_gaussian_smooth\"\n",
    "    else:\n",
    "        return \"no_smooth\"\n",
    "\n",
    "typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict = {}\n",
    "for type_html in types_html_files_list:\n",
    "    typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html] = {}\n",
    "    for smooth_no in smooth_no_list:\n",
    "        typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no] = {}\n",
    "\n",
    "for type_html in types_html_files_list:\n",
    "    \n",
    "    for smooth_no in smooth_no_list:\n",
    "        if smooth_no==0:\n",
    "            smooth_true=False\n",
    "        else:\n",
    "            smooth_true=True\n",
    "        fpr_fnr_plot_dict = {}\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            fpr_fnr_plot_dict[transformer] = []\n",
    "\n",
    "        total_fig_no  = 0\n",
    "        file_no = 0\n",
    "        filenames_list = []\n",
    "        if type_html == \"textbooks\":\n",
    "            filenames_list = textbooks_html_files\n",
    "        if type_html == \"papers\":\n",
    "            filenames_list = papers_html_files\n",
    "        if debug ==  1:\n",
    "            filenames_list = filenames_list_debug\n",
    "            \n",
    "        for filename in filenames_list:\n",
    "            try:\n",
    "                fp = open(filename, encoding=\"utf8\")\n",
    "                print(\"++++++++++++++++++++++++++++   START   +++++++++++++++++++++\")\n",
    "                print(filename)\n",
    "            except:\n",
    "                print(\"***************  Error while opening file   ***************\")\n",
    "                print(filename)\n",
    "                print(\"************************************************************\")\n",
    "            extension = filename.split(\".\")[-1]\n",
    "            if extension == \"xhtml\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   XML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml-xml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            elif extension == \"html\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   HTML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "            fp.close()\n",
    "            fig_no = 0\n",
    "            max_caption_words = 60\n",
    "            min_caption_words = 3\n",
    "\n",
    "            full_text = soup.get_text()\n",
    "\n",
    "            # uncomment for debugging \n",
    "            # output_file = open(\"output_full_text.txt\",\"w\")\n",
    "            # output_file.write(remove_non_ascii(full_text))\n",
    "            # output_file.close()\n",
    "            # output_file = open(\"output_full_text_str.txt\",\"w\")\n",
    "            # output_file.write(str(full_text.encode('utf8')))\n",
    "            # output_file.close()\n",
    "\n",
    "            full_text = remove_multi_newlines(full_text)\n",
    "            all_sent_list = split_text_to_sent(full_text)\n",
    "            \n",
    "            all_sentence_embeddings = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                all_sentence_embeddings[transformer] = sentence_transformer_model.encode(all_sent_list)\n",
    "\n",
    "            fpr_fnr_list_fig = []\n",
    "            fpr_fnr_dict_fig = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_dict_fig[transformer] = []\n",
    "            \n",
    "            all_fig = soup.find_all(string=re.compile(\"^Fig\"))\n",
    "\n",
    "            for fig in all_fig:\n",
    "                print(\"------- fig  \"+str(fig_no)+\"   --------\")\n",
    "                \n",
    "                fig_caption = extract_caption(fig, min_caption_words, max_caption_words)\n",
    "                image_path = extract_imagepath(fig, filename, type_html)\n",
    "                \n",
    "                # ocr_full_txt_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "                if type_html == \"textbooks\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if type_html == \"papers\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if debug != 1:\n",
    "                    plt.imshow(mpimg.imread(image_path))\n",
    "                    plt.show()\n",
    "                if os.path.isfile(ocr_json_path):\n",
    "                    print(\"    Json already exist :  \"+ocr_json_path)\n",
    "                    json_file = open(ocr_json_path)\n",
    "                    ocr_dict = json.load(json_file) \n",
    "                    #structure of ocr_dict ={\"block\":[ {\"text\":\"\",\"\",\"paragraph\":[{\"text\":\"\", \"word\":[\"w1\",\"w2\",..], \"bounding_poly\": [[],[],..]}, { .. }, ...],\"bounding_poly\":[[(x1,y1),..(x4,y4)],[]...]}, { ... }, ..],\"bounding_poly\":[[],[]...]}\n",
    "                else:\n",
    "                    print(\"  $$$$  Calling google vision API $$$$ \")\n",
    "                    ocr_dict = detect_document(image_path, type_html)\n",
    "                labels_list = []\n",
    "                print(\"~~~~~~~ Labels  ~~~~~~~~\")\n",
    "                if len(ocr_dict[\"block\"]) > 0:\n",
    "                    for i in range(0,len(ocr_dict[\"block\"])):\n",
    "                        block_dict = ocr_dict[\"block\"][i]\n",
    "                        print(block_dict[\"text\"])\n",
    "                        labels_list.append(block_dict[\"text\"].replace(\"-\",\"\"))\n",
    "                        # print(block_dict[\"text\"], \"  \",ocr_dict[\"bounding_poly\"][i])\n",
    "\n",
    "                        # for i in range(0,len(block_dict[\"paragraph\"])):\n",
    "                        #     para_dict = block_dict[\"paragraph\"][i]\n",
    "                        #     print(\"      -\",para_dict[\"text\"])\n",
    "                        #     print(\"      \",para_dict[\"text\"],\"   \",block_dict[\"bounding_poly\"][i])\n",
    "                else:\n",
    "                    print(\"  NO Labels  \")\n",
    "                print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "                if type_html == \"textbooks\":\n",
    "                    above_header = fig.parent.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if above_header is not None:\n",
    "                        above_header_text = above_header.get_text()\n",
    "                    else:\n",
    "                        above_header_text = \"\"\n",
    "                    while clean_string(above_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(above_header_text).replace(\" \",\"\"))<3:\n",
    "                        if above_header is not None:\n",
    "                            above_header = above_header.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if above_header is not None:\n",
    "                            above_header_text = above_header.get_text()\n",
    "                        else:\n",
    "                            above_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    below_header = fig.parent.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if below_header is not None:\n",
    "                        below_header_text = below_header.get_text()\n",
    "                    else:\n",
    "                        below_header_text = \"\"\n",
    "                    while clean_string(below_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(below_header_text).replace(\" \",\"\"))<3:\n",
    "                        if below_header is not None:\n",
    "                            below_header = below_header.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if below_header is not None:\n",
    "                            below_header_text = below_header.get_text()\n",
    "                        else:\n",
    "                            below_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    print(\"\\nabove header = \",above_header_text)\n",
    "                    print(\"fig caption = \",fig_caption)\n",
    "                    print(\"below header = \",below_header_text)\n",
    "\n",
    "                    section_text = above_header_text\n",
    "                    next_header = above_header.find_next()\n",
    "\n",
    "                    while next_header.get_text() != below_header.get_text():\n",
    "                        # section_text = section_text + next_header.text\n",
    "                        for child in next_header.contents:\n",
    "                            if isinstance(child, str):\n",
    "                                # if len(clean_string(child).replace(\" \",\"\")) > 1:\n",
    "                                section_text = section_text + child\n",
    "                                # print(\"---------------\\n\",clean_string(child),\"\\n\")\n",
    "                        next_header = next_header.find_next()\n",
    "                        section_text = section_text + \"\\n\"\n",
    "\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    # print(\"---------\\n\",section_text,\"\\n--------------\")\n",
    "                    # print(\"++++++\\n\",full_text.encode(\"utf-8\"),\"\\n++++++++\")\n",
    "                if type_html == \"papers\":\n",
    "                    print(\"Figure caption = \",fig_caption)\n",
    "                    relevant_txt_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\"_relevant.txt\"\n",
    "                    relevant_file = open(relevant_txt_path,\"r\", encoding='utf-8')\n",
    "                    print(relevant_txt_path + \"  read!\")\n",
    "                    section_text = relevant_file.read()\n",
    "                    # print(\"\\n!!! section text !!! \\n\", section_text, \"\\n!!!!!!!!\")\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    print (\"Length of sent list = \", len(sent_list))\n",
    "                    # for inx in range(0,len(sent_list)):\n",
    "                    #     print( str(inx)+\": \",f\"{bcolors.OKBLUE}{sent_list[inx]}{bcolors.ENDC}\")\n",
    "                    relevant_file.close()\n",
    "                                        \n",
    "                str_labels_list = str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "\n",
    "                if len(labels_list) > 0:\n",
    "                    fig_caption_sent = fig_caption +\". This figure contains \"+ str_labels_list +\".\"\n",
    "                    # fig_caption_sent = fig_caption + \" and this figure contains \"+ str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")+ \" as labels.\"\n",
    "                else:\n",
    "                    fig_caption_sent = fig_caption\n",
    "\n",
    "\n",
    "                fig_caption_sent = clean_string(fig_caption_sent)\n",
    "\n",
    "                print(\"Fig caption sentence = \",fig_caption_sent)\n",
    "\n",
    "\n",
    "                moving_average_no = smooth_no                 #takes only odd numbers\n",
    "\n",
    "                no_space_sent_list = []\n",
    "                for sent in sent_list:\n",
    "                    no_space_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                no_space_all_sent_list = []\n",
    "                for sent in all_sent_list:\n",
    "                    no_space_all_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                fig_caption_clean = remove_multi_newlines(fig_caption)\n",
    "                fig_caption_start = split_text_to_sent(fig_caption_clean)[0]\n",
    "                fig_caption_index = -1\n",
    "                if fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_all_sent_list:\n",
    "                    fig_caption_index = no_space_all_sent_list.index(fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "                    \n",
    "                for transformer in sentence_transformer_names_list:\n",
    "                    sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                    fig_caption_embedding = sentence_transformer_model.encode(fig_caption_sent)\n",
    "                    sentence_embeddings = sentence_transformer_model.encode(sent_list)\n",
    "\n",
    "\n",
    "                    cosine_list = cosine_similarity([fig_caption_embedding],sentence_embeddings)[0]\n",
    "                    all_cosine_list = cosine_similarity([fig_caption_embedding],all_sentence_embeddings[transformer])[0]\n",
    "\n",
    "                    sect_index_list = []\n",
    "                    # sect_cosine_list = []\n",
    "                    check_no_space_sent_list = no_space_sent_list.copy()\n",
    "                    for index in range(0,len(all_sent_list)):\n",
    "                        if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "                            sect_index_list.append(index)\n",
    "                            # sect_cosine_list.append(all_cosine_list[index])\n",
    "                            check_no_space_sent_list.remove(all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        print(\"index of caption in all_sent_list = \",fig_caption_index)\n",
    "                        print(\"width of boundary = \",no_bounding_sent)\n",
    "                        print(\"indices of section = \",sect_index_list)\n",
    "                        print(\"len of matched sent = \",len(sect_index_list))\n",
    "                        print(\"len of section sent = \",len(sent_list))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = \"non_smooth\")\n",
    "                        plt.vlines(x = sect_index_list[0], ymin = 0, ymax = 1, colors = 'green', label = 'section_start')\n",
    "                        plt.vlines(x = sect_index_list[-1], ymin = 0, ymax = 1, colors = 'red', label = 'section_end')\n",
    "\n",
    "                    if smooth_true:\n",
    "                        all_cosine_list = moving_average(np.array(all_cosine_list),moving_average_no).tolist()\n",
    "                        if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                            plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = smooth_label(smooth_no))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.title(\"cosine simlairity scores\")\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "\n",
    "                    cosine_threshold_list = np.arange(0.0, 1.0001 ,(1/no_points_fpr_fnr_plot))\n",
    "                    cosine_threshold_list = np.around(cosine_threshold_list, 4)\n",
    "\n",
    "                    fpr_fnr_list_threshold = []\n",
    "                        \n",
    "                    for cosine_threshold in cosine_threshold_list:\n",
    "                        no_sim_sent = 0\n",
    "                        no_sim_sent_in_sect = 0\n",
    "                        no_sent_in_sect = len(sect_index_list)\n",
    "                        no_sim_sent_not_in_sect = 0\n",
    "                        prediction_index_list=[]\n",
    "                        for index in range(0,len(all_sent_list)):\n",
    "                            cosine_value = all_cosine_list[index]\n",
    "                            # if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_sent_list:\n",
    "                            #     sect_index_list.append(index)\n",
    "                            if fig_caption_index!=-1:\n",
    "                                if cosine_value >= cosine_threshold and abs(index-fig_caption_index)<=no_bounding_sent:\n",
    "                                    prediction_index_list.append(index)\n",
    "                            else:\n",
    "                                if cosine_value >= cosine_threshold:\n",
    "                                    prediction_index_list.append(index)\n",
    "                        \n",
    "                        if fig_caption_index!=-1:\n",
    "                            if len(prediction_index_list)==0:\n",
    "                                new_prediction_index_list=[]\n",
    "                            else:\n",
    "                                new_prediction_index_list = list(range(prediction_index_list[0],prediction_index_list[-1]+1))\n",
    "                                no_sim_sent=len(new_prediction_index_list)\n",
    "                        else:\n",
    "                            new_prediction_index_list=prediction_index_list\n",
    "                            no_sim_sent=len(new_prediction_index_list)\n",
    "                            \n",
    "                        for index in new_prediction_index_list:\n",
    "                            if index in sect_index_list:\n",
    "                                no_sim_sent_in_sect = no_sim_sent_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "                            else:\n",
    "                                no_sim_sent_not_in_sect = no_sim_sent_not_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",all_sent_list[index])\n",
    "\n",
    "                        if no_sim_sent != 0:\n",
    "                            false_positive_rate = no_sim_sent_not_in_sect/no_sim_sent\n",
    "                        else:\n",
    "                            false_positive_rate = 0.0\n",
    "                        if no_sent_in_sect != 0:\n",
    "                            false_negative_rate = (no_sent_in_sect - no_sim_sent_in_sect)/no_sent_in_sect\n",
    "                        else:\n",
    "                            false_negative_rate = 0.0\n",
    "\n",
    "                        fpr_fnr_list_threshold.append((false_positive_rate,false_negative_rate))\n",
    "\n",
    "                        # print(\"no_sim_sent = \",no_sim_sent)\n",
    "                        # print(\"no_sent_in_sect = \",no_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_in_sect = \",no_sim_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_not_in_sect = \",no_sim_sent_not_in_sect)\n",
    "                        # print(\"false_positive_rate = \",false_positive_rate)\n",
    "                        # print(\"false_negative_rate = \",false_negative_rate)\n",
    "                        # print(\"cosine_threshold = \",cosine_threshold)\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "                        # for index in range(0,len(sent_list)):\n",
    "                        #     cosine_value = cosine_list[index]\n",
    "                        #     if cosine_value > cosine_threshold:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{sent_list[index]}{bcolors.ENDC}\")\n",
    "                        #     else:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",sent_list[index])\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "                    fpr_fnr_dict_fig[transformer].append(fpr_fnr_list_threshold)\n",
    "\n",
    "                section_text_caption_labels = section_text + \" \" +fig_caption_sent\n",
    "\n",
    "                fig_no = fig_no + 1\n",
    "\n",
    "            marker_dict = {'sentence-transformers/all-mpnet-base-v2':\"s\",'sentence-transformers/multi-qa-MiniLM-L6-cos-v1':\"o\",'sentence-transformers/all-MiniLM-L6-v2':\"P\",'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2':\"v\",'sentence-transformers/bert-base-nli-mean-tokens':\"X\"}\n",
    "\n",
    "            textbook_name = filename.split(\"/\")[4]+\"_\"+(filename.split(\"/\")[-1]).split(\".\")[0]\n",
    "            # class_name = filename.split(\"/\")[3]\n",
    "            class_name = smooth_label(smooth_no)+\"_\"+filename.split(\"/\")[3] # while smooth\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_list_fig = fpr_fnr_dict_fig[transformer]\n",
    "                np_fpr_fnr_list_fig = np.array(fpr_fnr_list_fig)\n",
    "                mean_fpr_fnr_list = (np.mean(np_fpr_fnr_list_fig, axis=0)).tolist()\n",
    "                fpr_fnr_plot_dict[transformer].append(mean_fpr_fnr_list)\n",
    "                plot_fpr_list = []\n",
    "                plot_fnr_list = []\n",
    "\n",
    "                for tup in mean_fpr_fnr_list:\n",
    "                    plot_fpr_list.append(tup[0])\n",
    "                    plot_fnr_list.append(tup[1])\n",
    "\n",
    "        #         plt.scatter(plot_fnr_list, plot_fpr_list, label = transformer.split(\"/\")[1], marker = marker_dict[transformer], c = cosine_threshold_list, cmap = \"jet\", vmin=0, vmax=1)\n",
    "        #         plt.xlabel('False Negative Rate (FNR)')\n",
    "        #         plt.ylabel('False Positve Rate (FPR)')\n",
    "        #         if moving_average_no == 0:\n",
    "        #             plt.title('FNR-FPR plot')\n",
    "        #         else:\n",
    "        #             plt.title(str(moving_average_no)+' smooth FNR-FPR plot')\n",
    "        #         plt.colorbar(label=\"Cosine treshold\", orientation=\"vertical\")\n",
    "        #         plt.grid(True)\n",
    "        #         plt.legend(loc=\"lower left\",fontsize=\"small\")\n",
    "\n",
    "        #         plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_\"+transformer.replace(\"/\",\"_\")+\"_scatter.png\",dpi=1000)\n",
    "        #         plt.show()\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                mean_plot_fpr_list = []\n",
    "                mean_plot_fnr_list = []\n",
    "                for tup in fpr_fnr_plot_dict[transformer][file_no]:\n",
    "                    mean_plot_fpr_list.append(tup[0])\n",
    "                    mean_plot_fnr_list.append(tup[1])\n",
    "                plt.plot(mean_plot_fnr_list,mean_plot_fpr_list, label = transformer.split(\"/\")[1])    \n",
    "\n",
    "            plt.xlabel('False Negative Rate (FNR)')\n",
    "            plt.ylabel('False Positve Rate (FPR)')\n",
    "            if smooth_true == False:\n",
    "                plt.title('H1 FNR-FPR plot of multiple models')\n",
    "            else:\n",
    "                plt.title(smooth_label(smooth_no)+' H1 FNR-FPR plot of multiple models')\n",
    "            plt.grid(True)\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.axis('square')\n",
    "            plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "            # plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_allmodels.png\",dpi=1000)\n",
    "            plt.show()\n",
    "\n",
    "            file_no = file_no + 1\n",
    "            total_fig_no = total_fig_no + fig_no\n",
    "            print(\"++++++++++++++++++++++++++++   END   +++++++++++++++++++++\\n\")\n",
    "\n",
    "        original_stdout = sys.stdout\n",
    "        if smooth_true:\n",
    "            out_file = open(\"plots/h1_\"+smooth_label(smooth_no)+'_'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        else:\n",
    "            out_file = open('plots/h1_'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        sys.stdout = out_file\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            mean_allfiles_fpr_fnr_list = (np.mean(np.array(fpr_fnr_plot_dict[transformer]), axis=0)).tolist()\n",
    "            typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer] = mean_allfiles_fpr_fnr_list \n",
    "            allfile_fpr_plot_list = []\n",
    "            allfile_fnr_plot_list = []\n",
    "            for tup in mean_allfiles_fpr_fnr_list:\n",
    "                allfile_fpr_plot_list.append(tup[0])\n",
    "                allfile_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allfile_fnr_plot_list, allfile_fpr_plot_list, label = transformer.split(\"/\")[1])\n",
    "            min_dist_index = -1\n",
    "            min_dist = 100\n",
    "            for i in range(0,len(cosine_threshold_list)):\n",
    "                dist = math.sqrt(allfile_fnr_plot_list[i]*allfile_fnr_plot_list[i] + allfile_fpr_plot_list[i]*allfile_fpr_plot_list[i])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_dist_index = i\n",
    "            print(\"-------  \", transformer, \"  ------------------\")\n",
    "            print(\" least distance cosine threshold = \", cosine_threshold_list[min_dist_index])\n",
    "            print(\" least distance = \", min_dist)\n",
    "            print(\" least distnant FNR = \", plot_fnr_list[min_dist_index])\n",
    "            print(\" least distnant FPR = \", plot_fpr_list[min_dist_index])\n",
    "            print(\" number of total figures = \", total_fig_no)\n",
    "\n",
    "        sys.stdout = original_stdout\n",
    "        out_file.close()\n",
    "\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        if smooth_true == False:\n",
    "            plt.title('H1 FNR-FPR plot of multiple models')\n",
    "        else:\n",
    "            plt.title(smooth_label(smooth_no)+' H1 FNR-FPR plot of multiple models')\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        if smooth_true == False:\n",
    "            plt.savefig(\"plots/h1_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        else:\n",
    "            plt.savefig(\"plots/h1_\"+smooth_label(smooth_no)+\"_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        plt.show()\n",
    "\n",
    "    for transformer in sentence_transformer_names_list:    \n",
    "        for smooth_no in smooth_no_list:\n",
    "            allsmooth_fpr_plot_list = []\n",
    "            allsmooth_fnr_plot_list = []\n",
    "            for tup in typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer]:\n",
    "                allsmooth_fpr_plot_list.append(tup[0])\n",
    "                allsmooth_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = smooth_label(smooth_no))\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        plt.title('H1 FNR-FPR plot of multiple smoothing of '+transformer.split(\"/\")[1])\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        # if smooth_true:\n",
    "        plt.savefig(\"plots/h1_\"+transformer.split(\"/\")[1]+\" \"+type_html+\"_all_smooth_average_allchapters_allmodels.png\",dpi=1000)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "for type_html in types_html_files_list: \n",
    "    for smooth_no in smooth_no_list:\n",
    "        pickle.dump( typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no], open( \"pickles/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32bc999-a26b-4021-9aff-b3007e7ed4a7",
   "metadata": {},
   "source": [
    "# test: all models accuracy test with heuristic 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba884e-55d9-4429-9d0c-3a25b389fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Heuristic 2 -------------------------------\n",
    "# Consider few sentences above and below the fig as boundary.\n",
    "# Subtract mean cosive value first from cosine value then find similarity \n",
    "# Next, consider similar sentence with in the boundary as similar sentences.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# filename = \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\"\n",
    "textbooks_html_files = [ \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/3_5239_3/OEBPS/Text/Untitled-1.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/4_5239_4/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/5_5239_5/OEBPS/Text/Untitled-3.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/6_5239_6/OEBPS/Text/Untitled-4.xhtml\"]\n",
    "papers_html_files = ['./data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-004.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-005.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-006.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-007.xhtml']\n",
    "\n",
    "filenames_list_debug = [ './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml']\n",
    "\n",
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# types_html_files_list = [\"papers\"]\n",
    "\n",
    "debug = 0\n",
    "\n",
    "if debug == 1:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_debug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_debug_dict\n",
    "if debug == 0:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_nondebug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_nondebug_dict\n",
    "\n",
    "\n",
    "smooth_true = True\n",
    "# if smooth_true:\n",
    "# smooth_no_list = [-11,-9,-7,-5,-3,0,3,5,7,9,11,100,101]\n",
    "smooth_no_list = [0,-7]\n",
    "# smooth_no_list = [0,3,9,13]\n",
    "# smooth_no_list = [0,100,101]\n",
    "# smooth_no_list = [-13,0,13,101]\n",
    "# else:\n",
    "#     smooth_no_list = [0]\n",
    "def smooth_label(smooth_no):\n",
    "    if smooth_no==100:\n",
    "        return \"9_max_smooth\"\n",
    "    elif smooth_no==101:\n",
    "        return \"13_max_smooth\"\n",
    "    elif smooth_no >0:\n",
    "        return str(smooth_no)+\"_uniform_smooth\"\n",
    "    elif smooth_no < 0:\n",
    "        return str(-1*smooth_no)+\"_gaussian_smooth\"\n",
    "    else:\n",
    "        return \"no_smooth\"\n",
    "\n",
    "typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict = {}\n",
    "for type_html in types_html_files_list:\n",
    "    typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html] = {}\n",
    "    for smooth_no in smooth_no_list:\n",
    "        typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no] = {}\n",
    "\n",
    "for type_html in types_html_files_list:\n",
    "    \n",
    "    for smooth_no in smooth_no_list:\n",
    "        if smooth_no==0:\n",
    "            smooth_true=False\n",
    "        else:\n",
    "            smooth_true=True\n",
    "        fpr_fnr_plot_dict = {}\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            fpr_fnr_plot_dict[transformer] = []\n",
    "\n",
    "        total_fig_no  = 0\n",
    "        file_no = 0\n",
    "        filenames_list = []\n",
    "        if type_html == \"textbooks\":\n",
    "            filenames_list = textbooks_html_files\n",
    "        if type_html == \"papers\":\n",
    "            filenames_list = papers_html_files\n",
    "        if debug ==  1:\n",
    "            filenames_list = filenames_list_debug\n",
    "            \n",
    "        for filename in filenames_list:\n",
    "            try:\n",
    "                fp = open(filename, encoding=\"utf8\")\n",
    "                print(\"++++++++++++++++++++++++++++   START   +++++++++++++++++++++\")\n",
    "                print(filename)\n",
    "            except:\n",
    "                print(\"***************  Error while opening file   ***************\")\n",
    "                print(filename)\n",
    "                print(\"************************************************************\")\n",
    "            extension = filename.split(\".\")[-1]\n",
    "            if extension == \"xhtml\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   XML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml-xml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            elif extension == \"html\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   HTML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "            fp.close()\n",
    "            fig_no = 0\n",
    "            max_caption_words = 60\n",
    "            min_caption_words = 3\n",
    "\n",
    "            full_text = soup.get_text()\n",
    "\n",
    "            # uncomment for debugging \n",
    "            # output_file = open(\"output_full_text.txt\",\"w\")\n",
    "            # output_file.write(remove_non_ascii(full_text))\n",
    "            # output_file.close()\n",
    "            # output_file = open(\"output_full_text_str.txt\",\"w\")\n",
    "            # output_file.write(str(full_text.encode('utf8')))\n",
    "            # output_file.close()\n",
    "\n",
    "            full_text = remove_multi_newlines(full_text)\n",
    "            all_sent_list = split_text_to_sent(full_text)\n",
    "\n",
    "            all_sentence_embeddings = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                all_sentence_embeddings[transformer] = sentence_transformer_model.encode(all_sent_list)\n",
    "\n",
    "            fpr_fnr_list_fig = []\n",
    "            fpr_fnr_dict_fig = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_dict_fig[transformer] = []\n",
    "            \n",
    "            all_fig = soup.find_all(string=re.compile(\"^Fig\"))\n",
    "\n",
    "            for fig in all_fig:\n",
    "                print(\"------- fig  \"+str(fig_no)+\"   --------\")\n",
    "                \n",
    "                fig_caption = extract_caption(fig, min_caption_words, max_caption_words)\n",
    "                image_path = extract_imagepath(fig, filename, type_html)\n",
    "\n",
    "                # ocr_full_txt_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "                if type_html == \"textbooks\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if type_html == \"papers\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if debug != 1:\n",
    "                    plt.imshow(mpimg.imread(image_path))\n",
    "                    plt.show()\n",
    "                if os.path.isfile(ocr_json_path):\n",
    "                    print(\"    Json already exist :  \"+ocr_json_path)\n",
    "                    json_file = open(ocr_json_path)\n",
    "                    ocr_dict = json.load(json_file) \n",
    "                    #structure of ocr_dict ={\"block\":[ {\"text\":\"\",\"\",\"paragraph\":[{\"text\":\"\", \"word\":[\"w1\",\"w2\",..], \"bounding_poly\": [[],[],..]}, { .. }, ...],\"bounding_poly\":[[(x1,y1),..(x4,y4)],[]...]}, { ... }, ..],\"bounding_poly\":[[],[]...]}\n",
    "                else:\n",
    "                    print(\"  $$$$  Calling google vision API $$$$ \")\n",
    "                    ocr_dict = detect_document(image_path, type_html)\n",
    "                labels_list = []\n",
    "                print(\"~~~~~~~ Labels  ~~~~~~~~\")\n",
    "                if len(ocr_dict[\"block\"]) > 0:\n",
    "                    for i in range(0,len(ocr_dict[\"block\"])):\n",
    "                        block_dict = ocr_dict[\"block\"][i]\n",
    "                        print(block_dict[\"text\"])\n",
    "                        labels_list.append(block_dict[\"text\"].replace(\"-\",\"\"))\n",
    "                        # print(block_dict[\"text\"], \"  \",ocr_dict[\"bounding_poly\"][i])\n",
    "\n",
    "                        # for i in range(0,len(block_dict[\"paragraph\"])):\n",
    "                        #     para_dict = block_dict[\"paragraph\"][i]\n",
    "                        #     print(\"      -\",para_dict[\"text\"])\n",
    "                        #     print(\"      \",para_dict[\"text\"],\"   \",block_dict[\"bounding_poly\"][i])\n",
    "                else:\n",
    "                    print(\"  NO Labels  \")\n",
    "                print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "                if type_html == \"textbooks\":\n",
    "                    above_header = fig.parent.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if above_header is not None:\n",
    "                        above_header_text = above_header.get_text()\n",
    "                    else:\n",
    "                        above_header_text = \"\"\n",
    "                    while clean_string(above_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(above_header_text).replace(\" \",\"\"))<3:\n",
    "                        if above_header is not None:\n",
    "                            above_header = above_header.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if above_header is not None:\n",
    "                            above_header_text = above_header.get_text()\n",
    "                        else:\n",
    "                            above_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    below_header = fig.parent.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if below_header is not None:\n",
    "                        below_header_text = below_header.get_text()\n",
    "                    else:\n",
    "                        below_header_text = \"\"\n",
    "                    while clean_string(below_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(below_header_text).replace(\" \",\"\"))<3:\n",
    "                        if below_header is not None:\n",
    "                            below_header = below_header.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if below_header is not None:\n",
    "                            below_header_text = below_header.get_text()\n",
    "                        else:\n",
    "                            below_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    print(\"\\nabove header = \",above_header_text)\n",
    "                    print(\"fig caption = \",fig_caption)\n",
    "                    print(\"below header = \",below_header_text)\n",
    "\n",
    "                    section_text = above_header_text\n",
    "                    next_header = above_header.find_next()\n",
    "\n",
    "                    while next_header.get_text() != below_header.get_text():\n",
    "                        # section_text = section_text + next_header.text\n",
    "                        for child in next_header.contents:\n",
    "                            if isinstance(child, str):\n",
    "                                # if len(clean_string(child).replace(\" \",\"\")) > 1:\n",
    "                                section_text = section_text + child\n",
    "                                # print(\"---------------\\n\",clean_string(child),\"\\n\")\n",
    "                        next_header = next_header.find_next()\n",
    "                        section_text = section_text + \"\\n\"\n",
    "\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    # print(\"---------\\n\",section_text,\"\\n--------------\")\n",
    "                    # print(\"++++++\\n\",full_text.encode(\"utf-8\"),\"\\n++++++++\")\n",
    "                if type_html == \"papers\":\n",
    "                    print(\"Figure caption = \",fig_caption)\n",
    "                    relevant_txt_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\"_relevant.txt\"\n",
    "                    relevant_file = open(relevant_txt_path,\"r\", encoding='utf-8')\n",
    "                    print(relevant_txt_path + \"  read!\")\n",
    "                    section_text = relevant_file.read()\n",
    "                    # print(\"\\n!!! section text !!! \\n\", section_text, \"\\n!!!!!!!!\")\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    print (\"Length of sent list = \", len(sent_list))\n",
    "                    # for inx in range(0,len(sent_list)):\n",
    "                    #     print( str(inx)+\": \",f\"{bcolors.OKBLUE}{sent_list[inx]}{bcolors.ENDC}\")\n",
    "                    relevant_file.close()\n",
    "                                        \n",
    "                str_labels_list = str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "\n",
    "                if len(labels_list) > 0:\n",
    "                    fig_caption_sent = fig_caption +\". This figure contains \"+ str_labels_list +\".\"\n",
    "                    # fig_caption_sent = fig_caption + \" and this figure contains \"+ str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")+ \" as labels.\"\n",
    "                else:\n",
    "                    fig_caption_sent = fig_caption\n",
    "\n",
    "\n",
    "                fig_caption_sent = clean_string(fig_caption_sent)\n",
    "\n",
    "                print(\"Fig caption sentence = \",fig_caption_sent)\n",
    "\n",
    "\n",
    "                moving_average_no = smooth_no                 #takes only odd numbers\n",
    "\n",
    "                no_space_sent_list = []\n",
    "                for sent in sent_list:\n",
    "                    no_space_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                no_space_all_sent_list = []\n",
    "                for sent in all_sent_list:\n",
    "                    no_space_all_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                fig_caption_clean = remove_multi_newlines(fig_caption)\n",
    "                fig_caption_start = split_text_to_sent(fig_caption_clean)[0]\n",
    "                fig_caption_index = -1\n",
    "                if fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_all_sent_list:\n",
    "                    fig_caption_index = no_space_all_sent_list.index(fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                for transformer in sentence_transformer_names_list:\n",
    "                    sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                    fig_caption_embedding = sentence_transformer_model.encode(fig_caption_sent)\n",
    "                    sentence_embeddings = sentence_transformer_model.encode(sent_list)\n",
    "\n",
    "\n",
    "                    cosine_list = cosine_similarity([fig_caption_embedding],sentence_embeddings)[0]\n",
    "                    all_cosine_list = cosine_similarity([fig_caption_embedding],all_sentence_embeddings[transformer])[0]\n",
    "\n",
    "                    sect_index_list = []\n",
    "                    sect_cosine_list = []\n",
    "                    check_no_space_sent_list = no_space_sent_list.copy()\n",
    "                    for index in range(0,len(all_sent_list)):\n",
    "                        if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "                            sect_index_list.append(index)\n",
    "                            sect_cosine_list.append(all_cosine_list[index])\n",
    "                            check_no_space_sent_list.remove(all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                    if len(sect_index_list) != len(sent_list):\n",
    "                        print(\"!!!!! ERROR !!!!\")\n",
    "                        print(\"start index of section = \",sect_index_list[0])\n",
    "                        print(\"end index of section = \",sect_index_list[-1])\n",
    "                        print(\"indices of section = \",sect_index_list)\n",
    "                        print(\"len of matched sent = \",len(sect_index_list))\n",
    "                        print(\"len of section sent = \",len(sent_list))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = \"non_smooth\")\n",
    "                        plt.vlines(x = sect_index_list[0], ymin = 0, ymax = 1, colors = 'green', label = 'section_start')\n",
    "                        plt.vlines(x = sect_index_list[-1], ymin = 0, ymax = 1, colors = 'red', label = 'section_end')\n",
    "\n",
    "                    if smooth_true:\n",
    "                        all_cosine_list = moving_average(np.array(all_cosine_list),moving_average_no).tolist()\n",
    "                        if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                            plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = smooth_label(smooth_no))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.title(\"cosine simlairity scores\")\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "\n",
    "                    cosine_threshold_list = np.arange(0.0, 1.0001 ,(1/no_points_fpr_fnr_plot))\n",
    "                    cosine_threshold_list = np.around(cosine_threshold_list, 4)\n",
    "                    average_cosine_value = np.average(all_cosine_list)\n",
    "                    fpr_fnr_list_threshold = []\n",
    "                    for cosine_threshold in cosine_threshold_list:\n",
    "                        no_sim_sent = 0\n",
    "                        no_sim_sent_in_sect = 0\n",
    "                        no_sent_in_sect = len(sect_index_list)\n",
    "                        no_sim_sent_not_in_sect = 0\n",
    "                        prediction_index_list=[]\n",
    "                        for index in range(0,len(all_sent_list)):\n",
    "                            cosine_value = all_cosine_list[index]\n",
    "                            # if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_sent_list:\n",
    "                            #     sect_index_list.append(index)\n",
    "                            if fig_caption_index!=-1:\n",
    "                                if cosine_value - average_cosine_value >= cosine_threshold and abs(index-fig_caption_index)<=no_bounding_sent:\n",
    "                                    prediction_index_list.append(index)\n",
    "                            else:\n",
    "                                if cosine_value - average_cosine_value >= cosine_threshold:\n",
    "                                    prediction_index_list.append(index)\n",
    "                        no_sim_sent = len(prediction_index_list)\n",
    "                        for index in prediction_index_list:\n",
    "                            if index in sect_index_list:\n",
    "                                no_sim_sent_in_sect = no_sim_sent_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "                            else:\n",
    "                                no_sim_sent_not_in_sect = no_sim_sent_not_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",all_sent_list[index])\n",
    "\n",
    "                        if no_sim_sent != 0:\n",
    "                            false_positive_rate = no_sim_sent_not_in_sect/no_sim_sent\n",
    "                        else:\n",
    "                            false_positive_rate = 0.0\n",
    "                        if no_sent_in_sect != 0:\n",
    "                            false_negative_rate = (no_sent_in_sect - no_sim_sent_in_sect)/no_sent_in_sect\n",
    "                        else:\n",
    "                            false_negative_rate = 0.0\n",
    "\n",
    "                        fpr_fnr_list_threshold.append((false_positive_rate,false_negative_rate))\n",
    "\n",
    "                        # print(\"no_sim_sent = \",no_sim_sent)\n",
    "                        # print(\"no_sent_in_sect = \",no_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_in_sect = \",no_sim_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_not_in_sect = \",no_sim_sent_not_in_sect)\n",
    "                        # print(\"false_positive_rate = \",false_positive_rate)\n",
    "                        # print(\"false_negative_rate = \",false_negative_rate)\n",
    "                        # print(\"cosine_threshold = \",cosine_threshold)\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "                        # for index in range(0,len(sent_list)):\n",
    "                        #     cosine_value = cosine_list[index]\n",
    "                        #     if cosine_value > cosine_threshold:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{sent_list[index]}{bcolors.ENDC}\")\n",
    "                        #     else:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",sent_list[index])\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "                    fpr_fnr_dict_fig[transformer].append(fpr_fnr_list_threshold)\n",
    "\n",
    "                section_text_caption_labels = section_text + \" \" +fig_caption_sent\n",
    "\n",
    "                fig_no = fig_no + 1\n",
    "\n",
    "            marker_dict = {'sentence-transformers/all-mpnet-base-v2':\"s\",'sentence-transformers/multi-qa-MiniLM-L6-cos-v1':\"o\",'sentence-transformers/all-MiniLM-L6-v2':\"P\",'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2':\"v\",'sentence-transformers/bert-base-nli-mean-tokens':\"X\"}\n",
    "\n",
    "            textbook_name = filename.split(\"/\")[4]+\"_\"+(filename.split(\"/\")[-1]).split(\".\")[0]\n",
    "            # class_name = filename.split(\"/\")[3]\n",
    "            class_name = smooth_label(smooth_no)+\"_\"+filename.split(\"/\")[3] # while smooth\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_list_fig = fpr_fnr_dict_fig[transformer]\n",
    "                np_fpr_fnr_list_fig = np.array(fpr_fnr_list_fig)\n",
    "                mean_fpr_fnr_list = (np.mean(np_fpr_fnr_list_fig, axis=0)).tolist()\n",
    "                fpr_fnr_plot_dict[transformer].append(mean_fpr_fnr_list)\n",
    "                plot_fpr_list = []\n",
    "                plot_fnr_list = []\n",
    "\n",
    "                for tup in mean_fpr_fnr_list:\n",
    "                    plot_fpr_list.append(tup[0])\n",
    "                    plot_fnr_list.append(tup[1])\n",
    "\n",
    "        #         plt.scatter(plot_fnr_list, plot_fpr_list, label = transformer.split(\"/\")[1], marker = marker_dict[transformer], c = cosine_threshold_list, cmap = \"jet\", vmin=0, vmax=1)\n",
    "        #         plt.xlabel('False Negative Rate (FNR)')\n",
    "        #         plt.ylabel('False Positve Rate (FPR)')\n",
    "        #         if moving_average_no == 0:\n",
    "        #             plt.title('FNR-FPR plot')\n",
    "        #         else:\n",
    "        #             plt.title(str(moving_average_no)+' smooth FNR-FPR plot')\n",
    "        #         plt.colorbar(label=\"Cosine treshold\", orientation=\"vertical\")\n",
    "        #         plt.grid(True)\n",
    "        #         plt.legend(loc=\"lower left\",fontsize=\"small\")\n",
    "\n",
    "        #         plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_\"+transformer.replace(\"/\",\"_\")+\"_scatter.png\",dpi=1000)\n",
    "        #         plt.show()\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                mean_plot_fpr_list = []\n",
    "                mean_plot_fnr_list = []\n",
    "                for tup in fpr_fnr_plot_dict[transformer][file_no]:\n",
    "                    mean_plot_fpr_list.append(tup[0])\n",
    "                    mean_plot_fnr_list.append(tup[1])\n",
    "                plt.plot(mean_plot_fnr_list,mean_plot_fpr_list, label = transformer.split(\"/\")[1])    \n",
    "\n",
    "            plt.xlabel('False Negative Rate (FNR)')\n",
    "            plt.ylabel('False Positve Rate (FPR)')\n",
    "            if smooth_true == False:\n",
    "                plt.title('H2 FNR-FPR plot of multiple models')\n",
    "            else:\n",
    "                plt.title(smooth_label(smooth_no)+' H2 FNR-FPR plot of multiple models')\n",
    "            plt.grid(True)\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.axis('square')\n",
    "            plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "            # plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_allmodels.png\",dpi=1000)\n",
    "            plt.show()\n",
    "\n",
    "            file_no = file_no + 1\n",
    "            total_fig_no = total_fig_no + fig_no\n",
    "            print(\"++++++++++++++++++++++++++++   END   +++++++++++++++++++++\\n\")\n",
    "\n",
    "        original_stdout = sys.stdout\n",
    "        if smooth_true:\n",
    "            out_file = open(\"plots/\"+smooth_label(smooth_no)+'_'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        else:\n",
    "            out_file = open('plots/'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        sys.stdout = out_file\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            mean_allfiles_fpr_fnr_list = (np.mean(np.array(fpr_fnr_plot_dict[transformer]), axis=0)).tolist()\n",
    "            typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer] = mean_allfiles_fpr_fnr_list \n",
    "            allfile_fpr_plot_list = []\n",
    "            allfile_fnr_plot_list = []\n",
    "            for tup in mean_allfiles_fpr_fnr_list:\n",
    "                allfile_fpr_plot_list.append(tup[0])\n",
    "                allfile_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allfile_fnr_plot_list, allfile_fpr_plot_list, label = transformer.split(\"/\")[1])\n",
    "            min_dist_index = -1\n",
    "            min_dist = 100\n",
    "            for i in range(0,len(cosine_threshold_list)):\n",
    "                dist = math.sqrt(allfile_fnr_plot_list[i]*allfile_fnr_plot_list[i] + allfile_fpr_plot_list[i]*allfile_fpr_plot_list[i])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_dist_index = i\n",
    "            print(\"-------  \", transformer, \"  ------------------\")\n",
    "            print(\" least distance cosine threshold = \", cosine_threshold_list[min_dist_index])\n",
    "            print(\" least distance = \", min_dist)\n",
    "            print(\" least distnant FNR = \", plot_fnr_list[min_dist_index])\n",
    "            print(\" least distnant FPR = \", plot_fpr_list[min_dist_index])\n",
    "            print(\" number of total figures = \", total_fig_no)\n",
    "\n",
    "        sys.stdout = original_stdout\n",
    "        out_file.close()\n",
    "\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        if smooth_true == False:\n",
    "            plt.title('H2 FNR-FPR plot of multiple models')\n",
    "        else:\n",
    "            plt.title(smooth_label(smooth_no)+' H2 FNR-FPR plot of multiple models')\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        if smooth_true == False:\n",
    "            plt.savefig(\"plots/h2_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        else:\n",
    "            plt.savefig(\"plots/h2_\"+smooth_label(smooth_no)+\"_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        plt.show()\n",
    "\n",
    "    for transformer in sentence_transformer_names_list:    \n",
    "        for smooth_no in smooth_no_list:\n",
    "            allsmooth_fpr_plot_list = []\n",
    "            allsmooth_fnr_plot_list = []\n",
    "            for tup in typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer]:\n",
    "                allsmooth_fpr_plot_list.append(tup[0])\n",
    "                allsmooth_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = smooth_label(smooth_no))\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        plt.title('H2 FNR-FPR plot of '+type_html+' of '+transformer.split(\"/\")[1])\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        # if smooth_true:\n",
    "        plt.savefig(\"plots/h2_\"+transformer.split(\"/\")[1]+\" \"+type_html+\"_all_smooth_average_allchapters_allmodels.png\",dpi=1000)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "for type_html in types_html_files_list: \n",
    "    for smooth_no in smooth_no_list:\n",
    "        pickle.dump( typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no], open( \"pickles/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65eecf-266f-49e3-8370-053647dc9e2c",
   "metadata": {},
   "source": [
    "# test: all models accuracy test with heuristic 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d837fce-b7d1-472a-a9d9-bc723d376ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Heuristic 3 ---------------------\n",
    "# Consider few sentences above and below the fig as boundary.\n",
    "# subract average cosine value of all_sent_list from the cosine value and also scaling using standard deviation\n",
    "# Next, consider similar sentence with in the boundary as similar sentences.\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# filename = \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\"\n",
    "textbooks_html_files = [ \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/3_5239_3/OEBPS/Text/Untitled-1.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/4_5239_4/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/5_5239_5/OEBPS/Text/Untitled-3.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/6_5239_6/OEBPS/Text/Untitled-4.xhtml\"]\n",
    "papers_html_files = ['./data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-004.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-005.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-006.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-007.xhtml']\n",
    "\n",
    "filenames_list_debug = [ './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml']\n",
    "\n",
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# types_html_files_list = [\"papers\"]\n",
    "\n",
    "debug = 0\n",
    "\n",
    "if debug == 1:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_debug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_debug_dict\n",
    "if debug == 0:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_nondebug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_nondebug_dict\n",
    "\n",
    "\n",
    "smooth_true = True\n",
    "# if smooth_true:\n",
    "# smooth_no_list = [-11,-9,-7,-5,-3,0,3,5,7,9,11,100,101]\n",
    "smooth_no_list = [0,-7]\n",
    "# smooth_no_list = [0,3,9,13]\n",
    "# smooth_no_list = [0,100,101]\n",
    "# smooth_no_list = [-13,0,13,101]\n",
    "# else:\n",
    "#     smooth_no_list = [0]\n",
    "def smooth_label(smooth_no):\n",
    "    if smooth_no==100:\n",
    "        return \"9_max_smooth\"\n",
    "    elif smooth_no==101:\n",
    "        return \"13_max_smooth\"\n",
    "    elif smooth_no >0:\n",
    "        return str(smooth_no)+\"_uniform_smooth\"\n",
    "    elif smooth_no < 0:\n",
    "        return str(-1*smooth_no)+\"_gaussian_smooth\"\n",
    "    else:\n",
    "        return \"no_smooth\"\n",
    "\n",
    "typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict = {}\n",
    "for type_html in types_html_files_list:\n",
    "    typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html] = {}\n",
    "    for smooth_no in smooth_no_list:\n",
    "        typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no] = {}\n",
    "\n",
    "for type_html in types_html_files_list:\n",
    "    \n",
    "    for smooth_no in smooth_no_list:\n",
    "        if smooth_no==0:\n",
    "            smooth_true=False\n",
    "        else:\n",
    "            smooth_true=True\n",
    "        fpr_fnr_plot_dict = {}\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            fpr_fnr_plot_dict[transformer] = []\n",
    "\n",
    "        total_fig_no  = 0\n",
    "        file_no = 0\n",
    "        filenames_list = []\n",
    "        if type_html == \"textbooks\":\n",
    "            filenames_list = textbooks_html_files\n",
    "        if type_html == \"papers\":\n",
    "            filenames_list = papers_html_files\n",
    "        if debug ==  1:\n",
    "            filenames_list = filenames_list_debug\n",
    "            \n",
    "        for filename in filenames_list:\n",
    "            try:\n",
    "                fp = open(filename, encoding=\"utf8\")\n",
    "                print(\"++++++++++++++++++++++++++++   START   +++++++++++++++++++++\")\n",
    "                print(filename)\n",
    "            except:\n",
    "                print(\"***************  Error while opening file   ***************\")\n",
    "                print(filename)\n",
    "                print(\"************************************************************\")\n",
    "            extension = filename.split(\".\")[-1]\n",
    "            if extension == \"xhtml\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   XML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml-xml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            elif extension == \"html\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   HTML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "            fp.close()\n",
    "            fig_no = 0\n",
    "            max_caption_words = 60\n",
    "            min_caption_words = 3\n",
    "\n",
    "            full_text = soup.get_text()\n",
    "\n",
    "            # uncomment for debugging \n",
    "            # output_file = open(\"output_full_text.txt\",\"w\")\n",
    "            # output_file.write(remove_non_ascii(full_text))\n",
    "            # output_file.close()\n",
    "            # output_file = open(\"output_full_text_str.txt\",\"w\")\n",
    "            # output_file.write(str(full_text.encode('utf8')))\n",
    "            # output_file.close()\n",
    "\n",
    "            full_text = remove_multi_newlines(full_text)\n",
    "            all_sent_list = split_text_to_sent(full_text)\n",
    "\n",
    "            all_sentence_embeddings = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                all_sentence_embeddings[transformer] = sentence_transformer_model.encode(all_sent_list)\n",
    "\n",
    "            fpr_fnr_list_fig = []\n",
    "            fpr_fnr_dict_fig = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_dict_fig[transformer] = []\n",
    "            \n",
    "            all_fig = soup.find_all(string=re.compile(\"^Fig\"))\n",
    "\n",
    "            for fig in all_fig:\n",
    "                print(\"------- fig  \"+str(fig_no)+\"   --------\")\n",
    "                \n",
    "                fig_caption = extract_caption(fig, min_caption_words, max_caption_words)\n",
    "                image_path = extract_imagepath(fig, filename, type_html)\n",
    "\n",
    "                # ocr_full_txt_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "                if type_html == \"textbooks\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if type_html == \"papers\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if debug != 1:\n",
    "                    plt.imshow(mpimg.imread(image_path))\n",
    "                    plt.show()\n",
    "                if os.path.isfile(ocr_json_path):\n",
    "                    print(\"    Json already exist :  \"+ocr_json_path)\n",
    "                    json_file = open(ocr_json_path)\n",
    "                    ocr_dict = json.load(json_file) \n",
    "                    #structure of ocr_dict ={\"block\":[ {\"text\":\"\",\"\",\"paragraph\":[{\"text\":\"\", \"word\":[\"w1\",\"w2\",..], \"bounding_poly\": [[],[],..]}, { .. }, ...],\"bounding_poly\":[[(x1,y1),..(x4,y4)],[]...]}, { ... }, ..],\"bounding_poly\":[[],[]...]}\n",
    "                else:\n",
    "                    print(\"  $$$$  Calling google vision API $$$$ \")\n",
    "                    ocr_dict = detect_document(image_path, type_html)\n",
    "                labels_list = []\n",
    "                print(\"~~~~~~~ Labels  ~~~~~~~~\")\n",
    "                if len(ocr_dict[\"block\"]) > 0:\n",
    "                    for i in range(0,len(ocr_dict[\"block\"])):\n",
    "                        block_dict = ocr_dict[\"block\"][i]\n",
    "                        print(block_dict[\"text\"])\n",
    "                        labels_list.append(block_dict[\"text\"].replace(\"-\",\"\"))\n",
    "                        # print(block_dict[\"text\"], \"  \",ocr_dict[\"bounding_poly\"][i])\n",
    "\n",
    "                        # for i in range(0,len(block_dict[\"paragraph\"])):\n",
    "                        #     para_dict = block_dict[\"paragraph\"][i]\n",
    "                        #     print(\"      -\",para_dict[\"text\"])\n",
    "                        #     print(\"      \",para_dict[\"text\"],\"   \",block_dict[\"bounding_poly\"][i])\n",
    "                else:\n",
    "                    print(\"  NO Labels  \")\n",
    "                print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "                if type_html == \"textbooks\":\n",
    "                    above_header = fig.parent.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if above_header is not None:\n",
    "                        above_header_text = above_header.get_text()\n",
    "                    else:\n",
    "                        above_header_text = \"\"\n",
    "                    while clean_string(above_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(above_header_text).replace(\" \",\"\"))<3:\n",
    "                        if above_header is not None:\n",
    "                            above_header = above_header.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if above_header is not None:\n",
    "                            above_header_text = above_header.get_text()\n",
    "                        else:\n",
    "                            above_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    below_header = fig.parent.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if below_header is not None:\n",
    "                        below_header_text = below_header.get_text()\n",
    "                    else:\n",
    "                        below_header_text = \"\"\n",
    "                    while clean_string(below_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(below_header_text).replace(\" \",\"\"))<3:\n",
    "                        if below_header is not None:\n",
    "                            below_header = below_header.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if below_header is not None:\n",
    "                            below_header_text = below_header.get_text()\n",
    "                        else:\n",
    "                            below_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    print(\"\\nabove header = \",above_header_text)\n",
    "                    print(\"fig caption = \",fig_caption)\n",
    "                    print(\"below header = \",below_header_text)\n",
    "\n",
    "                    section_text = above_header_text\n",
    "                    next_header = above_header.find_next()\n",
    "\n",
    "                    while next_header.get_text() != below_header.get_text():\n",
    "                        # section_text = section_text + next_header.text\n",
    "                        for child in next_header.contents:\n",
    "                            if isinstance(child, str):\n",
    "                                # if len(clean_string(child).replace(\" \",\"\")) > 1:\n",
    "                                section_text = section_text + child\n",
    "                                # print(\"---------------\\n\",clean_string(child),\"\\n\")\n",
    "                        next_header = next_header.find_next()\n",
    "                        section_text = section_text + \"\\n\"\n",
    "\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    # print(\"---------\\n\",section_text,\"\\n--------------\")\n",
    "                    # print(\"++++++\\n\",full_text.encode(\"utf-8\"),\"\\n++++++++\")\n",
    "                if type_html == \"papers\":\n",
    "                    print(\"Figure caption = \",fig_caption)\n",
    "                    relevant_txt_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\"_relevant.txt\"\n",
    "                    relevant_file = open(relevant_txt_path,\"r\", encoding='utf-8')\n",
    "                    print(relevant_txt_path + \"  read!\")\n",
    "                    section_text = relevant_file.read()\n",
    "                    # print(\"\\n!!! section text !!! \\n\", section_text, \"\\n!!!!!!!!\")\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    print (\"Length of sent list = \", len(sent_list))\n",
    "                    # for inx in range(0,len(sent_list)):\n",
    "                    #     print( str(inx)+\": \",f\"{bcolors.OKBLUE}{sent_list[inx]}{bcolors.ENDC}\")\n",
    "                    relevant_file.close()\n",
    "                                        \n",
    "                str_labels_list = str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "\n",
    "                if len(labels_list) > 0:\n",
    "                    fig_caption_sent = fig_caption +\". This figure contains \"+ str_labels_list +\".\"\n",
    "                    # fig_caption_sent = fig_caption + \" and this figure contains \"+ str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")+ \" as labels.\"\n",
    "                else:\n",
    "                    fig_caption_sent = fig_caption\n",
    "\n",
    "\n",
    "                fig_caption_sent = clean_string(fig_caption_sent)\n",
    "\n",
    "                print(\"Fig caption sentence = \",fig_caption_sent)\n",
    "\n",
    "\n",
    "                moving_average_no = smooth_no                 #takes only odd numbers\n",
    "\n",
    "                no_space_sent_list = []\n",
    "                for sent in sent_list:\n",
    "                    no_space_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                no_space_all_sent_list = []\n",
    "                for sent in all_sent_list:\n",
    "                    no_space_all_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                fig_caption_clean = remove_multi_newlines(fig_caption)\n",
    "                fig_caption_start = split_text_to_sent(fig_caption_clean)[0]\n",
    "                fig_caption_index = -1\n",
    "                if fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_all_sent_list:\n",
    "                    fig_caption_index = no_space_all_sent_list.index(fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                for transformer in sentence_transformer_names_list:\n",
    "                    sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                    fig_caption_embedding = sentence_transformer_model.encode(fig_caption_sent)\n",
    "                    sentence_embeddings = sentence_transformer_model.encode(sent_list)\n",
    "\n",
    "\n",
    "                    cosine_list = cosine_similarity([fig_caption_embedding],sentence_embeddings)[0]\n",
    "                    all_cosine_list = cosine_similarity([fig_caption_embedding],all_sentence_embeddings[transformer])[0]\n",
    "\n",
    "                    sect_index_list = []\n",
    "                    sect_cosine_list = []\n",
    "                    check_no_space_sent_list = no_space_sent_list.copy()\n",
    "                    for index in range(0,len(all_sent_list)):\n",
    "                        if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "                            sect_index_list.append(index)\n",
    "                            sect_cosine_list.append(all_cosine_list[index])\n",
    "                            check_no_space_sent_list.remove(all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                    if len(sect_index_list) != len(sent_list):\n",
    "                        print(\"!!!!! ERROR !!!!\")\n",
    "                        print(\"start index of section = \",sect_index_list[0])\n",
    "                        print(\"end index of section = \",sect_index_list[-1])\n",
    "                        print(\"indices of section = \",sect_index_list)\n",
    "                        print(\"len of matched sent = \",len(sect_index_list))\n",
    "                        print(\"len of section sent = \",len(sent_list))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = \"non_smooth\")\n",
    "                        plt.vlines(x = sect_index_list[0], ymin = 0, ymax = 1, colors = 'green', label = 'section_start')\n",
    "                        plt.vlines(x = sect_index_list[-1], ymin = 0, ymax = 1, colors = 'red', label = 'section_end')\n",
    "\n",
    "                    if smooth_true:\n",
    "                        all_cosine_list = moving_average(np.array(all_cosine_list),moving_average_no).tolist()\n",
    "                        if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                            plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = smooth_label(smooth_no))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.title(\"cosine simlairity scores\")\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "\n",
    "                    average_cosine_value = np.average(all_cosine_list)\n",
    "                    sigma_cosine_value = np.std(all_cosine_list)\n",
    "\n",
    "                    cosine_threshold_list = np.arange(0.0, 3.0001 ,(3.0/no_points_fpr_fnr_plot))\n",
    "                    cosine_threshold_list = np.around(cosine_threshold_list, 4)\n",
    "                    fpr_fnr_list_threshold = []\n",
    "                    for cosine_threshold in cosine_threshold_list:\n",
    "                        no_sim_sent = 0\n",
    "                        no_sim_sent_in_sect = 0\n",
    "                        no_sent_in_sect = len(sect_index_list)\n",
    "                        no_sim_sent_not_in_sect = 0\n",
    "                        prediction_index_list=[]\n",
    "                        for index in range(0,len(all_sent_list)):\n",
    "                            cosine_value = all_cosine_list[index]\n",
    "                            # if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_sent_list:\n",
    "                            #     sect_index_list.append(index)\n",
    "                            if fig_caption_index!=-1:\n",
    "                                if cosine_value - average_cosine_value  >= cosine_threshold*sigma_cosine_value and abs(index-fig_caption_index)<=no_bounding_sent:\n",
    "                                    prediction_index_list.append(index)\n",
    "                            else:\n",
    "                                if cosine_value - average_cosine_value  >= cosine_threshold*sigma_cosine_value:\n",
    "                                    prediction_index_list.append(index)\n",
    "                        no_sim_sent = len(prediction_index_list)\n",
    "                        for index in prediction_index_list:\n",
    "                            if index in sect_index_list:\n",
    "                                no_sim_sent_in_sect = no_sim_sent_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "                            else:\n",
    "                                no_sim_sent_not_in_sect = no_sim_sent_not_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",all_sent_list[index])\n",
    "\n",
    "                        if no_sim_sent != 0:\n",
    "                            false_positive_rate = no_sim_sent_not_in_sect/no_sim_sent\n",
    "                        else:\n",
    "                            false_positive_rate = 0.0\n",
    "                        if no_sent_in_sect != 0:\n",
    "                            false_negative_rate = (no_sent_in_sect - no_sim_sent_in_sect)/no_sent_in_sect\n",
    "                        else:\n",
    "                            false_negative_rate = 0.0\n",
    "\n",
    "                        fpr_fnr_list_threshold.append((false_positive_rate,false_negative_rate))\n",
    "\n",
    "                        # print(\"no_sim_sent = \",no_sim_sent)\n",
    "                        # print(\"no_sent_in_sect = \",no_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_in_sect = \",no_sim_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_not_in_sect = \",no_sim_sent_not_in_sect)\n",
    "                        # print(\"false_positive_rate = \",false_positive_rate)\n",
    "                        # print(\"false_negative_rate = \",false_negative_rate)\n",
    "                        # print(\"cosine_threshold = \",cosine_threshold)\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "                        # for index in range(0,len(sent_list)):\n",
    "                        #     cosine_value = cosine_list[index]\n",
    "                        #     if cosine_value > cosine_threshold:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{sent_list[index]}{bcolors.ENDC}\")\n",
    "                        #     else:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",sent_list[index])\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "                    fpr_fnr_dict_fig[transformer].append(fpr_fnr_list_threshold)\n",
    "\n",
    "                section_text_caption_labels = section_text + \" \" +fig_caption_sent\n",
    "\n",
    "                fig_no = fig_no + 1\n",
    "\n",
    "            marker_dict = {'sentence-transformers/all-mpnet-base-v2':\"s\",'sentence-transformers/multi-qa-MiniLM-L6-cos-v1':\"o\",'sentence-transformers/all-MiniLM-L6-v2':\"P\",'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2':\"v\",'sentence-transformers/bert-base-nli-mean-tokens':\"X\"}\n",
    "\n",
    "            textbook_name = filename.split(\"/\")[4]+\"_\"+(filename.split(\"/\")[-1]).split(\".\")[0]\n",
    "            # class_name = filename.split(\"/\")[3]\n",
    "            class_name = smooth_label(smooth_no)+\"_\"+filename.split(\"/\")[3] # while smooth\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_list_fig = fpr_fnr_dict_fig[transformer]\n",
    "                np_fpr_fnr_list_fig = np.array(fpr_fnr_list_fig)\n",
    "                mean_fpr_fnr_list = (np.mean(np_fpr_fnr_list_fig, axis=0)).tolist()\n",
    "                fpr_fnr_plot_dict[transformer].append(mean_fpr_fnr_list)\n",
    "                plot_fpr_list = []\n",
    "                plot_fnr_list = []\n",
    "\n",
    "                for tup in mean_fpr_fnr_list:\n",
    "                    plot_fpr_list.append(tup[0])\n",
    "                    plot_fnr_list.append(tup[1])\n",
    "\n",
    "        #         plt.scatter(plot_fnr_list, plot_fpr_list, label = transformer.split(\"/\")[1], marker = marker_dict[transformer], c = cosine_threshold_list, cmap = \"jet\", vmin=0, vmax=1)\n",
    "        #         plt.xlabel('False Negative Rate (FNR)')\n",
    "        #         plt.ylabel('False Positve Rate (FPR)')\n",
    "        #         if moving_average_no == 0:\n",
    "        #             plt.title('FNR-FPR plot')\n",
    "        #         else:\n",
    "        #             plt.title(str(moving_average_no)+' smooth FNR-FPR plot')\n",
    "        #         plt.colorbar(label=\"Cosine treshold\", orientation=\"vertical\")\n",
    "        #         plt.grid(True)\n",
    "        #         plt.legend(loc=\"lower left\",fontsize=\"small\")\n",
    "\n",
    "        #         plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_\"+transformer.replace(\"/\",\"_\")+\"_scatter.png\",dpi=1000)\n",
    "        #         plt.show()\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                mean_plot_fpr_list = []\n",
    "                mean_plot_fnr_list = []\n",
    "                for tup in fpr_fnr_plot_dict[transformer][file_no]:\n",
    "                    mean_plot_fpr_list.append(tup[0])\n",
    "                    mean_plot_fnr_list.append(tup[1])\n",
    "                plt.plot(mean_plot_fnr_list,mean_plot_fpr_list, label = transformer.split(\"/\")[1])    \n",
    "\n",
    "            plt.xlabel('False Negative Rate (FNR)')\n",
    "            plt.ylabel('False Positve Rate (FPR)')\n",
    "            if smooth_true == False:\n",
    "                plt.title('H3 FNR-FPR plot of multiple models')\n",
    "            else:\n",
    "                plt.title(smooth_label(smooth_no)+' H3 FNR-FPR plot of multiple models')\n",
    "            plt.grid(True)\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.axis('square')\n",
    "            plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "            # plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_allmodels.png\",dpi=1000)\n",
    "            plt.show()\n",
    "\n",
    "            file_no = file_no + 1\n",
    "            total_fig_no = total_fig_no + fig_no\n",
    "            print(\"++++++++++++++++++++++++++++   END   +++++++++++++++++++++\\n\")\n",
    "\n",
    "        original_stdout = sys.stdout\n",
    "        if smooth_true:\n",
    "            out_file = open(\"plots/h3_\"+smooth_label(smooth_no)+'_'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        else:\n",
    "            out_file = open('plots/h3_'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        sys.stdout = out_file\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            mean_allfiles_fpr_fnr_list = (np.mean(np.array(fpr_fnr_plot_dict[transformer]), axis=0)).tolist()\n",
    "            typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer] = mean_allfiles_fpr_fnr_list \n",
    "            allfile_fpr_plot_list = []\n",
    "            allfile_fnr_plot_list = []\n",
    "            for tup in mean_allfiles_fpr_fnr_list:\n",
    "                allfile_fpr_plot_list.append(tup[0])\n",
    "                allfile_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allfile_fnr_plot_list, allfile_fpr_plot_list, label = transformer.split(\"/\")[1])\n",
    "            min_dist_index = -1\n",
    "            min_dist = 100\n",
    "            for i in range(0,len(cosine_threshold_list)):\n",
    "                dist = math.sqrt(allfile_fnr_plot_list[i]*allfile_fnr_plot_list[i] + allfile_fpr_plot_list[i]*allfile_fpr_plot_list[i])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_dist_index = i\n",
    "            print(\"-------  \", transformer, \"  ------------------\")\n",
    "            print(\" least distance cosine threshold = \", cosine_threshold_list[min_dist_index])\n",
    "            print(\" least distance = \", min_dist)\n",
    "            print(\" least distnant FNR = \", plot_fnr_list[min_dist_index])\n",
    "            print(\" least distnant FPR = \", plot_fpr_list[min_dist_index])\n",
    "            print(\" number of total figures = \", total_fig_no)\n",
    "\n",
    "        sys.stdout = original_stdout\n",
    "        out_file.close()\n",
    "\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        if smooth_true == False:\n",
    "            plt.title('H3 FNR-FPR plot of multiple models')\n",
    "        else:\n",
    "            plt.title(smooth_label(smooth_no)+' H3 FNR-FPR plot of multiple models')\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        if smooth_true == False:\n",
    "            plt.savefig(\"plots/h3_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        else:\n",
    "            plt.savefig(\"plots/h3_\"+smooth_label(smooth_no)+\"_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        plt.show()\n",
    "\n",
    "    for transformer in sentence_transformer_names_list:    \n",
    "        for smooth_no in smooth_no_list:\n",
    "            allsmooth_fpr_plot_list = []\n",
    "            allsmooth_fnr_plot_list = []\n",
    "            for tup in typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer]:\n",
    "                allsmooth_fpr_plot_list.append(tup[0])\n",
    "                allsmooth_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = smooth_label(smooth_no))\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        plt.title('H3 FNR-FPR plot of '+type_html+' of '+transformer.split(\"/\")[1])\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        # if smooth_true:\n",
    "        plt.savefig(\"plots/h3_\"+transformer.split(\"/\")[1]+\" \"+type_html+\"_all_smooth_average_allchapters_allmodels.png\",dpi=1000)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "for type_html in types_html_files_list: \n",
    "    for smooth_no in smooth_no_list:\n",
    "        pickle.dump( typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no], open( \"pickles/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f10cd-5f03-4e7d-ae58-5189f4c2e404",
   "metadata": {},
   "source": [
    "# test: all models accuracy test with heuristic 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69961069-8c89-4652-8ae6-6256f182c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Heuristic 4 -------------------------------\n",
    "# Consider few sentences above and below the fig as boundary.\n",
    "# Subtract mean cosive value of bounding box from cosine values then find similarity \n",
    "# Next, consider similar sentence with in the boundary as similar sentences.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# filename = \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\"\n",
    "textbooks_html_files = [ \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/3_5239_3/OEBPS/Text/Untitled-1.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/4_5239_4/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/5_5239_5/OEBPS/Text/Untitled-3.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/6_5239_6/OEBPS/Text/Untitled-4.xhtml\"]\n",
    "papers_html_files = ['./data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-004.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-005.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-006.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-007.xhtml']\n",
    "\n",
    "filenames_list_debug = [ './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml']\n",
    "\n",
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# types_html_files_list = [\"papers\"]\n",
    "\n",
    "no_bounding_sent = 50\n",
    "debug = 0\n",
    "\n",
    "if debug == 1:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_debug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_debug_dict\n",
    "if debug == 0:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_nondebug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_nondebug_dict\n",
    "\n",
    "\n",
    "smooth_true = True\n",
    "# if smooth_true:\n",
    "# smooth_no_list = [-11,-9,-7,-5,-3,0,3,5,7,9,11,100,101]\n",
    "smooth_no_list = [0,-7]\n",
    "# smooth_no_list = [0,3,9,13]\n",
    "# smooth_no_list = [0,100,101]\n",
    "# smooth_no_list = [-13,0,13,101]\n",
    "# else:\n",
    "#     smooth_no_list = [0]\n",
    "def smooth_label(smooth_no):\n",
    "    if smooth_no==100:\n",
    "        return \"9_max_smooth\"\n",
    "    elif smooth_no==101:\n",
    "        return \"13_max_smooth\"\n",
    "    elif smooth_no >0:\n",
    "        return str(smooth_no)+\"_uniform_smooth\"\n",
    "    elif smooth_no < 0:\n",
    "        return str(-1*smooth_no)+\"_gaussian_smooth\"\n",
    "    else:\n",
    "        return \"no_smooth\"\n",
    "\n",
    "typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict = {}\n",
    "for type_html in types_html_files_list:\n",
    "    typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html] = {}\n",
    "    for smooth_no in smooth_no_list:\n",
    "        typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no] = {}\n",
    "\n",
    "for type_html in types_html_files_list:\n",
    "    \n",
    "    for smooth_no in smooth_no_list:\n",
    "        if smooth_no==0:\n",
    "            smooth_true=False\n",
    "        else:\n",
    "            smooth_true=True\n",
    "        fpr_fnr_plot_dict = {}\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            fpr_fnr_plot_dict[transformer] = []\n",
    "\n",
    "        total_fig_no  = 0\n",
    "        file_no = 0\n",
    "        filenames_list = []\n",
    "        if type_html == \"textbooks\":\n",
    "            filenames_list = textbooks_html_files\n",
    "        if type_html == \"papers\":\n",
    "            filenames_list = papers_html_files\n",
    "        if debug ==  1:\n",
    "            filenames_list = filenames_list_debug\n",
    "            \n",
    "        for filename in filenames_list:\n",
    "            try:\n",
    "                fp = open(filename, encoding=\"utf8\")\n",
    "                print(\"++++++++++++++++++++++++++++   START   +++++++++++++++++++++\")\n",
    "                print(filename)\n",
    "            except:\n",
    "                print(\"***************  Error while opening file   ***************\")\n",
    "                print(filename)\n",
    "                print(\"************************************************************\")\n",
    "            extension = filename.split(\".\")[-1]\n",
    "            if extension == \"xhtml\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   XML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml-xml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            elif extension == \"html\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   HTML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "            fp.close()\n",
    "            fig_no = 0\n",
    "            max_caption_words = 60\n",
    "            min_caption_words = 3\n",
    "\n",
    "            full_text = soup.get_text()\n",
    "\n",
    "            # uncomment for debugging \n",
    "            # output_file = open(\"output_full_text.txt\",\"w\")\n",
    "            # output_file.write(remove_non_ascii(full_text))\n",
    "            # output_file.close()\n",
    "            # output_file = open(\"output_full_text_str.txt\",\"w\")\n",
    "            # output_file.write(str(full_text.encode('utf8')))\n",
    "            # output_file.close()\n",
    "\n",
    "            full_text = remove_multi_newlines(full_text)\n",
    "            all_sent_list = split_text_to_sent(full_text)\n",
    "\n",
    "            all_sentence_embeddings = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                all_sentence_embeddings[transformer] = sentence_transformer_model.encode(all_sent_list)\n",
    "\n",
    "            fpr_fnr_list_fig = []\n",
    "            fpr_fnr_dict_fig = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_dict_fig[transformer] = []\n",
    "            \n",
    "            all_fig = soup.find_all(string=re.compile(\"^Fig\"))\n",
    "\n",
    "            for fig in all_fig:\n",
    "                print(\"------- fig  \"+str(fig_no)+\"   --------\")\n",
    "                \n",
    "                fig_caption = extract_caption(fig, min_caption_words, max_caption_words)\n",
    "                image_path = extract_imagepath(fig, filename, type_html)\n",
    "\n",
    "                # ocr_full_txt_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "                if type_html == \"textbooks\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if type_html == \"papers\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if debug != 1:\n",
    "                    plt.imshow(mpimg.imread(image_path))\n",
    "                    plt.show()\n",
    "                if os.path.isfile(ocr_json_path):\n",
    "                    print(\"    Json already exist :  \"+ocr_json_path)\n",
    "                    json_file = open(ocr_json_path)\n",
    "                    ocr_dict = json.load(json_file) \n",
    "                    #structure of ocr_dict ={\"block\":[ {\"text\":\"\",\"\",\"paragraph\":[{\"text\":\"\", \"word\":[\"w1\",\"w2\",..], \"bounding_poly\": [[],[],..]}, { .. }, ...],\"bounding_poly\":[[(x1,y1),..(x4,y4)],[]...]}, { ... }, ..],\"bounding_poly\":[[],[]...]}\n",
    "                else:\n",
    "                    print(\"  $$$$  Calling google vision API $$$$ \")\n",
    "                    ocr_dict = detect_document(image_path, type_html)\n",
    "                labels_list = []\n",
    "                print(\"~~~~~~~ Labels  ~~~~~~~~\")\n",
    "                if len(ocr_dict[\"block\"]) > 0:\n",
    "                    for i in range(0,len(ocr_dict[\"block\"])):\n",
    "                        block_dict = ocr_dict[\"block\"][i]\n",
    "                        print(block_dict[\"text\"])\n",
    "                        labels_list.append(block_dict[\"text\"].replace(\"-\",\"\"))\n",
    "                        # print(block_dict[\"text\"], \"  \",ocr_dict[\"bounding_poly\"][i])\n",
    "\n",
    "                        # for i in range(0,len(block_dict[\"paragraph\"])):\n",
    "                        #     para_dict = block_dict[\"paragraph\"][i]\n",
    "                        #     print(\"      -\",para_dict[\"text\"])\n",
    "                        #     print(\"      \",para_dict[\"text\"],\"   \",block_dict[\"bounding_poly\"][i])\n",
    "                else:\n",
    "                    print(\"  NO Labels  \")\n",
    "                print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "                if type_html == \"textbooks\":\n",
    "                    above_header = fig.parent.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if above_header is not None:\n",
    "                        above_header_text = above_header.get_text()\n",
    "                    else:\n",
    "                        above_header_text = \"\"\n",
    "                    while clean_string(above_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(above_header_text).replace(\" \",\"\"))<3:\n",
    "                        if above_header is not None:\n",
    "                            above_header = above_header.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if above_header is not None:\n",
    "                            above_header_text = above_header.get_text()\n",
    "                        else:\n",
    "                            above_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    below_header = fig.parent.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if below_header is not None:\n",
    "                        below_header_text = below_header.get_text()\n",
    "                    else:\n",
    "                        below_header_text = \"\"\n",
    "                    while clean_string(below_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(below_header_text).replace(\" \",\"\"))<3:\n",
    "                        if below_header is not None:\n",
    "                            below_header = below_header.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if below_header is not None:\n",
    "                            below_header_text = below_header.get_text()\n",
    "                        else:\n",
    "                            below_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    print(\"\\nabove header = \",above_header_text)\n",
    "                    print(\"fig caption = \",fig_caption)\n",
    "                    print(\"below header = \",below_header_text)\n",
    "\n",
    "                    section_text = above_header_text\n",
    "                    next_header = above_header.find_next()\n",
    "\n",
    "                    while next_header.get_text() != below_header.get_text():\n",
    "                        # section_text = section_text + next_header.text\n",
    "                        for child in next_header.contents:\n",
    "                            if isinstance(child, str):\n",
    "                                # if len(clean_string(child).replace(\" \",\"\")) > 1:\n",
    "                                section_text = section_text + child\n",
    "                                # print(\"---------------\\n\",clean_string(child),\"\\n\")\n",
    "                        next_header = next_header.find_next()\n",
    "                        section_text = section_text + \"\\n\"\n",
    "\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    # print(\"---------\\n\",section_text,\"\\n--------------\")\n",
    "                    # print(\"++++++\\n\",full_text.encode(\"utf-8\"),\"\\n++++++++\")\n",
    "                if type_html == \"papers\":\n",
    "                    print(\"Figure caption = \",fig_caption)\n",
    "                    relevant_txt_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\"_relevant.txt\"\n",
    "                    relevant_file = open(relevant_txt_path,\"r\", encoding='utf-8')\n",
    "                    print(relevant_txt_path + \"  read!\")\n",
    "                    section_text = relevant_file.read()\n",
    "                    # print(\"\\n!!! section text !!! \\n\", section_text, \"\\n!!!!!!!!\")\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    print (\"Length of sent list = \", len(sent_list))\n",
    "                    # for inx in range(0,len(sent_list)):\n",
    "                    #     print( str(inx)+\": \",f\"{bcolors.OKBLUE}{sent_list[inx]}{bcolors.ENDC}\")\n",
    "                    relevant_file.close()\n",
    "                                        \n",
    "                str_labels_list = str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "\n",
    "                if len(labels_list) > 0:\n",
    "                    fig_caption_sent = fig_caption +\". This figure contains \"+ str_labels_list +\".\"\n",
    "                    # fig_caption_sent = fig_caption + \" and this figure contains \"+ str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")+ \" as labels.\"\n",
    "                else:\n",
    "                    fig_caption_sent = fig_caption\n",
    "\n",
    "\n",
    "                fig_caption_sent = clean_string(fig_caption_sent)\n",
    "\n",
    "                print(\"Fig caption sentence = \",fig_caption_sent)\n",
    "\n",
    "\n",
    "                moving_average_no = smooth_no                 #takes only odd numbers\n",
    "\n",
    "                no_space_sent_list = []\n",
    "                for sent in sent_list:\n",
    "                    no_space_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                no_space_all_sent_list = []\n",
    "                for sent in all_sent_list:\n",
    "                    no_space_all_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                fig_caption_clean = remove_multi_newlines(fig_caption)\n",
    "                fig_caption_start = split_text_to_sent(fig_caption_clean)[0]\n",
    "                fig_caption_index = -1\n",
    "                if fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_all_sent_list:\n",
    "                    fig_caption_index = no_space_all_sent_list.index(fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                for transformer in sentence_transformer_names_list:\n",
    "                    sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                    fig_caption_embedding = sentence_transformer_model.encode(fig_caption_sent)\n",
    "                    sentence_embeddings = sentence_transformer_model.encode(sent_list)\n",
    "\n",
    "\n",
    "                    cosine_list = cosine_similarity([fig_caption_embedding],sentence_embeddings)[0]\n",
    "                    all_cosine_list = cosine_similarity([fig_caption_embedding],all_sentence_embeddings[transformer])[0]\n",
    "\n",
    "                    sect_index_list = []\n",
    "                    sect_cosine_list = []\n",
    "                    check_no_space_sent_list = no_space_sent_list.copy()\n",
    "                    for index in range(0,len(all_sent_list)):\n",
    "                        if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "                            sect_index_list.append(index)\n",
    "                            sect_cosine_list.append(all_cosine_list[index])\n",
    "                            check_no_space_sent_list.remove(all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                    if len(sect_index_list) != len(sent_list):\n",
    "                        print(\"!!!!! ERROR !!!!\")\n",
    "                        print(\"start index of section = \",sect_index_list[0])\n",
    "                        print(\"end index of section = \",sect_index_list[-1])\n",
    "                        print(\"indices of section = \",sect_index_list)\n",
    "                        print(\"len of matched sent = \",len(sect_index_list))\n",
    "                        print(\"len of section sent = \",len(sent_list))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = \"non_smooth\")\n",
    "                        plt.vlines(x = sect_index_list[0], ymin = 0, ymax = 1, colors = 'green', label = 'section_start')\n",
    "                        plt.vlines(x = sect_index_list[-1], ymin = 0, ymax = 1, colors = 'red', label = 'section_end')\n",
    "\n",
    "                    if smooth_true:\n",
    "                        all_cosine_list = moving_average(np.array(all_cosine_list),moving_average_no).tolist()\n",
    "                        if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                            plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = smooth_label(smooth_no))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.title(\"cosine simlairity scores\")\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "\n",
    "                    cosine_threshold_list = np.arange(-1.0, 1.0001 ,(2.0/no_points_fpr_fnr_plot))\n",
    "                    cosine_threshold_list = np.around(cosine_threshold_list, 4)\n",
    "                    average_cosine_value = np.average(all_cosine_list[max(0,fig_caption_index - no_bounding_sent):min(fig_caption_index + no_bounding_sent,len(all_cosine_list))])\n",
    "                    fpr_fnr_list_threshold = []\n",
    "                    for cosine_threshold in cosine_threshold_list:\n",
    "                        no_sim_sent = 0\n",
    "                        no_sim_sent_in_sect = 0\n",
    "                        no_sent_in_sect = len(sect_index_list)\n",
    "                        no_sim_sent_not_in_sect = 0\n",
    "                        prediction_index_list=[]\n",
    "                        for index in range(0,len(all_sent_list)):\n",
    "                            cosine_value = all_cosine_list[index]\n",
    "                            # if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_sent_list:\n",
    "                            #     sect_index_list.append(index)\n",
    "                            if fig_caption_index!=-1:\n",
    "                                if cosine_value - average_cosine_value >= cosine_threshold and abs(index-fig_caption_index)<=no_bounding_sent:\n",
    "                                    prediction_index_list.append(index)\n",
    "                            else:\n",
    "                                if cosine_value - average_cosine_value >= cosine_threshold:\n",
    "                                    prediction_index_list.append(index)\n",
    "                        no_sim_sent = len(prediction_index_list)\n",
    "                        for index in prediction_index_list:\n",
    "                            if index in sect_index_list:\n",
    "                                no_sim_sent_in_sect = no_sim_sent_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "                            else:\n",
    "                                no_sim_sent_not_in_sect = no_sim_sent_not_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",all_sent_list[index])\n",
    "\n",
    "                        if no_sim_sent != 0:\n",
    "                            false_positive_rate = no_sim_sent_not_in_sect/no_sim_sent\n",
    "                        else:\n",
    "                            false_positive_rate = 0.0\n",
    "                        if no_sent_in_sect != 0:\n",
    "                            false_negative_rate = (no_sent_in_sect - no_sim_sent_in_sect)/no_sent_in_sect\n",
    "                        else:\n",
    "                            false_negative_rate = 0.0\n",
    "\n",
    "                        fpr_fnr_list_threshold.append((false_positive_rate,false_negative_rate))\n",
    "\n",
    "                        # print(\"no_sim_sent = \",no_sim_sent)\n",
    "                        # print(\"no_sent_in_sect = \",no_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_in_sect = \",no_sim_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_not_in_sect = \",no_sim_sent_not_in_sect)\n",
    "                        # print(\"false_positive_rate = \",false_positive_rate)\n",
    "                        # print(\"false_negative_rate = \",false_negative_rate)\n",
    "                        # print(\"cosine_threshold = \",cosine_threshold)\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "                        # for index in range(0,len(sent_list)):\n",
    "                        #     cosine_value = cosine_list[index]\n",
    "                        #     if cosine_value > cosine_threshold:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{sent_list[index]}{bcolors.ENDC}\")\n",
    "                        #     else:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",sent_list[index])\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "                    fpr_fnr_dict_fig[transformer].append(fpr_fnr_list_threshold)\n",
    "\n",
    "                section_text_caption_labels = section_text + \" \" +fig_caption_sent\n",
    "\n",
    "                fig_no = fig_no + 1\n",
    "\n",
    "            marker_dict = {'sentence-transformers/all-mpnet-base-v2':\"s\",'sentence-transformers/multi-qa-MiniLM-L6-cos-v1':\"o\",'sentence-transformers/all-MiniLM-L6-v2':\"P\",'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2':\"v\",'sentence-transformers/bert-base-nli-mean-tokens':\"X\"}\n",
    "\n",
    "            textbook_name = filename.split(\"/\")[4]+\"_\"+(filename.split(\"/\")[-1]).split(\".\")[0]\n",
    "            # class_name = filename.split(\"/\")[3]\n",
    "            class_name = smooth_label(smooth_no)+\"_\"+filename.split(\"/\")[3] # while smooth\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_list_fig = fpr_fnr_dict_fig[transformer]\n",
    "                np_fpr_fnr_list_fig = np.array(fpr_fnr_list_fig)\n",
    "                mean_fpr_fnr_list = (np.mean(np_fpr_fnr_list_fig, axis=0)).tolist()\n",
    "                fpr_fnr_plot_dict[transformer].append(mean_fpr_fnr_list)\n",
    "                plot_fpr_list = []\n",
    "                plot_fnr_list = []\n",
    "\n",
    "                for tup in mean_fpr_fnr_list:\n",
    "                    plot_fpr_list.append(tup[0])\n",
    "                    plot_fnr_list.append(tup[1])\n",
    "\n",
    "        #         plt.scatter(plot_fnr_list, plot_fpr_list, label = transformer.split(\"/\")[1], marker = marker_dict[transformer], c = cosine_threshold_list, cmap = \"jet\", vmin=0, vmax=1)\n",
    "        #         plt.xlabel('False Negative Rate (FNR)')\n",
    "        #         plt.ylabel('False Positve Rate (FPR)')\n",
    "        #         if moving_average_no == 0:\n",
    "        #             plt.title('FNR-FPR plot')\n",
    "        #         else:\n",
    "        #             plt.title(str(moving_average_no)+' smooth FNR-FPR plot')\n",
    "        #         plt.colorbar(label=\"Cosine treshold\", orientation=\"vertical\")\n",
    "        #         plt.grid(True)\n",
    "        #         plt.legend(loc=\"lower left\",fontsize=\"small\")\n",
    "\n",
    "        #         plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_\"+transformer.replace(\"/\",\"_\")+\"_scatter.png\",dpi=1000)\n",
    "        #         plt.show()\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                mean_plot_fpr_list = []\n",
    "                mean_plot_fnr_list = []\n",
    "                for tup in fpr_fnr_plot_dict[transformer][file_no]:\n",
    "                    mean_plot_fpr_list.append(tup[0])\n",
    "                    mean_plot_fnr_list.append(tup[1])\n",
    "                plt.plot(mean_plot_fnr_list,mean_plot_fpr_list, label = transformer.split(\"/\")[1])    \n",
    "\n",
    "            plt.xlabel('False Negative Rate (FNR)')\n",
    "            plt.ylabel('False Positve Rate (FPR)')\n",
    "            if smooth_true == False:\n",
    "                plt.title('H4 FNR-FPR plot of multiple models')\n",
    "            else:\n",
    "                plt.title(smooth_label(smooth_no)+' H4 FNR-FPR plot of multiple models')\n",
    "            plt.grid(True)\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.axis('square')\n",
    "            plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "            # plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_allmodels.png\",dpi=1000)\n",
    "            plt.show()\n",
    "\n",
    "            file_no = file_no + 1\n",
    "            total_fig_no = total_fig_no + fig_no\n",
    "            print(\"++++++++++++++++++++++++++++   END   +++++++++++++++++++++\\n\")\n",
    "\n",
    "        original_stdout = sys.stdout\n",
    "        if smooth_true:\n",
    "            out_file = open(\"plots/\"+smooth_label(smooth_no)+'_'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        else:\n",
    "            out_file = open('plots/'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        sys.stdout = out_file\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            mean_allfiles_fpr_fnr_list = (np.mean(np.array(fpr_fnr_plot_dict[transformer]), axis=0)).tolist()\n",
    "            typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer] = mean_allfiles_fpr_fnr_list \n",
    "            allfile_fpr_plot_list = []\n",
    "            allfile_fnr_plot_list = []\n",
    "            for tup in mean_allfiles_fpr_fnr_list:\n",
    "                allfile_fpr_plot_list.append(tup[0])\n",
    "                allfile_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allfile_fnr_plot_list, allfile_fpr_plot_list, label = transformer.split(\"/\")[1])\n",
    "            min_dist_index = -1\n",
    "            min_dist = 100\n",
    "            for i in range(0,len(cosine_threshold_list)):\n",
    "                dist = math.sqrt(allfile_fnr_plot_list[i]*allfile_fnr_plot_list[i] + allfile_fpr_plot_list[i]*allfile_fpr_plot_list[i])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_dist_index = i\n",
    "            print(\"-------  \", transformer, \"  ------------------\")\n",
    "            print(\" least distance cosine threshold = \", cosine_threshold_list[min_dist_index])\n",
    "            print(\" least distance = \", min_dist)\n",
    "            print(\" least distnant FNR = \", plot_fnr_list[min_dist_index])\n",
    "            print(\" least distnant FPR = \", plot_fpr_list[min_dist_index])\n",
    "            print(\" number of total figures = \", total_fig_no)\n",
    "\n",
    "        sys.stdout = original_stdout\n",
    "        out_file.close()\n",
    "\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        if smooth_true == False:\n",
    "            plt.title('H4 FNR-FPR plot of multiple models')\n",
    "        else:\n",
    "            plt.title(smooth_label(smooth_no)+' H4 FNR-FPR plot of multiple models')\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        if smooth_true == False:\n",
    "            plt.savefig(\"plots/h4_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        else:\n",
    "            plt.savefig(\"plots/h4_\"+smooth_label(smooth_no)+\"_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        plt.show()\n",
    "\n",
    "    for transformer in sentence_transformer_names_list:    \n",
    "        for smooth_no in smooth_no_list:\n",
    "            allsmooth_fpr_plot_list = []\n",
    "            allsmooth_fnr_plot_list = []\n",
    "            for tup in typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer]:\n",
    "                allsmooth_fpr_plot_list.append(tup[0])\n",
    "                allsmooth_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = smooth_label(smooth_no))\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        plt.title('H4 FNR-FPR plot of '+type_html+' of '+transformer.split(\"/\")[1])\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        # if smooth_true:\n",
    "        plt.savefig(\"plots/h4_\"+transformer.split(\"/\")[1]+\" \"+type_html+\"_all_smooth_average_allchapters_allmodels.png\",dpi=1000)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "for type_html in types_html_files_list: \n",
    "    for smooth_no in smooth_no_list:\n",
    "        pickle.dump( typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no], open( \"pickles/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c78b7-dc21-4b43-b297-956d9a9f6b7e",
   "metadata": {},
   "source": [
    "# test: all models accuracy test with heuristic 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbb4f93-c869-4cca-b252-7937415acf26",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------- Heuristic 5 ---------------------\n",
    "# Consider few sentences above and below the fig as boundary.\n",
    "# subract average cosine value of all_sent_list from the cosine value of bounding box and also scaling using standard deviation of bounding box\n",
    "# Next, consider similar sentence with in the boundary as similar sentences.\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# filename = \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\"\n",
    "textbooks_html_files = [ \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/3_5239_3/OEBPS/Text/Untitled-1.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/4_5239_4/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/5_5239_5/OEBPS/Text/Untitled-3.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/6_5239_6/OEBPS/Text/Untitled-4.xhtml\"]\n",
    "papers_html_files = ['./data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-004.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-005.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-006.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-007.xhtml']\n",
    "\n",
    "filenames_list_debug = [ './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml']\n",
    "\n",
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# types_html_files_list = [\"papers\"]\n",
    "\n",
    "no_points_fpr_fnr_plot = 50\n",
    "no_bounding_sent = 30\n",
    "debug = 0\n",
    "\n",
    "if debug == 1:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_debug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_debug_dict\n",
    "if debug == 0:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_nondebug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_nondebug_dict\n",
    "\n",
    "\n",
    "smooth_true = True\n",
    "# if smooth_true:\n",
    "# smooth_no_list = [-11,-9,-7,-5,-3,0,3,5,7,9,11,100,101]\n",
    "smooth_no_list = [0,-7]\n",
    "# smooth_no_list = [0,3,9,13]\n",
    "# smooth_no_list = [0,100,101]\n",
    "# smooth_no_list = [-13,0,13,101]\n",
    "# else:\n",
    "#     smooth_no_list = [0]\n",
    "def smooth_label(smooth_no):\n",
    "    if smooth_no==100:\n",
    "        return \"9_max_smooth\"\n",
    "    elif smooth_no==101:\n",
    "        return \"13_max_smooth\"\n",
    "    elif smooth_no >0:\n",
    "        return str(smooth_no)+\"_uniform_smooth\"\n",
    "    elif smooth_no < 0:\n",
    "        return str(-1*smooth_no)+\"_gaussian_smooth\"\n",
    "    else:\n",
    "        return \"no_smooth\"\n",
    "\n",
    "typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict = {}\n",
    "for type_html in types_html_files_list:\n",
    "    typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html] = {}\n",
    "    for smooth_no in smooth_no_list:\n",
    "        typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no] = {}\n",
    "\n",
    "for type_html in types_html_files_list:\n",
    "    \n",
    "    for smooth_no in smooth_no_list:\n",
    "        if smooth_no==0:\n",
    "            smooth_true=False\n",
    "        else:\n",
    "            smooth_true=True\n",
    "        fpr_fnr_plot_dict = {}\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            fpr_fnr_plot_dict[transformer] = []\n",
    "\n",
    "        total_fig_no  = 0\n",
    "        file_no = 0\n",
    "        filenames_list = []\n",
    "        if type_html == \"textbooks\":\n",
    "            filenames_list = textbooks_html_files\n",
    "        if type_html == \"papers\":\n",
    "            filenames_list = papers_html_files\n",
    "        if debug ==  1:\n",
    "            filenames_list = filenames_list_debug\n",
    "            \n",
    "        for filename in filenames_list:\n",
    "            try:\n",
    "                fp = open(filename, encoding=\"utf8\")\n",
    "                print(\"++++++++++++++++++++++++++++   START   +++++++++++++++++++++\")\n",
    "                print(filename)\n",
    "            except:\n",
    "                print(\"***************  Error while opening file   ***************\")\n",
    "                print(filename)\n",
    "                print(\"************************************************************\")\n",
    "            extension = filename.split(\".\")[-1]\n",
    "            if extension == \"xhtml\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   XML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml-xml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            elif extension == \"html\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   HTML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "            fp.close()\n",
    "            fig_no = 0\n",
    "            max_caption_words = 60\n",
    "            min_caption_words = 3\n",
    "\n",
    "            full_text = soup.get_text()\n",
    "\n",
    "            # uncomment for debugging \n",
    "            # output_file = open(\"output_full_text.txt\",\"w\")\n",
    "            # output_file.write(remove_non_ascii(full_text))\n",
    "            # output_file.close()\n",
    "            # output_file = open(\"output_full_text_str.txt\",\"w\")\n",
    "            # output_file.write(str(full_text.encode('utf8')))\n",
    "            # output_file.close()\n",
    "\n",
    "            full_text = remove_multi_newlines(full_text)\n",
    "            all_sent_list = split_text_to_sent(full_text)\n",
    "\n",
    "            all_sentence_embeddings = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                all_sentence_embeddings[transformer] = sentence_transformer_model.encode(all_sent_list)\n",
    "\n",
    "            fpr_fnr_list_fig = []\n",
    "            fpr_fnr_dict_fig = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_dict_fig[transformer] = []\n",
    "            \n",
    "            all_fig = soup.find_all(string=re.compile(\"^Fig\"))\n",
    "\n",
    "            for fig in all_fig:\n",
    "                print(\"------- fig  \"+str(fig_no)+\"   --------\")\n",
    "                \n",
    "                fig_caption = extract_caption(fig, min_caption_words, max_caption_words)\n",
    "                image_path = extract_imagepath(fig, filename, type_html)\n",
    "\n",
    "                # ocr_full_txt_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "                if type_html == \"textbooks\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if type_html == \"papers\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if debug != 1:\n",
    "                    plt.imshow(mpimg.imread(image_path))\n",
    "                    plt.show()\n",
    "                if os.path.isfile(ocr_json_path):\n",
    "                    print(\"    Json already exist :  \"+ocr_json_path)\n",
    "                    json_file = open(ocr_json_path)\n",
    "                    ocr_dict = json.load(json_file) \n",
    "                    #structure of ocr_dict ={\"block\":[ {\"text\":\"\",\"\",\"paragraph\":[{\"text\":\"\", \"word\":[\"w1\",\"w2\",..], \"bounding_poly\": [[],[],..]}, { .. }, ...],\"bounding_poly\":[[(x1,y1),..(x4,y4)],[]...]}, { ... }, ..],\"bounding_poly\":[[],[]...]}\n",
    "                else:\n",
    "                    print(\"  $$$$  Calling google vision API $$$$ \")\n",
    "                    ocr_dict = detect_document(image_path, type_html)\n",
    "                labels_list = []\n",
    "                print(\"~~~~~~~ Labels  ~~~~~~~~\")\n",
    "                if len(ocr_dict[\"block\"]) > 0:\n",
    "                    for i in range(0,len(ocr_dict[\"block\"])):\n",
    "                        block_dict = ocr_dict[\"block\"][i]\n",
    "                        print(block_dict[\"text\"])\n",
    "                        labels_list.append(block_dict[\"text\"].replace(\"-\",\"\"))\n",
    "                        # print(block_dict[\"text\"], \"  \",ocr_dict[\"bounding_poly\"][i])\n",
    "\n",
    "                        # for i in range(0,len(block_dict[\"paragraph\"])):\n",
    "                        #     para_dict = block_dict[\"paragraph\"][i]\n",
    "                        #     print(\"      -\",para_dict[\"text\"])\n",
    "                        #     print(\"      \",para_dict[\"text\"],\"   \",block_dict[\"bounding_poly\"][i])\n",
    "                else:\n",
    "                    print(\"  NO Labels  \")\n",
    "                print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "                if type_html == \"textbooks\":\n",
    "                    above_header = fig.parent.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if above_header is not None:\n",
    "                        above_header_text = above_header.get_text()\n",
    "                    else:\n",
    "                        above_header_text = \"\"\n",
    "                    while clean_string(above_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(above_header_text).replace(\" \",\"\"))<3:\n",
    "                        if above_header is not None:\n",
    "                            above_header = above_header.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if above_header is not None:\n",
    "                            above_header_text = above_header.get_text()\n",
    "                        else:\n",
    "                            above_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    below_header = fig.parent.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if below_header is not None:\n",
    "                        below_header_text = below_header.get_text()\n",
    "                    else:\n",
    "                        below_header_text = \"\"\n",
    "                    while clean_string(below_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(below_header_text).replace(\" \",\"\"))<3:\n",
    "                        if below_header is not None:\n",
    "                            below_header = below_header.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if below_header is not None:\n",
    "                            below_header_text = below_header.get_text()\n",
    "                        else:\n",
    "                            below_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    print(\"\\nabove header = \",above_header_text)\n",
    "                    print(\"fig caption = \",fig_caption)\n",
    "                    print(\"below header = \",below_header_text)\n",
    "\n",
    "                    section_text = above_header_text\n",
    "                    next_header = above_header.find_next()\n",
    "\n",
    "                    while next_header.get_text() != below_header.get_text():\n",
    "                        # section_text = section_text + next_header.text\n",
    "                        for child in next_header.contents:\n",
    "                            if isinstance(child, str):\n",
    "                                # if len(clean_string(child).replace(\" \",\"\")) > 1:\n",
    "                                section_text = section_text + child\n",
    "                                # print(\"---------------\\n\",clean_string(child),\"\\n\")\n",
    "                        next_header = next_header.find_next()\n",
    "                        section_text = section_text + \"\\n\"\n",
    "\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    # print(\"---------\\n\",section_text,\"\\n--------------\")\n",
    "                    # print(\"++++++\\n\",full_text.encode(\"utf-8\"),\"\\n++++++++\")\n",
    "                if type_html == \"papers\":\n",
    "                    print(\"Figure caption = \",fig_caption)\n",
    "                    relevant_txt_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\"_relevant.txt\"\n",
    "                    relevant_file = open(relevant_txt_path,\"r\", encoding='utf-8')\n",
    "                    print(relevant_txt_path + \"  read!\")\n",
    "                    section_text = relevant_file.read()\n",
    "                    # print(\"\\n!!! section text !!! \\n\", section_text, \"\\n!!!!!!!!\")\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    print (\"Length of sent list = \", len(sent_list))\n",
    "                    # for inx in range(0,len(sent_list)):\n",
    "                    #     print( str(inx)+\": \",f\"{bcolors.OKBLUE}{sent_list[inx]}{bcolors.ENDC}\")\n",
    "                    relevant_file.close()\n",
    "                                        \n",
    "                str_labels_list = str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "\n",
    "                if len(labels_list) > 0:\n",
    "                    fig_caption_sent = fig_caption +\". This figure contains \"+ str_labels_list +\".\"\n",
    "                    # fig_caption_sent = fig_caption + \" and this figure contains \"+ str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")+ \" as labels.\"\n",
    "                else:\n",
    "                    fig_caption_sent = fig_caption\n",
    "\n",
    "\n",
    "                fig_caption_sent = clean_string(fig_caption_sent)\n",
    "\n",
    "                print(\"Fig caption sentence = \",fig_caption_sent)\n",
    "\n",
    "\n",
    "                moving_average_no = smooth_no                 #takes only odd numbers\n",
    "\n",
    "                no_space_sent_list = []\n",
    "                for sent in sent_list:\n",
    "                    no_space_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                no_space_all_sent_list = []\n",
    "                for sent in all_sent_list:\n",
    "                    no_space_all_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                fig_caption_clean = remove_multi_newlines(fig_caption)\n",
    "                fig_caption_start = split_text_to_sent(fig_caption_clean)[0]\n",
    "                fig_caption_index = -1\n",
    "                if fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_all_sent_list:\n",
    "                    fig_caption_index = no_space_all_sent_list.index(fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                for transformer in sentence_transformer_names_list:\n",
    "                    sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                    fig_caption_embedding = sentence_transformer_model.encode(fig_caption_sent)\n",
    "                    sentence_embeddings = sentence_transformer_model.encode(sent_list)\n",
    "\n",
    "\n",
    "                    cosine_list = cosine_similarity([fig_caption_embedding],sentence_embeddings)[0]\n",
    "                    all_cosine_list = cosine_similarity([fig_caption_embedding],all_sentence_embeddings[transformer])[0]\n",
    "\n",
    "                    sect_index_list = []\n",
    "                    sect_cosine_list = []\n",
    "                    check_no_space_sent_list = no_space_sent_list.copy()\n",
    "                    for index in range(0,len(all_sent_list)):\n",
    "                        if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "                            sect_index_list.append(index)\n",
    "                            sect_cosine_list.append(all_cosine_list[index])\n",
    "                            check_no_space_sent_list.remove(all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                    if len(sect_index_list) != len(sent_list):\n",
    "                        print(\"!!!!! ERROR !!!!\")\n",
    "                        print(\"start index of section = \",sect_index_list[0])\n",
    "                        print(\"end index of section = \",sect_index_list[-1])\n",
    "                        print(\"indices of section = \",sect_index_list)\n",
    "                        print(\"len of matched sent = \",len(sect_index_list))\n",
    "                        print(\"len of section sent = \",len(sent_list))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = \"non_smooth\")\n",
    "                        plt.vlines(x = sect_index_list[0], ymin = 0, ymax = 1, colors = 'green', label = 'section_start')\n",
    "                        plt.vlines(x = sect_index_list[-1], ymin = 0, ymax = 1, colors = 'red', label = 'section_end')\n",
    "\n",
    "                    if smooth_true:\n",
    "                        all_cosine_list = moving_average(np.array(all_cosine_list),moving_average_no).tolist()\n",
    "                        if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                            plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = smooth_label(smooth_no))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.title(\"cosine simlairity scores\")\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "\n",
    "                    average_cosine_value = np.average(all_cosine_list[max(0,fig_caption_index - no_bounding_sent):min(fig_caption_index + no_bounding_sent,len(all_cosine_list))])\n",
    "                    sigma_cosine_value = np.std(all_cosine_list[max(0,fig_caption_index - no_bounding_sent):min(fig_caption_index + no_bounding_sent,len(all_cosine_list))])\n",
    "\n",
    "                    cosine_threshold_list = np.arange(-3.0, 3.0001 ,(6.0/no_points_fpr_fnr_plot))\n",
    "                    cosine_threshold_list = np.around(cosine_threshold_list, 4)\n",
    "                    fpr_fnr_list_threshold = []\n",
    "                    for cosine_threshold in cosine_threshold_list:\n",
    "                        no_sim_sent = 0\n",
    "                        no_sim_sent_in_sect = 0\n",
    "                        no_sent_in_sect = len(sect_index_list)\n",
    "                        no_sim_sent_not_in_sect = 0\n",
    "                        prediction_index_list=[]\n",
    "                        for index in range(0,len(all_sent_list)):\n",
    "                            cosine_value = all_cosine_list[index]\n",
    "                            # if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_sent_list:\n",
    "                            #     sect_index_list.append(index)\n",
    "                            if fig_caption_index!=-1:\n",
    "                                if cosine_value - average_cosine_value  >= cosine_threshold*sigma_cosine_value and abs(index-fig_caption_index)<=no_bounding_sent:\n",
    "                                    prediction_index_list.append(index)\n",
    "                            else:\n",
    "                                if cosine_value - average_cosine_value  >= cosine_threshold*sigma_cosine_value:\n",
    "                                    prediction_index_list.append(index)\n",
    "                        no_sim_sent = len(prediction_index_list)\n",
    "                        for index in prediction_index_list:\n",
    "                            if index in sect_index_list:\n",
    "                                no_sim_sent_in_sect = no_sim_sent_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "                            else:\n",
    "                                no_sim_sent_not_in_sect = no_sim_sent_not_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",all_sent_list[index])\n",
    "\n",
    "                        if no_sim_sent != 0:\n",
    "                            false_positive_rate = no_sim_sent_not_in_sect/no_sim_sent\n",
    "                        else:\n",
    "                            false_positive_rate = 0.0\n",
    "                        if no_sent_in_sect != 0:\n",
    "                            false_negative_rate = (no_sent_in_sect - no_sim_sent_in_sect)/no_sent_in_sect\n",
    "                        else:\n",
    "                            false_negative_rate = 0.0\n",
    "\n",
    "                        fpr_fnr_list_threshold.append((false_positive_rate,false_negative_rate))\n",
    "\n",
    "                        # print(\"no_sim_sent = \",no_sim_sent)\n",
    "                        # print(\"no_sent_in_sect = \",no_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_in_sect = \",no_sim_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_not_in_sect = \",no_sim_sent_not_in_sect)\n",
    "                        # print(\"false_positive_rate = \",false_positive_rate)\n",
    "                        # print(\"false_negative_rate = \",false_negative_rate)\n",
    "                        # print(\"cosine_threshold = \",cosine_threshold)\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "                        # for index in range(0,len(sent_list)):\n",
    "                        #     cosine_value = cosine_list[index]\n",
    "                        #     if cosine_value > cosine_threshold:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{sent_list[index]}{bcolors.ENDC}\")\n",
    "                        #     else:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",sent_list[index])\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "                    fpr_fnr_dict_fig[transformer].append(fpr_fnr_list_threshold)\n",
    "\n",
    "                section_text_caption_labels = section_text + \" \" +fig_caption_sent\n",
    "\n",
    "                fig_no = fig_no + 1\n",
    "\n",
    "            marker_dict = {'sentence-transformers/all-mpnet-base-v2':\"s\",'sentence-transformers/multi-qa-MiniLM-L6-cos-v1':\"o\",'sentence-transformers/all-MiniLM-L6-v2':\"P\",'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2':\"v\",'sentence-transformers/bert-base-nli-mean-tokens':\"X\"}\n",
    "\n",
    "            textbook_name = filename.split(\"/\")[4]+\"_\"+(filename.split(\"/\")[-1]).split(\".\")[0]\n",
    "            # class_name = filename.split(\"/\")[3]\n",
    "            class_name = smooth_label(smooth_no)+\"_\"+filename.split(\"/\")[3] # while smooth\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_list_fig = fpr_fnr_dict_fig[transformer]\n",
    "                np_fpr_fnr_list_fig = np.array(fpr_fnr_list_fig)\n",
    "                mean_fpr_fnr_list = (np.mean(np_fpr_fnr_list_fig, axis=0)).tolist()\n",
    "                fpr_fnr_plot_dict[transformer].append(mean_fpr_fnr_list)\n",
    "                plot_fpr_list = []\n",
    "                plot_fnr_list = []\n",
    "\n",
    "                for tup in mean_fpr_fnr_list:\n",
    "                    plot_fpr_list.append(tup[0])\n",
    "                    plot_fnr_list.append(tup[1])\n",
    "\n",
    "        #         plt.scatter(plot_fnr_list, plot_fpr_list, label = transformer.split(\"/\")[1], marker = marker_dict[transformer], c = cosine_threshold_list, cmap = \"jet\", vmin=0, vmax=1)\n",
    "        #         plt.xlabel('False Negative Rate (FNR)')\n",
    "        #         plt.ylabel('False Positve Rate (FPR)')\n",
    "        #         if moving_average_no == 0:\n",
    "        #             plt.title('FNR-FPR plot')\n",
    "        #         else:\n",
    "        #             plt.title(str(moving_average_no)+' smooth FNR-FPR plot')\n",
    "        #         plt.colorbar(label=\"Cosine treshold\", orientation=\"vertical\")\n",
    "        #         plt.grid(True)\n",
    "        #         plt.legend(loc=\"lower left\",fontsize=\"small\")\n",
    "\n",
    "        #         plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_\"+transformer.replace(\"/\",\"_\")+\"_scatter.png\",dpi=1000)\n",
    "        #         plt.show()\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                mean_plot_fpr_list = []\n",
    "                mean_plot_fnr_list = []\n",
    "                for tup in fpr_fnr_plot_dict[transformer][file_no]:\n",
    "                    mean_plot_fpr_list.append(tup[0])\n",
    "                    mean_plot_fnr_list.append(tup[1])\n",
    "                plt.plot(mean_plot_fnr_list,mean_plot_fpr_list, label = transformer.split(\"/\")[1])    \n",
    "\n",
    "            plt.xlabel('False Negative Rate (FNR)')\n",
    "            plt.ylabel('False Positve Rate (FPR)')\n",
    "            if smooth_true == False:\n",
    "                plt.title('H5 FNR-FPR plot of multiple models')\n",
    "            else:\n",
    "                plt.title(smooth_label(smooth_no)+' H5 FNR-FPR plot of multiple models')\n",
    "            plt.grid(True)\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.axis('square')\n",
    "            plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "            # plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_allmodels.png\",dpi=1000)\n",
    "            plt.show()\n",
    "\n",
    "            file_no = file_no + 1\n",
    "            total_fig_no = total_fig_no + fig_no\n",
    "            print(\"++++++++++++++++++++++++++++   END   +++++++++++++++++++++\\n\")\n",
    "\n",
    "        original_stdout = sys.stdout\n",
    "        if smooth_true:\n",
    "            out_file = open(\"plots/h5_\"+smooth_label(smooth_no)+'_'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        else:\n",
    "            out_file = open('plots/h5_'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        sys.stdout = out_file\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            mean_allfiles_fpr_fnr_list = (np.mean(np.array(fpr_fnr_plot_dict[transformer]), axis=0)).tolist()\n",
    "            typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer] = mean_allfiles_fpr_fnr_list \n",
    "            allfile_fpr_plot_list = []\n",
    "            allfile_fnr_plot_list = []\n",
    "            for tup in mean_allfiles_fpr_fnr_list:\n",
    "                allfile_fpr_plot_list.append(tup[0])\n",
    "                allfile_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allfile_fnr_plot_list, allfile_fpr_plot_list, label = transformer.split(\"/\")[1])\n",
    "            min_dist_index = -1\n",
    "            min_dist = 100\n",
    "            for i in range(0,len(cosine_threshold_list)):\n",
    "                dist = math.sqrt(allfile_fnr_plot_list[i]*allfile_fnr_plot_list[i] + allfile_fpr_plot_list[i]*allfile_fpr_plot_list[i])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_dist_index = i\n",
    "            print(\"-------  \", transformer, \"  ------------------\")\n",
    "            print(\" least distance cosine threshold = \", cosine_threshold_list[min_dist_index])\n",
    "            print(\" least distance = \", min_dist)\n",
    "            print(\" least distnant FNR = \", plot_fnr_list[min_dist_index])\n",
    "            print(\" least distnant FPR = \", plot_fpr_list[min_dist_index])\n",
    "            print(\" number of total figures = \", total_fig_no)\n",
    "\n",
    "        sys.stdout = original_stdout\n",
    "        out_file.close()\n",
    "\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        if smooth_true == False:\n",
    "            plt.title('H5 FNR-FPR plot of multiple models')\n",
    "        else:\n",
    "            plt.title(smooth_label(smooth_no)+' H5 FNR-FPR plot of multiple models')\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        if smooth_true == False:\n",
    "            plt.savefig(\"plots/h5_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        else:\n",
    "            plt.savefig(\"plots/h5_\"+smooth_label(smooth_no)+\"_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        plt.show()\n",
    "\n",
    "    for transformer in sentence_transformer_names_list:    \n",
    "        for smooth_no in smooth_no_list:\n",
    "            allsmooth_fpr_plot_list = []\n",
    "            allsmooth_fnr_plot_list = []\n",
    "            for tup in typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer]:\n",
    "                allsmooth_fpr_plot_list.append(tup[0])\n",
    "                allsmooth_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = smooth_label(smooth_no))\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        plt.title('H5 FNR-FPR plot of '+type_html+' of '+transformer.split(\"/\")[1])\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        # if smooth_true:\n",
    "        plt.savefig(\"plots/h5_\"+transformer.split(\"/\")[1]+\" \"+type_html+\"_all_smooth_average_allchapters_allmodels.png\",dpi=1000)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "for type_html in types_html_files_list: \n",
    "    for smooth_no in smooth_no_list:\n",
    "        pickle.dump( typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no], open( \"pickles/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5d9ad-1230-4bea-83cf-c002cbdb441a",
   "metadata": {},
   "source": [
    "# test: all models accuracy test with heuristic 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3d9a5-3490-4385-8d66-1bb9951d5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Heuristic 6 -------------------------------\n",
    "# Consider few sentences above and below the fig as boundary.\n",
    "# Subtract multiple of mean cosive value first from cosine value then find similarity \n",
    "# Next, consider similar sentence with in the boundary as similar sentences.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# filename = \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\"\n",
    "textbooks_html_files = [ \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/3_5239_3/OEBPS/Text/Untitled-1.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/4_5239_4/OEBPS/Text/Untitled-2.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/5_5239_5/OEBPS/Text/Untitled-3.xhtml\", \"./data/e_pathshala_epub_debug/class_9_science/6_5239_6/OEBPS/Text/Untitled-4.xhtml\"]\n",
    "papers_html_files = ['./data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-004.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-005.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-006.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-007.xhtml']\n",
    "\n",
    "filenames_list_debug = [ './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-001.xhtml', './data/technical_papers/ACMBook1/OEBPS/xhtml/b_9781450384827-chapter-002.xhtml']\n",
    "\n",
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# types_html_files_list = [\"papers\"]\n",
    "\n",
    "debug = 0\n",
    "no_bounding_sent = 50\n",
    "\n",
    "if debug == 1:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_debug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_debug_dict\n",
    "if debug == 0:\n",
    "    sentence_transformer_names_list = sentence_transformer_names_nondebug_list\n",
    "    sentence_transformer_model_dict = sentence_transformer_model_nondebug_dict\n",
    "\n",
    "\n",
    "smooth_true = True\n",
    "# if smooth_true:\n",
    "# smooth_no_list = [-11,-9,-7,-5,-3,0,3,5,7,9,11,100,101]\n",
    "smooth_no_list = [0,-7]\n",
    "# smooth_no_list = [0,3,9,13]\n",
    "# smooth_no_list = [0,100,101]\n",
    "# smooth_no_list = [-13,0,13,101]\n",
    "# else:\n",
    "#     smooth_no_list = [0]\n",
    "def smooth_label(smooth_no):\n",
    "    if smooth_no==100:\n",
    "        return \"9_max_smooth\"\n",
    "    elif smooth_no==101:\n",
    "        return \"13_max_smooth\"\n",
    "    elif smooth_no >0:\n",
    "        return str(smooth_no)+\"_uniform_smooth\"\n",
    "    elif smooth_no < 0:\n",
    "        return str(-1*smooth_no)+\"_gaussian_smooth\"\n",
    "    else:\n",
    "        return \"no_smooth\"\n",
    "\n",
    "typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict = {}\n",
    "for type_html in types_html_files_list:\n",
    "    typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html] = {}\n",
    "    for smooth_no in smooth_no_list:\n",
    "        typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no] = {}\n",
    "\n",
    "for type_html in types_html_files_list:\n",
    "    \n",
    "    for smooth_no in smooth_no_list:\n",
    "        if smooth_no==0:\n",
    "            smooth_true=False\n",
    "        else:\n",
    "            smooth_true=True\n",
    "        fpr_fnr_plot_dict = {}\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            fpr_fnr_plot_dict[transformer] = []\n",
    "\n",
    "        total_fig_no  = 0\n",
    "        file_no = 0\n",
    "        filenames_list = []\n",
    "        if type_html == \"textbooks\":\n",
    "            filenames_list = textbooks_html_files\n",
    "        if type_html == \"papers\":\n",
    "            filenames_list = papers_html_files\n",
    "        if debug ==  1:\n",
    "            filenames_list = filenames_list_debug\n",
    "            \n",
    "        for filename in tqdm(filenames_list):\n",
    "            try:\n",
    "                fp = open(filename, encoding=\"utf8\")\n",
    "                print(\"++++++++++++++++++++++++++++   START   +++++++++++++++++++++\")\n",
    "                print(filename)\n",
    "            except:\n",
    "                print(\"***************  Error while opening file   ***************\")\n",
    "                print(filename)\n",
    "                print(\"************************************************************\")\n",
    "            extension = filename.split(\".\")[-1]\n",
    "            if extension == \"xhtml\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   XML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml-xml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "            elif extension == \"html\":\n",
    "                try:\n",
    "                    print(\"@@@@@@@@@@@@   HTML parser @@@@@@@@@@@@\")\n",
    "                    soup = BeautifulSoup(fp, 'lxml')\n",
    "                except Exception as e:\n",
    "                    print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "                    print(e)\n",
    "                    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "            fp.close()\n",
    "            fig_no = 0\n",
    "            max_caption_words = 60\n",
    "            min_caption_words = 3\n",
    "\n",
    "            full_text = soup.get_text()\n",
    "\n",
    "            # uncomment for debugging \n",
    "            # output_file = open(\"output_full_text.txt\",\"w\")\n",
    "            # output_file.write(remove_non_ascii(full_text))\n",
    "            # output_file.close()\n",
    "            # output_file = open(\"output_full_text_str.txt\",\"w\")\n",
    "            # output_file.write(str(full_text.encode('utf8')))\n",
    "            # output_file.close()\n",
    "\n",
    "            full_text = remove_multi_newlines(full_text)\n",
    "            all_sent_list = split_text_to_sent(full_text)\n",
    "\n",
    "            all_sentence_embeddings = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                all_sentence_embeddings[transformer] = sentence_transformer_model.encode(all_sent_list)\n",
    "\n",
    "            fpr_fnr_list_fig = []\n",
    "            fpr_fnr_dict_fig = {}\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_dict_fig[transformer] = []\n",
    "            \n",
    "            all_fig = soup.find_all(string=re.compile(\"^Fig\"))\n",
    "\n",
    "            for fig in tqdm(all_fig):\n",
    "                print(\"------- fig  \"+str(fig_no)+\"   --------\")\n",
    "                \n",
    "                fig_caption = extract_caption(fig, min_caption_words, max_caption_words)\n",
    "                image_path = extract_imagepath(fig, filename, type_html)\n",
    "\n",
    "                # ocr_full_txt_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "                if type_html == \"textbooks\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if type_html == \"papers\":\n",
    "                    ocr_json_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\".json\"\n",
    "                if debug != 1:\n",
    "                    plt.imshow(mpimg.imread(image_path))\n",
    "                    plt.show()\n",
    "                if os.path.isfile(ocr_json_path):\n",
    "                    print(\"    Json already exist :  \"+ocr_json_path)\n",
    "                    json_file = open(ocr_json_path)\n",
    "                    ocr_dict = json.load(json_file) \n",
    "                    #structure of ocr_dict ={\"block\":[ {\"text\":\"\",\"\",\"paragraph\":[{\"text\":\"\", \"word\":[\"w1\",\"w2\",..], \"bounding_poly\": [[],[],..]}, { .. }, ...],\"bounding_poly\":[[(x1,y1),..(x4,y4)],[]...]}, { ... }, ..],\"bounding_poly\":[[],[]...]}\n",
    "                else:\n",
    "                    print(\"  $$$$  Calling google vision API $$$$ \")\n",
    "                    ocr_dict = detect_document(image_path, type_html)\n",
    "                labels_list = []\n",
    "                print(\"~~~~~~~ Labels  ~~~~~~~~\")\n",
    "                if len(ocr_dict[\"block\"]) > 0:\n",
    "                    for i in range(0,len(ocr_dict[\"block\"])):\n",
    "                        block_dict = ocr_dict[\"block\"][i]\n",
    "                        print(block_dict[\"text\"])\n",
    "                        labels_list.append(block_dict[\"text\"].replace(\"-\",\"\"))\n",
    "                        # print(block_dict[\"text\"], \"  \",ocr_dict[\"bounding_poly\"][i])\n",
    "\n",
    "                        # for i in range(0,len(block_dict[\"paragraph\"])):\n",
    "                        #     para_dict = block_dict[\"paragraph\"][i]\n",
    "                        #     print(\"      -\",para_dict[\"text\"])\n",
    "                        #     print(\"      \",para_dict[\"text\"],\"   \",block_dict[\"bounding_poly\"][i])\n",
    "                else:\n",
    "                    print(\"  NO Labels  \")\n",
    "                print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "                if type_html == \"textbooks\":\n",
    "                    above_header = fig.parent.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if above_header is not None:\n",
    "                        above_header_text = above_header.get_text()\n",
    "                    else:\n",
    "                        above_header_text = \"\"\n",
    "                    while clean_string(above_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(above_header_text).replace(\" \",\"\"))<3:\n",
    "                        if above_header is not None:\n",
    "                            above_header = above_header.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if above_header is not None:\n",
    "                            above_header_text = above_header.get_text()\n",
    "                        else:\n",
    "                            above_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    below_header = fig.parent.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                    if below_header is not None:\n",
    "                        below_header_text = below_header.get_text()\n",
    "                    else:\n",
    "                        below_header_text = \"\"\n",
    "                    while clean_string(below_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(below_header_text).replace(\" \",\"\"))<3:\n",
    "                        if below_header is not None:\n",
    "                            below_header = below_header.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "                        if below_header is not None:\n",
    "                            below_header_text = below_header.get_text()\n",
    "                        else:\n",
    "                            below_header_text = \"\"\n",
    "                            break\n",
    "\n",
    "                    print(\"\\nabove header = \",above_header_text)\n",
    "                    print(\"fig caption = \",fig_caption)\n",
    "                    print(\"below header = \",below_header_text)\n",
    "\n",
    "                    section_text = above_header_text\n",
    "                    next_header = above_header.find_next()\n",
    "\n",
    "                    while next_header.get_text() != below_header.get_text():\n",
    "                        # section_text = section_text + next_header.text\n",
    "                        for child in next_header.contents:\n",
    "                            if isinstance(child, str):\n",
    "                                # if len(clean_string(child).replace(\" \",\"\")) > 1:\n",
    "                                section_text = section_text + child\n",
    "                                # print(\"---------------\\n\",clean_string(child),\"\\n\")\n",
    "                        next_header = next_header.find_next()\n",
    "                        section_text = section_text + \"\\n\"\n",
    "\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    # print(\"---------\\n\",section_text,\"\\n--------------\")\n",
    "                    # print(\"++++++\\n\",full_text.encode(\"utf-8\"),\"\\n++++++++\")\n",
    "                if type_html == \"papers\":\n",
    "                    print(\"Figure caption = \",fig_caption)\n",
    "                    relevant_txt_path = image_path.split(\"/OEBPS/images/\")[0]+\"/OEBPS/images/\"+image_path.split(\"/OEBPS/images/\")[1].split(\".\")[0]+\"_relevant.txt\"\n",
    "                    relevant_file = open(relevant_txt_path,\"r\", encoding='utf-8')\n",
    "                    print(relevant_txt_path + \"  read!\")\n",
    "                    section_text = relevant_file.read()\n",
    "                    # print(\"\\n!!! section text !!! \\n\", section_text, \"\\n!!!!!!!!\")\n",
    "                    section_text = remove_multi_newlines(section_text)\n",
    "                    sent_list = split_text_to_sent(section_text)\n",
    "                    print (\"Length of sent list = \", len(sent_list))\n",
    "                    # for inx in range(0,len(sent_list)):\n",
    "                    #     print( str(inx)+\": \",f\"{bcolors.OKBLUE}{sent_list[inx]}{bcolors.ENDC}\")\n",
    "                    relevant_file.close()\n",
    "                                        \n",
    "                str_labels_list = str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "\n",
    "                if len(labels_list) > 0:\n",
    "                    fig_caption_sent = fig_caption +\". This figure contains \"+ str_labels_list +\".\"\n",
    "                    # fig_caption_sent = fig_caption + \" and this figure contains \"+ str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")+ \" as labels.\"\n",
    "                else:\n",
    "                    fig_caption_sent = fig_caption\n",
    "\n",
    "\n",
    "                fig_caption_sent = clean_string(fig_caption_sent)\n",
    "\n",
    "                print(\"Fig caption sentence = \",fig_caption_sent)\n",
    "\n",
    "\n",
    "                moving_average_no = smooth_no                 #takes only odd numbers\n",
    "\n",
    "                no_space_sent_list = []\n",
    "                for sent in sent_list:\n",
    "                    no_space_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                no_space_all_sent_list = []\n",
    "                for sent in all_sent_list:\n",
    "                    no_space_all_sent_list.append(sent.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                fig_caption_clean = remove_multi_newlines(fig_caption)\n",
    "                fig_caption_start = split_text_to_sent(fig_caption_clean)[0]\n",
    "                fig_caption_index = -1\n",
    "                if fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_all_sent_list:\n",
    "                    fig_caption_index = no_space_all_sent_list.index(fig_caption_start.replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                for transformer in sentence_transformer_names_list:\n",
    "                    sentence_transformer_model = sentence_transformer_model_dict[transformer]\n",
    "                    fig_caption_embedding = sentence_transformer_model.encode(fig_caption_sent)\n",
    "                    sentence_embeddings = sentence_transformer_model.encode(sent_list)\n",
    "\n",
    "\n",
    "                    cosine_list = cosine_similarity([fig_caption_embedding],sentence_embeddings)[0]\n",
    "                    all_cosine_list = cosine_similarity([fig_caption_embedding],all_sentence_embeddings[transformer])[0]\n",
    "\n",
    "                    sect_index_list = []\n",
    "                    sect_cosine_list = []\n",
    "                    check_no_space_sent_list = no_space_sent_list.copy()\n",
    "                    for index in range(0,len(all_sent_list)):\n",
    "                        if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in check_no_space_sent_list:\n",
    "                            sect_index_list.append(index)\n",
    "                            sect_cosine_list.append(all_cosine_list[index])\n",
    "                            check_no_space_sent_list.remove(all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "                    if len(sect_index_list) != len(sent_list):\n",
    "                        print(\"!!!!! ERROR !!!!\")\n",
    "                        print(\"start index of section = \",sect_index_list[0])\n",
    "                        print(\"end index of section = \",sect_index_list[-1])\n",
    "                        print(\"indices of section = \",sect_index_list)\n",
    "                        print(\"len of matched sent = \",len(sect_index_list))\n",
    "                        print(\"len of section sent = \",len(sent_list))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = \"non_smooth\")\n",
    "                        plt.vlines(x = sect_index_list[0], ymin = 0, ymax = 1, colors = 'green', label = 'section_start')\n",
    "                        plt.vlines(x = sect_index_list[-1], ymin = 0, ymax = 1, colors = 'red', label = 'section_end')\n",
    "\n",
    "                    if smooth_true:\n",
    "                        all_cosine_list = moving_average(np.array(all_cosine_list),moving_average_no).tolist()\n",
    "                        if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                            plt.plot(list(range(0,len(all_sent_list))), all_cosine_list, label = smooth_label(smooth_no))\n",
    "                    if transformer == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "                        plt.title(\"cosine simlairity scores\")\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "\n",
    "                    cosine_threshold_list = np.arange(-1.0, 3.0001 ,(2/no_points_fpr_fnr_plot))\n",
    "                    cosine_threshold_list = np.around(cosine_threshold_list, 4)\n",
    "                    average_cosine_value = np.average(all_cosine_list)\n",
    "                    fpr_fnr_list_threshold = []\n",
    "                    for cosine_threshold in cosine_threshold_list:\n",
    "                        no_sim_sent = 0\n",
    "                        no_sim_sent_in_sect = 0\n",
    "                        no_sent_in_sect = len(sect_index_list)\n",
    "                        no_sim_sent_not_in_sect = 0\n",
    "                        prediction_index_list=[]\n",
    "                        for index in range(0,len(all_sent_list)):\n",
    "                            cosine_value = all_cosine_list[index]\n",
    "                            # if all_sent_list[index].replace(\" \",\"\").replace(\"\\t\",\"\") in no_space_sent_list:\n",
    "                            #     sect_index_list.append(index)\n",
    "                            if fig_caption_index!=-1:\n",
    "                                if cosine_value - average_cosine_value >= cosine_threshold*average_cosine_value and abs(index-fig_caption_index)<=no_bounding_sent:\n",
    "                                    prediction_index_list.append(index)\n",
    "                            else:\n",
    "                                if cosine_value - average_cosine_value >= cosine_threshold*average_cosine_value:\n",
    "                                    prediction_index_list.append(index)\n",
    "                        no_sim_sent = len(prediction_index_list)\n",
    "                        for index in prediction_index_list:\n",
    "                            if index in sect_index_list:\n",
    "                                no_sim_sent_in_sect = no_sim_sent_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{all_sent_list[index]}{bcolors.ENDC}\")\n",
    "                            else:\n",
    "                                no_sim_sent_not_in_sect = no_sim_sent_not_in_sect + 1\n",
    "                                # print(cosine_value,\" # \",cosine_threshold,\" # \",all_sent_list[index])\n",
    "\n",
    "                        if no_sim_sent != 0:\n",
    "                            false_positive_rate = no_sim_sent_not_in_sect/no_sim_sent\n",
    "                        else:\n",
    "                            false_positive_rate = 0.0\n",
    "                        if no_sent_in_sect != 0:\n",
    "                            false_negative_rate = (no_sent_in_sect - no_sim_sent_in_sect)/no_sent_in_sect\n",
    "                        else:\n",
    "                            false_negative_rate = 0.0\n",
    "\n",
    "                        fpr_fnr_list_threshold.append((false_positive_rate,false_negative_rate))\n",
    "\n",
    "                        # print(\"no_sim_sent = \",no_sim_sent)\n",
    "                        # print(\"no_sent_in_sect = \",no_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_in_sect = \",no_sim_sent_in_sect)\n",
    "                        # print(\"no_sim_sent_not_in_sect = \",no_sim_sent_not_in_sect)\n",
    "                        # print(\"false_positive_rate = \",false_positive_rate)\n",
    "                        # print(\"false_negative_rate = \",false_negative_rate)\n",
    "                        # print(\"cosine_threshold = \",cosine_threshold)\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "                        # for index in range(0,len(sent_list)):\n",
    "                        #     cosine_value = cosine_list[index]\n",
    "                        #     if cosine_value > cosine_threshold:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",f\"{bcolors.OKGREEN}{sent_list[index]}{bcolors.ENDC}\")\n",
    "                        #     else:\n",
    "                        #         print(cosine_value,\" # \",cosine_threshold,\" # \",sent_list[index])\n",
    "                        # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "                    fpr_fnr_dict_fig[transformer].append(fpr_fnr_list_threshold)\n",
    "\n",
    "                section_text_caption_labels = section_text + \" \" +fig_caption_sent\n",
    "\n",
    "                fig_no = fig_no + 1\n",
    "\n",
    "            marker_dict = {'sentence-transformers/all-mpnet-base-v2':\"s\",'sentence-transformers/multi-qa-MiniLM-L6-cos-v1':\"o\",'sentence-transformers/all-MiniLM-L6-v2':\"P\",'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2':\"v\",'sentence-transformers/bert-base-nli-mean-tokens':\"X\"}\n",
    "\n",
    "            textbook_name = filename.split(\"/\")[4]+\"_\"+(filename.split(\"/\")[-1]).split(\".\")[0]\n",
    "            # class_name = filename.split(\"/\")[3]\n",
    "            class_name = smooth_label(smooth_no)+\"_\"+filename.split(\"/\")[3] # while smooth\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                fpr_fnr_list_fig = fpr_fnr_dict_fig[transformer]\n",
    "                np_fpr_fnr_list_fig = np.array(fpr_fnr_list_fig)\n",
    "                mean_fpr_fnr_list = (np.mean(np_fpr_fnr_list_fig, axis=0)).tolist()\n",
    "                fpr_fnr_plot_dict[transformer].append(mean_fpr_fnr_list)\n",
    "                plot_fpr_list = []\n",
    "                plot_fnr_list = []\n",
    "\n",
    "                for tup in mean_fpr_fnr_list:\n",
    "                    plot_fpr_list.append(tup[0])\n",
    "                    plot_fnr_list.append(tup[1])\n",
    "\n",
    "        #         plt.scatter(plot_fnr_list, plot_fpr_list, label = transformer.split(\"/\")[1], marker = marker_dict[transformer], c = cosine_threshold_list, cmap = \"jet\", vmin=0, vmax=1)\n",
    "        #         plt.xlabel('False Negative Rate (FNR)')\n",
    "        #         plt.ylabel('False Positve Rate (FPR)')\n",
    "        #         if moving_average_no == 0:\n",
    "        #             plt.title('FNR-FPR plot')\n",
    "        #         else:\n",
    "        #             plt.title(str(moving_average_no)+' smooth FNR-FPR plot')\n",
    "        #         plt.colorbar(label=\"Cosine treshold\", orientation=\"vertical\")\n",
    "        #         plt.grid(True)\n",
    "        #         plt.legend(loc=\"lower left\",fontsize=\"small\")\n",
    "\n",
    "        #         plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_\"+transformer.replace(\"/\",\"_\")+\"_scatter.png\",dpi=1000)\n",
    "        #         plt.show()\n",
    "\n",
    "            for transformer in sentence_transformer_names_list:\n",
    "                mean_plot_fpr_list = []\n",
    "                mean_plot_fnr_list = []\n",
    "                for tup in fpr_fnr_plot_dict[transformer][file_no]:\n",
    "                    mean_plot_fpr_list.append(tup[0])\n",
    "                    mean_plot_fnr_list.append(tup[1])\n",
    "                plt.plot(mean_plot_fnr_list,mean_plot_fpr_list, label = transformer.split(\"/\")[1])    \n",
    "\n",
    "            plt.xlabel('False Negative Rate (FNR)')\n",
    "            plt.ylabel('False Positve Rate (FPR)')\n",
    "            if smooth_true == False:\n",
    "                plt.title('H6 FNR-FPR plot of multiple models')\n",
    "            else:\n",
    "                plt.title(smooth_label(smooth_no)+' H6 FNR-FPR plot of multiple models')\n",
    "            plt.grid(True)\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.axis('square')\n",
    "            plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "            # plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_allmodels.png\",dpi=1000)\n",
    "            plt.show()\n",
    "\n",
    "            file_no = file_no + 1\n",
    "            total_fig_no = total_fig_no + fig_no\n",
    "            print(\"++++++++++++++++++++++++++++   END   +++++++++++++++++++++\\n\")\n",
    "\n",
    "        original_stdout = sys.stdout\n",
    "        if smooth_true:\n",
    "            out_file = open(\"plots/\"+smooth_label(smooth_no)+'_'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        else:\n",
    "            out_file = open('plots/'+type_html+'_allfiles_best_metrics.txt', 'w')\n",
    "        sys.stdout = out_file\n",
    "        for transformer in sentence_transformer_names_list:\n",
    "            mean_allfiles_fpr_fnr_list = (np.mean(np.array(fpr_fnr_plot_dict[transformer]), axis=0)).tolist()\n",
    "            typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer] = mean_allfiles_fpr_fnr_list \n",
    "            allfile_fpr_plot_list = []\n",
    "            allfile_fnr_plot_list = []\n",
    "            for tup in mean_allfiles_fpr_fnr_list:\n",
    "                allfile_fpr_plot_list.append(tup[0])\n",
    "                allfile_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allfile_fnr_plot_list, allfile_fpr_plot_list, label = transformer.split(\"/\")[1])\n",
    "            min_dist_index = -1\n",
    "            min_dist = 100\n",
    "            for i in range(0,len(cosine_threshold_list)):\n",
    "                dist = math.sqrt(allfile_fnr_plot_list[i]*allfile_fnr_plot_list[i] + allfile_fpr_plot_list[i]*allfile_fpr_plot_list[i])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_dist_index = i\n",
    "            print(\"-------  \", transformer, \"  ------------------\")\n",
    "            print(\" least distance cosine threshold = \", cosine_threshold_list[min_dist_index])\n",
    "            print(\" least distance = \", min_dist)\n",
    "            print(\" least distnant FNR = \", plot_fnr_list[min_dist_index])\n",
    "            print(\" least distnant FPR = \", plot_fpr_list[min_dist_index])\n",
    "            print(\" number of total figures = \", total_fig_no)\n",
    "\n",
    "        sys.stdout = original_stdout\n",
    "        out_file.close()\n",
    "\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        if smooth_true == False:\n",
    "            plt.title('H6 FNR-FPR plot of multiple models')\n",
    "        else:\n",
    "            plt.title(smooth_label(smooth_no)+' H6 FNR-FPR plot of multiple models')\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        if smooth_true == False:\n",
    "            plt.savefig(\"plots/h6_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        else:\n",
    "            plt.savefig(\"plots/h6_\"+smooth_label(smooth_no)+\"_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "        plt.show()\n",
    "\n",
    "    for transformer in sentence_transformer_names_list:    \n",
    "        for smooth_no in smooth_no_list:\n",
    "            allsmooth_fpr_plot_list = []\n",
    "            allsmooth_fnr_plot_list = []\n",
    "            for tup in typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer]:\n",
    "                allsmooth_fpr_plot_list.append(tup[0])\n",
    "                allsmooth_fnr_plot_list.append(tup[1])\n",
    "            plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = smooth_label(smooth_no))\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        plt.title('H6 FNR-FPR plot of '+type_html+' of '+transformer.split(\"/\")[1])\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "        # if smooth_true:\n",
    "        plt.savefig(\"plots/h6_\"+transformer.split(\"/\")[1]+\" \"+type_html+\"_all_smooth_average_allchapters_allmodels.png\",dpi=1000)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "for type_html in types_html_files_list: \n",
    "    for smooth_no in smooth_no_list:\n",
    "        pickle.dump( typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no], open( \"pickles/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55368937-a216-4930-bc06-87a03119eb48",
   "metadata": {},
   "source": [
    "# Load pickle files and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1244fbe9-f690-49b2-b431-d4365b639c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# smooth_no_list = [-13,-11,-9,-7,-5,-3,0,3,5,7,9,11,13,100,101]\n",
    "smooth_no_list = [-7]\n",
    "no_points_fpr_fnr_plot = 500\n",
    "\n",
    "cosine_threshold_list = np.arange(-1.0, 3.0001 ,(2/no_points_fpr_fnr_plot))\n",
    "cosine_threshold_list = np.around(cosine_threshold_list, 4)\n",
    "heuristic_no = 6\n",
    "marker_dict = {'sentence-transformers/all-mpnet-base-v2':\"s\",'sentence-transformers/multi-qa-MiniLM-L6-cos-v1':\"o\",'sentence-transformers/all-MiniLM-L6-v2':\"P\",'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2':\"v\",'sentence-transformers/bert-base-nli-mean-tokens':\"X\"}\n",
    "\n",
    "for type_html in types_html_files_list: \n",
    "#     for smooth_no in smooth_no_list:\n",
    "#         transformer_allfiles_mean_fpr_fnr_dict = pickle.load( open(\"pickles/heuristic_\"+str(heuristic_no)+\"/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"rb\" ) )\n",
    "#         for transformer in sentence_transformer_names_list:\n",
    "#             mean_allfiles_fpr_fnr_list = transformer_allfiles_mean_fpr_fnr_dict[transformer]\n",
    "#             allfile_fpr_plot_list = []\n",
    "#             allfile_fnr_plot_list = []\n",
    "#             for tup in mean_allfiles_fpr_fnr_list:\n",
    "#                 allfile_fpr_plot_list.append(tup[0])\n",
    "#                 allfile_fnr_plot_list.append(tup[1])\n",
    "#             plt.plot(allfile_fnr_plot_list, allfile_fpr_plot_list, label = transformer.split(\"/\")[1])\n",
    "#             min_dist_index = -1\n",
    "#             min_dist = 100\n",
    "#             for i in range(0,len(cosine_threshold_list)):\n",
    "#                 dist = math.sqrt(allfile_fnr_plot_list[i]*allfile_fnr_plot_list[i] + allfile_fpr_plot_list[i]*allfile_fpr_plot_list[i])\n",
    "#                 if dist <= min_dist:\n",
    "#                     min_dist = dist\n",
    "#                     min_dist_index = i\n",
    "#             if transformer == 'sentence-transformers/all-MiniLM-L6-v2' and smooth_no == -7:\n",
    "#                 print(\"-------  \", transformer, \"  ------------------\")\n",
    "#                 print(\"---smooth no = \", smooth_no, \"  ------------------\")\n",
    "#                 print(\"---type_html = \", type_html, \"  ------------------\")\n",
    "#                 print(\" least distance cosine threshold = \", cosine_threshold_list[min_dist_index])\n",
    "#                 print(\" least distance = \", min_dist)\n",
    "#                 print(\" least distnant FNR = \", allfile_fnr_plot_list[min_dist_index])\n",
    "#                 print(\" least distnant FPR = \", allfile_fpr_plot_list[min_dist_index])\n",
    "#                 print(\" number of total figures = \", total_fig_no)\n",
    "        \n",
    "#         plt.xlabel('False Negative Rate (FNR)')\n",
    "#         plt.ylabel('False Positve Rate (FPR)')\n",
    "#         plt.title(smooth_label(smooth_no)+' of '+type_html+' FNR-FPR plot of multiple models')\n",
    "#         plt.grid(True)\n",
    "#         plt.xlim([0, 1])\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.axis('square')\n",
    "#         plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "#         # plt.savefig(\"plots/\"+smooth_label(smooth_no)+\"_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "#         plt.show()\n",
    "    # for transformer in sentence_transformer_names_list:    \n",
    "    #     for smooth_no in smooth_no_list:\n",
    "    #         allsmooth_fpr_plot_list = []\n",
    "    #         allsmooth_fnr_plot_list = []\n",
    "    #         typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict = pickle.load( open( \"pickles/\"+str(type_html)+\"_\"+str(smooth_no)+\"_typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict.p\", \"rb\" ) )\n",
    "    #         for tup in typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer]:\n",
    "    #             allsmooth_fpr_plot_list.append(tup[0])\n",
    "    #             allsmooth_fnr_plot_list.append(tup[1])\n",
    "    #         plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = smooth_label(smooth_no))\n",
    "    #     plt.xlabel('False Negative Rate (FNR)')\n",
    "    #     plt.ylabel('False Positve Rate (FPR)')\n",
    "    #     plt.title('FNR-FPR plot of '+'uniform'+' smoothings of '+type_html+' & '+transformer.split(\"/\")[1])\n",
    "    #     plt.grid(True)\n",
    "    #     plt.xlim([0, 1])\n",
    "    #     plt.ylim([0, 1])\n",
    "    #     plt.axis('square')\n",
    "    #     plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "    #     plt.savefig(\"plots/\"+transformer.split(\"/\")[1]+\" \"+type_html+\"_all_smooth_average_allchapters_allmodels.png\",dpi=1000)\n",
    "    #     plt.show()\n",
    "    \n",
    "    # for transformer in ['sentence-transformers/all-MiniLM-L6-v2','sentence-transformers/multi-qa-MiniLM-L6-cos-v1']:    \n",
    "    for transformer in ['sentence-transformers/multi-qa-MiniLM-L6-cos-v1']:    \n",
    "        for smooth_no in smooth_no_list:\n",
    "            allsmooth_fpr_plot_list = []\n",
    "            allsmooth_fnr_plot_list = []\n",
    "            transformer_allfiles_mean_fpr_fnr_dict = pickle.load( open( \"pickles/30_with_boundingbox/heuristic_\"+str(heuristic_no)+\"/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"rb\" ) )\n",
    "            tup_index = 0\n",
    "            print(\" -------------------- \"+type_html+\"   \"+transformer.split(\"/\")[1]+\" ------------------\")\n",
    "            for tup in transformer_allfiles_mean_fpr_fnr_dict[transformer]:\n",
    "                allsmooth_fpr_plot_list.append(tup[0])\n",
    "                allsmooth_fnr_plot_list.append(tup[1])\n",
    "                if (cosine_threshold_list[tup_index] < 0.6) and (cosine_threshold_list[tup_index] > 0.1):\n",
    "                    print(\"(fnr,fpr) = \",(np.around(tup[1], 4),np.around(tup[0], 4)),\" cosine value = \",cosine_threshold_list[tup_index])\n",
    "                tup_index = tup_index + 1\n",
    "            plt.scatter(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = transformer.split(\"/\")[1], marker = marker_dict[transformer], c = cosine_threshold_list, cmap = \"jet\", vmin=-1, vmax=3)\n",
    "    plt.xlabel('False Negative Rate (FNR)')\n",
    "    plt.ylabel('False Positve Rate (FPR)')\n",
    "    plt.title('FNR-FPR plot of '+'uniform'+' smoothings of '+type_html)\n",
    "    # plt.title('FNR-FPR plot of '+'uniform'+' smoothings of '+type_html+' & '+transformer.split(\"/\")[1])\n",
    "    plt.colorbar(label=\"Cosine treshold\", orientation=\"vertical\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.axis('square')\n",
    "    plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "    # plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_\"+transformer.replace(\"/\",\"_\")+\"_scatter.png\",dpi=1000)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939fd3b-f9be-48ec-bd6d-2ec4b831378d",
   "metadata": {},
   "source": [
    "## comparision of heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb1dad-738c-4caf-973f-93d5df679e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# smooth_no_list = [-13,-11,-9,-7,-5,-3,0,3,5,7,9,11,13,100,101]\n",
    "smooth_no_list = [-7]\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# cosine_threshold_list = np.arange(0.0, 1.0001 ,(1/no_points_fpr_fnr_plot))\n",
    "# cosine_threshold_list = np.around(cosine_threshold_list, 4)\n",
    "heuristic_no_list = [2,6]\n",
    "\n",
    "for type_html in types_html_files_list: \n",
    "#         for smooth_no in smooth_no_list:\n",
    "#             transformer_allfiles_mean_fpr_fnr_dict = pickle.load( open(\"pickles/heuristic_\"+str(heuristic_no)+\"/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"rb\" ) )\n",
    "#             for transformer in sentence_transformer_names_list:\n",
    "#                 mean_allfiles_fpr_fnr_list = transformer_allfiles_mean_fpr_fnr_dict[transformer]\n",
    "#                 allfile_fpr_plot_list = []\n",
    "#                 allfile_fnr_plot_list = []\n",
    "#                 for tup in mean_allfiles_fpr_fnr_list:\n",
    "#                     allfile_fpr_plot_list.append(tup[0])\n",
    "#                     allfile_fnr_plot_list.append(tup[1])\n",
    "#                 plt.plot(allfile_fnr_plot_list, allfile_fpr_plot_list, label = transformer.split(\"/\")[1])\n",
    "#                 min_dist_index = -1\n",
    "#                 min_dist = 100\n",
    "#                 for i in range(0,len(cosine_threshold_list)):\n",
    "#                     dist = math.sqrt(allfile_fnr_plot_list[i]*allfile_fnr_plot_list[i] + allfile_fpr_plot_list[i]*allfile_fpr_plot_list[i])\n",
    "#                     if dist <= min_dist:\n",
    "#                         min_dist = dist\n",
    "#                         min_dist_index = i\n",
    "#                 if transformer == 'sentence-transformers/all-MiniLM-L6-v2' and smooth_no == -7:\n",
    "#                     print(\"-------  \", transformer, \"  ------------------\")\n",
    "#                     print(\"---smooth no = \", smooth_no, \"  ------------------\")\n",
    "#                     print(\"---type_html = \", type_html, \"  ------------------\")\n",
    "#                     print(\" least distance cosine threshold = \", cosine_threshold_list[min_dist_index])\n",
    "#                     print(\" least distance = \", min_dist)\n",
    "#                     print(\" least distnant FNR = \", allfile_fnr_plot_list[min_dist_index])\n",
    "#                     print(\" least distnant FPR = \", allfile_fpr_plot_list[min_dist_index])\n",
    "#                     print(\" number of total figures = \", total_fig_no)\n",
    "\n",
    "#             plt.xlabel('False Negative Rate (FNR)')\n",
    "#             plt.ylabel('False Positve Rate (FPR)')\n",
    "#             plt.title(smooth_label(smooth_no)+' of '+type_html+' FNR-FPR plot of multiple models')\n",
    "#             plt.grid(True)\n",
    "#             plt.xlim([0, 1])\n",
    "#             plt.ylim([0, 1])\n",
    "#             plt.axis('square')\n",
    "#             plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "#             # plt.savefig(\"plots/\"+smooth_label(smooth_no)+\"_\"+type_html+\"_average_allchapters_allmodels.png\",dpi=1000)\n",
    "#             plt.show()\n",
    "    # for transformer in sentence_transformer_names_list:    \n",
    "    #     for smooth_no in smooth_no_list:\n",
    "    #         allsmooth_fpr_plot_list = []\n",
    "    #         allsmooth_fnr_plot_list = []\n",
    "    #         typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict = pickle.load( open( \"pickles/\"+str(type_html)+\"_\"+str(smooth_no)+\"_typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict.p\", \"rb\" ) )\n",
    "    #         for tup in typehtml_smooth_transformer_allfiles_mean_fpr_fnr_dict[type_html][smooth_no][transformer]:\n",
    "    #             allsmooth_fpr_plot_list.append(tup[0])\n",
    "    #             allsmooth_fnr_plot_list.append(tup[1])\n",
    "    #         plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = smooth_label(smooth_no))\n",
    "    #     plt.xlabel('False Negative Rate (FNR)')\n",
    "    #     plt.ylabel('False Positve Rate (FPR)')\n",
    "    #     plt.title('FNR-FPR plot of '+'uniform'+' smoothings of '+type_html+' & '+transformer.split(\"/\")[1])\n",
    "    #     plt.grid(True)\n",
    "    #     plt.xlim([0, 1])\n",
    "    #     plt.ylim([0, 1])\n",
    "    #     plt.axis('square')\n",
    "    #     plt.legend(loc=\"lower left\",fontsize=\"x-small\")\n",
    "    #     plt.savefig(\"plots/\"+transformer.split(\"/\")[1]+\" \"+type_html+\"_all_smooth_average_allchapters_allmodels.png\",dpi=1000)\n",
    "    #     plt.show()\n",
    "\n",
    "    for transformer in ['sentence-transformers/multi-qa-MiniLM-L6-cos-v1']:    \n",
    "        for smooth_no in smooth_no_list:\n",
    "            for heuristic_no in heuristic_no_list:\n",
    "                allsmooth_fpr_plot_list = []\n",
    "                allsmooth_fnr_plot_list = []\n",
    "                transformer_allfiles_mean_fpr_fnr_dict = pickle.load( open( \"pickles/30_with_boundingbox/heuristic_\"+str(heuristic_no)+\"/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"rb\" ) )\n",
    "                for tup in transformer_allfiles_mean_fpr_fnr_dict[transformer]:\n",
    "                    allsmooth_fpr_plot_list.append(tup[0])\n",
    "                    allsmooth_fnr_plot_list.append(tup[1])\n",
    "                plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = \"heuristic_\"+str(heuristic_no))\n",
    "    plt.xlabel('False Negative Rate (FNR)')\n",
    "    plt.ylabel('False Positve Rate (FPR)')\n",
    "    plt.title(transformer.split(\"/\")[1]+'[30BB] FNR-FPR plot of '+'gaussian'+' of '+type_html)\n",
    "    # plt.title('FNR-FPR plot of '+'uniform'+' smoothings of '+type_html+' & '+transformer.split(\"/\")[1])\n",
    "    # plt.colorbar(label=\"Cosine treshold\", orientation=\"vertical\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.axis('square')\n",
    "    plt.legend(loc=\"upper right\",fontsize=\"x-small\")\n",
    "    plt.savefig(\"plots/30BB_all_heuritstics_\"+type_html+\"_\"+transformer.replace(\"/\",\"_\")+\".png\",dpi=1000)\n",
    "    plt.show()\n",
    "    for transformer in ['sentence-transformers/multi-qa-MiniLM-L6-cos-v1']:    \n",
    "        for smooth_no in smooth_no_list:\n",
    "            for heuristic_no in heuristic_no_list:\n",
    "                allsmooth_fpr_plot_list = []\n",
    "                allsmooth_fnr_plot_list = []\n",
    "                transformer_allfiles_mean_fpr_fnr_dict = pickle.load( open( \"pickles/50_with_boundingbox/heuristic_\"+str(heuristic_no)+\"/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"rb\" ) )\n",
    "                for tup in transformer_allfiles_mean_fpr_fnr_dict[transformer]:\n",
    "                    allsmooth_fpr_plot_list.append(tup[0])\n",
    "                    allsmooth_fnr_plot_list.append(tup[1])\n",
    "                plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = \"heuristic_\"+str(heuristic_no))\n",
    "    plt.xlabel('False Negative Rate (FNR)')\n",
    "    plt.ylabel('False Positve Rate (FPR)')\n",
    "    plt.title(transformer.split(\"/\")[1]+'[50BB] FNR-FPR plot of '+'gaussian'+' of '+type_html)\n",
    "    # plt.title('FNR-FPR plot of '+'uniform'+' smoothings of '+type_html+' & '+transformer.split(\"/\")[1])\n",
    "    # plt.colorbar(label=\"Cosine treshold\", orientation=\"vertical\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.axis('square')\n",
    "    plt.legend(loc=\"upper right\",fontsize=\"x-small\")\n",
    "    plt.savefig(\"plots/50BB_all_heuritstics_\"+type_html+\"_\"+transformer.replace(\"/\",\"_\")+\".png\",dpi=1000)\n",
    "    plt.show()\n",
    "    # for transformer in ['sentence-transformers/multi-qa-MiniLM-L6-cos-v1']:    \n",
    "    #     for smooth_no in smooth_no_list:\n",
    "    #         for heuristic_no in heuristic_no_list:\n",
    "    #             allsmooth_fpr_plot_list = []\n",
    "    #             allsmooth_fnr_plot_list = []\n",
    "    #             transformer_allfiles_mean_fpr_fnr_dict = pickle.load( open( \"pickles/without_boundingbox/heuristic_\"+str(heuristic_no)+\"/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"rb\" ) )\n",
    "    #             for tup in transformer_allfiles_mean_fpr_fnr_dict[transformer]:\n",
    "    #                 allsmooth_fpr_plot_list.append(tup[0])\n",
    "    #                 allsmooth_fnr_plot_list.append(tup[1])\n",
    "    #             plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = \"heuristic_\"+str(heuristic_no))\n",
    "    # plt.xlabel('False Negative Rate (FNR)')\n",
    "    # plt.ylabel('False Positve Rate (FPR)')\n",
    "    # plt.title(transformer.split(\"/\")[1]+' NO [BB] FNR-FPR plot of '+'gaussian'+' of '+type_html)\n",
    "    # # plt.title('FNR-FPR plot of '+'uniform'+' smoothings of '+type_html+' & '+transformer.split(\"/\")[1])\n",
    "    # # plt.colorbar(label=\"Cosine treshold\", orientation=\"vertical\")\n",
    "    # plt.grid(True)\n",
    "    # plt.xlim([0, 1])\n",
    "    # plt.ylim([0, 1])\n",
    "    # plt.axis('square')\n",
    "    # plt.legend(loc=\"upper right\",fontsize=\"x-small\")\n",
    "    # plt.savefig(\"plots/No_BB_all_heuritstics_\"+type_html+\"_\"+transformer.replace(\"/\",\"_\")+\".png\",dpi=1000)\n",
    "    # # plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_\"+transformer.replace(\"/\",\"_\")+\"_scatter.png\",dpi=1000)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec39678-5368-4f86-8ba6-d15a03726f72",
   "metadata": {},
   "source": [
    "## comparision of each heuristic with various bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d992b0f-f037-4af2-8734-701c28a90586",
   "metadata": {},
   "outputs": [],
   "source": [
    "types_html_files_list = [\"textbooks\",\"papers\"]\n",
    "# smooth_no_list = [-13,-11,-9,-7,-5,-3,0,3,5,7,9,11,13,100,101]\n",
    "smooth_no_list = [-7]\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# cosine_threshold_list = np.arange(0.0, 1.0001 ,(1/no_points_fpr_fnr_plot))\n",
    "# cosine_threshold_list = np.around(cosine_threshold_list, 4)\n",
    "heuristic_no_list = [1]\n",
    "\n",
    "transformer = 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1'\n",
    "smooth_no = -7\n",
    "for heuristic_no in heuristic_no_list:\n",
    "    for type_html in types_html_files_list:\n",
    "        allsmooth_fpr_plot_list = []\n",
    "        allsmooth_fnr_plot_list = []\n",
    "        transformer_allfiles_mean_fpr_fnr_dict = pickle.load( open( \"pickles/30_with_boundingbox/heuristic_\"+str(heuristic_no)+\"/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"rb\" ) )\n",
    "        for tup in transformer_allfiles_mean_fpr_fnr_dict[transformer]:\n",
    "            allsmooth_fpr_plot_list.append(tup[0])\n",
    "            allsmooth_fnr_plot_list.append(tup[1])\n",
    "        plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = \"30_with_boundingbox\")\n",
    "        allsmooth_fpr_plot_list = []\n",
    "        allsmooth_fnr_plot_list = []\n",
    "        transformer_allfiles_mean_fpr_fnr_dict = pickle.load( open( \"pickles/50_with_boundingbox/heuristic_\"+str(heuristic_no)+\"/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"rb\" ) )\n",
    "        for tup in transformer_allfiles_mean_fpr_fnr_dict[transformer]:\n",
    "            allsmooth_fpr_plot_list.append(tup[0])\n",
    "            allsmooth_fnr_plot_list.append(tup[1])\n",
    "        plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = \"50_with_boundingbox\")\n",
    "        # allsmooth_fpr_plot_list = []\n",
    "        # allsmooth_fnr_plot_list = []\n",
    "        # transformer_allfiles_mean_fpr_fnr_dict = pickle.load( open( \"pickles/without_boundingbox/heuristic_\"+str(heuristic_no)+\"/\"+str(type_html)+\"_\"+str(smooth_no)+\"_transformer_allfiles_mean_fpr_fnr_dict.p\", \"rb\" ) )\n",
    "        # for tup in transformer_allfiles_mean_fpr_fnr_dict[transformer]:\n",
    "        #     allsmooth_fpr_plot_list.append(tup[0])\n",
    "        #     allsmooth_fnr_plot_list.append(tup[1])\n",
    "        # plt.plot(allsmooth_fnr_plot_list, allsmooth_fpr_plot_list, label = \"without_boundingbox\")\n",
    "        plt.xlabel('False Negative Rate (FNR)')\n",
    "        plt.ylabel('False Positve Rate (FPR)')\n",
    "        plt.title(\"H\"+str(heuristic_no)+\"_\"+transformer.split(\"/\")[1]+' FNR-FPR plot of '+'gaussian'+' of '+type_html)\n",
    "        # plt.title('FNR-FPR plot of '+'uniform'+' smoothings of '+type_html+' & '+transformer.split(\"/\")[1])\n",
    "        # plt.colorbar(label=\"Cosine treshold\", orientation=\"vertical\")\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.axis('square')\n",
    "        plt.legend(loc=\"upper right\",fontsize=\"x-small\")\n",
    "        plt.savefig(\"plots/h\"+str(heuristic_no)+\"_\"+type_html+\"_\"+transformer.replace(\"/\",\"_\")+\".png\",dpi=1000)\n",
    "        # plt.savefig(\"plots/\"+class_name+\"_\"+textbook_name+\"_\"+transformer.replace(\"/\",\"_\")+\"_scatter.png\",dpi=1000)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b18a3-387e-421c-bd61-8856990b82f2",
   "metadata": {},
   "source": [
    "# test: text summarization and CLIP testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a725d-7bb5-4aca-96ae-ea9ff9b6da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testimg1 = \"./data/e_pathshala_epub_debug/class_10_science/26_5240_11/OEBPS/Images/806.png\"\n",
    "testimg2 = \"./data/e_pathshala_epub_debug/class_11_biology/89_5156_6/OEBPS/Images/1897.png\"\n",
    "testimg3 = \"./data/e_pathshala_epub_debug/class_10_science/26_5240_11/OEBPS/Images/1192.png\"\n",
    "\n",
    "testimg4 = \"./data/e_pathshala_epub_debug/class_10_science/26_5240_11/OEBPS/Images/1099.png\"\n",
    "\n",
    "testimg5 = \"./data/e_pathshala_epub_debug/class_8_science/37_5238_6/OEBPS/Images/image29.png\"\n",
    "\n",
    "\n",
    "filename = \"./data/e_pathshala_epub_debug/class_9_science/2_5239_2/OEBPS/Text/Untitled-2.xhtml\"\n",
    "\n",
    "try:\n",
    "    fp = open(filename, encoding=\"utf8\")\n",
    "    print(\"++++++++++++++++++++++++++++   START   +++++++++++++++++++++\")\n",
    "    print(filename)\n",
    "except:\n",
    "    print(\"***************  Error while opening file   ***************\")\n",
    "    print(filename)\n",
    "    print(\"************************************************************\")\n",
    "extension = filename.split(\".\")[-1]\n",
    "if extension == \"xhtml\":\n",
    "    try:\n",
    "        print(\"@@@@@@@@@@@@   XML parser @@@@@@@@@@@@\")\n",
    "        soup = BeautifulSoup(fp, 'lxml-xml')\n",
    "    except Exception as e:\n",
    "        print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "        print(e)\n",
    "        print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "elif extension == \"html\":\n",
    "    try:\n",
    "        print(\"@@@@@@@@@@@@   HTML parser @@@@@@@@@@@@\")\n",
    "        soup = BeautifulSoup(fp, 'lxml')\n",
    "    except Exception as e:\n",
    "        print(\" ^^^^^^^^^^^ Error while html parsing ^^^^^^^^^^^^^^^^^^^\")\n",
    "        print(e)\n",
    "        print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "all_fig = soup.find_all(string=re.compile(\"^Fig\"))\n",
    "fig_no = 1\n",
    "max_caption_words = 60\n",
    "min_caption_words = 6\n",
    "\n",
    "full_text = soup.get_text()\n",
    "\n",
    "output_file = open(\"output_full_text.txt\",\"w\")\n",
    "output_file.write(remove_non_ascii(full_text))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(\"output_full_text_str.txt\",\"w\")\n",
    "output_file.write(str(full_text.encode('utf8')))\n",
    "output_file.close()\n",
    "\n",
    "def clean_string(txt):\n",
    "    return remove_non_ascii(txt).replace(\"Fig. \", \"Figure \").replace(\"fig. \",\"Figure \").replace(\"\\t\",\" \").replace(\"\\n\",\" \")\n",
    "\n",
    "def extract_lastfig(txt):\n",
    "    txt = remove_non_ascii(txt).strip().replace(\"\\n\",\" \")\n",
    "    last_index = [_.start() for _ in re.finditer(r\"Fig\", txt)][-1]\n",
    "    return txt[last_index:]\n",
    "\n",
    "for fig in all_fig:\n",
    "    print(\"------- fig  \"+str(fig_no)+\"   --------\")\n",
    "\n",
    "    caption_parent_text = extract_lastfig(fig.parent.parent.get_text())\n",
    "    caption_grandparent_text = extract_lastfig(fig.parent.parent.parent.get_text())\n",
    "    fig_text = extract_lastfig(fig)\n",
    "    fig_caption = \"\"\n",
    "    if len(fig_text.split()) >= max_caption_words:\n",
    "        continue\n",
    "    elif len(fig_text.split()) >= min_caption_words:\n",
    "        # print(f\"fig\\nlength = {len(fig.split())}\")\n",
    "        print(\"fig\")\n",
    "        fig_caption = fig_text\n",
    "    elif len(caption_parent_text.split()) >= max_caption_words:\n",
    "        # print(f\"fig\\nlength = {len(fig.split())}\")\n",
    "        print(\"fig\")\n",
    "        fig_caption = fig_text\n",
    "    elif len(caption_grandparent_text.split()) >= max_caption_words:\n",
    "        # print(f\" fig + parent\\n length = {len(caption_parent_text.split())}\")\n",
    "        print(\"fig + parent\")\n",
    "        fig_caption = caption_parent_text\n",
    "    else:\n",
    "        # print( f\"fig + grandparent \\nlength = {len(caption_grandparent_text.split())}\")\n",
    "        print(\"fig + grandparent\")\n",
    "        fig_caption = caption_grandparent_text\n",
    "    \n",
    "    same_el_img = fig.parent.find_all(\"img\")\n",
    "    prev_img = fig.parent.find_previous(\"img\")\n",
    "\n",
    "    def get_image_path(html_path,img_src): # get image path from (html path) and (image path in html file)\n",
    "        oebps_path = html_path.split(\"/Text/\")[0]\n",
    "        img_src = img_src[2:]\n",
    "        return unquote(oebps_path+img_src)\n",
    "\n",
    "    if same_el_img == []:\n",
    "        if prev_img == None:\n",
    "            print(\"No image found\")\n",
    "        else:\n",
    "            image_path = get_image_path(filename,prev_img[\"src\"])\n",
    "            # image_path = convert_trans_to_white_bg(image_path)\n",
    "            print(\"previous image = \"+ image_path)\n",
    "    else:\n",
    "        image_path = get_image_path(filename,same_el_img[-1][\"src\"])\n",
    "        # image_path = convert_trans_to_white_bg(image_path)\n",
    "        print(\"same tag image = \"+ image_path)\n",
    "\n",
    "    # ocr_full_txt_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\"_full.txt\"\n",
    "    ocr_json_path = image_path.split(\"/OEBPS/Images/\")[0]+\"/OEBPS/Images/\"+image_path.split(\"/OEBPS/Images/\")[1].split(\".\")[0]+\".json\"\n",
    "    plt.imshow(mpimg.imread(image_path))\n",
    "    plt.show()\n",
    "    if os.path.isfile(ocr_json_path):\n",
    "        print(\"    Json already exist :  \"+ocr_json_path)\n",
    "        json_file = open(ocr_json_path)\n",
    "        ocr_dict = json.load(json_file) \n",
    "        #structure of ocr_dict ={\"block\":[ {\"text\":\"\",\"\",\"paragraph\":[{\"text\":\"\", \"word\":[\"w1\",\"w2\",..], \"bounding_poly\": [[],[],..]}, { .. }, ...],\"bounding_poly\":[[(x1,y1),..(x4,y4)],[]...]}, { ... }, ..],\"bounding_poly\":[[],[]...]}\n",
    "    else:\n",
    "        print(\"  $$$$  Calling google vision API $$$$ \")\n",
    "        ocr_dict = detect_document(image_path)\n",
    "    labels_list = []\n",
    "    print(\"~~~~~~~ Labels  ~~~~~~~~\")\n",
    "    if len(ocr_dict[\"block\"]) > 0:\n",
    "        for i in range(0,len(ocr_dict[\"block\"])):\n",
    "            block_dict = ocr_dict[\"block\"][i]\n",
    "            print(block_dict[\"text\"])\n",
    "            labels_list.append(block_dict[\"text\"].replace(\"-\",\"\"))\n",
    "            # print(block_dict[\"text\"], \"  \",ocr_dict[\"bounding_poly\"][i])\n",
    "            \n",
    "            # for i in range(0,len(block_dict[\"paragraph\"])):\n",
    "            #     para_dict = block_dict[\"paragraph\"][i]\n",
    "            #     print(\"      -\",para_dict[\"text\"])\n",
    "            #     print(\"      \",para_dict[\"text\"],\"   \",block_dict[\"bounding_poly\"][i])\n",
    "    else:\n",
    "        print(\"  NO Labels  \")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "    above_header = fig.parent.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "    above_header_text = above_header.get_text()\n",
    "    while clean_string(above_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(above_header_text).replace(\" \",\"\"))<3:\n",
    "        above_header = above_header.find_previous([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "        above_header_text = above_header.get_text()\n",
    "\n",
    "    below_header = fig.parent.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "    below_header_text = below_header.get_text()\n",
    "    while clean_string(below_header_text).lower().find(\"Activity\".lower()) != -1 or len(clean_string(below_header_text).replace(\" \",\"\"))<3:\n",
    "        below_header = below_header.find_next([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "        below_header_text = below_header.get_text()\n",
    "    \n",
    "    print(\"\\nabove header = \",above_header_text)\n",
    "    print(\"fig caption = \",fig_caption)\n",
    "    print(\"below header = \",below_header_text)\n",
    "    \n",
    "    section_text = above_header_text\n",
    "    next_header = above_header.find_next()\n",
    "    \n",
    "    while next_header.get_text() != below_header.get_text():\n",
    "        for child in next_header.contents:\n",
    "            if isinstance(child, str):\n",
    "                if len(clean_string(child).replace(\" \",\"\")) > 1:\n",
    "                    section_text = section_text +\"\\n\"+ clean_string(child)\n",
    "                    # print(\"---------------\\n\",clean_string(child),\"\\n\")\n",
    "        next_header = next_header.find_next()    \n",
    "    \n",
    "    sent_list =  re.split('\\. |\\? |\\n',section_text)\n",
    "\n",
    "    for sent in sent_list:\n",
    "        if len(sent) <= 3:\n",
    "            sent_list.remove(sent)\n",
    "\n",
    "    class bcolors:\n",
    "        HEADER = '\\033[95m'\n",
    "        OKBLUE = '\\033[94m'\n",
    "        OKCYAN = '\\033[96m'\n",
    "        OKGREEN = '\\033[92m'\n",
    "        WARNING = '\\033[93m'\n",
    "        FAIL = '\\033[91m'\n",
    "        ENDC = '\\033[0m'\n",
    "        BOLD = '\\033[1m'\n",
    "        UNDERLINE = '\\033[4m'\n",
    "\n",
    "    sentence_embeddings = sentence_transformer_model.encode(sent_list)\n",
    "    str_labels_list = str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "\n",
    "    if len(labels_list) > 0:\n",
    "        fig_caption_sent = fig_caption +\". This figure contains \"+ str_labels_list +\".\"\n",
    "        # fig_caption_sent = fig_caption + \" and this figure contains \"+ str(labels_list).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")+ \" as labels.\"\n",
    "    else:\n",
    "        fig_caption_sent = fig_caption\n",
    "\n",
    "\n",
    "    fig_caption_sent = fig_caption_sent.replace(\"\\t\", \" \").replace(\"Fig. \", \"Figure \").replace(\"fig. \",\"Figure \").lower()\n",
    "\n",
    "    print(\"Fig caption sentence = \",fig_caption_sent)\n",
    "    fig_caption_embedding = sentence_transformer_model.encode(fig_caption_sent)\n",
    "\n",
    "    cosine_list = cosine_similarity([fig_caption_embedding],sentence_embeddings)[0]\n",
    "\n",
    "    # for index in range(0,len(sent_list)):\n",
    "    #     print(\"* index \", index, \", cosine= \",cosine_list[index], \" - \", sent_list[index])\n",
    "\n",
    "    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "    top_n2 = 20\n",
    "    top_n1 = 10\n",
    "    top_n0 = 5\n",
    "    list_indices_n2 = sorted(range(len(cosine_list)), key = lambda sub: -cosine_list[sub])[top_n1:top_n2]\n",
    "    list_indices_n1 = sorted(range(len(cosine_list)), key = lambda sub: -cosine_list[sub])[top_n0:top_n1]\n",
    "    list_indices_n0 = sorted(range(len(cosine_list)), key = lambda sub: -cosine_list[sub])[:top_n0]\n",
    "    # print(\"Len of simlarity sent list = \", len(list_indices))\n",
    "    for index in list_indices_n0:\n",
    "        print(\"index \", index, \", cosine= \",cosine_list[index], \" - \", sent_list[index])\n",
    "    for index in list_indices_n1:\n",
    "        print(\"index \", index, \", cosine= \",cosine_list[index], \" - \", sent_list[index])\n",
    "    for index in list_indices_n2:\n",
    "        print(\"index \", index, \", cosine= \",cosine_list[index], \" - \", sent_list[index])\n",
    "\n",
    "\n",
    "    for index in range(0,len(sent_list)):\n",
    "        if index in list_indices_n2:\n",
    "            print(f\"{bcolors.WARNING}{sent_list[index]}{bcolors.ENDC}\")\n",
    "        elif index in list_indices_n1:\n",
    "            print(f\"{bcolors.OKCYAN}{sent_list[index]}{bcolors.ENDC}\")\n",
    "        elif index in list_indices_n0:\n",
    "            print( f\"{bcolors.OKGREEN}{sent_list[index]}{bcolors.ENDC}\")\n",
    "        else:\n",
    "            print(sent_list[index])\n",
    "\n",
    "    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "    section_text_caption_labels = section_text + \" \" +fig_caption_sent\n",
    "\n",
    "    # print(\"\\n!!!Section text = \\n\",section_text_caption_labels,\"!!!\\n\")\n",
    "    # print(\"\\n***Section summary text(True)  = \\n\",pipeline_summarizer(section_text_caption_labels, max_length=120, min_length=30, do_sample=True),\"***\\n\")\n",
    "    # print(\"\\n***Section summary text(False) = \\n\",pipeline_summarizer(section_text_caption_labels, max_length=120, min_length=30, do_sample=False),\"***\\n\")\n",
    "    \n",
    "    fig_no = fig_no + 1\n",
    "\n",
    "print(\"++++++++++++++++++++++++++++   END   +++++++++++++++++++++\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9eb88a-6b6c-4277-be6d-243c5e2d2dfb",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579ccd7-438e-4889-900c-6707f33dc3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \"\"\"\n",
    "In order to solve this problem, the authors defined three main tasks: (1) Frequent pattern mining and association rules for chronic diseases and related treatments\n",
    "\n",
    "(2) Frequent sequence mining to analyze complex sequences of events with different time spans between them\n",
    "\n",
    "(3) Periodic event mining for finding periodical sequences of certain diseases\n",
    "\n",
    "Figure 5.1High-level architecture of a pattern mining system for understanding comorbidities of multiple chronic diseases\n",
    "\n",
    "Reproduced from Boytcheva et al\n",
    "\n",
    "[2019].\n",
    "\n",
    "Although the authors did not provide any specific algorithm for each task, they applied their system to find associations among schizophrenia, diabetes mellitus type 2, and hyperprolactinemia\n",
    "\n",
    "Figure 5.1 shows their proposed high-level architecture.\n",
    "figure 5.1 high-level architecture of a pattern mining system for understanding comorbidities of multiple chronic diseases. reproduced from boytcheva et al. [2019].\"\"\"\n",
    "\n",
    "full_text = full_text.replace(\"Fig. \", \"Figure \").replace(\"\\t\",\" \").replace(\"\\n\",\" \")\n",
    "print(pipeline_summarizer(full_text, max_length=80, min_length=30, do_sample=True))\n",
    "print(pipeline_summarizer(full_text, max_length=100, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb7296-0bba-49f5-8ba4-2dc392985383",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [2.56422, 3.77284, 3.52623, 3.51468, 3.02199]\n",
    "z = [0.15, 0.3, 0.45, 0.6, 0.75]\n",
    "n = [58, 651, 393, 203, 123]\n",
    "\n",
    "print(np.arange(0, 1.001,10))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(z, y)\n",
    "\n",
    "for i, txt in enumerate(n):\n",
    "    ax.annotate(txt, (z[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b37f50-fd66-4312-888e-e0c1b5f487d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "a = np.array([1,2,1,3,2,3,4,44,-43,2,1])\n",
    "print(a,len(a))\n",
    "b = np.pad(a, (1, 1), 'constant')\n",
    "print(b,len(b))\n",
    "print(moving_average(b,3).tolist(), len(moving_average(b,3)))\n",
    "\n",
    "u = np.array([1,2,3])\n",
    "i = np.array([1,2,3])\n",
    "y = np.array([])\n",
    "\n",
    "print(int(-7/2))\n",
    "\n",
    "a = np.array([[(1,2),(3,4)],[(5,6),(7,8)]])\n",
    "print(a)\n",
    "print(np.mean(a,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03880b0-8c8c-4e45-bf00-94c3a26ff21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split('\\. |\\? |\\n|\\.\\n', \"as. asd? ddls\\nas.\\nfdf\\nasd\\n.adl.?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf5c31-e6df-40f7-b4c4-e7751a1a9991",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "print(np.average(a))\n",
    "print(a.index(4))\n",
    "print(list(range(0,4)))\n",
    "fig_caption_index = 3\n",
    "no_bounding_sent = 11\n",
    "print(np.average(a[max(0,fig_caption_index - no_bounding_sent):min(fig_caption_index + no_bounding_sent,len(a))]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
