<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="bmtitle"><a id="page_517"/><b>Volume 2 Glossary</b></p>
<p class="glo"><b>Accumulative GSR</b> refers to the summation of GSR values over the task time. If the GSR is considered as a continuous signal over time, the accumulative GSR is the integration of GSR values over the task time.</p>
<p class="glo"><b>Action units</b> and action descriptors are the smallest visually discriminable facial movements. Action units are movements for which the anatomic basis is known [Cohn, Ambadar, &#38; Ekman, 2007]. They are represented as either binary events (presence vs. absence) or with respect to five levels of ordinal intensity. Action units individually or in combinations can represent nearly all possible facial expressions.</p>
<p class="glo"><b>Active Appearance Model (AAM)</b>. An AAM is a statistical model of shape and grey-level appearance that can generalize to almost any face [Edwards, Taylor, &#38; Cootes, 1998; Matthews &#38; Baker, 2004]. An AAM seeks to find the model parameters that can generate a synthetic image as close as possible to a target image [Cootes &#38; Taylor, 2004]. AAMs are learned from hand-labelled training data.</p>
<p class="glo"><b>Affect</b>. Broad term encompassing constructs such as emotions, moods, and feelings. Is not the same as personality, motivation, and other related terms.</p>
<p class="glo"><b>Affect and social signals</b> can be described as temporal patterns of a multiplicity of non-verbal behavioral cues which last for a short time [Vinciarelli et al. 2009] and are expressed as changes in neuromuscular and physiological activity [Vinciarelli et al. 2008a]. Sometimes, we consciously draw on affect and social signals to alter the interpretation of a situation, e.g. by saying something in a sarcastic voice to signal that we actually mean the opposite. At another time, we use them without being aware of it, e.g., by showing sympathy towards our counterpart by mimicking his or her verbal and nonverbal expressions. See <a href="19_Chapter07.xhtml">Chapter 7</a> of this volume for an overview of affective and social signals for which automated recognition approaches have been proposed.</p>
<p class="glo"><a id="page_518"/><b>Affect annotation</b>. The process of assigning affective labels (e.g., bored, confused, aroused) or values (e.g., arousal = 5) to data (e.g., video, audio, text).</p>
<p class="glo"><b>Affective computing</b>. Computing techniques and applications involving emotion or affect.</p>
<p class="glo"><b>Affective computing and social signal processing</b> aims at perceiving and interpreting nonverbal behavior with a machine by detecting affect and social signals just as humans do among themselves. This will lead to a new generation of computers that is perceived as more natural, efficacious and trustworthy [Vinciarelli et al. 2008b, Vinciarelli et al. 2009], and it makes room for a more human-like and intuitive interaction [Pantic et al. 2007].</p>
<p class="glo"><b>Affective experience-expression link</b>. The relationship between experiencing an affective state (e.g., feeling confused) and expressing it (e.g., displaying a furrowed brow).</p>
<p class="glo"><b>Affective ground truth</b>. Objective reality involving the &#8220;true&#8221; affective state. Is a misleading term for psychological constructs like affect.</p>
<p class="glo"><b>Alignment</b> A third challenge is to identify the direct relations between (sub)-elements from two or more different modalities. For example, we may want to align the steps in a recipe to a video showing the dish being made. To tackle this challenge we need to measure similarity between different modalities and deal with possible long-range dependencies and ambiguities.</p>
<p class="glo"><b>Classifier</b>: in pattern recognition and machine learning, it is a function that maps an object of interest (represented through a set of physical measurements called features) into one of the classes or categories such an object can belong to (the number of classes or categories is finite).</p>
<p class="glo"><b>Bag-of-words (BoW)</b> is a data-driven algorithm for summarizing large volumes of features. It can be thought of as a histogram whose bins are determined by partitions (or clusters) of the feature space.</p>
<p class="glo"><b>Canonical Correlation Analysis (CCA)</b> is a tool to infer information based on cross-covariance matrices. It can be used to identify correlation across heterogeneous modalities or sensor signals. Let each modality be represented by a feature vector and let us assume there is correlation across these, such as is in audiovisual speech recognition. Then, CCA will identify linear combinations of the individual features with maximum correlation amongst each other.</p>
<p class="glo"><b>Classifier Combination</b>: in pattern recognition and machine learning, it is a body of methodologies aimed at jointly using multiple classifiers to achieve a collective <a id="page_519"/>performance higher&#8212;to a statistically significant extent&#8212;than the individual performance of any individual classifier.</p>
<p class="glo"><b>Classifier Diversity</b>: in a set of classifiers that are being combined combined, the diversity is the tendency of different classifiers to have different performance in different regions of the input space.</p>
<p class="glo"><b>Cognitive load</b> is a multidimensional construct that refers to the momentary working memory load experienced by a person while performing a cognitive task. It can be increased by a wide variety of factors, including the difficulty of a task, the materials or tools used (e.g., computer interface), the situational context (e.g., distracting vs. quiet setting), the social context (e.g., working individually, vs. jointly with a group), a person&#8217;s expertise in the domain, a person&#8217;s physiological stress level, and so forth. Cognitive Load Theory describes cognitive load as having three components&#8212;intrinsic load, extraneous load, and germane load [Sweller et al. 2011]. For a detailed discussion of the dynamic and often non-linear interplay between cognitive load and domain expertise, including how cognitive load can either expand or minimize the performance gap between low- and high-performing students, see Oviatt [2013].</p>
<p class="glo"><b>Cognitive load measurement</b> refers to the methods to quantitatively discriminate the different levels of cognitive load experienced by the user. Usually the cognitive load is induced with varied task difficulty (i.e. the extraneous load is manipulated), and the methods to discriminate cognitive load include subjective methods, performance-based methods, physiological methods and behavioral methods.</p>
<p class="glo"><b>Co-learning</b> A fifth challenge is to transfer knowledge between modalities, their representation, and their predictive models. This is exemplified by algorithms of co-training, conceptual grounding, and zero shot learning. Co-learning explores how knowledge learning from one modality can help a computational model trained on a different modality. This challenge is particularly relevant when one of the modalities has limited resources (e.g., annotated data).</p>
<p class="glo"><b>Communication</b>: process between two or more agents aimed at the exchange of information or at the mutual modification of beliefs, shared or individual.</p>
<p class="glo"><b>Confidence measure</b> is the information on the assumed certainty of a decision made by a machine learning algorithm.</p>
<p class="glo"><b>Congruent expressions of affects</b>. A multimodal combination is said to involve congruent expressions of affects if each modality is conveying the same affect <a id="page_520"/>in terms of the category or the dimension (e.g., the facial expressions express anger and the hand gestures also express anger).</p>
<p class="glo"><b>Cooperative learning</b> in machine learning is a combination of active learning and semi-supervised learning. In the semi-supervised learning part, the machine learning algorithm labels unlabeled data based on its own previously learned model. In the active learning part, it identifies which unlabeled data is most important to be labeled by humans. Usually, cooperative learning tries to minimize the amount of data to be labeled by humans while maximizing the gain in accuracy of a learning algorithm. This can be based on confidence measures such that the machine labels unlabeled data itself as long as it is sufficiently confident in its decisions. It asks humans for help only where its confidence is insufficient, but the data seem to be highly informative.</p>
<p class="glo"><b>Construct</b>. A conceptual variable that cannot be directly observed (e.g., intelligence, personality).</p>
<p class="glo"><b>Continuous or discrete (i.e., categorical) representation</b> refer to the modeling of a user state or trait. As an example, the age of a user can be modeled as continuum such as the age in years. As opposed to this, a discretized representation would be broader age classes such as &#8220;young,&#8221; &#8220;adult&#8217;s&#8221;, and &#8220;elderly.&#8221; In addition, the time can be discretized or continuous (in fact, it is always discretized in some respect&#8212;at least by the sample rate of the digitized sampling of the sensor signals). However, one would speak of continuous measurement if processing is delivering a continuous output stream on a (short) frame-by-frame basis rather than an asynchronous processing of (larger) segments or chunks of the signal such as per spoken word or per body gesture.</p>
<p class="glo"><b>A Convolutional Neural Network (CNN)</b> is a neural network that contains one or more convolutional layers. A <i>convolutional layer</i> is a layer that processes an image (or any other data comprised of points with a notion of distance between these points, such as an audio signal) by convolving it with a number of kernels.</p>
<p class="glo"><b>A correlation</b> is a single number that describes the degree of relationship between two variables (signals). It most often refers to how close two variables are to having a linear relationship with each other.</p>
<p class="glo"><b>A dense layer</b> is the basic type of layer in a neural network. The layer takes a one-dimensional vector as input and transforms it to another one-dimensional vector by multiplying it by a <i>weight matrix</i> and adding a <i>bias vector</i>.</p>
<p class="glo"><b>Depression</b> refers broadly to the persistence over an extended period of time of several of the following symptoms: lowered mood, interest, or pleasure; psychomotor <a id="page_521"/>retardation; psychomotor agitation; diminished ability to think/concentrate; increased indecisiveness; fatigue or loss of energy; insomnia; hypersomnia; significant weight loss or weight gain; feelings of worthlessness or excessive guilt; and recurrent thoughts of death or recurrent suicidal ideation. It is important to note that there are multiple definitions of depression (see references in <a href="25_Chapter12.xhtml#ch12_2">Section 12.2</a>).</p>
<p class="glo"><b>Domain adaptation</b> refers to machine learning methods that learn from a source data distribution a well performing model on a different (but related) target data distribution.</p>
<p class="glo"><b>Domain expertise</b> refers to the level of working knowledge and problem-solving competence within a specific subject matter, such as algebra. It is a relatively stable state that influences how a person perceives and strategizes solving a problem. For the same task, a more expert student will group elements within it into higher-level patterns, or perceive the problem in a more integrated manner. A person&#8217;s level of domain expertise influences a variety of problem-solving behaviors (e.g., fluency, effort expended, accuracy). A more expert person also will experience lower cognitive load when working on the same problem as a less expert person. Most people experience domain expertise in some subjects. This everyday experience of domain expertise is distinct from elite expert performance that occurs in a small minority of virtuosos or prodigies, which can take a decade or lifetime to achieve.</p>
<p class="glo"><b>Dynamic Time Warping (DTW)</b> is a machine learning algorithm to align two time series such as feature vectors extracted over time based on similarity measurement. This similarity is often measured by distance measures such as Euclidean distance or based on correlation such as when aligning heterogenous modalities. A classical application example is speech recognition, where words spoken at different speed are aligned in time to measure their similarity. DTW aims at a maximized match between the two observation sequences usually based on local and global alignment path search restrictions.</p>
<p class="glo"><b>In early combination</b>, the inputs from all the different modalities are concatenated and fed to a single model. In <i>late combination</i>, for each modality there is a separate model that makes a prediction based on its modality, and these model predictions are later fused by a combining model.</p>
<p class="glo"><b>Early fusion</b> models are models for processing multimodal or multisensorial data, in which a model is processing the concatenation of all the data representations from the different modalities. In <i>late fusion</i> models, there is a unimodal model <a id="page_522"/>for each modality, and the outputs of all unimodal models are then combined to a final prediction based on all modalities.</p>
<p class="glo"><b>Encoder-decoder</b> architectures in deep learning start with an encoder neural network which&#8212;based on its input&#8212;usually outputs a feature map or vector. The second part&#8212;the decoder&#8212;is a further network that&#8212;based on the feature vector from the encoder&#8212;provides the closest match either to the input or an intended output. The decoder is in most cases employing the same network structure but in opposite orientation. Usually, the training is carried out unsupervised data, i.e., without labels. The target for learning is to minimize the reconstruction error, i.e., the delta between the input to the encoder and the output of the decoder. A typical application is to use encoderdecoder architectures for sequence-to-sequence mapping, such as in machine translation where the encoder is trained on sequences (phrases) in one language and the decoder is trained to map its representation to a sequence (phrase) in another language.</p>
<p class="glo"><b>An ensemble</b> is a set of models and we want the models in the set to differ in their predictions so that they make different errors. If we consider the space defined by the three dimensions that define a model as we defined above, the idea is to sample smartly from that space of learners. We want the individual models to be as accurate as possible individually, and at the same time, we want them to complement each other. How these two criteria affect the accuracy of the ensemble depends on the way we do the combination.</p>
<p class="gloi">From another perspective, we can view each particular model as one noisy estimate to the real (unknown) underlying problem. For example, in a classification task, each base classifier, depending on its model, hyper-parameters, and input features, learns one noisy estimator to the real discriminant. In such a perspective, the ensemble approach corresponds to constructing a final estimator from these noisy estimators&#8212;for example, voting corresponds to averaging them.</p>
<p class="gloi">When the different models use inputs in different modalities, there are three ways in which the predictions of models can be combined, namely, early, late, and intermediate combination/integration/fusion.</p>
<p class="glo"><b>Extraneous load</b> refers to the level of working memory load that a person experiences due to the properties of materials or computer interfaces they are using [Oviatt 2017].</p>
<p class="glo"><b>FACS</b> refers to the Facial Action Coding System [Ekman &#38; Friesen, 1978; Ekman, Friesen, &#38; Hager, 2002]. FACS describes facial activity in terms of anatomically <a id="page_523"/>based action units (AUs). Depending on the version of FACS, there are 33 to 44 AUs and a large number of additional &#8220;action descriptors&#8221; and other movements.</p>
<p class="glo"><b>Feature-level multimodal fusion</b>. The process of integrating features from different modalities using diverse methodologies such as concatenating the features together (early fusion) or combining the models obtained from each modality at decision level (late fusion).</p>
<p class="glo"><b>Fusion</b> A fourth challenge is to join information from two or more modalities to perform a prediction. For example, for audio-visual speech recognition, the visual description of the lip motion is fused with the speech signal to predict spoken words. The information coming from different modalities may have varying predictive power and noise topology, with possibly missing data in at least one of the modalities.</p>
<p class="glo"><b>Galvanic Skin Response (GSR)</b> refers to galvanic skin response which is a measure of the conductivity of human skin, and can provide an indication of changes in the human sympathetic nervous system during the cognitive task time.</p>
<p class="glo"><b>Gaussian mixture models (GMMs)</b> are probability density functions comprising a weighted sum of individual Gaussian components, each with their own mean and covariance. They are commonly employed to compactly characterize arbitrary distributions (e.g. of features) that are not well-fitted by a single Gaussian.</p>
<p class="glo"><b>Germane load</b> refers to the level of a person&#8217;s effort and activity compatible with mastering new domain content during learning. It pertains to the cognitive resources dedicated to constructing new schema in long-term memory.</p>
<p class="glo"><b>Gross errors</b> refer to non-Gaussian noise of large magnitude. Gross errors are often in abundance in audio-visual data due to incorrect localisation and tracking, presence of partial occlusions, enviromental noise etc.</p>
<p class="glo"><b>Hand-crafted features</b> refer to features developed to extract a specific type of information, usually as part of a hypothesis-driven research study. By contrast, data-driven features are those extracted automatically from raw signal data by algorithms (e.g., neural networks), whose physical interpretation often cannot easily be described.</p>
<p class="glo"><b>Incongruent expressions of affects</b>. A multimodal combination is said to involve incongruent expressions of affects if the combined modalities are conveying different affects in terms of the category or the dimension (e.g., the facial expressions express joy, while hand gestures express anger). Such combinations are also called <b>blends of emotions</b>. They might occur even in a single modality <a id="page_524"/>such as the facial expressions (e.g., the upper part of the face may express a certain emotion, while the bottom part of the face conveys a different emotion).</p>
<p class="gloi">Researchers exploring the perception of human (or computer-generated) multimodal expressions of affects usually consider the following attributes related to perception and affects that are impacted by or do impact multimodal perception:</p>
<p class="square">&#9632;&#160;&#160;Recognition rate</p>
<p class="square">&#9632;&#160;&#160;Reaction time</p>
<p class="square">&#9632;&#160;&#160;Affect categories</p>
<p class="square">&#9632;&#160;&#160;Affect dimensions</p>
<p class="square">&#9632;&#160;&#160;Multimodal integration patterns</p>
<p class="square">&#9632;&#160;&#160;Inter-individual differences and personality</p>
<p class="square">&#9632;&#160;&#160;Timing: synchrony vs. sequential presentation of the signals in different modalities</p>
<p class="square">&#9632;&#160;&#160;Modality dominance</p>
<p class="square">&#9632;&#160;&#160;Context: environment and task related information (e.g., food or violent scenes, and associated applications for specific users with food disorders or PTSD)</p>
<p class="square">&#9632;&#160;&#160;Task difficulty</p>
<p class="glot"><b>In intermediate combination</b>, each modality is first processed to get a more abstract representation and then all such representations from different modalities are fed together to a single model. This processing can be in the form of a <b>kernel function</b>, which is a measure of similarity, and such an approach is called <b>multiple kernel learning</b>. Or the intermediate processing may be done by one or more layers of a neural network, and such an approach corresponds to a <b>deep neural network</b>.</p>
<p class="gloi">The level of combination depends on the level we expect to see a <b>dependency</b> between the inputs in different modalities. Early combination assumes a dependency at the lowest level of input features; intermediate combination assumes a dependency at a more abstract or semantic level that is extracted after some processing of the raw input; late combination assumes no dependency in the input but only at the level of decisions.</p>
<p class="glot"><b>Intrinsic load</b> is the inherent difficulty level and related working memory load associated with the material being processed during a user&#8217;s primary task.</p>
<p class="glo"><a id="page_525"/><b>Learning analytics</b> involves the collection, analysis, and reporting of data about learners, including their activities and contexts of learning, in order to understand and provide better support for effective learning. First-generation learning analytics focused exclusively on computer-mediated learning using keyboard-and-mouse computer interfaces. This limited analyses to click-stream activity patterns and linguistic content. Early learning analytic techniques mainly have been applied to managing educational systems (e.g., attendance tracking, tracking work completion), advising students based on their individual profiles (e.g., courses taken, grades received, time spent in learning activities), and improving educational technologies. Learning analytics data typically are summarized on dashboards for educators, such as teachers or administrators.</p>
<p class="glo"><b>Leave-one-out cross validation</b>. Cross validation is the process of dividing a dataset into batches where one batch is reserved for testing and all the other batches are used for training a system. Leave-one-out means each batch is formed of a single instance.</p>
<p class="glo">The user <b>(long-term) traits</b> include biological trait primitives (e.g., age, gender, height, weight), cultural trait primitives in the sense of group/ethnicity membership (e.g,. culture, race, social class, or linguistic concepts such as dialect or first language), personality traits (e.g., the &#8220;OCEAN big five&#8221; dimensions openness, conscientiousness, extraversion, agreeableness, and neuroticism or likability), and traits that constitute subject idiosyncrasy, i.e., ID.</p>
<p class="glo"><b>A longer-term state</b> can subsume (partly self-induced) non-permanent, yet longer-term states (e.g., sleepiness, intoxication, mood such as depression (see also <a href="25_Chapter12.xhtml">Chapter 12</a> the health state such as having a flu), structural (behavioral, interactional, social) signals (e.g., role in dyads and groups, friendship and identity, positive/negative attitude, intimacy, interest, politeness), and (nonverbal) social signals (see <a href="19_Chapter07.xhtml">Chapters 7</a> and <a href="20_Chapter08.xhtml">8</a> and discrepant signals (e.g., deception (see also <a href="26_Chapter13.xhtml">Chapter 13</a>) irony, sarcasm, sincerity).</p>
<p class="glo"><b>Longitudinal data</b> refers to multiple recordings of the same type from the same individual at different points in time, between which it is likely that the individual&#8217;s state (e.g., depression score) has changed.</p>
<p class="glo"><i>L</i>() is the <b>loss function</b> that measures how far the prediction <i>g</i>(<i>x<sup>t</sup></i>&#124;<i>&#952;</i>) is from the desired value <i>r<sup>t</sup></i>. The complexity of this optimization problem depends on the particular <i>g</i>() and <i>L</i>(). Different learning algorithms in the machine learning literature differ either in the model they use, the loss function they employ, or the how the optimization problem is solved.</p>
<p class="gloi"><a id="page_526"/>This step above optimizes the parameters given a model. Each model has an inductive bias that is, it comes with a set of assumptions about the data and the model is accurate if its assumptions match the characteristics of the data. This implies that we also need a process of <i>model selection</i> where we optimize the model structure. This model structure depends on dimensions such as (i) the learning algorithm, (ii) the hyper-parameters of the model (that define model complexity), and (iii) the input features and representation, or modality. Each model corresponds to one particular combination of these dimensions.</p>
<p class="glo">In <b>machine learning</b>, the <b>learner</b> is a model that takes an input <i>x</i> and learns to give out the correct output <i>y</i>. In <b>pattern recognition</b>, typically we have a classification task where <i>y</i> is a class code; for example in face recognition, <i>x</i> is the face image and <i>y</i> is the index of the person whose face it is we are classifying.</p>
<p class="gloi">In building a learner, we start from a data set <i>&#967;</i> = {<i>x<sup>t</sup>, r<sup>t</sup></i>}, <i>t</i> = 1, &#8230;, <i>N</i> that contains training pairs of instances <i>x<sup>t</sup></i> and the desired output values <i>r<sup>t</sup></i> (e.g., class labels) for them. We assume that there is a dependency between <i>x</i> and <i>r</i> but that it is unknown&#8212;If it were known, there would be no need to do any learning and we would just write down the code for the mapping.</p>
<p class="gloi">Typically, <i>x<sup>t</sup></i> is not enough to uniquely identify <i>r<sup>t</sup></i>; we call <i>x<sup>t</sup></i> the observables and there may also be unobservables that affect <i>r<sup>t</sup></i> and we model their effect as noise. This implies that each training pair gives us only a limited amount of information. Another related problem is that in most applications, <i>x</i> has a very high dimensionality and our training set samples this high dimensional space very sparsely.</p>
<p class="gloi">Our prediction is given by our predictor <i>g</i>(<i>x<sup>t</sup></i>|<i>&#952;</i>) where <i>g</i>() is the model and <i>&#952;</i> is its set of parameters. Learning corresponds to finding that best <i>&#952;</i>* that makes our predictions as close as possible to the desired values on the training set:</p>
<p class="eqna"><img src="../images/pg526_01.png" alt="Image"/></p>
<p class="glo"><b>Mel frequency cepstral coefficients (MFCCs)</b> are features that compactly represent the short-term speech spectrum, including formant information, and are widely used to characterize both spoken content (for automatic speech recognition) and speaker-specific qualities (for automatic speaker verification). Briefly, a mel-scale frequency-domain filterbank is applied to the spectrum to obtain mel filterbank energies, the log of which is transformed to a lower-dimensional representation using the discrete cosine transform.</p>
<p class="glo"><a id="page_527"/><b>Metacognitive awareness</b> involves higher-level self-regulatory behaviors that guide the learning process, such as an individual&#8217;s awareness of what type of problem they are working on and how to approach solving it, the ability to diagnose error states, or understanding what tools are best suited for a particular task.</p>
<p class="glo"><b>Moment of insight</b> refers to one of several phases during the process of problem solving. It involves the interval of time immediately before and after a person consciously realizes the solution to a problem they&#8217;ve been working on. This idea represents what the person believes is the solution, although it may or may not be correct.</p>
<p class="glo"><b>Multi-level multimodal learning analytics</b> refers to the different levels of analysis enabled by multimodal data collection. For example, speech and handwriting can be analyzed at the signal, activity pattern, representational, or metacognitive levels. During research on learning, it frequently is valuable to analyze data across behavioral and physiological/neural levels for a deeper understanding of any learning effects. In this regard, multi-level multimodal learning analytics can support a more comprehensive systems-level view of the complex process of learning.</p>
<p class="glo"><b>A multimodal corpus</b> targets the recording and annotation of multiple communication modalities including speech, hand gesture, facial expression, body posture, etc. Today, most corpora that are multimodal consist of audio-visual data. Other modalities such as 3D body and gaze tracking, or physiological signals are hardly present, but are needed to provide a broader picture of human interaction. The collection of large databases rich of social behavior expressed through a variety of modalities is key to model the complexity of social interaction [Vinciarelli et al. 2012, Eerekoviae 2014].</p>
<p class="glo"><b>Multimodal fusion</b> is the process of combining information from multiple modalities, such as audio and video, into a homogenous and consistent representation. Combining affective and social cues across channels is important to resolve situations where social behavior is expressed in a complementary [Zeng et al. 2009] or even contradictory way [Douglas-Cowie et al. 2005]. This also involves a proper modeling of the complex temporal relationships that exist between the diverse channels. It can help to achieve higher precision such as in cognitive load measurement, or better reliability via overcoming the limitations of individual signal or interaction modalities. The fusion can be done at different stages: mid-fusion and late-fusion. Mid-fusion refers to the fusion of features extracted from multimodalities before classifications, while late-fusion is the fusion of classification scores from single modality decisions.</p>
<p class="glo"><a id="page_528"/><b>Multimodal learning analytics</b> is an emerging area that analyzes students&#8217; natural communication patterns (e.g., speech, writing, gaze, non-verbal behavior), activity patterns (e.g., number of hours spent studying), and physiological and neural patterns (e.g., EEG, fNIRS) in order to predict learning-oriented behaviors during educational activities. These rich data can be analyzed at multiple levels, for example at the signal level (e.g., speech amplitude), activity level (e.g., frequency of speaking), representational level (e.g., linguistic content), and others. These second-generation learning analytic techniques are capable of predicting mental states during the process of learning, in some cases automatically and in real time.</p>
<p class="glo"><b>Multimodal or multiview signals</b> are sets of heterogeneous data, captured by different sensors, such as various types of cameras, microphones, and tactile sensors and in different contexts.</p>
<p class="glo"><b>Online recognition</b> means that a system is able to detect and analyze affective and social cues on-the-fly from the raw sensor input. Decisions based on the perceived user state need to be made fast enough to allow for a fluent interaction and it is not possible to look ahead in time. Setting up an online system is more complex than processing data offline.</p>
<p class="glo"><b>Overfitting</b> is a problem that occurs when the training or estimation of a machine learning method is performed on data with too few training examples relative to the number of parameters to be estimated. The resulting problem is that the method becomes too closely tuned to the training data, and generalizes poorly to unseen test data.</p>
<p class="glo"><b>Physiological sensor</b>. A device that uses a transducer and a biological element to collect physiological responses, such as heart rate and skin conductance, and convert them into an electrical signal. The measures obtained with such devices provide quantitative feedback about physiological changes or processes experienced by research subjects.</p>
<p class="glo"><b>Prerequisites for learning</b> are precursors for successful learning to occur, which can provide early markers. They include attention to the learning materials, emotional and motivational predisposition to learn, and active engagement with the learning activitites.</p>
<p class="glo"><b>A pseudo-multimodal</b> approach exploits a modality not only by itself, but in addition to estimate another modality&#8217;s behavior to replace it. An example is estimating the heart rate from speech parameters and using it alongside (other) speech parameters.</p>
<p class="glo"><a id="page_529"/><b>A Recurrent Neural Network (RNN)</b> is a neural network that contains one or more recurrent layers. A <i>recurrent layer</i> is a layer that takes a sequence <i>x</i> indexed by <i>t</i> and processes it element by a element, while maintaining a <i>hidden state</i> for each unit in the layer: <i>h<sub>t</sub></i> = <i>RNN</i>(<i>h</i><sub><i>t</i>&#8722;1</sub>, <i>x<sub>t</sub></i>), where <i>h<sub>t</sub></i> is the hidden state at step <i>t</i>, and <i>RNN</i> is a transition function to compute the next hidden state, that depends on the type of hidden layer.</p>
<p class="glo"><b>Redundancy</b>: tendency of multiple signals or communication channels to carry the same or widely overlapping information.</p>
<p class="glo"><b>Representation</b> A first fundamental challenge is learning how to represent and summarize multimodal data in a way that exploits the complementarity and redundancy of multiple modalities. The heterogeneity of multimodal data makes it challenging to construct such representations. For example, language is often symbolic while audio and visual modalities will be represented as signals.</p>
<p class="glo"><b>A sequence-to-sequence</b> model is a neural network that processes a sequence as its input and produces another sequence as its output. Example of such models include neural machine translation and end-to-end speech recognition models.</p>
<p class="glo"><b>Shared hidden layer</b> is a layer within a neural network which is shared within the topology. For example, different modalities, or different output classes, or even different databases could be trained within parts of the network mostly. In the shared hidden layer, however, they would share neurons by according connections. This can be an important approach to model diverse information types largely independently but provide mutual information exchange at some point in the topology of a neural network.</p>
<p class="glo"><b>A short-term state</b> includes the mode (e.g., speaking style and voice quality), emotions, and affects (e.g., confidence, stress, frustration, pain, uncertainty, see also <a href="18_Chapter06.xhtml">Chapters 6</a> and <a href="20_Chapter08.xhtml">8</a>.</p>
<p class="glo"><b>Skilled performance</b> involves the acquisition of skilled action patterns, for example when learning to become an expert athlete or musician. Skilled performance also can involve the acquisition of communication skills, as in developing expertise in writing compositions or giving oral presentations. The role of deliberate practice has been emphasized in acquiring skilled performance.</p>
<p class="glo"><b>Social Signals</b>: constellations of nonverbal behavioural cues aimed at conveying socially relevant information such as attitudes, personality, intentions, etc.</p>
<p class="glo"><b>Social Signal Processing</b>: computing domain aimed at modeling, analysis and synthesis of social signals in human-human and human-machine interactions.</p>
<p class="glo"><a id="page_530"/><b>A softmax layer</b> is a dense layer followed by the softmax nonlinearity. The softmax nonlinearity takes a one-dimensional vector <i>v</i> of real numbers and normalizes it into a probability distribution, by applying <img src="../images/inline530.png" alt="Image"/>, where the sum is over all coordinates of the vector <i>v</i>.</p>
<p class="glo"><b>Spatio-temporal</b> features are those which have both a spatial and a time dimension. For example, the intensity of pixels in a video vary both in terms of their position within a given frame (spatial dimension) and in terms of the frame number for a given pixel coordinate (temporal dimension).</p>
<p class="glo"><b>Support vector machine (SVM)</b> is a widely used discriminative classification method, which defines a separating hyperplane between two classes of features, which is defined in terms of particular feature instances that are close to the class boundaries, called support vectors.</p>
<p class="glo"><b>Support vector regression (SVR)</b> is a commonly used method for multivariate regression, which concentrates on fitting a model by considering only training features that are not very close to the model prediction.</p>
<p class="glo"><b>Temporal dynamics of facial expression</b>: rather than being like a single snapshot, facial appearance changes as a function of time. Two main factors affecting temporal dynamics of facial expression is the speed with which they unfold and the changes of their intensity over time.</p>
<p class="glo"><b>Transfer learning</b> helps to reuse knowledge gained in one task in another task in machine learning. It can be executed on different levels, such as the feature or model level. For example, a neural network can be trained on a related task to the task of interest at first. Then, the actual task of interest is trained &#8220;on top&#8221; of this pre-training of the network. Likewise, rather than starting to train the target task of interest based on a random initialization of a network, related data could be used to provide a better starting point.</p>
<p class="glo"><b>Transfer of learning</b> refers to students&#8217; ability to recognize parallells and make use of learned information in new contexts, for example to apply learned information outside of the classroom, in contexts not resembling the original framing of the problem, with different people present, and so forth. This requires generalizing the learned information beyond its original concrete context.</p>
<p class="glo"><b>Translation</b> A second challenge addresses how to translate (map) data from one modality to another. Not only is the data heterogeneous, but the relationship between modalities is often open-ended or subjective. For example, there exist <a id="page_531"/>a number of <i>correct</i> ways to describe an image and one perfect translation may not exist.</p>
<p class="glo"><b>T-unit analysis</b>. The analysis of terminable units of language (T-unit), which is the smallest group of words that could be considered as a grammatical sentence, regardless of how is punctuated. T-unit analysis is used extensively to measure the overall complexity of both speech and writing samples and consists mainly on measuring different aspects of their syntactic construction in text such as mean length of the t-units, and number of clauses present in each unit, among others.</p>
<p class="glo"><b>User-independent model</b> A model that generalizes to a different set of users beyond those used to develop the model.</p>
<p class="glo"><b>Voice quality</b> refers to the type of phonation during voiced speech. Depending on the physical movement of the vocal folds during phonation, the perceived quality of speech can change, even for the same speech sound uttered at the same pitch. Descriptors such as &#8220;creaky&#8221; and &#8220;breathy&#8221; are applied to specific modes of vocal fold vibration.</p>
<p class="glo"><b>Vowel space area</b> is a term given to the two dimensional area enclosed by lines connecting pairs of vowels in the formant (F1/F2) space.</p>
<p class="glo"><b>Zero-shot learning</b> is a method in machine learning to learn a new task without any training examples for this task. An example could be recognizing a new type of object without any visual example but based on a semantic description such as specific features that describe the object.</p>
</body>
</html>