<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="fmtitle"><a id="page_xxi"/><b>Figure Credits</b></p>
<p class="tocfig"><a href="15_Chapter04.xhtml#fig4_3"><b>Figure 4.3</b></a> Based on: O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 2015c. Show and tell: A neural image caption generator. In <i>Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</i>, pp. 3156&#8211;3164. IEEE.</p>
<p class="tocfig"><a href="15_Chapter04.xhtml#fig4_4"><b>Figure 4.4</b></a> Based on: O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 2015c. Show and tell: A neural image caption generator. In <i>Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</i>, pp. 3156&#8211;3164. IEEE</p>
<p class="tocfig"><a href="15_Chapter04.xhtml#fig4_5"><b>Figure 4.5</b></a> From: E. Mansimov, E. Parisotto, J. L. Ba, and R. Salakhutdinov. 2015. Generating images from captions with attention. In <i>Proceedings of the International Conference on Learning Representations</i>. Courtesy of the authors. Used with permission.</p>
<p class="tocfig"><a href="15_Chapter04.xhtml#fig4_6"><b>Figure 4.6</b></a> From: D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural machine translation by jointly learning to align and translate. In <i>Proceedings of the International Conference on Learning Representations</i>. Courtesy of the authors. Used with permission.</p>
<p class="tocfig"><a href="15_Chapter04.xhtml#fig4_7"><b>Figure 4.7</b></a> From: K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In <i>Proceedings of the International Conference on Machine Learning</i>, pp. 2048&#8211;2057. Courtesy of the authors. Used with permission.</p>
<p class="tocfig"><a href="15_Chapter04.xhtml#fig4_8"><b>Figure 4.8</b></a> From: A. E. Mousa. 2014. Sub-Word Based Language Modeling of Morphologically Rich Languages for LVCSR. Ph.D. thesis, Computer Science Department, RWTH Aachen University, Aachen, Germany. Courtesy of Amr Ibrahim El-Desoky Mousa. Used with permission.</p>
<p class="tocfig"><a href="15_Chapter04.xhtml#fig4_12"><b>Figure 4.12</b></a> From: A. Karpathy and L. Fei-Fei. 2015. Deep visual-semantic alignments for generating image descriptions. In <i>Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</i>, pp. 3128&#8211;3137. Copyright &#169; 2015 IEEE. Used with permission.</p>
<p class="tocfig"><a href="15_Chapter04.xhtml#fig4_13"><b>Figure 4.13</b></a> From: S. Reed, Z. Akata, H. Lee, and B. Schiele. 2016. Learning deep representations of fine-grained visual descriptions. In <i>Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</i>. Copyright &#169; 2016 IEEE. Used with permission.</p>
<p class="tocfig"><a href="21_Chapter09.xhtml#fig9_1"><b>Figure 9.1</b></a> Based on: M. Y. Tsalamlal, M-A. Amorim, J-C. Martin, and M. Ammi. 2017. Combining facial expression and touch for perceiving emotional valence. <i>IEEE Transactions on Affective Computing</i>, 99. IEEE.</p>
<p class="tocfig"><a href="23_Chapter10.xhtml#fig10_4"><b>Figure 10.4</b></a> From: NASA-TLX APP on iOS. <a href="http://itunes.apple.com/us/app/nasa-tlx/id1168110608">http://itunes.apple.com/us/app/nasa-tlx/id1168110608</a>. Copyright &#169; 2018 NASA. Used with permission.</p>
<p class="tocfig"><a id="page_xxii"/><a href="23_Chapter10.xhtml#fig10_5"><b>Figure 10.5</b></a> From: Y. Shi, E. Choi, R. Taib, and F. Chen. 2010. Designing Cognition-adaptive human&#8211;computer interface for mission-critical systems. In G. A. Papadopoulos, W. Wojtkowski, G. Wojtkowski, S. Wrycza, and J. Zupan?i?, editors, <i>Information Systems Development</i>, pp. 111&#8211;119. Copyright &#169; 2010 Springer. Used with permission.</p>
<p class="tocfig"><a href="23_Chapter10.xhtml#fig10_8"><b>Figure 10.8</b></a> From: N. Nourbakhsh, Y. Wang, F. Chen, and R. A. Calvo. 2012. Using Galvanic skin response for cognitive load measurement in arithmetic and reading tasks. In <i>Proceedings of the 24th Australian Computer-Human Interaction Conference</i>, OzCHI &#8217;12 pp. 420&#8211;423. Copyright &#169; 2012 ACM. Used with permission.</p>
<p class="tocfig"><a href="23_Chapter10.xhtml#fig10_9"><b>Figure 10.9</b></a> From: W. Wang, Z. Li, Y. Wang, and F. Chen. 2013. Indexing cognitive workload based on pupillary response under luminance and emotional changes. In <i>Proceedings of the 2013 International Conference on Intelligent User Interfaces</i>, IUI &#8217;13. pp. 247&#8211;256. Copyright &#169; 2013 ACM. Used with permission.</p>
<p class="tocfig"><a href="23_Chapter10.xhtml#fig10_10"><b>Figure 10.10</b></a> From: F. Chen et al. 2012. Multimodal behavior and interaction as indicators of cognitive load. In <i>ACM Transactions on Interactive Intelligent Systems</i> 2(4):22:1&#8211;22:36. Copyright &#169; 2012 ACM. Used with permission.</p>
<p class="tocfig"><a href="23_Chapter10.xhtml#fig10_12"><b>Figure 10.12</b></a> From: F. Chen et al. 2012. Multimodal behavior and interaction as indicators of cognitive load. In <i>ACM Transactions on Interactive Intelligent Systems</i> 2(4):22:1&#8211;22:36. Copyright &#169; 2012 ACM. Used with permission.</p>
<p class="tocfig"><a href="23_Chapter10.xhtml#fig10_13"><b>Figure 10.13</b></a> From: F. Chen et al. 2012. Multimodal behavior and interaction as indicators of cognitive load. In <i>ACM Transactions on Interactive Intelligent Systems</i> 2(4):22:1&#8211;22:36. Copyright &#169; 2012 ACM. Used with permission.</p>
<p class="tocfig"><a href="24_Chapter11.xhtml#fig11_1"><b>Figure 11.1</b></a> From: S. Oviatt and A. Cohen. 2013. Written and multimodal representations as predictors of expertise and problem-solving success in mathematics. In <i>Proceedings of the 15th ACM International Conference on Multimodal Interaction</i>, pp. 599&#8211;606. Copyright &#169; 2013 ACM. Used with permission.</p>
<p class="tocfig"><a href="24_Chapter11.xhtml#fig11_2"><b>Figure 11.2</b></a> From: S. Oviatt and A. Cohen. 2013. Written and multimodal representations as predictors of expertise and problem-solving success in mathematics. In <i>Proceedings of the 15th ACM International Conference on Multimodal Interaction</i>, pp. 599&#8211;606. Copyright &#169; 2013 ACM. Used with permission.</p>
<p class="tocfig"><a href="24_Chapter11.xhtml#fig11_3"><b>Figure 11.3</b></a> From: C. Leong, L. Chen, G. Feng, C. Lee, and M. Mulholland. 2015. Utilizing depth sensors for analyzing multimodal presentations: Hardware, software and toolkits. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 547&#8211;556. Copyright &#169; 2015 ACM. Used with permission.</p>
<p class="tocfig"><a href="24_Chapter11.xhtml#fig11_4"><b>Figure 11.4</b></a> From: M. Raca and P. Dillenbourg. 2014. Holistic analysis of the classroom. In <i>Proceedings of the ACM International Data-Driven Grand Challenge workshop on Multimodal Learning Analytics</i>, pp. 13&#8211;20. Copyright &#169; 2014 ACM. Used with permission.</p>
<p class="tocfig"><a href="24_Chapter11.xhtml#fig11_5"><b>Figure 11.5</b></a> From: F. Dominguez, V. Echeverria, K. Chiluiza, and X. Ochoa. 2015. Multimodal selfies: Designing a multimodal recording device for students in traditional classrooms. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 567&#8211;574. Copyright &#169; 2015 ACM. Used with permission.</p>
<p class="tocfig"><a id="page_xxiii"/><a href="24_Chapter11.xhtml#fig11_6"><b>Figure 11.6</b></a> From: A. Ezen-Can, J. F. Grafsgaard, J. C. Lester, and K. E. Boyer. 2015. Classifying student dialogue acts with multimodal learning analytics. In <i>Proceedings of the Fifth International Conference on Learning Analytics And Knowledge</i>, pp. 280&#8211;289. Copyright &#169; 2015 ACM. Used with permission.</p>
<p class="tocfig"><a href="25_Chapter12.xhtml#fig12_2"><b>Figure 12.2</b></a> Based on: H. Dibeklioglu, Z. Hammal, Y. Yang, and J.F. Cohn. 2015. Multimodal detection of depression in clinical interviews. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, Seattle, WA. ACM.</p>
<p class="tocfig"><a href="25_Chapter12.xhtml#fig12_3"><b>Figure 12.3</b></a> From: J. Joshi, A. Dhall, R. Goecke, M. Breakspear, and G. Parker. 2012. Neuralnet classification for spatio-temporal descriptor based depression analysis. In <i>Proceedings of the International Conference on Pattern Recognition</i>, Tsukuba, Japan, pp. 2634&#8211;2638. Copyright &#169; 2012 IEEE. Used with permission.</p>
<p class="tocfig"><a href="25_Chapter12.xhtml#fig12_4"><b>Figure 12.4</b></a> From: N. Cummins. 2016. Automatic assessment of depression from speech: paralinguistic analysis, modelling and machine learning. Ph.D. Thesis, UNSW Australia. 375, 377, 384. Courtesy of Nicholas Peter Cummins. Used with permission.</p>
<p class="tocfig"><a href="25_Chapter12.xhtml#fig12_5"><b>Figure 12.5</b></a> From: J. Joshi, A. Dhall, R. Goecke, and J.F. Cohn. 2013c. Relative body parts movement for automatic depression analysis. In <i>Proceedings of the Conference on Affective Computing and Intelligent Interaction</i>, pp. 492&#8211;497. Copyright &#169; Springer 2013. Used with permission.</p>
<p class="tocfig"><a href="26_Chapter13.xhtml#fig13_3"><b>Figure 13.3</b></a> From: P. Tsiamyrtzis, J. Dowdall, D. Shastri, I. T. Pavlidis, M. G. Frank, and P. Ekman. 2007. Imaging facial physiology for the detection of deceit. <i>International Journal of Computer Vision</i>, 71(2): 197&#8211;214. Copyright &#169; Springer 2007. Used with permission.</p>
</body>
</html>