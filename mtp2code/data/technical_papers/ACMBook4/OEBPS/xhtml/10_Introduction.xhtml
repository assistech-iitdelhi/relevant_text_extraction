<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="fmtitle"><a id="page_1"/><b>Introduction: Trends in Intelligent Multimodal-Multisensorial Interfaces: Cognition, Emotion, Social Signals, Deep Learning, and More</b></p>
<p class="noindent">Close your eyes and imagine the beginning of Human-Computer Interaction (HCI). What do you see? In one&#8217;s mind, these early days of computer interfaces are usually marked by a direct mapping of user input via buttons, keyboards, joysticks, etc. In addition, in this picture of early-day HCI, Artificial Intelligence (AI)&#8212;or even artificial emotional and social intelligence&#8212;would hardly play a role. Since then, things have certainly changed dramatically&#8212;we now include all of AI when we interact with computing systems, e.g., by speech or gestures. The picture will soon change once more so as to consider the emotional and social capabilities of computer interfaces, as will be introduced in detail in this volume. However, let us start with the history of HCI and AI. After that, the volume will be introduced chapter by chapter.</p>
<p class="h1n"><a id="int_1"/><b>A Very Brief History of HCI and AI&#8212;and Their Relationship in Time</b></p>
<p class="noindent">Looking at the history of HCI, what followed keyboards such as those used in teleprinters as an interaction-changing paradigm&#8212;as early as in 1926&#8212;were arguably the first two-axes joysticks. The first trackballs for (analog at the time) computers are said to have appeared after the end of World War II. The first patent dates to 1947 when secret military implementations were prevailing at the time. Light pens were next, first occurring right in the middle of the 1950s (during the Whirlwind project at MIT). The first computer mouse arrived only in 1964 [English et al. 1965]. On the other hand, as early as 1952, Bell Labs realized a first prototype <a id="page_2"/>of a speaker-dependent speech recognizer [Juang and Rabiner 2005]. Even though the vocabularies at the time hardly surpassed ten words, it seems worth setting the above into temporal relation. The roots of Voice User Interfaces (VUI)&#8212;clearly demanding AI for their realization&#8212;followed surprisingly quickly the roots of the Graphical User Interface (GUI). Accordingly, one could argue that AI was used in ambitions toward improving computer interaction surprisingly early on. Noteworthy in this context is that the term Artificial Intelligence itself was only born in the 1955 Dartmouth summer research project proposal [McCarthy et al. 2006].</p>
<p class="h1n"><a id="int_2"/><b>Increasingly Robust AI as a Game-Changer for HCI</b></p>
<p class="noindent">Over the last two decades, AI&#8217;s usage in computer interfaces has increased dramatically. Think, for example, of the rise of increasingly naturalistic interaction via handwriting, speech, and video-based interaction, to name three &#8220;AI-heavy&#8221; frequently used interaction examples. To stay with VUIs as an example, in 2015 65% of smartphone owners in the United States are said to have used such technology, resembling a usage increase by a factor above two over two years.<sup><a id="rfn1" href="#fn1">1</a></sup> In fact, AI itself has changed dramatically in the last decade. One could argue that it was this change that increased acceptance of &#8220;smarter&#8221; interaction technology by users, as the improvements in AI performance made AI-empowered interaction more usable. Using speech once more as an example, in 2018 Microsoft and IBM had surpassed (single) human transcription abilities on the popular telephone conversation Switchboard<sup><a id="rfn2" href="#fn2">2</a></sup> speech recognition task at a word error rate slightly above 5%, i.e., 1 out of 20 words is erroneously transcribed. This is comparable to human-parity and exceeds a single human expert&#8217;s speech recognition abilities. To put this into perspective, on the same task in 1995 the word error rate was around 43%, in 2004 it was around 15%, and in 2016 it was 8%. Clearly, this improvement in robustness is a decisive factor when it comes to one&#8217;s choice of modality for interaction. This change in AI&#8217;s reliability in interaction use cases is usually attributed largely to the recent achievements in deep learning. While this term appeared in the 1980s [Dechter 1986] and was used in the context of artificial neural networks (ANNs), the underlying machine learning paradigm of today&#8217;s deep learning appeared later. Important early breakthroughs came largely from solutions provided by Hochreiter and Schmidhuber [1997] and Hinton et al. [2006]. These contributions support efficiently training &#8220;deeper&#8221; ANNs with more than three or even up <a id="page_3"/>to hundreds of information-processing layers. Furthermore, the networks can also be efficiently trained in recurrent topologies. This allows for better model time series data such as typical interaction data. Overall, these deeper networks thus can likewise unleash their potential in efficiently learning from larger amounts of data and largely unlabeled data. Up to millions or even billions of learning parameters can be trained in these networks. In fact, the number of neurons of the human brain has already been surpassed. Such high numbers of learning parameters allow for the formation of highly complex decision boundaries as occur in real-world demanding interaction tasks, e.g., robust speech recognition or natural language understanding.</p>
<p class="h1n"><a id="int_3"/><b>Multimodal Signal Processing, Architectures and Deep Learning</b></p>
<p class="noindent">Multimodal processing of human (input) signals and their synergistic combination holds several promises to make interaction more robust against failure. For example, if one or several modalities should suffer from adverse conditions, information from the other modalities can still be exploited. Further, multimodal interaction has repeatedly been shown to contribute to more efficient, natural, and enjoyable interaction. Likewise, it is not surprising that the idea to go multimodal is &#8220;out there&#8221; since longer, such as in a mid-1980s work that combined the keyboard with speech input [Mitchell and Forren 1987]. Multimodal interaction does, however, not always live up to expectations [Oviatt 1999]. A crucial factor is the optimal integration of the information sources. Accordingly, a plethora of machine learning strategies usually along the range of early fusion of signals and late fusion after making decisions per signal source has been introduced.</p>
<p class="indent">Perhaps the first attempt to model multimodal human interaction&#8212;albeit between humans&#8212;exploiting deep learning appeared in the early days of this branch of AI [Reiter et al. 2006]. Therefore, deep learning has broadly found its way into multimodal signal processing for human communication and interaction analysis. In fact, deep learning has increasingly replaced conventional expert-crafted signal processing of human input. Unsupervised representation learning and learning &#8220;end-to-end&#8221; from raw data replaced traditional pre-processing or extraction of features. Deep neural networks do not only learn to make a final decision based on extracted features, but also learn feature extraction and pre-processing directly from (user) data.</p>
<p class="indent">Thanks to all this progress, it is indeed becoming more and more an everyday reality that interaction has become multimodal. For example, one can speak to a smartphone to enter a navigation target and use finger gestures to zoom or move the map or use gestures and speech to control a smart TV or video console.</p>
<div class="box">
<p class="bhead"><a id="page_4"/><b>Glossary</b></p>
<p class="hangbx"><b>Canonical Correlation Analysis (CCA)</b> is a tool to infer information based on cross-covariance matrices. It can be used to identify correlation across heterogeneous modalities or sensor signals. Let each modality be represented by a feature vector and let us assume there is correlation across these, such as is in audiovisual speech recognition. Then, CCA will identify linear combinations of the individual features with maximum correlation amongst each other.</p>
<p class="hangbx"><b>Confidence measure</b> is the information on the assumed certainty of a decision made by a machine learning algorithm.</p>
<p class="hangbx"><b>Cooperative learning</b> in machine learning is a combination of active learning and semi-supervised learning. In the semi-supervised learning part, the machine learning algorithm labels unlabeled data based on its own previously learned model. In the active learning part, it identifies which unlabeled data is most important to be labeled by humans. Usually, cooperative learning tries to minimize the amount of data to be labeled by humans while maximizing the gain in accuracy of a learning algorithm. This can be based on confidence measures such that the machine labels unlabeled data itself as long as it is sufficiently confident in its decisions. It asks humans for help only where its confidence is insufficient, but the data seem to be highly informative.</p>
<p class="hangbx"><b>Dynamic Time Warping (DTW)</b> is a machine learning algorithm to align two time series such as feature vectors extracted over time based on similarity measurement. This similarity is often measured by distance measures such as Euclidean distance or based on correlation such as when aligning heterogenous modalities. A classical application example is speech recognition, where words spoken at different speed are aligned in time to measure their similarity. DTW aims at a maximized match between the two observation sequences usually based on local and global alignment path search restrictions.</p>
<p class="hangbx"><b>Encoder-decoder</b> architectures in deep learning start with an encoder neural network which&#8212;based on its input&#8212;usually outputs a feature map or vector. The second part&#8212;the decoder&#8212;is a further network that&#8212;based on the feature vector from the encoder&#8212;provides the closest match either to the input or an intended output. The decoder is in most cases employing the same network structure but in opposite orientation. Usually, the training is carried out on unsupervised data, i.e., without labels. The target for learning is to minimize the reconstruction error, i.e., the delta between the input to the encoder and the output of the decoder. A typical application is to use encoder-decoder architectures for sequence-to-sequence mapping, such as in machine translation where the encoder is trained on sequences (phrases) in one language and the decoder is trained to map its representation to a sequence (phrase) in another language.</p>
<p class="hangbx"><a id="page_5"/><b>Shared hidden</b> layer is a layer within a neural network which is shared within the topology. For example, different modalities, or different output classes, or even different databases could be trained within parts of the network mostly. In the shared hidden layer, however, they would share neurons by according connections. This can be an important approach to model diverse information types largely independently but provide mutual information exchange at some point in the topology of a neural network.</p>
<p class="hangbx"><b>Transfer learning</b> helps to reuse knowledge gained in one task in another task in machine learning. It can be executed on different levels, such as the feature or model level. For example, a neural network can be trained on a related task to the task of interest at first. Then, the actual task of interest is trained &#8220;on top&#8221; of this pre-training of the network. Likewise, rather than starting to train the target task of interest based on a random initialization of a network, related data could be used to provide a better starting point.</p>
<p class="hangbx"><b>Zero-shot learning</b> is a method in machine learning to learn a new task without any training examples for this task. An example could be recognizing a new type of object without any visual example but based on a semantic description such as specific features that describe the object.</p>
</div>
<p class="h1n"><a id="int_4"/><b>The Advent of Artificial Emotional and Social Intelligence</b></p>
<p class="noindent">The increasingly &#8220;intelligent&#8221; interaction in more and more multimodal ways is, however, still lacking a major factor as observed in human-to-human interaction and communication these days: the emotional and social intelligence. With the rise of Affective Computing [Picard 1995], the technical means of integrating user emotion recognition or computer emotion simulation as in dialogs has become available. In fact, first patents or research implementations date even earlier, such as the 1978 patent &#8220;<i>determine the emotional state of a person</i>&#8221; by a speech analyzer [Williamson 1978]. The first computer-based implementations started in the 1990s for emotional speech analysis [Chiu et al. 1994] or emotional speech synthesis [Cahn 1990]. Implementations of facial emotion recognition appeared at a similar time as [Mase 1991, Kobayashi and Hara 1992], as well as ground-laying work on facial expression synthesis [Parke 1972]. Today, the field has widened to modeling a <a id="page_6"/>large variety also of human cognitive states such as human cognitive load, attention, or interest. To recognize such information, exploitation of multimodal information is now looking back on more than two decades of experience. Early ideas have been formulated as blueprint, e.g., [De Silva et al. 1997], and implemented [Chen et al. 1998]. Therefore, it is largely acknowledged that a multimodal approach is highly synergistic, such as the voice acoustics being well suited to reveal the arousal of a user, and the facial expression or verbal content to indicate the valence. In addition, artificial social intelligence recognizing and interpreting behavioral cues of users such as social signals [Pentland 2007] is becoming a further aid to better model and understand a user and her behavior. The difference lies largely in a focus on dyadic or multi-party communication and interaction. Likewise, instead of analyzing the speech of a user in isolation, the communication flow or adaptation between communication partners would be of interest. This could include the turn-taking behavior or the change of mean pitch to adapt to the conversational partner. An example from the visual domain would be the presence or absence of eye contact with a robot when it comes to interaction. Also, spatial relations would be of interest, such as head orientation, distance to the conversational partner, or orientation to the conversational partner. Further, rather than focusing on states as emotion, social signal processing often deals with short events such as eye blinks or laughter.</p>
<p class="h1n"><a id="int_5"/><b>Insights in the Chapters Ahead</b></p>
<p class="noindent">This handbook presents chapters that summarize basic research and development of multimodal-multisensor systems, including their status today and rapidly growing future directions. The initial volume [Oviatt et al. 2017] introduced relevant theory and neuroscience foundations, approaches to design and user modeling, and an in-depth look at some common modality combinations. The present second volume summarizes multimodal-multisensor system signal processing, architectures, and the emerging use of these systems for detecting emotional and cognitive states. The third volume [Oviatt et al. 2018] presents multimodal language and dialogue processing, software tools and platforms, commercialization of applications, and emerging technology trends and societal implications. Collectively, these handbook chapters address a comprehensive range of central issues in this rapidly changing field. In addition, each volume includes selected challenge topics, in which an international panel of experts exchanges their views on some especially consequential, timely, and controversial problem in the field that is in need of insightful resolution. We hope these challenge topics will stimulate talented students <a id="page_7"/>to tackle these important societal problems and motivate the rest of us to envision and plan for our technology future.</p>
<p class="indent">Information presented in these three volumes is intended to provide a comprehensive state-of-the-art resource for professionals, business strategists and technology funders, interested lay readers, and advanced undergraduate and graduate students in this multidisciplinary computational field. To enhance its pedagogical value to the readers, many chapters include valuable digital resources such as pointers to open-source tools, databases, video demonstrations, and case study walkthroughs to assist in designing, building, and evaluating multimodal-multisensor systems. Each handbook chapter defines the basic technical terms required to understand its topic. Educational resources, such as focus questions, are included to support readers in mastering newly presented materials.</p>
<p class="h1n"><b>Multimodal Signal Processing and Architectures</b></p>
<p class="noindent">The first of three parts of this volume is divided into the topics of multimodal signal processing and architectures for multimodal and multisensorial information fusion. In <a href="12_Chapter01.xhtml">Chapter 1</a>, Baltrusaitis et al. introduce a taxonomy of multimodal machine learning divided into the five aspects of representation, namely: (1) to represent and summarize multimodal data &#8220;<i>in a way that exploits the complementarity and redundancy of multiple modalities</i>&#8221;; (2) the alignment (&#8220;<i>identify the direct relations between (sub) elements from two or more different modalities</i>&#8221;); (3) the translation (&#8220;<i>how to map data from one modality to another</i>&#8221;); (4) the fusion (&#8220;<i>join information from two or more modalities</i>&#8221;); and (5) the co-learning (&#8220;<i>transfer knowledge between modalities, their representation, and their predictive models</i>&#8221;). The authors highlight that there might not be just one optimal machine learning solution for all of these aspects depending on the task. They further argue that the current increase in research on representation and translation benefits novel solutions and applications for multimodality. The taxonomy shall, according to the authors, help catalog research contributions according to the scheme. To illustrate this interesting perspective, illustrative examples are featured in the chapter including speech recognition and synthesis, event detection, emotion and affect, media description, and multimedia retrieval. The taxonomy of five aspects of representation is each used to identify core challenges for these tasks, as in the case of multimodal representation. Multimodal representation is broken down into two aspects: (1) joint multimodal representation by projecting the multimodal data into one space&#8212;mostly if all modalities are present&#8212;and (2) coordinated multimodal representation by projecting individual modalities into separate but coordinated spaces&#8212;usually if only one modality is <a id="page_8"/>present. A final major challenge is co-learning where the modeling of a resource poor modality is aided by a resource-rich modality.</p>
<p class="indent">In <a href="13_Chapter02.xhtml">Chapter 2</a>, Alpaydin explains how multimodal data from different sensors and sources can be fused intelligently such as to best exploit complementary information with the aim to reach higher accuracies in classification tasks. The prevailing early fusion on feature level, and late fusion on semantic decision level are introduced, alongside intermediate solutions. In this latter compromise between early and late fusion, only one classifier is learned to be suitably processed, yet in a rather abstract form of the input per modality. To this end, multiple kernel learning uniting a predefined set of kernels, e.g., for support vector learning, is presented as a first paradigm. In this case, a kernel tailored to each modality is applied as such. In addition to this, an according intermediate fusion strategy based on deep learning is introduced. This bases on <i><b>shared hidden layers</b></i> within a neural network, i.e., per modality early layers in the network are available separately, whereas later layers combine the information from the different modalities. The chapter provides an extensive discussion on the advantages and shortcomings of each form to integrate the information from the different sources. The outcome of this discussion, however, is that an optimal solution usually needs to be tailored to the needs of the use case. The chapter stresses the importance of the layer of abstraction on which the correlation between features is expected or is best exploitable across modalities. At the chapter&#8217;s end, perspectives on potential future endeavors toward improved fusion are given.</p>
<p class="indent"><a href="14_Chapter03.xhtml">Chapter 3</a> is largely dedicated to the same topic, i.e., optimal fusion of information sources, yet highlighting further aspects. Panagakis et al. stress the importance of temporal information, context, and adaptability in multimodal solutions. Similar to the previous chapter, the focus is on the best ways to fuse multimodal and multisensorial data in a synergistic manner. The chapter goes beyond an abstract description of fusion strategies and focuses on a series of suited approaches when dealing with the often-met case of heterogeneous, but correlated, time series data. This could, for example, be the case in audiovisual speech recognition. Such data is not only noisy but often misaligned in time across the involved modalities or sensor signals. To solve this, the authors present correlation-based methods for heterogeneous, yet complementary, modalities or multi-sensorial information such as coming from different types of video cameras or audio and video stemming from the same scene. Starting from <i><b>canonical correlation analysis,</b></i> further suited solutions and their shortcomings and advantages are discussed. In combination with <i><b>dynamic time warping (DTW)</b></i>, alignment of different types of, yet correlated information streams become feasible. Next, robust and individual component analysis <a id="page_9"/>is introduced and further featured in combination with time warping. For illustration purposes, one example is facial expression analysis, including evolution over time. This includes segmentation and estimation of the intensity of the expression. The authors further argue for the importance of context integration in the fusion and discuss machine learning methods to incorporate it into the learning process. In closing, they then explore avenues for domain adaptation across various contextual factors and modalities.</p>
<p class="indent"><a href="15_Chapter04.xhtml">Chapter 4</a>, the fourth and final chapter of <a href="11_Part01.xhtml">Part I</a>, is entirely dedicated to deep learning methods. Whereas, for example, in <a href="13_Chapter02.xhtml">Chapter 2</a> shared hidden layers in a deep neural network are presented as a solution for multimodal fusion, this chapter by Keren et al. takes a broad view on multimodal and multisensorial fusion architectures, which the present deep learning methods offer. The authors promote the usage of deep learning not only because of its significant empirical success across a range of AI tasks, but also, because it is often &#8220;<i>simpler and easier to design</i>&#8221;, able to learn &#8220;end-to-end&#8221;, and provides reusable building blocks across tasks which support easy transfer of knowledge. The well-supported and maintained software landscape further benefits the usage of deep learning. The authors assume basic knowledge in deep neural networks and address specific topologies and methods in this chapter. As in <a href="13_Chapter02.xhtml">Chapter 2</a>, this chapter discusses early, late, and intermediate fusion, but, from a slightly different and neural network-focused perspective. The chapter introduces <i><b>encoder-decoder</b></i> topologies such as sequence-to-sequence approaches including attention mechanisms. It also highlights embeddings for multimodal modeling as is illustrated for images and text analyses. In an outlook, a number of future avenues are given. Concluding the chapter, the authors stress the bottleneck of little labeled data. Such interaction data together with a learning target is usually needed in a large quantity for deep learning architectures&#8217; success in multimodal fusion.</p>
<p class="h1n"><b>Multimodal Processing of Social and Emotional States</b></p>
<p class="noindent"><a href="16_Part02.xhtml">Part II</a> bundles five chapters on affective computing and social signal processing for the realization of artificial socioemotionally intelligent interaction.</p>
<p class="indent">An introduction is given in <a href="17_Chapter05.xhtml">Chapter 5</a> to introduce the broader picture on &#8220;profiling&#8221; the user in terms of her states and traits via various modalities. The author motivates the need for such user modeling by economist Peter Drucker&#8217;s quote &#8220;<i>The most important thing in communication is hearing what isn&#8217;t said</i>&#8221;. Potential characteristics of a user to recognize are shown in an overview, featuring extensive examples which have been targeted in a multimodal approach in the literature. For traits, these include a person&#8217;s age, attractiveness, ethnicity, gender, height, <a id="page_10"/>identity, leader traits, likability, (degree of) nativeness, or personality. As to states, alertness, cognitive load, deception, depression, distraction, drowsiness, emotion, engagement, interest, laughter, physical activity, sentiment, stress, swallowing, and violence are listed. The reader is then guided in detail along the path of processing of a typical user state and trait assessment engine. This includes all typical and also less-typical, yet beneficiary, steps from pre-processing to decision making and integration in an application context. Modern aspects such as <i><b>cooperative learning</b></i> and <i><b>transfer learning,</b></i> or (multimodal) <i><b>confidence measure</b></i> estimation are further discussed. A focus is thereby placed on fusion of different modalities for user state and trait modeling. The author advocates a modern view and provides guidance on how to simplify, unify, and cluster these steps to allow for a more seamless integration of information in the processing. Spoken and written language, video information&#8212;such as facial expression, body posture, and movement&#8212;and physiological signals as well as tactile interaction data are then compared regarding their strengths and weaknesses for state and trait recognition. After introducing tools in the field, the reader is also walked through a didactic example: the (fictional case of) arrogance recognition. Concluding this chapter, recent trends and potential for future improvement are given: in the wild processing, diversity of culture and language, multi-subject processing, and linking analysis with synthesis.</p>
<p class="indent">In <a href="18_Chapter06.xhtml">Chapter 6</a>, D&#8217;Mello et al. focus on affect as a state to recognize. The concept of affect is first viewed from an affective sciences perspective. Similar to the chapters in <a href="11_Part01.xhtml">Part I</a>, the authors then deal with ways to fuse information, here, however, for the particular use case of affect detection. Alongside basic approaches such as data, feature, decision, and hybrid fusion, model fusion is also introduced. The authors present statistical approaches as by dynamic Bayesian networks besides further variants of deep networks complementing the kernel- and deep network-based approaches shown in <a href="11_Part01.xhtml">Part I</a>. Three walk-through examples of multisensor-multimodal affect detection systems exemplify the usage: (1) the case of feature-level fusion targeting basic emotions; (2) decision-level fusion for learning-centered affective states; and (3) model-based fusion is shown for a continuous learning by affective dimensions. The authors also discuss current trends in the field. They further give a detailed overview of modality combinations. A highlight to give an impression of the current performances are the results from the leading-in-the-field challenge. To conclude, the authors compare the state-of-the-field vs. ten years into the past, speculating also about ten years into the future expecting &#8220;<i>advances in theoretical sophistication, data sources, and computational techniques</i>&#8221;, while calling for advances in the science of validation in the field so it can advance to broad everyday usage.</p>
<p class="indent"><a id="page_11"/>In <a href="19_Chapter07.xhtml">Chapter 7</a>, Vinciarelli and Esposito lead the reader into the related and complementary field of multimodal analysis of social signals. Rather than dealing with affective states as in the previous chapter, this chapter deals with multimodal communication between living beings. It takes both the perspectives of the life sciences as well as computer science regarding technical implications and optimal realization of system solutions. The life science side organizes multimodal communication patterns as occurring in nature into a coherent taxonomy. Taking the technical stance, the authors consider early and late fusion as in previous chapters in this volume, but with a focus on social signals in communication. A meta-analysis over five years in the recent literature provides insight into the current best practices in this field. In this analysis, the authors observe that early fusion is the dominant approach, and that combining synergistic heterogeneous modalities can lead to performances exceeding the best single modality.</p>
<p class="indent">In <a href="20_Chapter08.xhtml">Chapter 8</a>, Wagner and Andr&#233; introduce their practical view on the two tasks introduced in detail in the previous two chapters: affect and social signals and their recognition. The real-time aspect is thereby crucial for a natural communication of humans with technical systems that make use of such information. The chapter first deals with a basic necessity which is often seen as the main limiting factor in the two fields: data for training and its collection. The chapter also lists a number of popular databases for these tasks before turning to fusion in a multimodal context. It also features asynchronous fusion approaches. Timing is respected for online-enabled processing, and potentially missing information. The chapter then formulates a set of requirements for a practical multimodal framework implementation, namely, multisensory abilities (catering also for potentially novel and &#8220;exotic&#8221; sensors), synchronization for modalities operating on different time scales, automated transcription (of supplementary descriptions), multimodal fusion, missing data handling, continuous classification, and real-time ability. The authors then introduce the Social Signal Interpretation Framework as tool and guide the reader through very detailed examples before making final conclusions.</p>
<p class="indent"><a href="21_Chapter09.xhtml">Chapter 9</a> by Martin et al. rounds <a href="16_Part02.xhtml">Part II</a> off by taking the opposite view on affect in a technical context as compared to the previous <a href="17_Chapter05.xhtml">Chapters 5</a>, 6, and 8. Rather than targeting analysis of human affect, this chapter highlights the synthesis of affect and its perception by humans. Following a general introduction, the authors deal with emotions and their expressions. Related to the focus of these three volumes on multimodality, the authors then turn to human perception of combinations of such affective expressions across modalities. This includes facial and bodily as well as vocal expressions. These expressions are, for example, frequently met in artificial conversational agents. In addition to discussing audio and video, combinations <a id="page_12"/>with haptic expression of affect are included. Further, the impact of context on the perception of multimodal expression of affect by technical systems is discussed. The authors support the discussion by presenting results from diverse experiments. In their conclusion, the authors stress the challenge of &#8220;<i>generating and controlling synchronized expressions of affects across several modalities</i>&#8221; and summarize including that different modalities are differently suited for different affects&#8217; display, as was in previous chapters in this volume similarly found from the affect analysis perspective.</p>
<p class="h1n"><b>Multimodal Processing of Cognitive States</b></p>
<p class="noindent"><a href="22_Part03.xhtml">Part III</a> of this volume emphasizes cognitive states in four chapters, each of which target a different example.</p>
<p class="indent">In <a href="23_Chapter10.xhtml">Chapter 10</a>, Zhou et al. first discuss cognitive load. They introduce the principle of cognitive load measurement and the current four primary methods: subjective self-reported measures, performance measures, physiological measures, and behavioral measures. The chapter then presents the state-of-the-art in terms of theories and prevailing approaches for cognitive load measurement involving applications. Measurement based on behavioral cues follows, including pen as well as speech and linguistic features. Thereafter, a range of physiological measures is presented, with emphasis on pupil dilation and galvanic skin response. The authors also introduce a feedback loop to the user in applications where automatic cognitive load measurement is included with the intention of supporting maximization of cognitive capacity. The chapter is concluded by final considerations also on future needs and efforts to be made.</p>
<p class="indent"><a href="24_Chapter11.xhtml">Chapter 11</a> by Oviatt et al. focuses on students&#8217; mental state during the process of learning&#8212;a complex activity to be evaluated over time. As in previous chapters in this volume, analysis based on a multimodal approach is presented to overcome the limitations of click-stream and textual analyses. The chapter thoroughly introduces the concept of multimodal learning analytics, including setting its current emergence and the advocated multimodal approach into temporal context and laying out the main objectives. An important part of practical relevance is the description of major available multimodal corpora in this field, including a discussion on their limitations. Subsequently, the main findings in the field are presented distilling five major advantages of using multiple modalities and concluding that linguistic content analysis is not necessarily required for successful prediction of domain expertise. The chapter highlights the theoretical basis of multimodal learning analytics discussing future directions for the field.</p>
<p class="indent"><a id="page_13"/>In <a href="25_Chapter12.xhtml">Chapter 12</a>, Cohn et al. introduce the mental disorder of depression from a multimodal perspective, aiming at its automated recognition. The chapter first thoroughly introduces depression as a phenomenon, and then turns to multimodal techniques for identifying behavioral and physiological indices of it in users. The chapter details the extraction of feature information sensitive to depression from facial cues, the speech signal, and body movement and other sensor signals. This discussion also includes the presentation of the key machine learning methods applied in this context so far which are largely identical to those for the recognition of other cognitive states. As in the other chapters, emphasis is particularly placed on the fusion of information from the various modalities. Such fusion is thereby handled in the context of classification, but also prediction. Toward the chapter&#8217;s end, aspects as related to the implementation as contextual embedding in an application are considered. Further, a number of broadly used corpora are briefly reviewed.</p>
<p class="indent">Burzo et al. introduce a final cognitive state example in <a href="26_Chapter13.xhtml">Chapter 13</a>. After introducing and motivating the topic, individual modalities are discussed: the perspective of psychological experiments is first, followed by language, vision, and physiology as examples of suited deception information carriers for automated detection. The chapter then presents selected combination examples and results from according studies, namely thermal imaging in combination with physiological sensors, and language analysis, followed by language and acoustics, and finally vision and language in combination. The authors finally discuss deficits in the field, including the need for larger datasets with more and cross-cultural subjects best in out of the lab conditions. Furthermore, the authors argue for an increased number of modalities. They also see deep learning methods as have been introduced in this volume&#8212;in particular in <a href="15_Chapter04.xhtml">Chapter 4</a>&#8212;as a promising avenue.</p>
<p class="h1n"><b>Multidisciplinary Challenge Topic: Perspectives on Predictive Power of Multimodal Deep Learning: Surprises and Future Directions</b></p>
<p class="noindent">The Challenge Topic addressed in this volume in <a href="28_Chapter14.xhtml">Chapter 14</a> features a discussion between Samy Bengio (an expert in deep architectures for sequences and other structured objects, understanding training and generalization of deep architectures, and adversarial training with application in image captioning), Li Deng (expertise in AI, mathematical modeling, deep learning, big data analytics, speech recognition, natural language processing, and image captioning), Louis-Philippe Morency (expertise in multimodal machine learning for modeling of acoustic, visual, and verbal modalities, with an interest in human communication dynamics, and health behavior informatics), and Bj&#246;rn Schuller (expertise in deep learning, <a id="page_14"/>recurrent networks, and end-2-end learning with application in multimodal and multisensorial affective computing and mobile health).</p>
<p class="indent">The discussion was initiated around <a href="15_Chapter04.xhtml">Chapter 4</a> that deals with deep learning in the context of multimodal and multisensorial interaction and introduces a range of topologies and solutions by according deep net inventory. Five questions were discussed starting with: How have deep learning techniques shown that they can be a catalyst for scientific discovery? Deng names machine reading comprehension as a starting point for even greater discovery once machines can interpret &#8220;<i>greater and greater scientific literature</i>&#8221; also with cross-disciplinary linkage.</p>
<p class="indent">Discussing next the surpluses but also potential downsides of multimodal deep learning as compared to conventional methods of machine learning for multimodal fusion, Bengio and Morency highlight the ability to learn better representations as particular strengths. Morency also adds transfer learning on the positive side. In a similar vein, Bengio remarks that deep learning has shown &#8220;<i>promises into finding common representations of concepts, irrespective of how they are sensed</i>&#8221;, thus bearing crucial relevance for multimodal and multisensorial use cases. Along these lines, Deng states that &#8220;<i>multimodal signals can be effectively exploited to enhance the predictive power by enriching the supervision signals across different modalities</i>&#8221; which he names as distant-supervised learning and perceives as closer to human learning such as for a child learning spoken language. Bengio also expects cross-modal generation of concepts such as by generative adversarial networks. He and Schuller also expect their ability to exploit big amounts of data to be a key advantage in times where multimodal and multisensorial data can easily be collected at such scale. Deng agrees to this stating that conventional methods lack sufficient learning capacity and representation power to enable multimodal fusion.</p>
<p class="indent">Asked about surprises multimodal deep learning might bring along, Deng names drastic reduction of labeling cost in large-scale prediction tasks. Bengio adds that deep learning may surprise us as simultaneous access to many modalities should benefit the robustness of learning representations. He adds, he would be &#8220;<i>really surprised the day a deep learning model can generate things like humor!</i>&#8221; Schuller hopes for emergent behavior, once several task-specific networks start to work together in a large multimodal context.</p>
<p class="indent">As to future directions for multimodal deep learning, Morency highlights co-learning exploiting knowledge from one modality to help perform a task in a second modality such as by <i><b>zero-shot learning</b></i>. Bengio thinks deep networks can &#8220;<i>learn to generate one modality from another</i>&#8221;, and Deng wants to see further exploration of effective architectures for multimodal information fusion.</p>
<p class="indent"><a id="page_15"/>Finally, dealing with transparency of multimodal deep learning and potential ethical and societal implications, Morency first highlights privacy and ownership of one&#8217;s (multimodal) data. Bengio sees an additional problem in the fact that such deep models can be fooled and largely blames &#8220;<i>bad data</i>&#8221; such as with erroneous labeling or biased in some way for &#8220;<i>bad decisions</i>&#8221;. As to transparency, Schuller argues that one can already explain for example learned time-profiles or compare learned features to traditional ones. Hope in this respect is further spread by Deng from an algorithmic point of view by designing &#8220;<i>special neural cell architectures which explicitly represent some known properties of the input signals</i>&#8221;. All agree that transparency is a crucial step.</p>
<p class="h1n"><a id="int_6"/><b>References</b></p>
<p class="ref">J. E. Cahn. 1990. The generation of affect in synthesized speech. <i>Journal of the American Voice I/O Society</i>, 8: 1&#8211;19. DOI: 10.1.1.52.5802. 5</p>
<p class="ref">L. S. Chen, T. S. Huang, T. Miyasato, and R. Nakatsu. 1998. Multimodal human emotion/expression recognition. In <i>Proceedings of the Third IEEE International Conference on Automatic Face and Gesture Recognition</i>, pp. 366&#8211;371. IEEE. DOI: 10.1109/AFGR.1998.670976. 6</p>
<p class="ref">C. Chiu, Y. Chang, and Y. Lai. 1994. The analysis and recognition of human vocal emotions. In <i>Proceedings of the International Computer Symposium</i>, pp. 83&#8211;88. 5</p>
<p class="ref">R. Dechter. 1986. Learning while searching in constraint-satisfaction problems, pp. 178&#8211;183. University of California, Computer Science Department, Cognitive Systems Laboratory. 2</p>
<p class="ref">L. C. De Silva, T. Miyasato, and R. Nakatsu. 1997. Facial emotion recognition using multimodal information. In <i>Proceedings of the 1997 International Conference on Information, Communications and Signal Processing</i>, vol. 1, pp. 397&#8211;401. IEEE. DOI: 10.1109/ICICS.1997.647126. 6</p>
<p class="ref">W. K. English, D. C. Engelbart, and B. Huddart. 1965. Computer-aided display control. Final Report, Contract NASl-3988, SRI Project, 5061. 1</p>
<p class="ref">G. E. Hinton, S. Osindero, and T. W. Teh. 2006. A fast learning algorithm for deep belief nets. <i>Neural computation</i>, 18(7): 1527&#8211;1554. DOI: 10.1162/neco.2006.18.7.1527. 2</p>
<p class="ref">S. Hochreiter and J. Schmidhuber. 1997. Long short-term memory. <i>Neural Computation</i>, 9(8): 1735&#8211;1780. 2</p>
<p class="ref">B. H. Juang and L. R. Rabiner. 2005. Automatic speech recognition&#8211;a brief history of the technology development. Georgia Institute of Technology, Atlanta Rutgers University and the University of California. Santa Barbara, 1: 67. DOI: 10.1.1.90.5614. 2</p>
<p class="ref">H. Kobayashi and F. Hara. 1992. Recognition of six basic facial expression and their strength by neural network. In <i>Proceedings IEEE International Workshop on Robot and Human Communication</i>, pp. 381&#8211;386. IEEE. DOI: 10.1109/ROMAN.1992.253857. 5</p>
<p class="ref"><a id="page_16"/>K. Mase. 1991. Recognition of facial expression from optical flow. <i>IEICE Transactions (E)</i>, 74: 3474&#8211;3483. 5</p>
<p class="ref">J. McCarthy, M. L. Minsky, N. Rochester, and C. E. Shannon. 2006. A proposal for the Dartmouth Summer Research Project on Artificial Intelligence August 31, 1955, <i>AI Magazine</i>, 27(4): 12&#8211;14. AAAI. 2</p>
<p class="ref">C. B. Mirick. 1926. Electrical distant-control system. <i>U.S. Patent</i>, no. 1,597,416, 1926. Washington, DC: U.S. Patent and Trademark Office.</p>
<p class="ref">C. M. Mitchell and M. G. Forren. 1987. Multimodal user input to supervisory control systems: voice-augmented keyboard. <i>IEEE Transactions on Systems, Man, and Cybernetics</i>, 17(4): 594&#8211;607. DOI: 10.1109/TSMC.1987.289349. 3</p>
<p class="ref">S. Oviatt. 1999. Ten myths of multimodal interaction. <i>Communications of the ACM</i>, 42(11): 74&#8211;81. DOI: 10.1145/319382.319398. 3</p>
<p class="ref">S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger. Editors. 2017. <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 1: Foundations, User Modeling and Common Modality Combinations</i>. San Rafael, CA: Morgan &#38; Claypool Publishers. DOI: 10.1145/3015783. 6</p>
<p class="ref">S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger. Editors. 2018. <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 3: Language Processing, Software, Commercialization, and Emerging Directions</i>. San Rafael, CA: Morgan &#38; Claypool Publishers. 6</p>
<p class="ref">F. I. Parke. 1972. Computer generated animation of faces. In <i>Proceedings of the ACM annual Conference</i>, 1: 451&#8211;457. ACM. DOI: 10.1145/800193.569955. 5</p>
<p class="ref">A. Pentland. 2007. Social signal processing [exploratory DSP]. <i>IEEE Signal Processing Magazine</i>, 24(4): 108&#8211;111. DOI: 10.1109/MSP.2007.4286569. 6</p>
<p class="ref">R. W. Picard. 1995. <i>Affective Computing</i>. MIT Press, Cambridge, MA. 5</p>
<p class="ref">S. Reiter, B. Schuller, and G. Rigoll. 2006. A combined LSTM-RNN-HMM-approach for meeting event segmentation and recognition. In <i>Proceedings of the International Conference on Acoustics, Speech and Signal Processing</i>, volume 2. IEEE. DOI: 10.1109/ICASSP.2006.1660362. 3</p>
<p class="ref">J. D. Williamson. 1978. Speech analyzer for analyzing pitch or frequency perturbations in individual speech pattern to determine the emotional state of the person. <i>U.S. Patent</i> No. 4,093,821. U.S. Patent and Trademark Office, Washington, DC. 5</p>
<p class="line"/>
<p class="note"><a id="fn1" href="#rfn1">1</a>.&#160;&#160;<i>Business Insider</i> on June 10, 2016. Databasis: Kleiner Perkins.</p>
<p class="note"><a id="fn2" href="#rfn2">2</a>.&#160;&#160;Linguistic Data Consortium, 1993/1997.</p>
</body>
</html>