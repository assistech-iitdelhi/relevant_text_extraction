<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="fmtitle"><a id="page_473"/><b>Index</b></p>
<p class="noindent">The index that appeared in the print version of this title was intentionally removed from the eBook. Please use the search function on your eReading device to search for terms of interest. For your reference, the terms that appear in the print index are listed below.</p>
<p class="indext">1-hot encoding of text</p>
<p class="index1">3D tracking in facial analysis</p>
<p class="indext">AAM (Active Appearance Model)</p>
<p class="index2">defined</p>
<p class="index2">depression behavioral signals</p>
<p class="index1">Abdominal respiration in deception detection</p>
<p class="index1">Accumulative GSR, defined</p>
<p class="index1">ACII (Affective Computing and Intelligent Interaction) conference series</p>
<p class="index1">ACM International Conference on Multimodal Interaction (ICMI)</p>
<p class="index1">Acoustics</p>
<p class="index2">as contextual cue</p>
<p class="index2">deception detection</p>
<p class="index2">speech recognition</p>
<p class="index1">Action classification, challenges and limitations</p>
<p class="index1">Action units (AUs). <i>See also</i> Facial Action Coding System (FACS)</p>
<p class="index2">defined</p>
<p class="index2">depression detection</p>
<p class="index2">facial analysis</p>
<p class="index2">facial expressions</p>
<p class="index2">recognition pipeline</p>
<p class="index2">segmentation</p>
<p class="index2">video</p>
<p class="index1">Active Appearance Model (AAM)</p>
<p class="index2">defined</p>
<p class="index2">depression behavioral signals</p>
<p class="index1">Active learning in userstate and trait recognition</p>
<p class="index1">Activity theory in multimodal learning analytics</p>
<p class="index1">Adaboost</p>
<p class="index2">AU intensity estimation</p>
<p class="index2">cognitive load indicators</p>
<p class="index2">domain adaptation</p>
<p class="index2">facial expressions</p>
<p class="index2">training learners</p>
<p class="index1">Adaptability in multimodal interfaces</p>
<p class="index1">Adaptation in userstate and trait recognition</p>
<p class="index1">Adaptive fuzzy systems</p>
<p class="index1">Affect</p>
<p class="index2">challenges and limitations</p>
<p class="index2">defined</p>
<p class="index2">human perception of expressions</p>
<p class="index2">multimodal expressions of. <i>See</i> Multimodal expressions of affects</p>
<p class="index2">overview</p>
<p class="index1">Affect annotations</p>
<p class="index2">defined</p>
<p class="index2">description</p>
<p class="index1">Affect detection</p>
<p class="index2">affect overview</p>
<p class="index2">affective experience-expression link</p>
<p class="index2">AVEC challenge</p>
<p class="index2">discussion</p>
<p class="index2"><a id="page_474"/>focus questions</p>
<p class="index2">ground truth</p>
<p class="index2">introduction</p>
<p class="index2">modality fusion</p>
<p class="index2">multimodal coordination of affective responses</p>
<p class="index2">multimodal expressions. <i>See</i> Multimodal expressions of affects</p>
<p class="index2">multimodal learning analytics</p>
<p class="index2">real-time sensing. <i>See</i> Real-time sensing of social signals</p>
<p class="index2">references</p>
<p class="index2">trends and state of art</p>
<p class="index2">walk-throughs</p>
<p class="index1">Affect-sensitive multimodal interfaces. <i>See</i> Multimodal and affect-sensitive interfaces</p>
<p class="index1">Affect signals, defined</p>
<p class="index1">Affective computing (AC)</p>
<p class="index2">animated agents</p>
<p class="index2">deep learning</p>
<p class="index2">defined</p>
<p class="index2">description</p>
<p class="index2">rise of</p>
<p class="index2">state of</p>
<p class="index1">Affective Computing and Intelligent Interaction (ACII) conference series</p>
<p class="index1">Affective experience-expression link</p>
<p class="index2">defined</p>
<p class="index2">overview</p>
<p class="index1">Affective ground truth</p>
<p class="index2">defined</p>
<p class="index2">overview</p>
<p class="index1">Affective pictures databases</p>
<p class="index1">Age groups in cognitive load measurement</p>
<p class="index1">AHMM (Asynchronous Hidden Markov model)</p>
<p class="index1">AI. <i>See</i> Artificial Intelligence (AI)</p>
<p class="index1">AISReact application</p>
<p class="index1">Alignment</p>
<p class="index2">challenges</p>
<p class="index2">correlation analysis methods</p>
<p class="index2">defined</p>
<p class="index2">Dynamic Time Warping</p>
<p class="index2">early integration</p>
<p class="index2">encoder-decoder models</p>
<p class="index2">multimodal machine learning</p>
<p class="index2">social signals</p>
<p class="index1">Alternative encoders and decoders</p>
<p class="index1">Alzheimer&#8217;s disease</p>
<p class="index1">AMI Meeting Corpus</p>
<p class="index2">description</p>
<p class="index2">social interactions</p>
<p class="index1">Analytic applications for multimodal learning analytics</p>
<p class="index1">Anger</p>
<p class="index2">affect detection</p>
<p class="index2">expressions</p>
<p class="index1">Animals, multimodal communication by</p>
<p class="index1">Animated agents</p>
<p class="index2">affective computing</p>
<p class="index2">Autism Spectrum Disorders</p>
<p class="index2">human perception of expressions</p>
<p class="index1">ANNs (artificial neural networks)</p>
<p class="index1">ANVIL tool</p>
<p class="index2">affect and social signals</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Appearance-based modeling in userstate and trait recognition</p>
<p class="index1">Apraxia</p>
<p class="index1">Architectures</p>
<p class="index2">multimodal signal processing</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Arousal aspect in deception detection</p>
<p class="index1">Artificial Intelligence (AI)</p>
<p class="index2">emotional</p>
<p class="index2">HCI relationship</p>
<p class="index2">increased usage</p>
<p class="index2">machine learning. <i>See</i> Machine learning</p>
<p class="index1">Artificial neural networks (ANNs)</p>
<p class="index1">ASD (Autism Spectrum Disorders)</p>
<p class="index2">contextual cues</p>
<p class="index2">human-robot interactions for social learning</p>
<p class="index1"><a id="page_475"/>Asynchronous Hidden Markov model (AHMM)</p>
<p class="index1">Asynchronous sensor measurements</p>
<p class="index1">Attention</p>
<p class="index2">encoder-decoder models</p>
<p class="index2">learning prerequisites</p>
<p class="index1">Audio</p>
<p class="index2">deception detection</p>
<p class="index2">deep learning</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Audio/Visual Emotion Challenge (AVEC)</p>
<p class="index2">affect detection</p>
<p class="index2">deep learning</p>
<p class="index2">social interactions</p>
<p class="index2">speech analysis</p>
<p class="index2">speech and depression</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Audio-visual speech recognition (AVSR)</p>
<p class="index2">challenges and limitations</p>
<p class="index2">motivation for</p>
<p class="index1">Augmented Multiparty Interaction with Distance Access (AMIDA)</p>
<p class="index1">AUs. <i>See</i> Action units (AUs); Facial Action Coding System (FACS)</p>
<p class="index1">Autism Spectrum Disorders (ASD)</p>
<p class="index2">contextual cues</p>
<p class="index2">human-robot interactions for social learning</p>
<p class="index1">Autoencoders</p>
<p class="index2">multimodal deep learning</p>
<p class="index2">neural networks</p>
<p class="index1">Automated transcription requirements</p>
<p class="index1">AVEC. <i>See</i> Audio/Visual Emotion Challenge (AVEC)</p>
<p class="index1">Average rule in training learners</p>
<p class="index1">AVSR (audio-visual speech recognition)</p>
<p class="index2">challenges and limitations</p>
<p class="index2">motivation for</p>
<p class="indext">Bag-of-words (BoW) algorithm</p>
<p class="index2">deception detection</p>
<p class="index2">defined</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">depression detection</p>
<p class="index2">document comparisons</p>
<p class="index2">facial analysis</p>
<p class="index2">text representation</p>
<p class="index1">Bayesian Networks (BNs)</p>
<p class="index1">Beck Depression Index (BDI)</p>
<p class="index1">Behavioral cues and measures</p>
<p class="index2">cognitive load indicators</p>
<p class="index2">depression. <i>See</i> Depression behavioral signals</p>
<p class="index2">social signals</p>
<p class="index1">Belfast Naturalistic Database</p>
<p class="index1">Belfast Story Telling corpus</p>
<p class="index2">description</p>
<p class="index2">enjoyment recognition</p>
<p class="index1">Bi-directional Long-Short-Term Memory Neural Networks</p>
<p class="index1">Bi-directional LSTMs (BLSTMs)</p>
<p class="index1">Bias vectors</p>
<p class="index2">dense layers</p>
<p class="index2">image representation</p>
<p class="index1">Bimodality communication</p>
<p class="index1">Bipolar depression</p>
<p class="index1">Black boxes in deep learning</p>
<p class="index1">Black Dog database</p>
<p class="index1">Blends of emotions</p>
<p class="index1">Blobs in thermal infrared imagery</p>
<p class="index1">Blood volume pulse (BVP) sensors in deception detection</p>
<p class="index1">BLSTM-NN (Dynamic Bayesian Networks) classifiers</p>
<p class="index1">BLSTMs (bi-directional LSTMs)</p>
<p class="index1">Blush detection in deception detection</p>
<p class="index1">BNs (Bayesian Networks)</p>
<p class="index1">Body language and expressions</p>
<p class="index2">as contextual cue</p>
<p class="index2">deception detection</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">human perception of</p>
<p class="index1">BodyANT sensors</p>
<p class="index1">Boltzmann machines</p>
<p class="index1">BoW. <i>See</i> Bag-of-words (BoW) algorithm</p>
<p class="index1"><a id="page_476"/>Brain activity and regions</p>
<p class="index2">cognitive load</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Bridge Correlational Neural Network</p>
<p class="index1">Broadcast material</p>
<p class="index2">databases</p>
<p class="index2">real-time sensing</p>
<p class="index1">BROMP (Baker-Rodrigo Observation Method Protocol)</p>
<p class="index1">BVP (blood volume pulse) sensors in deception detection</p>
<p class="indext">Cameras in social signals analysis</p>
<p class="index1">CAMI (Cognition-Adaptive Multimodal Interface)</p>
<p class="index1">Canal9 corpus</p>
<p class="index1">Canonical correlation analysis (CCA)</p>
<p class="index2">coordinated representations</p>
<p class="index2">description</p>
<p class="index2">multimodal interfaces</p>
<p class="index1">Canonical Time Warping (CTW)</p>
<p class="index2">correlation analysis methods</p>
<p class="index2">multimodal interfaces</p>
<p class="index1">Captions</p>
<p class="index2">encoder-decoder models</p>
<p class="index2">media description</p>
<p class="index2">multilingual images</p>
<p class="index1">Capture process in userstate and trait recognition</p>
<p class="index1">Cascading model in learner training</p>
<p class="index1">Case study for cognitive load indicators</p>
<p class="index1">Categorical representation in userstate and trait recognition</p>
<p class="index1">CBOW (continuous bag-of-words) model</p>
<p class="index1">CCA (canonical correlation analysis)</p>
<p class="index2">coordinated representations</p>
<p class="index2">description</p>
<p class="index2">multimodal interfaces</p>
<p class="index1">Center for Epidemiological Studies Depression Scale</p>
<p class="index1">CERT (Computer Expression Recognition Toolbox)</p>
<p class="index2">deception detection</p>
<p class="index2">multimodal learning analytics</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Children verbal responses in deception detection</p>
<p class="index1">Chittaranjan, G. and Hung, H., role-playing games</p>
<p class="index1">Chunking clusters in userstate and trait recognition</p>
<p class="index1">Classification algorithms in deception detection</p>
<p class="index1">Classifier combinations, defined</p>
<p class="index1">Classifier diversity</p>
<p class="index2">defined</p>
<p class="index2">social signals</p>
<p class="index1">Classifiers, defined</p>
<p class="index1">Classifying multimodal data</p>
<p class="index2">conclusions and future work</p>
<p class="index2">focus questions</p>
<p class="index2">integration</p>
<p class="index2">introduction</p>
<p class="index2">multimodal deep learning</p>
<p class="index2">multiple kernel learning</p>
<p class="index2">overview</p>
<p class="index2">references</p>
<p class="index1">Click-stream data in multimodal learning analytics</p>
<p class="index1">Clinic-based multimodal assessment of depression</p>
<p class="index1">CLT (Cognitive Load Theory)</p>
<p class="index2">multimodality and cognitive load</p>
<p class="index2">unidimensional scales</p>
<p class="index2">working memory based</p>
<p class="index1">CMS (Continuous Measurement System)</p>
<p class="index1">CNNs. <i>See</i> Convolutional Neural Networks (CNNs)</p>
<p class="index1">Co-clustering approach for social signals</p>
<p class="index1">Co-learning</p>
<p class="index2">deep learning</p>
<p class="index2">defined</p>
<p class="index2">discussion</p>
<p class="index2">hybrid data</p>
<p class="index2">multimodal machine learning</p>
<p class="index2"><a id="page_477"/>non-parallel data</p>
<p class="index2">overview</p>
<p class="index2">parallel data</p>
<p class="index2">social signals</p>
<p class="index1">Co-training</p>
<p class="index1">COBE (Common Orthogonal Basis Extraction)</p>
<p class="index1">Cognition-Adaptive Multimodal Interface (CAMI)</p>
<p class="index1">Cognitive load, defined</p>
<p class="index1">Cognitive Load Component Survey</p>
<p class="index1">Cognitive load indicators</p>
<p class="index2">behavioral measures</p>
<p class="index2">case study</p>
<p class="index2">conclusion</p>
<p class="index2">focus questions</p>
<p class="index2">introduction</p>
<p class="index2">multimodal signals and data fusion</p>
<p class="index2">references</p>
<p class="index2">state-of-the-art theories</p>
<p class="index2">subjective measures</p>
<p class="index1">Cognitive load measurement</p>
<p class="index2">applications</p>
<p class="index2">defined</p>
<p class="index2">factors</p>
<p class="index2">performance</p>
<p class="index2">physiological</p>
<p class="index2">purpose</p>
<p class="index1">Cognitive Load Theory (CLT)</p>
<p class="index2">multimodality and cognitive load</p>
<p class="index2">unidimensional scales</p>
<p class="index2">working memory based</p>
<p class="index1">Cognitive processing in deception detection</p>
<p class="index1">Columbia-SRI-Colorado (CSC) corpus</p>
<p class="index1">Combinations, classification</p>
<p class="index1">Common Orthogonal Basis Extraction (COBE)</p>
<p class="index1">Communication, defined</p>
<p class="index1">Compatibility function for joint representation</p>
<p class="index1">Competence in Oral Presentation Corpus</p>
<p class="index1">Compression auto encoders</p>
<p class="index1">Computational methods in multimodal analysis</p>
<p class="index1">Computer Expression Recognition Toolbox (CERT)</p>
<p class="index2">deception detection</p>
<p class="index2">multimodal learning analytics</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Computer-mediated learning</p>
<p class="index1">Computing Adaptive Testing</p>
<p class="index1">Concatenation in joint representations</p>
<p class="index1">Concept Net database</p>
<p class="index1">Conceptual grounding in non-parallel data</p>
<p class="index1">Conditional Ordinal Random Field (CORF)</p>
<p class="index1">Conditional Random Fields (CRFs) for facial expressions</p>
<p class="index1">Confidence measure and estimation</p>
<p class="index2">description</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Congruent conditions in emotional expressions</p>
<p class="index1">Congruent modalities</p>
<p class="index1">Construct</p>
<p class="index2">defined</p>
<p class="index2">description</p>
<p class="index1">Context</p>
<p class="index2">affect expressions</p>
<p class="index2">databases</p>
<p class="index2">encoder-decoder model attention mechanisms</p>
<p class="index2">multimodal interfaces</p>
<p class="index1">Context-sensitive CORF (cs-CORF) model</p>
<p class="index1">Continuous bag-of-words (CBOW) model</p>
<p class="index1">Continuous classification in multimodal frameworks</p>
<p class="index1">Continuous Measurement System (CMS)</p>
<p class="index1">Continuous representation, defined</p>
<p class="index1">Continuous skip-gram model</p>
<p class="index1">Control aspect in deception detection</p>
<p class="index1"><a id="page_478"/>Control Question Test (CQT)</p>
<p class="index1">Convolutional layers</p>
<p class="index2">defined</p>
<p class="index2">intermediate fusion</p>
<p class="index2">multimodal deep learning</p>
<p class="index1">Convolutional Neural Networks (CNNs)</p>
<p class="index2">deep learning</p>
<p class="index2">defined</p>
<p class="index2">early fusion</p>
<p class="index2">encoder-decoder models</p>
<p class="index2">image representation</p>
<p class="index2">intermediate fusion</p>
<p class="index2">joint representation</p>
<p class="index2">multimodal representations</p>
<p class="index1">Cooperative learning</p>
<p class="index2">description</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Coordinated representations</p>
<p class="index2">multimodal</p>
<p class="index2">social signals</p>
<p class="index1">Coordinated responses in affect detection</p>
<p class="index1">Copula functions for facial expressions</p>
<p class="index1">CORF (Conditional Ordinal Random Field)</p>
<p class="index1">Correlation analysis methods for multimodal interfaces</p>
<p class="index2">CCA</p>
<p class="index2">DTW and CTW</p>
<p class="index2">JIVE</p>
<p class="index2">overview</p>
<p class="index2">RCICA</p>
<p class="index2">RCITW</p>
<p class="index1">Correlations</p>
<p class="index2">defined</p>
<p class="index2">multimodal interfaces</p>
<p class="index1">Corruptions in RCICA</p>
<p class="index1">Coupled Hidden Markov Models (CHMMs)</p>
<p class="index2">affect detection</p>
<p class="index2">multimodal fusion</p>
<p class="index1">COVAREP toolkit</p>
<p class="index1">CQT (Control Question Test)</p>
<p class="index1">CRFs (Conditional Random Fields) for facial expressions</p>
<p class="index1">Cross modal hashing</p>
<p class="index2">challenges and limitations</p>
<p class="index2">coordinated representations</p>
<p class="index1">Cross modal retrieval, challenges and limitations</p>
<p class="index1">Cross validation, defined</p>
<p class="index1">Crowd sourcing in userstate and trait recognition</p>
<p class="index1">cs-CORF (Context-sensitive CORF) model</p>
<p class="index1">CSC (Columbia-SRI-Colorado) corpus</p>
<p class="index1">CTW (Canonical Time Warping)</p>
<p class="index2">correlation analysis methods</p>
<p class="index2">multimodal interfaces</p>
<p class="index1">Cues in social signals</p>
<p class="index1">Culture factors in userstate and trait recognition</p>
<p class="index1">CURRENNT tool</p>
<p class="indext">DAIC (Distress Assessment Interview Corpus)</p>
<p class="index1">DAMSL (Dialog Act Markup in Several Layers)</p>
<p class="index1">Darwin, Charles</p>
<p class="index1">Data, databases, and coding</p>
<p class="index2">deception detection</p>
<p class="index2">Math Data Corpus</p>
<p class="index2">multimodal learning analytics</p>
<p class="index2">Oral Presentation Corpus</p>
<p class="index2">real-time sensing of social signals</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Data-driven approach</p>
<p class="index2">cognitive load measurement</p>
<p class="index2">word embeddings</p>
<p class="index1">Data fusion</p>
<p class="index2">affect detection</p>
<p class="index2">cognitive load indicators</p>
<p class="index1">DataShop repository</p>
<p class="index1">DBMs (deep Boltzmann machines)</p>
<p class="index1">DBNs (Dynamic Bayesian Networks)</p>
<p class="index2">affect detection</p>
<p class="index2">facial expressions</p>
<p class="index1"><a id="page_479"/>DCCA (deep canonical correlation analysis)</p>
<p class="index2">affect detection</p>
<p class="index2">KCCA alternative</p>
<p class="index1">DCT (Discrete Cosine Transform) for facial expressions</p>
<p class="index1">Deception detection</p>
<p class="index2">datasets and devices</p>
<p class="index2">focus questions</p>
<p class="index2">future</p>
<p class="index2">individual modalities</p>
<p class="index2">introduction</p>
<p class="index2">language and acoustics</p>
<p class="index2">language overview</p>
<p class="index2">multimodal feature extraction</p>
<p class="index2">multiple modalities</p>
<p class="index2">physiology</p>
<p class="index2">psychology</p>
<p class="index2">results</p>
<p class="index2">thermal imaging, physiological sensors, and language analysis</p>
<p class="index2">vision and language</p>
<p class="index2">vision overview</p>
<p class="index1">Decision-level fusion</p>
<p class="index2">affect detection</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">early studies</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Decision process in userstate and trait recognition</p>
<p class="index1">Decision Tree (DT) algorithm in deception detection</p>
<p class="index1">Deep architectures in deep learning</p>
<p class="index1">Deep asymmetric structured joint embedding</p>
<p class="index1">Deep Boltzmann machines (DBMs)</p>
<p class="index1">Deep canonical correlation analysis (DCCA)</p>
<p class="index2">affect detection</p>
<p class="index2">KCCA alternative</p>
<p class="index1">Deep learning</p>
<p class="index2">affect detection</p>
<p class="index2">as catalyst for scientific discovery</p>
<p class="index2">conclusion</p>
<p class="index2">deep architectures</p>
<p class="index2">encoder-decoder models</p>
<p class="index2">focus questions</p>
<p class="index2">fusion models</p>
<p class="index2">future</p>
<p class="index2">image representation</p>
<p class="index2">introduction</p>
<p class="index2">multimodal embedding models</p>
<p class="index2">multimodal joint representation</p>
<p class="index2">multimodal signal processing</p>
<p class="index2">overview</p>
<p class="index2">perspectives</p>
<p class="index2">references</p>
<p class="index2">responsibility</p>
<p class="index2">text representation</p>
<p class="index1">Deep neural networks</p>
<p class="index2">classification</p>
<p class="index2">machine learning</p>
<p class="index2">modality fusion in MMAD systems</p>
<p class="index2">multimodal deep learning</p>
<p class="index1">Deep Reinforcement Learning</p>
<p class="index1">Deep structured joint embedding</p>
<p class="index1">deep visual-semantic embedding (DeViSE)</p>
<p class="index1">Denoising autoencoders</p>
<p class="index1">Dense layers</p>
<p class="index2">defined</p>
<p class="index2">early fusion</p>
<p class="index1">Department of Defense Polygraph Institute</p>
<p class="index1">Dependencies</p>
<p class="index2">classification</p>
<p class="index2">context</p>
<p class="index2">multimodal deep learning</p>
<p class="index2">multimodal interfaces</p>
<p class="index1">Depression behavioral signals</p>
<p class="index2">analysis</p>
<p class="index2">assessment</p>
<p class="index2">body movement</p>
<p class="index2">conclusion and current challenges</p>
<p class="index2">depression overview</p>
<p class="index2"><a id="page_480"/>facial analysis</p>
<p class="index2">focus questions</p>
<p class="index2">implementation-related considerations and elicitation approaches</p>
<p class="index2">introduction</p>
<p class="index2">multimodal fusion</p>
<p class="index2">signal processing systems</p>
<p class="index2">speech analysis</p>
<p class="index1">Description feature for databases</p>
<p class="index1">Detection style classification papers for speech and depression</p>
<p class="index1">DeViSE (deep visual-semantic embedding)</p>
<p class="index1">Dialog Act Markup in Several Layers (DAMSL)</p>
<p class="index1">Dimensional spaces in mental states</p>
<p class="index1">Dindar, M., and cognitive load measurement</p>
<p class="index1">Discrete Cosine Transform (DCT) for facial expressions</p>
<p class="index1">Discrete representation, defined</p>
<p class="index1">Disgust expressions</p>
<p class="index1">Distant-supervised learning</p>
<p class="index1">Distress Assessment Interview Corpus (DAIC)</p>
<p class="index1">Diverse social signals</p>
<p class="index1">DLPFC (dorsolateral prefrontal cortex) and cognitive load</p>
<p class="index1">Domain adaptation</p>
<p class="index2">defined</p>
<p class="index2">facial behavior analysis</p>
<p class="index2">multimodal interfaces</p>
<p class="index1">Domain expertise</p>
<p class="index2">analytics</p>
<p class="index2">defined</p>
<p class="index2">multimodal learning analytics</p>
<p class="index2">problem solving</p>
<p class="index1">Domain-trained approaches in userstate and trait recognition</p>
<p class="index1">Dominance</p>
<p class="index2">feeling ranges</p>
<p class="index2">non-redundant signals</p>
<p class="index1">Dorsolateral prefrontal cortex (DLPFC) and cognitive load</p>
<p class="index1">Driver distraction in cognitive load</p>
<p class="index1">DT (Decision Tree) algorithm in deception detection</p>
<p class="index1">DTW. <i>See</i> Dynamic Time Warping (DTW)</p>
<p class="index1">Dual-task paradigm in cognitive load measurement</p>
<p class="index1">Dynamic Bayesian Networks (BLSTM-NN) classifiers</p>
<p class="index1">Dynamic Bayesian Networks (DBNs)</p>
<p class="index2">affect detection</p>
<p class="index2">facial expressions</p>
<p class="index1">Dynamic classifiers in multimodal fusion</p>
<p class="index1">Dynamic Time Warping (DTW)</p>
<p class="index2">correlation analysis methods</p>
<p class="index2">description</p>
<p class="index2">multimodal fusion</p>
<p class="index2">RCICA</p>
<p class="index1">Dynamics of expressions</p>
<p class="index1">Dysarthria in speech analysis</p>
<p class="indext">EARL (Emotion Annotation and Representation Language)</p>
<p class="index1">Early combinations, defined</p>
<p class="index1">Early fusion</p>
<p class="index2">affect detection</p>
<p class="index2">defined</p>
<p class="index2">joint representations</p>
<p class="index2">overview</p>
<p class="index2">social signals</p>
<p class="index2">studies</p>
<p class="index1">Early integration in classifying multimodal data</p>
<p class="index1">ECG in cognitive load measurement</p>
<p class="index1">EDA (electrodermal activity)</p>
<p class="index1">Educational activity</p>
<p class="index2">Math Data Corpus</p>
<p class="index2">Oral Presentation Corpus</p>
<p class="index1">Educational management systems in multimodal learning analytics</p>
<p class="index1">Educational Testing Service</p>
<p class="index2">multimodal learning analytics</p>
<p class="index2"><a id="page_481"/>Oral Presentation Corpus</p>
<p class="index1">ELAN (EUDICO Linguistic Annotator)</p>
<p class="index1">Electrodermal activity (EDA)</p>
<p class="index1">Electromyography (EMG) system</p>
<p class="index2">facial expressions</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Elicitation methods for depression behavioral signals</p>
<p class="index1">Embedding models</p>
<p class="index2">multimodal. <i>See</i> Multimodal embedding models</p>
<p class="index2">sequence-to-sequence encoder-decoder</p>
<p class="index2">text representation</p>
<p class="index1">EMBODI-EMO database</p>
<p class="index1">Embodied Cognition theory</p>
<p class="index1">Emergence in non-redundant signals</p>
<p class="index1">Emergency management in cognitive load</p>
<p class="index1">EMG (electromyography) system</p>
<p class="index2">facial expressions</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">EMMA (Extensible MultiModal Annotation) markup language</p>
<p class="index1">Emotion Annotation and Representation Language (EARL)</p>
<p class="index1">Emotion Markup Language (EML)</p>
<p class="index1">Emotional state in learning prerequisites</p>
<p class="index1">Emotional valence ratings of facial expressions</p>
<p class="index1">EmotionML standard</p>
<p class="index2">databases</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Emotions</p>
<p class="index2">affect detection. <i>See</i> Affect detection</p>
<p class="index2">body movement and depression</p>
<p class="index2">challenges and limitations</p>
<p class="index2">deception detection</p>
<p class="index2">expressions</p>
<p class="index2">facial expressions</p>
<p class="index2">haptic expressions of affects</p>
<p class="index2">social interactions</p>
<p class="index1">Emotive music databases</p>
<p class="index1">EmotiW challenge</p>
<p class="index1">EmoVoice tool</p>
<p class="index1">Encoder-decoder models</p>
<p class="index2">alternative</p>
<p class="index2">attention mechanisms</p>
<p class="index2">description</p>
<p class="index2">sequence-to-sequence models</p>
<p class="index1">Encoding in userstate and trait recognition</p>
<p class="index1">Energy variability in speech analysis</p>
<p class="index1">Engagement as learning prerequisite</p>
<p class="index1">Enhancement</p>
<p class="index2">redundant signals</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Enjoyment recognition, multimodal</p>
<p class="index1">Ensembles</p>
<p class="index2">classification</p>
<p class="index2">facial expressions</p>
<p class="index2">multimodal data</p>
<p class="index1">Equivalence in redundant signals</p>
<p class="index1">EUDICO Linguistic Annotator (ELAN)</p>
<p class="index1">Event detection, challenges and limitations</p>
<p class="index1">Event-driven fusion</p>
<p class="index2">enjoyment recognition</p>
<p class="index2">multimodal</p>
<p class="index1">EXMARaLDA (Extensible Markup Language for Discourse Annotation)</p>
<p class="index1"><i>Expression of Emotion in Animals and Man</i> (Darwin)</p>
<p class="index1">Expressions</p>
<p class="index2">affects. <i>See</i> Multimodal expressions of affects</p>
<p class="index2">deception detection</p>
<p class="index2">emotions</p>
<p class="index1">Extensible Markup Language for Discourse Annotation (EXMARaLDA)</p>
<p class="index1">Extensible MultiModal Annotation (EMMA) markup language</p>
<p class="index1"><a id="page_482"/>Extraction of behavioral cues for social signals</p>
<p class="index1">Extraneous loads</p>
<p class="index2">cognitive load indicators</p>
<p class="index2">defined</p>
<p class="index1">Eye activity</p>
<p class="index2">cognitive load</p>
<p class="index2">depression behavioral signals</p>
<p class="index1">EyesWeb XMI Expressive Gesture Processing Library</p>
<p class="indext">FACET program</p>
<p class="index1">Facial Action Coding System (FACS)</p>
<p class="index2">databases</p>
<p class="index2">deception detection</p>
<p class="index2">defined</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">facial expressions</p>
<p class="index2">multimodal learning analytics</p>
<p class="index1">Facial analysis</p>
<p class="index2">affect detection</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">domain adaptation</p>
<p class="index2">multimodal interfaces</p>
<p class="index1">Facial electromyography</p>
<p class="index1">Facial expressions and behavior</p>
<p class="index2">deception detection</p>
<p class="index2">Emotional Valence ratings</p>
<p class="index2">human perception of</p>
<p class="index2">intensity estimation</p>
<p class="index2">social signals</p>
<p class="index2">temporal modeling</p>
<p class="index2">temporal segmentation</p>
<p class="index1">Facial thermal patterns in deception detection</p>
<p class="index1">FACS. <i>See</i> Facial Action Coding System (FACS)</p>
<p class="index1">Fake resume paradigm</p>
<p class="index1">Feature-based approaches in userstate and trait recognition</p>
<p class="index1">Feature-level fusion</p>
<p class="index2">affect detection</p>
<p class="index2">deception detection</p>
<p class="index2">defined</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">early studies</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Feed-forward neural network language model (FFNN-LM)</p>
<p class="index1">Feelings in affective ground truth</p>
<p class="index1">FEELtrace tool</p>
<p class="index1">Felt emotion aspect in deception detection</p>
<p class="index1">FFNN-LM (feed-forward neural network language model)</p>
<p class="index1">Flat speech</p>
<p class="index1">Fluency Disorders</p>
<p class="index1">Formant features in speech analysis</p>
<p class="index1">Frame-level features for userstate and trait recognition</p>
<p class="index1">Full video for deception detection</p>
<p class="index1">Functional magnetic resonance imaging (fMRI) technology</p>
<p class="index1">Fusion and fusion models</p>
<p class="index2">affect detection</p>
<p class="index2">cognitive load indicators</p>
<p class="index2">deep learning</p>
<p class="index2">defined</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">early fusion</p>
<p class="index2">framework requirements</p>
<p class="index2">intermediate fusion</p>
<p class="index2">joint representations</p>
<p class="index2">late fusion</p>
<p class="index2">multimodal machine learning</p>
<p class="index2">overview</p>
<p class="index2">real-time sensing of social signals</p>
<p class="index2">social signals</p>
<p class="index2">studies</p>
<p class="index2">userstate and trait recognition</p>
<p class="indext">GAD (Generalized Anxiety Disorder)</p>
<p class="index1">Galvanic Skin Response (GSR)</p>
<p class="index2">cognitive load</p>
<p class="index2">cognitive load measurement</p>
<p class="index2">defined</p>
<p class="index2"><a id="page_483"/>userstate and trait recognition</p>
<p class="index1">Games</p>
<p class="index2">databases</p>
<p class="index2">real-time sensing</p>
<p class="index1">GANs (Generative Adversarial Networks)</p>
<p class="index1">Garbage classes</p>
<p class="index1">Gating units in deep learning</p>
<p class="index1">Gaussian Mixture Models (GMMs)</p>
<p class="index2">defined</p>
<p class="index2">speech and depression</p>
<p class="index1">Gaussian Staircase Regression (GSR) approach</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">speech and depression</p>
<p class="index1">GAVAM (Generalized Adaptive View-based Appearance Model)</p>
<p class="index1">Gaze</p>
<p class="index2">as contextual cue</p>
<p class="index2">depression behavioral signals</p>
<p class="index1">GBM (Generalized Boosted Regression Models)</p>
<p class="index1">Gender groups in cognitive load measurement</p>
<p class="index1">General Inquirer database</p>
<p class="index1">General Trace (GTrace) program</p>
<p class="index2">data annotation</p>
<p class="index2">databases</p>
<p class="index1">Generalized Adaptive View-based Appearance Model (GAVAM)</p>
<p class="index1">Generalized Anxiety Disorder (GAD)</p>
<p class="index1">Generalized Boosted Regression Models (GBM)</p>
<p class="index1">Generative Adversarial Networks (GANs)</p>
<p class="index1">GentleBoost for facial expressions</p>
<p class="index1">Germane loads</p>
<p class="index2">cognitive load indicators</p>
<p class="index2">defined</p>
<p class="index1">Gestures</p>
<p class="index2">as contextual cue</p>
<p class="index2">deception detection</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">emotion categories</p>
<p class="index2">online recognition</p>
<p class="index1">GKT (Guilty Knowledge Test)</p>
<p class="index1">GMMs (Gaussian Mixture Models)</p>
<p class="index2">defined</p>
<p class="index2">speech and depression</p>
<p class="index1">Graphical models in joint representations</p>
<p class="index1">Graphical User Interfaces (GUIs)</p>
<p class="index1">Gross errors</p>
<p class="index2">correlation analysis methods</p>
<p class="index2">defined</p>
<p class="index1">Ground truth, affective</p>
<p class="index1">Grounding non-parallel data</p>
<p class="index1">GSR (Galvanic Skin Response). <i>See</i> Galvanic Skin Response (GSR)</p>
<p class="index1">GSR (Gaussian Staircase Regression) approach</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">speech and depression</p>
<p class="index1">GTrace (General Trace) program</p>
<p class="index2">data annotation</p>
<p class="index2">databases</p>
<p class="index1">Guilty Knowledge Test (GKT)</p>
<p class="index1">GUIs (Graphical User Interfaces)</p>
<p class="indext">H&#38;H Theory</p>
<p class="index1">Hamilton Rating Scale for Depression (HRSD)</p>
<p class="index1">Hamming space</p>
<p class="index1">Hand-crafted features</p>
<p class="index2">defined</p>
<p class="index2">facial analysis</p>
<p class="index1">Hand gestures</p>
<p class="index2">as contextual cue</p>
<p class="index2">deception detection</p>
<p class="index1">Handwriting</p>
<p class="index2">AI usage</p>
<p class="index2">cognitive load indicators</p>
<p class="index2">multimodal learning analytics</p>
<p class="index1">Happiness</p>
<p class="index2">Autism Spectrum Disorders</p>
<p class="index2">expressions</p>
<p class="index1">Haptic expressions of affects</p>
<p class="index1">Hashing</p>
<p class="index1"><a id="page_484"/>HCI (Human-Computer Interaction)</p>
<p class="index2">AI increased usage</p>
<p class="index2">AI relationship</p>
<p class="index2">cognitive load indicators</p>
<p class="index1">HCRF (Hidden Conditional Random Field) for facial expressions</p>
<p class="index1">Head behavior in social signals</p>
<p class="index1">Heart rate</p>
<p class="index2">cognitive load</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Hidden Conditional Random Field (HCRF) for facial expressions</p>
<p class="index1">Hidden layers</p>
<p class="index2">description</p>
<p class="index2">neural networks</p>
<p class="index1">Hidden Markov Models (HMMs)</p>
<p class="index2">affect detection</p>
<p class="index2">audio-visual speech recognition</p>
<p class="index2">facial expressions</p>
<p class="index2">multimodal fusion</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Hidden states</p>
<p class="index2">encoder-decoder models</p>
<p class="index2">Recurrent Neural Networks</p>
<p class="index1">Hierarchical functionals in userstate and trait recognition</p>
<p class="index1">Histogram of Oriented Gradients (HOG)</p>
<p class="index1">HMMs. <i>See</i> Hidden Markov Models (HMMs)</p>
<p class="index1">Homeostatic Property Clusters</p>
<p class="index1">HRSD (Hamilton Rating Scale for Depression)</p>
<p class="index1">Human-Computer Interaction (HCI)</p>
<p class="index2">AI increased usage</p>
<p class="index2">AI relationship</p>
<p class="index2">cognitive load indicators</p>
<p class="index1">Human motion in cognitive load indicators</p>
<p class="index1">Human perception of combinations of expressions</p>
<p class="index2">facial and bodily expressions</p>
<p class="index2">haptic expressions of affects</p>
<p class="index2">speech and other modalities</p>
<p class="index1">Human performance in limited-resource theories</p>
<p class="index1">Human-robot interactions</p>
<p class="index2">haptic expressions of affects</p>
<p class="index2">social learning</p>
<p class="index1">Humanoid robot trends</p>
<p class="index1">Hundred year emotion war</p>
<p class="index1">Hybrid data in co-learning</p>
<p class="index1">Hybrid fusion for affect detection</p>
<p class="index1">Hybrid SVM-HMM model</p>
<p class="indext">IAPS (International Affective Picture System)</p>
<p class="index1">Idle conditions in emotional expressions</p>
<p class="index1">IEMOCAP (Interactive Emotional Dyadic Motion Capture) database</p>
<p class="index1">iHEARu-EAT corpus</p>
<p class="index1">iHEARu-PLAY platform</p>
<p class="index1">IIT (Information Integration Theory)</p>
<p class="index1">ILHAIRE project</p>
<p class="index1">Image-sentence pairs in joint representation</p>
<p class="index1">Images</p>
<p class="index2">challenges and limitations</p>
<p class="index2">deep learning</p>
<p class="index2">joint representation</p>
<p class="index2">media description</p>
<p class="index2">multimodal embedding models</p>
<p class="index2">order-embeddings</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Imitation learning in userstate and trait recognition</p>
<p class="index1">Incongruent modalities in multimodal expressions of affects</p>
<p class="index1">Incremental processing in online recognition</p>
<p class="index1">Independence</p>
<p class="index2">non-redundant signals</p>
<p class="index2">social signals</p>
<p class="index1"><a id="page_485"/>Indexing and retrieval in multimodal applications</p>
<p class="index1">Individual components in RCICA</p>
<p class="index1">Inductive bias for learning models</p>
<p class="index1">Inflectional languages in text representation</p>
<p class="index1">Information Integration Theory (IIT)</p>
<p class="index1">Information retrieval system design in cognitive load measurement</p>
<p class="index1">Insight in problem solving</p>
<p class="index1">Integration in classifying multimodal data</p>
<p class="index1">Intensity</p>
<p class="index2">facial expressions</p>
<p class="index2">human perception of expressions</p>
<p class="index1">Interactive Emotional Dyadic Motion Capture (IEMOCAP) database</p>
<p class="index1">Interfaces</p>
<p class="index2">multimodal and affect-sensitive. <i>See</i> Multimodal and affect-sensitive interfaces</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Intermediate classification combinations</p>
<p class="index1">Intermediate fusion</p>
<p class="index1">Intermediate integration in classifying multimodal data</p>
<p class="index1">Intermodal context in databases</p>
<p class="index1">International Affective Picture System (IAPS)</p>
<p class="index1">Intrinsic loads</p>
<p class="index2">cognitive load indicators</p>
<p class="index2">defined</p>
<p class="indext">Joint and Individual Variation Explained (JIVE)</p>
<p class="index1">Joint representations</p>
<p class="index2">multimodal</p>
<p class="index2">social signals</p>
<p class="index1">Joysticks</p>
<p class="indext">k-NN classifiers</p>
<p class="index1">Kalman filtering</p>
<p class="index1">Kernel canonical correlation analysis (KCCA)</p>
<p class="index2">description</p>
<p class="index2">multimodal interfaces</p>
<p class="index1">Kernel learning</p>
<p class="index2">defined</p>
<p class="index2">multiple</p>
<p class="index1">Kernel mean matching (KMM)</p>
<p class="index1">Kinect device</p>
<p class="index2">description</p>
<p class="index2">enjoyment recognition</p>
<p class="index2">Oral Presentation Corpus</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">KMM (kernel mean matching)</p>
<p class="index1">Knowledge in userstate and trait recognition</p>
<p class="index1">Kohavi-Wolpert variance</p>
<p class="indext">LAK (Learning Analytics and Knowledge Conference)</p>
<p class="index1">Landmark detection systems</p>
<p class="index1">Language</p>
<p class="index2">cognitive load</p>
<p class="index2">data-driven word embeddings</p>
<p class="index2">deception detection</p>
<p class="index2">deception detection, and acoustics</p>
<p class="index2">deception detection, approach</p>
<p class="index2">deception detection, overview</p>
<p class="index2">deep learning</p>
<p class="index2">grounding</p>
<p class="index2">order-embeddings</p>
<p class="index2">userstate and trait recognition</p>
<p class="index2">video description</p>
<p class="index1">Late classification combinations</p>
<p class="index1">Late fusion</p>
<p class="index2">affect detection</p>
<p class="index2">social signals</p>
<p class="index2">studies</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Late integration in classifying multimodal data</p>
<p class="index1"><a id="page_486"/>Latent Trees for facial expressions</p>
<p class="index1">Laughter</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">incongruent modalities</p>
<p class="index2">recognizing</p>
<p class="index1">Layers</p>
<p class="index2">hidden</p>
<p class="index2">intermediate fusion</p>
<p class="index2">late fusion</p>
<p class="index1">LBP-TOP method</p>
<p class="index1">LBPs (Local Binary Patterns)</p>
<p class="index2">facial expressions</p>
<p class="index2">multimodal frameworks</p>
<p class="index1">Learners</p>
<p class="index2">description</p>
<p class="index2">mental state assessment. <i>See</i> Multimodal learning analytics</p>
<p class="index2">multimodal data</p>
<p class="index2">training</p>
<p class="index1">Learning Analytics and Knowledge Conference (LAK)</p>
<p class="index1">Learning and learning analytics</p>
<p class="index2">defined</p>
<p class="index2">multimodal. <i>See</i> Multimodal learning analytics</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Learning-centered affective states</p>
<p class="index1">Learning-oriented behaviors</p>
<p class="index1">Leave-one-out cross validation</p>
<p class="index2">deception detection</p>
<p class="index2">defined</p>
<p class="index1">Life and human sciences, multimodal communication in</p>
<p class="index1">Light pens</p>
<p class="index1">Limited-resource theories in multimodal learning analytics</p>
<p class="index1">Lindblom, B., H&#38;H Theory</p>
<p class="index1">Linguistic Inquiry and Word Count (LIWC)</p>
<p class="index1">Linguistic modality, description</p>
<p class="index1">Linking</p>
<p class="index2">affective experience-expression link</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">LIWC (Linguistic Inquiry and Word Count)</p>
<p class="index1">Local Binary Patterns (LBPs)</p>
<p class="index2">facial expressions</p>
<p class="index2">multimodal frameworks</p>
<p class="index1">Locally Linear Reconstruction</p>
<p class="index1">Log-based models for affect detection</p>
<p class="index1">Logistic Regression for facial expressions</p>
<p class="index1">Long-short term memory (LSTM) models</p>
<p class="index2">affect detection</p>
<p class="index2">coordinated representations</p>
<p class="index2">non-parallel data</p>
<p class="index2">sequential representations</p>
<p class="index2">text representation</p>
<p class="index1">Long Short-Term Memory Neural Networks (LSTM-NNs)</p>
<p class="index1">Long-term traits, defined</p>
<p class="index1">Longer-term traits, defined</p>
<p class="index1">Longitudinal data</p>
<p class="index2">body movement and depression</p>
<p class="index2">defined</p>
<p class="index1">Loosely coupled components in affect detection</p>
<p class="index1">Loss function in classification</p>
<p class="index1">Low-rank matrices in RCICA</p>
<p class="index1">LSTM models. <i>See</i> Long-short term memory (LSTM) models</p>
<p class="index1">LSTM-NNs (Long Short-Term Memory Neural Networks)</p>
<p class="index1">Lying. <i>See</i> Deception detection</p>
<p class="indext">M.I.N.I. interviews</p>
<p class="index1">Machine learning</p>
<p class="index2">analytics</p>
<p class="index2">deep learning</p>
<p class="index2">learners</p>
<p class="index2">multimodal. <i>See</i> Multimodal machine learning</p>
<p class="index1">Machine translation in encoder-decoder models</p>
<p class="index1">Macro-bimodality in communication</p>
<p class="index1">MAHNOB Mimicry Database</p>
<p class="index1"><a id="page_487"/>Major Depressive Disorder (MDD)</p>
<p class="index2">diagnostic criteria</p>
<p class="index2">prevalence</p>
<p class="index1">Majority voting</p>
<p class="index2">social signals analysis</p>
<p class="index2">training learners</p>
<p class="index1">Mapping modalities in encoder-decoder models</p>
<p class="index1">MAPTRAITS challenge</p>
<p class="index1">Math Data Corpus</p>
<p class="index2">limited-resource theories</p>
<p class="index2">multimodal learning analytics</p>
<p class="index2">overview</p>
<p class="index1">Matrices</p>
<p class="index2">dense layers</p>
<p class="index2">RCICA</p>
<p class="index1">Maximum Rule in social signals analysis</p>
<p class="index1">McGurk effect</p>
<p class="index1">MCLM (Multimodal Cognitive Load Measurement) system</p>
<p class="index1">MCS (Multiple Classifiers System) for social signals</p>
<p class="index1">MDD (Major Depressive Disorder)</p>
<p class="index2">diagnostic criteria</p>
<p class="index2">prevalence</p>
<p class="index1">MDIST models</p>
<p class="index1">Mechanical Turk</p>
<p class="index2">crowd sourcing</p>
<p class="index2">deception detection</p>
<p class="index1">MED (multimedia event detection) tasks</p>
<p class="index1">Media description</p>
<p class="index2">challenges and limitations</p>
<p class="index2">multimodal applications</p>
<p class="index1">Media summarization, challenges and limitations</p>
<p class="index1">MediaEval challenge</p>
<p class="index1">Median Rule</p>
<p class="index2">social signals analysis</p>
<p class="index2">training learners</p>
<p class="index1">Mel frequency cepstral coefficients (MFCCs)</p>
<p class="index2">defined</p>
<p class="index2">multimodality and cognitive load</p>
<p class="index2">speech and depression</p>
<p class="index2">superseded</p>
<p class="index1">Mental disorders in depression. <i>See</i> Depression behavioral signals</p>
<p class="index1">Mental state assessment</p>
<p class="index2">introduction</p>
<p class="index2">multimodal learning analytics. <i>See</i> Multimodal learning analytics</p>
<p class="index1">Mental status</p>
<p class="index2">expressions</p>
<p class="index2">galvanic skin response</p>
<p class="index1">Metacognitive awareness</p>
<p class="index2">defined</p>
<p class="index2">emergence</p>
<p class="index1">MFCCs. <i>See</i> Mel frequency cepstral coefficients (MFCCs)</p>
<p class="index1">MFHMM (Multi-stream Fused Hidden Markov Model)</p>
<p class="index1">Micro-bimodality in communication</p>
<p class="index1">Micro-expressions in deception detection</p>
<p class="index1">Microphones in social signals analysis</p>
<p class="index1">Microsoft Kinect device</p>
<p class="index2">description</p>
<p class="index2">enjoyment recognition</p>
<p class="index2">Oral Presentation Corpus</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">MindReading database</p>
<p class="index1">Minimum rule in training learners</p>
<p class="index1">Missing data</p>
<p class="index2">multimodal frameworks</p>
<p class="index2">online recognition</p>
<p class="index1">Mixture of experts model in training learners</p>
<p class="index1">Mixture of Parts model</p>
<p class="index1">MMAD system. <i>See</i> Multisensor-multimodal Affect Detection (MMAD)</p>
<p class="index1">Mobile device sensors for depression behavioral</p>
<p class="index1">Modalities</p>
<p class="index2">description</p>
<p class="index2">Math Data Corpus</p>
<p class="index2">Oral Presentation Corpus</p>
<p class="index1"><a id="page_488"/>Modality fusion in affect detection</p>
<p class="index1">Model adaptation for multimodal interfaces</p>
<p class="index1">Model-based fusion for affect detection</p>
<p class="index1">Modeling tasks in multimodal learning analytics</p>
<p class="index1">Modulation of non-redundant signals</p>
<p class="index1">Moments of insight</p>
<p class="index2">defined</p>
<p class="index2">problem solving</p>
<p class="index1">Monotonous speech</p>
<p class="index1">Motion</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Motor control in speech analysis</p>
<p class="index1">Mouse</p>
<p class="index1">MRD (Multimodal Recording Device)</p>
<p class="index1">MRI scanners in deception detection</p>
<p class="index1">Multi-level multimodal learning analytics</p>
<p class="index2">defined</p>
<p class="index2">description</p>
<p class="index1">Multi-stream Fused Hidden Markov Model (MFHMM)</p>
<p class="index1">Multi-subject processing in userstate and trait recognition</p>
<p class="index1">Multidimensional scales for cognitive load indicators</p>
<p class="index1">Multimedia event detection (MED) tasks</p>
<p class="index1">Multimedia retrieval, challenges and limitations</p>
<p class="index1">Multimedia videos, indexing and retrieval</p>
<p class="index1">Multimodal and affect-sensitive interfaces</p>
<p class="index2">adaptability</p>
<p class="index2">conclusion</p>
<p class="index2">context</p>
<p class="index2">context dependency</p>
<p class="index2">correlation analysis methods</p>
<p class="index2">focus questions</p>
<p class="index2">introduction</p>
<p class="index2">model adaptation</p>
<p class="index2">temporal information</p>
<p class="index2">temporal modeling of facial expressions</p>
<p class="index1">Multimodal assessment of depression. <i>See</i> Depression behavioral signals</p>
<p class="index1">Multimodal Cognitive Load Measurement (MCLM) system</p>
<p class="index1">Multimodal communication</p>
<p class="index1">Multimodal coordination of affective responses</p>
<p class="index1">Multimodal data</p>
<p class="index2">classifying. <i>See</i> Classifying multimodal data</p>
<p class="index2">databases</p>
<p class="index2">defined</p>
<p class="index1">Multimodal deep learning</p>
<p class="index1">Multimodal embedding models</p>
<p class="index2">image representation</p>
<p class="index2">multimodal joint representation</p>
<p class="index2">overview</p>
<p class="index2">text representation</p>
<p class="index1">Multimodal expressions of affects</p>
<p class="index2">conclusion</p>
<p class="index2">context impact</p>
<p class="index2">emotions and expressions</p>
<p class="index2">focus questions</p>
<p class="index2">introduction</p>
<p class="index2">perception of combinations</p>
<p class="index2">references</p>
<p class="index1">Multimodal fusion</p>
<p class="index2">affect detection</p>
<p class="index2">defined</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">framework requirements</p>
<p class="index2">real-time sensing of social signals</p>
<p class="index1">Multimodal indicators of cognitive load. <i>See</i> Cognitive load indicators</p>
<p class="index1">Multimodal joint representation</p>
<p class="index1">Multimodal learning analytics</p>
<p class="index2">advantages</p>
<p class="index2">analysis techniques</p>
<p class="index2">challenges and limitations</p>
<p class="index2">conclusions and future directions</p>
<p class="index2"><a id="page_489"/>dataset limitations</p>
<p class="index2">defined</p>
<p class="index2">focus questions</p>
<p class="index2">history</p>
<p class="index2">infrastructure</p>
<p class="index2">introduction</p>
<p class="index2">Math Data Corpus</p>
<p class="index2">objectives</p>
<p class="index2">Oral Presentation Corpus</p>
<p class="index2">overview</p>
<p class="index2">prerequisites of learning</p>
<p class="index2">research findings</p>
<p class="index2">theoretical basis</p>
<p class="index1">Multimodal machine learning</p>
<p class="index2">co-learning</p>
<p class="index2">conclusion</p>
<p class="index2">focus questions</p>
<p class="index2">introduction</p>
<p class="index2">multimodal applications</p>
<p class="index2">multimodal representations</p>
<p class="index2">references</p>
<p class="index1">Multimodal Machine Learning Workshop</p>
<p class="index1">Multimodal Recording Device (MRD)</p>
<p class="index1">Multimodal representations</p>
<p class="index2">coordinated</p>
<p class="index2">discussion</p>
<p class="index2">joint</p>
<p class="index2">overview</p>
<p class="index1">Multimodal signals</p>
<p class="index2">cognitive load indicators</p>
<p class="index2">processing architectures</p>
<p class="index1">Multimodal term, ACM references to</p>
<p class="index1">Multimodal zero shot learning</p>
<p class="index1">Multiple Classifiers System (MCS) for social signals</p>
<p class="index1">Multiple kernel learning</p>
<p class="index2">classification</p>
<p class="index2">overview</p>
<p class="index1">Multisensor-multimodal Affect Detection (MMAD)</p>
<p class="index2">accuracy</p>
<p class="index2">affect detection</p>
<p class="index2">modality fusion</p>
<p class="index2">trends</p>
<p class="index1">Multisensory requirements in multimodal frameworks</p>
<p class="index1">MUMIN coding scheme</p>
<p class="index2">databases</p>
<p class="index2">deception detection</p>
<p class="index1">Music databases</p>
<p class="indext">N-grams in deception detection</p>
<p class="index1">Naive Bayes in deception detection</p>
<p class="index1">Nao robots</p>
<p class="index1">NAQ (Normalized Amplitude Quotient)</p>
<p class="index1">NASA Task Load Index (NASA-TLX)</p>
<p class="index1">National Institute of Mental Health (NIMH)</p>
<p class="index1">Natural language</p>
<p class="index2">data-driven word embeddings</p>
<p class="index2">video description</p>
<p class="index1">Naturalness of databases</p>
<p class="index1">Neural Networks (NNs)</p>
<p class="index2">affect detection</p>
<p class="index2">coordinated representations</p>
<p class="index2">facial expressions</p>
<p class="index2">fusion models</p>
<p class="index2">joint representations</p>
<p class="index2">multimodal deep learning</p>
<p class="index2">recurrent. <i>See</i> Recurrent Neural Networks (RNNs)</p>
<p class="index2">text representations</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Neural representations in deep learning</p>
<p class="index1">Neuro-machine-translation (NMT)</p>
<p class="index1">Neuroticism, Extraversion, Openness, and Five Factor Inventory</p>
<p class="index1">Neutral expressions in affects</p>
<p class="index1">NIMH (National Institute of Mental Health)</p>
<p class="index1">NMT (neuro-machine-translation)</p>
<p class="index1">NNs. <i>See</i> Neural Networks (NNs)</p>
<p class="index1">Noise suppression in deception detection</p>
<p class="index1"><a id="page_490"/>Non-parallel data</p>
<p class="index2">co-learning</p>
<p class="index2">grounding</p>
<p class="index1">Non-prototypical behavior in online recognition</p>
<p class="index1">Non-redundant signals in multimodal communication</p>
<p class="index1">Nonverbal behavioral social signals</p>
<p class="index1">Normalized Amplitude Quotient (NAQ)</p>
<p class="index1">Nursing education in cognitive load theory</p>
<p class="indext">Offline learning in userstate and trait recognition</p>
<p class="index1">Online interaction in deception detection</p>
<p class="index1">Online learning in userstate and trait recognition</p>
<p class="index1">Online recognition systems</p>
<p class="index2">defined</p>
<p class="index2">real-time sensing</p>
<p class="index1">Open-Face tools</p>
<p class="index1">OpenBlissART tool</p>
<p class="index1">openSMILE library and toolkit</p>
<p class="index2">speech analysis</p>
<p class="index2">speech and depression</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Optimization in userstate and trait recognition</p>
<p class="index1">Oral Presentation Corpus</p>
<p class="index1">Order-embeddings of images and language</p>
<p class="index1">Overfitting</p>
<p class="index2">defined</p>
<p class="index2">speech analysis</p>
<p class="indext">Pairwise diversity in social signals</p>
<p class="index1">Parallel data in co-learning</p>
<p class="index1">Parkinson&#8217;s disease in cognitive load measurement</p>
<p class="index1">Part of speech tags (POS) in deception detection</p>
<p class="index1">Partially observed HCRF</p>
<p class="index1">Patient Health Questionnaire (PHQ-9)</p>
<p class="index1">Pattern recognition in classification tasks</p>
<p class="index1">PAULA XML (Potsdam Exchange Format for Linguistic Annotations)</p>
<p class="index1">Pavlidis, I., deception detection</p>
<p class="index1">PC/laptop-based multimodal assessment of depression</p>
<p class="index1">PCFG (Probabilistic Context Free Grammar) parse trees</p>
<p class="index1">Perception-action dynamic theories</p>
<p class="index1">Performance</p>
<p class="index2">analytics</p>
<p class="index2">cognitive load indicators</p>
<p class="index2">defined</p>
<p class="index2">problem solving</p>
<p class="index1">Person-specific classifiers for multimodal interfaces</p>
<p class="index1">PHANTOM Desktop arm</p>
<p class="index1">PHOG (Pyramid of Histogram of Gradient)</p>
<p class="index1">PHQ-9 (Patient Health Questionnaire)</p>
<p class="index1">Physics Playground game</p>
<p class="index1">Physiological measures</p>
<p class="index2">affect detection</p>
<p class="index2">cognitive load</p>
<p class="index2">deception detection</p>
<p class="index2">defined</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Pitch variability in speech analysis</p>
<p class="index1">Pittsburgh database</p>
<p class="index1">PixelCNNs network</p>
<p class="index1">Planar Travelling Salesman Problem</p>
<p class="index1">Pleasure continuum</p>
<p class="index1">Polygraph tests</p>
<p class="index1">POS (part of speech tags) in deception detection</p>
<p class="index1">Post-Traumatic Stress Disorder (PTSD)</p>
<p class="index2">body movement and depression</p>
<p class="index2">depression behavioral signals</p>
<p class="index1">Postures as contextual cue</p>
<p class="index1">Potsdam Exchange Format for Linguistic Annotations (PAULA XML)</p>
<p class="index1">Praat tool</p>
<p class="index1"><a id="page_491"/>Pre-processing in userstate and trait recognition</p>
<p class="index1">Prediction power and results</p>
<p class="index2">cognitive load indicators</p>
<p class="index2">depression</p>
<p class="index2">late fusion</p>
<p class="index2">machine learning</p>
<p class="index2">multimodal data classification</p>
<p class="index2">multimodal deep learning</p>
<p class="index2">multimodal learning analytics</p>
<p class="index2">social signals</p>
<p class="index1">Prerequisites for learning</p>
<p class="index2">defined</p>
<p class="index2">mental states</p>
<p class="index2">multimodal learning analytics</p>
<p class="index1">Pressure sensors for userstate and trait recognition</p>
<p class="index1">Privacy issues in multimodal learning analytics</p>
<p class="index1">Probabilistic Context Free Grammar (PCFG) parse trees</p>
<p class="index1">Probabilistic graphical models for joint representations</p>
<p class="index1">Problem solving in multimodal learning analytics</p>
<p class="index1">Product reviews, deception detection for</p>
<p class="index1">Product rule in training learners</p>
<p class="index1">Projective methods in RCICA</p>
<p class="index1">Proprioception</p>
<p class="index1">Prosody</p>
<p class="index2">social signals</p>
<p class="index2">speech analysis</p>
<p class="index1">Pseudo-modality in userstate and trait recognition</p>
<p class="index1">Pseudo-multimodal approach in userstate and trait recognition</p>
<p class="index1">Psychology in deception detection</p>
<p class="index1">Psychomotor retardation</p>
<p class="index1">PT (Pursuit Test)</p>
<p class="index1">PTSD (Post-Traumatic Stress Disorder)</p>
<p class="index2">body movement and depression</p>
<p class="index2">depression behavioral signals</p>
<p class="index1">Pupillary response</p>
<p class="index2">cognitive load</p>
<p class="index2">depression behavioral signals</p>
<p class="index1">Pursuit Test (PT)</p>
<p class="index1">Push-to-talk online recognition</p>
<p class="index1">Pyramid of Histogram of Gradient (PHOG)</p>
<p class="indext">Quasi-Open-Quotient (QOQ)</p>
<p class="indext">Random forest model in multimodal learning</p>
<p class="index1">Random Forest (RF) algorithm in deception detection</p>
<p class="index1">Rapid Facial Reactions (RFRs)</p>
<p class="index1">Rate of speech in speech analysis</p>
<p class="index1">RBM (restricted Boltzmann machines)</p>
<p class="index1">RCICA (Robust Correlated and Independent Component Analysis)</p>
<p class="index2">correlation analysis methods</p>
<p class="index2">multimodal interfaces</p>
<p class="index1">RCICA with Time Warpings (RCITW)</p>
<p class="index1">RCNNs (Regional Convolutional Neural Networks)</p>
<p class="index1">RDoC (Research Domain Criteria) project</p>
<p class="index1">Readability index in deception detection</p>
<p class="index1">Real-time requirements in multimodal frameworks</p>
<p class="index1">Real-time sensing of social signals</p>
<p class="index2">database collection</p>
<p class="index2">focus questions</p>
<p class="index2">introduction</p>
<p class="index2">multimodal frameworks</p>
<p class="index2">multimodal fusion</p>
<p class="index2">online recognition</p>
<p class="index2">Social Signal Interpretation framework</p>
<p class="index1">Recognition pipeline walk-through</p>
<p class="index1">RECOLA database</p>
<p class="index2">affect detection</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1"><a id="page_492"/>Recurrent connections in multimodal deep learning</p>
<p class="index1">Recurrent dense layers</p>
<p class="index1">Recurrent neural network language model (RNN-LM)</p>
<p class="index1">Recurrent Neural Networks (RNNs)</p>
<p class="index2">dense layers</p>
<p class="index2">early fusion</p>
<p class="index2">encoder-decoder models</p>
<p class="index2">image representation</p>
<p class="index2">joint representation</p>
<p class="index2">multimodal fusion</p>
<p class="index2">multimodal representations</p>
<p class="index2">sequential representations</p>
<p class="index2">text representation</p>
<p class="index1">Reduction of features in userstate and trait recognition</p>
<p class="index1">Redundancy</p>
<p class="index2">defined</p>
<p class="index2">multimodal communication signals</p>
<p class="index2">multimodal expressions of affects</p>
<p class="index1">Regional Convolutional Neural Networks (RCNNs)</p>
<p class="index1">Regressions</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">facial expressions</p>
<p class="index2">multimodal learning analytics</p>
<p class="index2">speech and depression</p>
<p class="index1">Reinforcement Learning</p>
<p class="index1">Reliabilities in multimodal learning analytics</p>
<p class="index1">Representation</p>
<p class="index2">defined</p>
<p class="index2">multimodal machine learning</p>
<p class="index2">social signals</p>
<p class="index1">Research Domain Criteria (RDoC) project</p>
<p class="index1">Research findings in multimodal learning analytics</p>
<p class="index1">Respiration rate</p>
<p class="index2">deception detection</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Restricted Boltzmann machines (RBM)</p>
<p class="index1">RF (Random Forest) algorithm in deception detection</p>
<p class="index1">RFRs (Rapid Facial Reactions)</p>
<p class="index1">RMSE (root mean square error) methods in speech and depression</p>
<p class="index1">RNN-LM (recurrent neural network language model)</p>
<p class="index1">RNNs. <i>See</i> Recurrent Neural Networks (RNNs)</p>
<p class="index1">Robust Correlated and Independent Component Analysis (RCICA)</p>
<p class="index2">correlation analysis methods</p>
<p class="index2">multimodal interfaces</p>
<p class="index1">Robust Regression</p>
<p class="index1">Role-playing games (RPGs) in deception detection</p>
<p class="index1">Root mean square error (RMSE) methods in speech and depression</p>
<p class="index1">Rule-based classifiers for facial expressions</p>
<p class="indext">Sadness</p>
<p class="index2">Autism Spectrum Disorders</p>
<p class="index2">basic emotion</p>
<p class="index1">SAL (Sensitive Artificial Listener) corpus</p>
<p class="index1">Scale invariant feature transform (SIFT)</p>
<p class="index1">Scattered X&#8217;s test (SX)</p>
<p class="index1">SCHMMs (semi-coupled Hidden Markov models)</p>
<p class="index1">SCID-5 interviews for depression</p>
<p class="index1">Scope of databases</p>
<p class="index1">Search function for userstate and trait recognition</p>
<p class="index1">Second International Workshop and Data-Driven Grand Challenge on Multimodal Learning Analytics</p>
<p class="index1">Segmentation</p>
<p class="index2">facial expressions</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Selection and generation in userstate and trait recognition</p>
<p class="index1"><a id="page_493"/>Self-adaptation in userstate and trait recognition</p>
<p class="index1">SEMAINE corpus</p>
<p class="index2">affect detection</p>
<p class="index2">real-time sensing of social signals</p>
<p class="index2">social interactions</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Semi-coupled Hidden Markov models (SCHMMs)</p>
<p class="index1">Semi-supervized learning in userstate and trait recognition</p>
<p class="index1">SensAble PHANTOM Desktop arm</p>
<p class="index1">Sensitive Artificial Listener (SAL) corpus</p>
<p class="index1">Sensors</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">multimodality and cognitive load</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Sensory modalities</p>
<p class="index1">Sentence-image pairs in joint representation</p>
<p class="index1">Sequence-to-sequence encoder-decoder models</p>
<p class="index1">Sequential joint representations</p>
<p class="index1">Severity style classification papers for speech and depression</p>
<p class="index1">Shape contours in userstate and trait recognition</p>
<p class="index1">Shared hidden layers</p>
<p class="index2">description</p>
<p class="index2">neural networks</p>
<p class="index1">SHORE tools</p>
<p class="index1">Short-term traits, defined</p>
<p class="index1">Shot-boundary detection</p>
<p class="index1">SIFT (scale invariant feature transform)</p>
<p class="index1">Signal-level fusion in userstate and trait recognition</p>
<p class="index1">Signal-level predictive results in multimodal learning analytics</p>
<p class="index1">Signal processing systems for depression behavioral</p>
<p class="index1">Silent video for deception detection</p>
<p class="index1">Similarity joint representations</p>
<p class="index1">Similarity models for coordinated representations</p>
<p class="index1">Skilled performance</p>
<p class="index2">analytics</p>
<p class="index2">defined</p>
<p class="index2">problem solving</p>
<p class="index1">Skin conductance</p>
<p class="index2">cognitive load</p>
<p class="index2">deception detection</p>
<p class="index1">Skin temperature in deception detection</p>
<p class="index1">SmartKom corpus</p>
<p class="index1">Smartphones</p>
<p class="index2">AI in</p>
<p class="index2">depression assessment</p>
<p class="index1">Smiling as depression behavioral signal</p>
<p class="index1">Social anxiety</p>
<p class="index1">Social connectivity for depression behavioral signals</p>
<p class="index1">Social intelligence, advent of</p>
<p class="index1">Social interactions in multimodal applications</p>
<p class="index1">Social networks in deception detection</p>
<p class="index1">Social Signal Interpretation (SSI) framework</p>
<p class="index2">basic concepts</p>
<p class="index2">multimodal enjoyment recognition</p>
<p class="index2">overview</p>
<p class="index2">recognition pipeline</p>
<p class="index1">Social Signal Processing (SSP), defined</p>
<p class="index1">Social signals</p>
<p class="index2">classifier diversity</p>
<p class="index2">defined</p>
<p class="index2">focus questions</p>
<p class="index2">introduction</p>
<p class="index2">multimodal analysis</p>
<p class="index2">multimodal communication in life and human sciences</p>
<p class="index2">next steps</p>
<p class="index2">real-time sensing. <i>See</i> Real-time sensing of social signals</p>
<p class="index2"><a id="page_494"/>State-of-the-art</p>
<p class="index1">Soft attention mechanisms in encoderdecoder models</p>
<p class="index1">Softmax layers</p>
<p class="index2">defined</p>
<p class="index2">feed-forward neural networks</p>
<p class="index2">intermediate fusion</p>
<p class="index2">late fusion</p>
<p class="index2">recurrent neural networks</p>
<p class="index1">Spanish essays, deception detection in</p>
<p class="index1">Sparse CCA</p>
<p class="index1">Sparse corruptions in RCICA</p>
<p class="index1">Sparse matrices in RCICA</p>
<p class="index1">Spatio-temporal features</p>
<p class="index2">defined</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">facial analysis</p>
<p class="index1">Spatio-temporal interest points (STIP) method</p>
<p class="index1">Speech and speech analysis</p>
<p class="index2">AI usage</p>
<p class="index2">cognitive load indicators</p>
<p class="index2">cognitive load measurement</p>
<p class="index2">deception detection</p>
<p class="index2">depression behavioral signals</p>
<p class="index2">emotion determination by</p>
<p class="index2">human perception of</p>
<p class="index1">Speech-gesture redundancy in affect expressions</p>
<p class="index1">Speech recognition and synthesis</p>
<p class="index2">audio-visual speech recognition</p>
<p class="index2">challenges and limitations</p>
<p class="index2">development of</p>
<p class="index2">encoder-decoder models</p>
<p class="index2">multimodal learning</p>
<p class="index1">Speech syntactic complexity in deception detection</p>
<p class="index1">Speech synthesis challenges and limitations</p>
<p class="index1">Spoken language in userstate and trait recognition</p>
<p class="index1">Spontaneous speech in depression behavioral signals</p>
<p class="index1">SSI framework. <i>See</i> Social Signal Interpretation (SSI) framework</p>
<p class="index1">SSI tool</p>
<p class="index1">SSP (Social Signal Processing), defined</p>
<p class="index1">Stacking model in training learners</p>
<p class="index1">Stakeholder approval and control for multimodal learning analytics</p>
<p class="index1">State-of-the-art theories for cognitive load indicators</p>
<p class="index1">Static rule-based classifiers for facial expressions</p>
<p class="index1">Stationary settings in multimodal learning analytics</p>
<p class="index1">Still conditions in emotional expressions</p>
<p class="index1">STIP (spatio-temporal interest points) method</p>
<p class="index1">Stream-level fusion in affect detection</p>
<p class="index1">Structured coordinated space models</p>
<p class="index1">Structured joint representations</p>
<p class="index1">Subject-specific features in multimodal interfaces</p>
<p class="index1">Subjective (self-report) measures for cognitive load</p>
<p class="index1">Subset selection in classifying multimodal data</p>
<p class="index1">Sum Rule</p>
<p class="index2">social signals analysis</p>
<p class="index2">training learners</p>
<p class="index1">Support Vector Machines (SVMs)</p>
<p class="index2">deception detection</p>
<p class="index2">defined</p>
<p class="index2">domain adaptation</p>
<p class="index2">facial expressions</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Support Vector Regression (SVR)</p>
<p class="index2">defined</p>
<p class="index2">domain adaptation</p>
<p class="index2">facial expressions</p>
<p class="index2">online recognition</p>
<p class="index2">speech and depression</p>
<p class="index1"><a id="page_495"/>Supra-segmental features in userstate and trait recognition</p>
<p class="index1">Surprise expressions</p>
<p class="index1">SVMs. <i>See</i> Support Vector Machines (SVMs)</p>
<p class="index1">SVR. <i>See</i> Support Vector Regression (SVR)</p>
<p class="index1">Switchboard speech recognition</p>
<p class="index1">SX (Scattered X&#8217;s test)</p>
<p class="index1">Synchronization requirements</p>
<p class="index2">multimodal frameworks</p>
<p class="index2">Social Signal Interpretation framework</p>
<p class="index1">Syntactic constituency parsing in encoderdecoder models</p>
<p class="index1">Syntactic patterns in deception detection</p>
<p class="index1">Synthesis</p>
<p class="index2">challenges and limitations</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Systems-level learning theory</p>
<p class="indext">T-SNE technique in deep learning</p>
<p class="index1">T-unit analysis</p>
<p class="index2">deception detection</p>
<p class="index2">defined</p>
<p class="index1">Tabletop-enhanced classrooms in multimodal learning analytics</p>
<p class="index1">Tactile signals for userstate and trait recognition</p>
<p class="index1">Tandem tracking in deception detection</p>
<p class="index1">Temporal consistency of AUs in facial expressions</p>
<p class="index1">Temporal context for databases</p>
<p class="index1">Temporal discrepancies in correlation analysis methods</p>
<p class="index1">Temporal dynamics of facial expression, defined</p>
<p class="index1">Temporal information for multimodal interfaces</p>
<p class="index1">Temporal modeling in facial expressions</p>
<p class="index1">Text representation and analysis</p>
<p class="index2">continuous bag-of-words model</p>
<p class="index2">continuous skip-gram model</p>
<p class="index2">deception detection</p>
<p class="index2">feed-forward neural network language model</p>
<p class="index2">multimodal embedding models</p>
<p class="index2">recurrent neural networks</p>
<p class="index1">Theory of Least Collaborative Effort</p>
<p class="index1">Thermal imaging</p>
<p class="index2">deception detection</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">THMM (Tripled Hidden Markov Model)</p>
<p class="index1">Tools for userstate and trait recognition</p>
<p class="index1">Topic models in social signals</p>
<p class="index1">Trackballs</p>
<p class="index1">Training learners</p>
<p class="index1">Transcription requirements in multimodal frameworks</p>
<p class="index1">Transfer learning</p>
<p class="index2">deep learning</p>
<p class="index2">defined</p>
<p class="index2">multimodal learning analytics</p>
<p class="index2">non-parallel data</p>
<p class="index2">parallel data</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Translation</p>
<p class="index2">defined</p>
<p class="index2">multimodal machine learning</p>
<p class="index2">social signals</p>
<p class="index1">Travelling Salesman Problem</p>
<p class="index1">TrecVid initiative</p>
<p class="index1">Trimodal models in learning predictions</p>
<p class="index1">Tripled Hidden Markov Model (THMM)</p>
<p class="index1">Truth bias in deception detection</p>
<p class="index1">Tsalamlal, M. Y., haptic expressions of affects</p>
<p class="index1">Twitter feeds for depression behavioral signal</p>
<p class="indext">Unidimensional scales for cognitive load indicators</p>
<p class="index1"><a id="page_496"/>Unimodal affect detection (UMAD)</p>
<p class="index2">accuracy</p>
<p class="index2">affect detection</p>
<p class="index1">Unimodal zero shot learning</p>
<p class="index1">Unipolar depression</p>
<p class="index1">Unsupervised approaches</p>
<p class="index2">deep learning</p>
<p class="index2">social signals</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">User-independent models, defined</p>
<p class="index1">Userstate and trait recognition</p>
<p class="index2">architectures</p>
<p class="index2">attempts overview</p>
<p class="index2">audio and spoken and written language</p>
<p class="index2">emerging trends and future directions</p>
<p class="index2">focus questions</p>
<p class="index2">images and video</p>
<p class="index2">introduction</p>
<p class="index2">learning</p>
<p class="index2">modalities</p>
<p class="index2">modeling</p>
<p class="index2">modern architecture perspective</p>
<p class="index2">optimization</p>
<p class="index2">physiology</p>
<p class="index2">pseudo-multimodality</p>
<p class="index2">tactile signals</p>
<p class="index2">tools</p>
<p class="index2">walk-through</p>
<p class="indext">VAM corpus</p>
<p class="index1">Variable-state Latent CRF (VSL-CRF) model</p>
<p class="index1">Velten mood induction databases</p>
<p class="index1">Video</p>
<p class="index2">affect detection</p>
<p class="index2">AI usage</p>
<p class="index2">deception detection</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">Video description</p>
<p class="index2">challenges and limitations</p>
<p class="index2">encoder-decoder models</p>
<p class="index2">natural language</p>
<p class="index1">Virtual agents in multimodal expressions of affects</p>
<p class="index1">Virtual characters in emotions and expressions</p>
<p class="index1">Visage tools for multimodal learning analytics</p>
<p class="index1">Vision</p>
<p class="index2">deception detection</p>
<p class="index2">deep learning</p>
<p class="index1">Visual clues in deception detection</p>
<p class="index1">Visual modality, description</p>
<p class="index1">Visual question-answering (VQA)</p>
<p class="index2">challenges and limitations</p>
<p class="index2">multimodal applications</p>
<p class="index1">Vocal modality, description</p>
<p class="index1">Vocal Tract Coordination (VTC) feature</p>
<p class="index1">Voice and voice quality</p>
<p class="index2">cognitive load measurement</p>
<p class="index2">defined</p>
<p class="index2">speech analysis</p>
<p class="index1">Voice User Interfaces (VUIs)</p>
<p class="index1">Voting in training learners</p>
<p class="index1">Vowel Space Area (VSA)</p>
<p class="index2">defined</p>
<p class="index2">speech and depression</p>
<p class="index1">VQA (visual question-answering)</p>
<p class="index2">challenges and limitations</p>
<p class="index2">multimodal applications</p>
<p class="index1">VSA (Vowel Space Area)</p>
<p class="index2">defined</p>
<p class="index2">speech and depression</p>
<p class="index1">VSL-CRF (Variable-state Latent CRF) model</p>
<p class="index1">VTC (Vocal Tract Coordination) feature</p>
<p class="index1">VUIs (Voice User Interfaces)</p>
<p class="indext">W3C EmotionML standard</p>
<p class="index1">W4 quadruplet for multimodal interfaces</p>
<p class="index1">W5+ context dependency model</p>
<p class="index1"><a id="page_497"/>W5+ sextuplet for multimodal interfaces</p>
<p class="index1">Walking speed in cognitive load measurement</p>
<p class="index1">Wearable-based assessment of depression</p>
<p class="index1">Web scale annotation by image embedding (WSABIE) model</p>
<p class="index1">Weight matrices for dense layers</p>
<p class="index1">Weighted voting in training learners</p>
<p class="index1">WEKA 3 tool</p>
<p class="index1">Whirlwind project</p>
<p class="index1">White-boxing in deep learning</p>
<p class="index1">Wizard-of-Oz scenario</p>
<p class="index2">databases</p>
<p class="index2">real-time sensing</p>
<p class="index1">Word statistics in deception detection</p>
<p class="index1">Wordnet Affect</p>
<p class="index2">database</p>
<p class="index2">deception detection</p>
<p class="index1">Working memory based on cognitive load theory</p>
<p class="index1">Working memory resources limitations</p>
<p class="index1">Working Memory theory</p>
<p class="index1">Wrapper searches in userstate and trait recognition</p>
<p class="index1">Writing</p>
<p class="index2">AI usage</p>
<p class="index2">cognitive load indicators</p>
<p class="index2">multimodal learning analytics</p>
<p class="index2">userstate and trait recognition</p>
<p class="index1">WSABIE (web scale annotation by image embedding) model</p>
<p class="indext">Zeno robots</p>
<p class="index1">Zero shot learning (ZSL)</p>
<p class="index2">description</p>
<p class="index2">non-parallel data</p>
</body>
</html>