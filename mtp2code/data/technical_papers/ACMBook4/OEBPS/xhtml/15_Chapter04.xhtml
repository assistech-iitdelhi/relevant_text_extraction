<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_99"/>4</p>
<p class="chtitle"><b>Deep Learning for Multisensorial and Multimodal Interaction</b></p>
<p class="chauthor"><b>Gil Keren, Amr El-Desoky Mousa, Olivier Pietquin, Stefanos Zafeiriou, Bj&#246;rn Schuller</b></p>
<p class="h1"><a id="ch4_1"/><b><span class="bg1">4.1</span>&#160;&#160;&#160;&#160;Introduction</b></p>
<p class="noindent">In recent years, neural networks have demonstrated large empirical success in various machine learning tasks over large datasets, such as object recognition [Krizhevsky et al. 2012, Szegedy et al. 2015], machine translation [Bahdanau et al. 2015], and speech recognition [Amodei et al. 2016, Hinton et al. 2012, Deng and Yu 2014]. In addition to the empirical success, models of neural networks are often simpler and easier to design. Indeed, in many cases learning is performed simultaneously for all parts of the model, from feature extraction all the way to prediction, in an &#8220;end-to-end&#8221; manner. Another factor contributing to the simplicity of neural network models is that the same model building blocks (the different types of layers) are generally used in many different applications. Using the same building blocks for a large variery of tasks makes the adaptation of models used for one task or data to another task or data relatively easy. In addition, software kits have been developed to allow faster and more efficient implementation of these models, such as Theano [Al-Rfou et al. 2016] and Tensorflow [Abadi et al. 2016]. For these reasons, nowadays neural networks are a prominent method of choice for a large variety of machine learning tasks over large datasets.</p>
<p class="indent">This chapter focuses on serveral types of neural networks models that can be used for multimodal or multisensorial machine learning. It is assumed that the reader is familiar with the basic building blocks of neural networks, such as convolutional, recurrent, dense and softmax layers (for an introduction to these topic, <a id="page_100"/>please refer to Goodfellow et al. [2016]). In <a href="#ch4_2">Section 4.2</a>, we cover fusion models, mostly used for classification or regression of multimodal or multisensorial data, by fusing different data representation from different modalities or sensors at different levels of the processing pipeline. In <a href="#ch4_3">Section 4.3</a>, we cover encoder-decoder models, that are used for translation of data from one modality to another, such as translation of text between two languages, generating a caption for an image and generating an image for a caption. These models perform this multimodal translation by encoding the unimodal data representation and then decoding it to the appropriate representation in the other modality. In <a href="#ch4_4">Section 4.4</a>, we cover models that embed data from different modalities in a shared embedding space. This embedding space is to create a semantically meaningful space that can correlate these different modalities together. Finally, in <a href="#ch4_5">Section 4.5</a> we give a brief overview of the importance of reinforcement learning to the field of multimodal machine learning. This chapter does not cover the exhaustive list of topics in the intersection of deep learning and multimodal machine learning and some of these topics are not included in this text, such as the deep temporal alignment of two time sequences [Trigeorgis et al. 2016].</p>
<p class="h1"><a id="ch4_2"/><b><span class="bg1">4.2</span>&#160;&#160;&#160;&#160;Fusion Models</b></p>
<p class="noindent">When the input to a given model is comprised of data representations originating from multiple modalities or sensors, to compute a single model output, one has to fuse the multiple data representations at a certain point of the processing pipeline.</p>
<p class="indent">In addition to standard <i><b>early</b></i> and <i><b>late fusion</b></i> models, neural networks allow for fusing the different modalities or sensors at any intermediate level of the network. Indeed, as neural networks are typically comprised of a hierarchy of layers, it is possible to feed each input source into a number of dedicated layer, and fuse the representations of the different sources extracted from the top of these layers. For simplicity of the text, we consider the task of classification based on two modalities, but the following can be easily adapted to other tasks and multiple modalities. A schematic diagram of the three fusion strategies we discuss in the section can be found in <a href="#fig4_1">Figure 4.1</a>.</p>
<p class="indent">The subject of fusion methods goes beyond the scope of neural network models, and in this section we will focus on those fusing methods that are most relevant for neural network models, and neural network models that are fusion methods themselves. For an extensive survey about fusion method for multimodal data, please refer to Atrey et al. [2010].</p>
<div class="box">
<p class="bhead"><a id="page_101"/><b>Glossary</b></p>
<p class="hangbx">A <b>Convolutional Neural Network (CNN)</b> is a neural network that contains one or more convolutional layers. A <i>convolutional layer</i> is a layer that processes an image (or any other data comprised of points with a notion of distance between these points, such as an audio signal) by convolving it with a number of kernels.</p>
<p class="hangbx">A <b>dense layer</b> is the basic type of layer in a neural network. The layer takes a one-dimensional vector as input and transforms it to another one-dimensional vector by multiplying it by a <i>weight matrix</i> and adding a <i>bias vector</i>.</p>
<p class="hangbx">A <b>Recurrent Neural Network (RNN)</b> is a neural network that contains one or more recurrent layers. A <i>recurrent layer</i> is a layer that takes a sequence <i>x</i> indexed by <i>t</i> and processes it element by a element, while maintaining a <i>hidden state</i> for each unit in the layer: <i>h<sub>t</sub></i> = <i>RNN</i>(<i>h</i><sub><i>t</i>&#8722;1</sub>, <i>x<sub>t</sub></i>), where <i>h<sub>t</sub></i> is the hidden state at step <i>t</i>, and <i>RNN</i> is a transition function to compute the next hidden state, that depends on the type of hidden layer.</p>
<p class="hangbx">A <b>sequence-to-sequence</b> model is a neural network that processes a sequence as its input and produces another sequence as its output. Example of such models include neural machine translation and end-to-end speech recognition models.</p>
<p class="hangbx">A <b>softmax layer</b> is a dense layer followed by the softmax nonlinearity. The softmax nonlinearity takes a one-dimensional vector <i>v</i> of real numbers and normalizes it into a probability distribution, by applying <img src="../images/inline101_1.png" alt="Image"/>, where the sum is over all coordinates of the vector <i>v</i>.</p>
<p class="hangbx"><b>Early fusion</b> models are models for processing multimodal or multisensorial data, in which a model is processing the concatenation of all the data representations from the different modalities. In <i>late fusion</i> models, there is a unimodal model for each modality, and the outputs of all unimodal models are then combined to a final prediction based on all modalities.</p>
</div>
<p class="h2"><a id="ch4_2_1"/><b><span class="bg2">4.2.1</span>&#160;&#160;&#160;&#160;Early Fusion</b></p>
<p class="noindent">The term early fusion, also called fusion in feature space, refers to the case where, for one example, unimodal features from the two modalities are concatenated <i>before</i> being fed to the model (<a href="#fig4_1">Figure 4.1a</a>). The model, in the scope of this text, can be any kind of neural network model used for the task at hand, most commonly a standard feed-forward network comprised of <i><b>dense layer</b></i>, a <i><b>Convolutional Neural Network(CNN)</b></i> or a <i><b>Recurrent Neural Network (RNN).</b></i> In many cases, preprocessing is performed on the concatenated features, such as mean and variance normalization, so that all features share some statistical properties.</p>
<div class="cap" id="fig4_1">
<p class="image"><a id="page_102"/><img src="../images/fig4_1.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 4.1</b>&#160;&#160;&#160;&#160;Different fusion strategies for two modalities. (a) Early fusion, (b) intermediate fusion, and (c) late fusion.</p>
</div>
<p class="indent">The main drawback of the early fusion method is that often simply concatenating features from the two modalities is not possible. Consider, for example, the case where the features of one modality are represented as a time sequence and the second modality is an image. Another example is two time sequences of different lengths or sampling rates. One na&#239;ve solution to this concatenation problem is flattening each representation to a one-dimensional vector, and then concatenating the representations. This na&#239;ve solution is, in most cases, undesired; any structure the data had before flattening is lost. On the other hand, early fusion can always be performed in case the two modalities are represented each as a one-dimensional vector, as in (Paleari and Huet [2008]). In what follows we further elaborate on the difficulties of using early fusion models, for the common cases where the data is an image or a time sequence.</p>
<p class="indent">For processing a modality that is an image, a CNN is most commonly used. In case the other modality is represented as an unordered one-dimensional vector or as a time sequence, it cannot be concatenated with the image in a plausible way such that the result of the concatenation can be fed to a CNN, and one should consider other fusion strategies, such as intermediate or late fusion.</p>
<p class="indent">For processing time sequences, an RNN is the most commonly used model. When the two modalities are represented as time sequences of the same sampling rate but different length, one can pad the shorter one with zeros to fill the missing <a id="page_103"/>time steps, and feed the concatenated sequence to the RNN. In case the two time sequences are of similar length but not the same sampling rates, such as two sensors producing measurements every 2 and 2.5 s respectively, one can use a sampling rate conversion method of choice, to apply on one of the time sequences before concatenating and feeding to the RNN. When the sampling rates of the two time sequences are very different from each other, for example when one modality is a measurement-per-day sequence and another is a measurement-per-minute sequence, early fusion methods might not be the most suitable choice. A successful application of early fusion of two time sequences can be found in Eyben et al. [2010], where low-level acoustic features are fused with linguistic features before being fed to the RNN.</p>
<p class="h2"><a id="ch4_2_2"/><b><span class="bg2">4.2.2</span>&#160;&#160;&#160;&#160;Late Fusion</b></p>
<p class="noindent">In late fusion models, each set of unimodal features is fed to its own classifier, and the two outputs of the two classifiers are then fused to emit the final output for the multimodal prediction (for illustration, see <a href="#fig4_1">Figure 4.1c</a>). By doing so, late fusion methods focus on the individual predictive strength of each modality. Learning is typically being done in two steps: (1) learning the unimodal classifiers and (2) learning the fusion model (in case it is a model that needs to be learned). Compared to early fusion, late fusion can be applied to a broader set of learning problems, as it does not suffer from the problem of fusing features in representation space. Indeed, even if the representations of the different modalities are of different forms, the outputs of multiple unimodal classifiers can be more easily fused, as they are in many cases class labels or class confidence measures of the same form.</p>
<p class="indent">When the output of each unimodal classifier is a class label, majority voting can be applied on the different classifier predictions to decide upon the final prediction of the multimodal classifier. Alternatively, in the case of a neural network with a <i>softmax layer</i>, the output of this classifier is a discrete probability distribution over the available classes. The output of multiple such classifiers can be combined into one multimodal class prediction by averaging (as in Kahou et al. [2016], Keren et al. [2016]):</p>
<p class="eqna"><img src="../images/pg103_01.png" alt="Image"/></p>
<p class="noindent">or by following the most confident prediction in all models (as in Keren et al. [2016]):</p>
<p class="eqna"><img src="../images/pg103_02.png" alt="Image"/></p>
<p class="noindent"><a id="page_104"/>where <i>n</i> is the number of modalities, <i>j</i> is the class index, and <img src="../images/inline104_1.png" alt="Image"/> is the probability assigned by the unimodal neural network classifier for modality <i>i</i> to class <i>j</i>.</p>
<p class="indent">Another option for fusing outputs of unimodal classifiers is to use a fusing neural network. This network is typically comprised of dense layers and a softmax layer for classification; its input is the outputs of the different unimodal classifiers and its output is the probabilities for the different classes. Using this approach, the fusing neural network attempts to learn in which cases to &#8220;trust&#8221; the output of each unimodal classifier. To further facilitate the learning of &#8220;when is each unimodal classifier more reliable&#8221;, additional fixed-size inputs can be introduced to the fusing network, such as metadata about the current training example (e.g., data about the user related to the example, context of the example, etc.).</p>
<p class="indent">In addition to the late fusion methods presented above, there exist many methods that can be used for the late fusion of unimodal neural network or non-neural network classifiers. For an extensive review of such methods, please refer to Atrey et al. [2010].</p>
<p class="h2"><a id="ch4_2_3"/><b><span class="bg2">4.2.3</span>&#160;&#160;&#160;&#160;Intermediate Fusion</b></p>
<p class="noindent">As neural networks are comprised of a hierarchy of layers, one can apply a few layers of processing on each unimodal representation before fusing the different modalities together. We refer to intermediate fusion models as models that process each unimodal representation using one or more dedicated layers of a neural network, concatenate the outputs of the top unimodal layers from all modalities, and feed the concatenated representation to an additional one or more layers of a neural network multimodal classifier. Learning is then typically performed in an end-to-end manner, optimizing for the parameters of the unimodal and the multimodal layers simultaneously. For illustration, see <a href="#fig4_1">Figure 4.1b</a>.</p>
<p class="indent">The method of intermediate fusion can be advantageous over early fusion, as the unimodal processing layers can differ in a way matching the nature of each modality. In addition, the concatenated representation can be fed to any number of layers, allowing a deep processing procedure of this multimodal representation, thus giving the intermediate fusion an advantage over late fusion models.</p>
<p class="indent">One example of an intermediate fusion model would be when one modality is an image, and another modality is a one-dimensional vector containing metadata. A CNN can be used to process the image, where the input to the softmax layer is then the last layer in the CNN (flattend) concatenated to the metadata vector. Another example would be a multimodal learning problem where one modality is an image, and another is a time sequence (for example, measurements taken by some senser). A few <i>convolutional layers</i> can be used to process the image, while one or more <a id="page_105"/>recurrent layers can be used to process the time sequence. The output of the top convolutional layer is then flattend and concatenated to the output of the last time step in the top recurrent layer, and this concatenated representation is fed to a few dense layers followed by a softmax layer for classification. In Eitel et al. [2015], Cheng et al. [2016], and Park et al. [2016], an intermediate fusion model is used for two different image modalities.</p>
<p class="h1"><a id="ch4_3"/><b><span class="bg1">4.3</span>&#160;&#160;&#160;&#160;Encoder-Decoder Models</b></p>
<p class="noindent">Multiple applications of machine learning can be described as a problem of translation between two modalities of the same object. Common examples are translating text in one language to another language, describing an image or a video using natural language, transcribing an audio signal, and transcribing speech to text. <i>Encoder-decoder</i> models approach the problem of translation between modalities by splitting it into two parts: encoding the input, a representation of the object in one modality, into a representation <i>h</i> in some high dimensional space: <i>h</i> = <i>f</i>(<i>x</i>; <i>&#952;<sub>f</sub></i>), and decoding a given representation into an output <i>y</i>, a representation of the object in another modality: <i>y</i> = <i>g</i>(<i>h</i>; <i>&#952;<sub>g</sub></i>). Typically, each of the two parts is a neural network, and learning is performed in an end-to-end manner: a loss function <i>L</i>(<i>g</i>(<i>f</i>(<i>x</i>); <i>&#952;<sub>f</sub>, &#952;<sub>g</sub></i>) is defined on top of the decoder output, and all model parameters (<i>&#952;<sub>f</sub>, &#952;<sub>g</sub></i>) are optimized simultaneously using a gradient-based optimization method (such as Stochastic Gradient Decent [Bottou 2010], Momentum [Polyak 1964], Adam [Kingma and Ba 2015], error sensitive optimization [Keren et al. 2017a], or fast single-logit classification optimization [Keren et al. 2017b]).</p>
<p class="h2"><a id="ch4_3_1"/><b><span class="bg2">4.3.1</span>&#160;&#160;&#160;&#160;Sequence-to-Sequence Models</b></p>
<p class="noindent">The framework of <i><b>sequence-to-sequence</b></i> was introduced in Sutskever et al. [2014] for the task of machine translation. As sentences are sequences of words, they can be processed with an RNN [Hochreiter and Schmidhuber 1997b, Cho et al. 2014, Keren and Schuller 2016]. In an RNN, a sequence <i>x</i> = (<i>x</i>, &#8230;, <i>x<sub>T</sub></i>) is comprised of <i>T</i> time steps, and is processed sequentially while maintaining a <i>hidden state: h</i><sub><i>t</i>+1</sub> = <i>RNN</i>(<i>h<sub>t</sub>, x</i><sub><i>t</i>+1</sub>), where <i>RNN</i> is a differentiable transition function that depends on the type of recurrent layer, and <i>h</i><sub>0</sub> is set to some initial state.</p>
<p class="indent">For the task of machine translation, sequence-to-sequence models are typically constructed as following. A translation database is comprised of source-target sentence pairs: <img src="../images/inline105_1.png" alt="Image"/>, where each <i>s<sub>i</sub></i> or <i>t<sub>i</sub></i> is an index of a word in the dictionary of the source or the target respectively. As word indices are meaningless discrete values, each word index is replaced with its learnable <i>embedding</i>, <a id="page_106"/>which is a <i>k</i>-dimensional vector. Note, in the rest of the section, feeding words to an RNN will actually mean feeding their respective <i>k</i>-dimensional embedding to the RNN, and for simplicity we will use these two terms interchangeably. The encoder <i>f</i> is a (possibly multi-layer) RNN that processes a sentence in the origin language (a sequence of embeddings of words) and outputs its representation at the last time step: <img src="../images/inline106_4.png" alt="Image"/>. The decoder in this model is an RNN as well. Given the encoded sentence <i>h</i>, at time step <i>n</i> the decoder emits a probability distribution for the <i>n</i>-th word in the translation, over all possible words in the target language, and a special end-of-sentence symbol:</p>
<p class="eqn"><a id="eq4_1"/><img src="../images/eq4_1.png" alt="Image"/></p>
<p class="noindent">such that <i>d<sub>n</sub></i> is the hidden state of the decoder RNN at the <i>n</i>-th time step, <i>W</i> is a weight matrix, <i>t</i><sub>0</sub> is set to a special &#60;EOS&#62; symbol, and the initial state of the decoder RNN is set to the encoded input: <i>d</i><sub>0</sub> = <i>h</i>. In words, at the first time step, the decoder RNN takes a special &#60;EOS&#62; symbol that marks the beginning of the translation and the encoded sentence <img src="../images/inline106_5.png" alt="Image"/>, and outputs a vector <img src="../images/inline106_1.png" alt="Image"/> in which the <i>i</i>-th element is the (unnormalized) probability that the first word in the translation is the <i>i</i>-th word in the target language dictionary. At the <i>n</i>-th time step, the decoder takes its current hidden state and the <i>n</i> &#8722; 1-th word in the correct translation <i>t</i><sub><i>n</i>&#8722;1</sub>, and outputs the vector <img src="../images/inline106_2.png" alt="Image"/> that contains the (unnormalized) probabilities of each word in the target language to be the <i>n</i>-th word in the translation. In addition to probabilities for all words in the target language dictionary, the vector <img src="../images/inline106_3.png" alt="Image"/> also contains the probability of the special element &#60;EOS&#62;, which is the probability for the end of the translation. Each vector <img src="../images/inline106_3.png" alt="Image"/> is then normalized to a vector <i>p<sub>n</sub></i> which is a probability distribution. A schematic figure of the model can be found in <a href="#fig4_2">Figure 4.2</a>.</p>
<p class="indent">Given the sequence <img src="../images/inline106_6.png" alt="Image"/>, the probability of a sentence <img src="../images/inline106_7.png" alt="Image"/> to be the model&#8217;s translation is given by:</p>
<p class="eqn"><a id="eq4_2"/><img src="../images/eq4_2.png" alt="Image"/></p>
<p class="noindent">and the network is then trained to maximize the probability <i>P</i>(<i>t</i>) of the correct translation, for all sentence pairs in the training set. Note that, the decoder outputs a probability distribution over all possible translations for a given sentence, and not only a single output translation.</p>
<p class="indent">At inference time, since the output of the decoder described above depends on a known correct translation of the sentence <i>t</i>, a minor adaption is needed to produce a translation for unseen sentences: In <a href="#eq4_1">Equation 4.1</a>, instead of feeding the real translation back to the model, we feed a sampled predicted translation by the model, namely <i>t</i><sub><i>n</i>&#8722;1</sub> from the known translation is replaced by <i>t</i><sub><i>n</i>&#8722;1</sub> ~ <i>p</i><sub><i>n</i>&#8722;1</sub>.</p>
<div class="cap" id="fig4_2">
<p class="image"><img src="../images/fig4_2.png" alt="Image"/></p>
<p class="figcaption"><a id="page_107"/><b>Figure 4.2</b>&#160;&#160;&#160;&#160;The sequence-to-sequence model. An input sentence in the original language, comprised of three words &#8220;A,B,C,&#8221; is processed by the encoder RNN. Then, the decoder RNN takes the last hidden state of the encoder and a special &#60;EOS&#62; symbol, and starts emitting the translation words &#8220;W, X, Y, Z.&#8221; The symbol &#60;EOS&#62; is emitted again to indicate the end of the translation.</p>
</div>
<p class="indent">The sequence-to-sequence framework is not limited to translation between only human spoken languages. In Vinyals et al. [2015b], the task of syntactic constituency parsing is formulated using the terms of the above described model (in fact, the model used was an enhanced version of the above model, and will be discussed in <a href="#ch4_3_3">Section 4.3.3</a>). Indeed, the problem of creating a parse tree from a sentence can be seen as a sequence-to-sequence problem, where the parse tree is linearized (cf. <a href="#fig4_3">Figure 4.3</a>). In this case, the target language is not a spoken language, but a language consisting of a set of symbols specialized for this task. Using a similar method, in Mei et al. [2016] navigational instructions in natural language are translated to a sequence of actions.</p>
<p class="h2"><a id="ch4_3_2"/><b><span class="bg2">4.3.2</span>&#160;&#160;&#160;&#160;Alternative Encoders or Decoders</b></p>
<p class="noindent">The sequence-to-sequence model described previously is an encoder-decoder model used for translation between two modalities: text in source and target languages. In that model, both the encoder and the decoder are recurrent neural networks. In order to translate between other modalities, it is possible to replace the encoder or the decoder with ones that are suitable for the desired modality. For example, in Vinyals et al. [2015c] the authors tackle the problem of image description. In image description, a training dataset is comprised of pairs of image <i>x</i> and its description <i>t</i>, a sequence of words in natural language: <img src="../images/inline107_1.png" alt="Image"/>. For this task, the decoder is the same as in the machine translation task, but the encoder from the sequence-to-sequence models is replaced by a CNN [LeCun et al. 1989]: <i>h</i> = CNN(<i>x</i>), where CNN(<i>x</i>) feeds the input <i>x</i> through the network and extracts a representation of the image in a top layer. The representation <i>h</i> is then used as the initial hidden state of the decoder. From this point on, the model is exactly the same as the machine translation model: at the <i>n</i>-th time step the decoder emits the probability distribution for the <i>n</i>-th word in the image description, and the model is trained to maximize the probability of the correct descriptions. At inference time, as in the sequence-to-sequence models, the model&#8217;s predicted description is being fed back to it, instead of the correct description. <a href="#fig4_4">Figure 4.4</a> contains a schematic illustration of the model.</p>
<div class="cap" id="fig4_3">
<p class="image"><img src="../images/fig4_3.png" alt="Image"/></p>
<p class="figcaption"><a id="page_108"/><b>Figure 4.3</b>&#160;&#160;&#160;&#160;A parse tree is linearized into a sequence of symbols (from Vinyals et al. [2015b]).</p>
</div>
<p class="indent">Another example application is the task of video description using natural language. In Venugopalan et al. [2015a], each frame of the video is fed to a CNN to extract its representation, and then the sequence of representations is fed to a recurrent neural network. The hidden state of the recurrent network at the last time step is the encoded video representation used by the decoder. In Venugopalan et al. [2015b], a slightly different encoder is used for the same task, where the extracted CNN representation is averaged across the video&#8217;s frames to create the encoder&#8217;s output.</p>
<p class="indent">In the applications discussed above, the encoder varies to match the task at hand, but the decoder is always a recurrent neural language decoder, which is a generative model that generates a natural language text given the encoded input. For the task of generating images from captions [Mansimov et al. 2015], the decoder has been replaced as well. In this work, the decoder is a generative model that generates an image conditioned on the encoded representation of its caption. <a href="#fig4_5">Figure 4.5</a> contains sample outputs of this model.</p>
<div class="cap" id="fig4_4">
<p class="image"><img src="../images/fig4_4.png" alt="Image"/></p>
<p class="figcaption"><a id="page_109"/><b>Figure 4.4</b>&#160;&#160;&#160;&#160;A CNN encoder followed by an RNN decoder, for generating an image description (from Vinyals et al. [2015c]).</p>
</div>
<div class="cap" id="fig4_5">
<p class="image"><img src="../images/fig4_5.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 4.5</b>&#160;&#160;&#160;&#160;Captions and their matching generated images (from Mansimov et al. [2015]).</p>
</div>
<p class="h2"><a id="ch4_3_3"/><b><span class="bg2">4.3.3</span>&#160;&#160;&#160;&#160;The Attention Mechanism</b></p>
<p class="noindent">In the models presented so far, the input to the decoder is a single vector of predefined dimensionality. The encoder has to compress all the relevant information from the input into this vector, and the decoder has no access to the different parts of the input. For example, when using the above-presented models in machine translation, the decoder is not able to access the different words in the source sentence and to interact independently with each of them, but only with the fixed representation of the sentence created by the encoder. Similarly, in the image captioning model described above, the decoder is not able to access the different parts of the image. While in theory it is possible for the encoder to encode all the relevant <a id="page_110"/>information into the fixed size vector representation (when the dimension of the vector is big enough), it was demonstrated in several works [Bahdanau et al. 2015, Xu et al. 2015] that it is preferable, in terms of task performance, to allow the decoder to interact with the different parts of the input.</p>
<p class="indent">The <i>soft attention mechanism</i> [Bahdanau et al. 2015, Cho et al. 2015] allows the decoder access to a structured form of the input. First, the encoder and decoder must conform to a certain form that allows for the usage of the attention mechanism. The encoder output is changed from a fixed size representation to a structured representation</p>
<p class="eqna"><img src="../images/pg110_01.png" alt="Image"/></p>
<p class="noindent">such that each <i>c<sub>i</sub></i> is a representation of a different part of the input, and <i>r</i> is the total number of parts. The separation to the different parts and the total number of parts is chosen according to the nature of the input. For example, in natural language applications, the different parts of the input to encode can be words, and in computer vision, different regions of an image can be the different parts to encode. On the decoder side, the decoder must be a recurrent model that at each time step <i>n</i> holds some hidden state <i>z<sub>n</sub></i> and emits an output <i>y<sub>n</sub></i> (the RNN decoder from the sequence-to-sequence models presented above meets these requirements and is most commonly used). The soft attention mechanism itself operates typically as follows. At time step <i>n</i>, the different parts of encoded input are weighted and then combined:</p>
<p class="eqn"><a id="eq4_3"/><img src="../images/eq4_3.png" alt="Image"/></p>
<p class="eqn"><a id="eq4_4"/><img src="../images/eq4_4.png" alt="Image"/></p>
<p class="eqn"><a id="eq4_5"/><img src="../images/eq4_5.png" alt="Image"/></p>
<p class="noindent">where <i>ATT</i> is a parameterized attention function (typically a multi-layer neural network) that determines the (unnormalized) relevance measure <i>e<sub>i</sub></i> of the encoded part <i>c<sub>i</sub></i> to the current decoder operation, given the current decoder hidden state <i>z</i><sub><i>n</i>&#8211;1</sub>. In other words, after <i>ATT</i> creates the vector <i>e</i> of the relevancy of the different input parts to the current operation, <i>e</i> is normalized to <i>&#945;</i>, and <i>&#945;</i> is used to compute a weighted sum of the encoded parts, which is the <i>context vector m<sub>n</sub></i> and the output of the attention mechanism at time step <i>n</i>.</p>
<p class="indent">Using the attention mechanism&#8217;s output <i>m<sub>n</sub></i>, to emit the next decoder output <i>y<sub>n</sub></i>, the decoder typically computes its next hidden state and output:</p>
<p class="eqna"><img src="../images/pg111_01.png" alt="Image"/></p>
<p class="noindent"><a id="page_111"/>where <i>&#981;, &#947;</i> are decoder-dependent functions (with a standard RNN decoder, these are the standard RNN transition and output functions). When all of the above functions are differentiable, the model can be learned in an end-to-end manner using a gradient-based optimization method, optimizing all parameters of the encoder, decoder, and attention model simultaneously.</p>
<p class="indent">The main advantage of using the attention mechanism is that it allows the decoder to learn to focus on the most relevant parts of the input at each time step. For example, in machine translation [Bahdanau et al. 2015], the decoder can attend to different words or phrases in the input sentence at each step of the translation. In image captioning [Xu et al. 2015], the decoder can attend to different parts of the image when emitting different parts of the caption sentence.</p>
<p class="indent">Encoder-decoder models with an attention mechanism and varients thereof have been used for various applications: machine translation [Bahdanau et al. 2015], where the encoder is a bidirectional recurrent neural network [Schuster and Paliwal 1997], image captioning [Xu et al. 2015] where the encoder is a CNN extracting features for parts of the input image, speech recognition [Chorowski et al. 2015], video description [Yao et al. 2015], image generation from captions [Mansimov et al. 2015], producing sequence of actions from natural language instructions [Mei et al. 2016], syntactic constituency parsing [Vinyals et al. 2015b], and even for finding approximate solutions to discrete optimization problems such as finding planar convex hulls and planar Travelling Salesman Problem [Vinyals et al. 2015a].</p>
<p class="indent">Another interesting advantage of Encoder-decoder models with attention mechanism is that these models are often able to learn an interpretable complex mapping between two modalities, without any explicit supervision. An example of such learned mapping for the machine translation [Bahdanau et al. 2015] and image captioning models [Xu et al. 2015], can be found in <a href="#fig4_6">Figure 4.6</a> and <a href="#fig4_7">Figure 4.7</a> respectively.</p>
<p class="h1"><a id="ch4_4"/><b><span class="bg1">4.4</span>&#160;&#160;&#160;&#160;Multimodal Embedding Models</b></p>
<p class="noindent">Another approach to multimodal machine learning is to learn to embed data from different modalities in a shared embedding space. The goal is to create a semantically meaningful space that can correlate these different modalities together. Even though similar concepts might appear very differently in different sensor modalities, humans are still able to understand that they map to the same concept. For <a id="page_112"/>example, we are able to correlate the appearance of a banana with its spoken name, or a natural language description of an image with its visual contents. There is a strong evidence that we do so through a common representation between different modalities [Erdogan et al. 2014]. Obtaining such a common representation is a challenging task because every modality might intrinsically have very different statistical properties. In addition, it would be very difficult to design joint features between such disparate modalities even with expert knowledge [Sung et al. 2015]. Models that rely on learning a common representation of different modalities in a shared embedding space are called <i>multimodal embedding models</i>.</p>
<div class="cap" id="fig4_6">
<p class="image"><img src="../images/fig4_6.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 4.6</b>&#160;&#160;&#160;&#160;Alignment learned with the attention mechanism, for a English-French translation model. The x-axis corresponds to words in the source sentence (English) and the y-axis corresponds to words in the generated translation. For each word in the generated translation, pixel <i>i</i> in its row corresponds to <i>&#945;<sub>i</sub></i> (grayscale, 0 is black, white is 1) in the step emitting the translated word. In other words, white pixel means that the English word in the pixel&#8217;s column was attended to strongly while emitting the French word in the pixel&#8217;s row (from Bahdanau et al. [2015]).</p>
</div>
<div class="cap" id="fig4_7">
<p class="image"><img src="../images/fig4_7.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 4.7</b>&#160;&#160;&#160;&#160;Alignment learned with the attention mechanism, for an image captioning model. The marked word is aligned with the bright area in the image (from Xu et al. [2015]).</p>
</div>
<p class="indent"><a id="page_113"/>In this section, we discuss one example of such models, where the task is to correlate images with their natural language descriptions. The goal is to show how to design a model that is rich enough to simultaneously reason about the visual contents of images and their corresponding textual captions. An example of a useful application in this domain is the auto-captioning of images for blind users on the web which can be very useful with the help of a text reader to read the generated captions for blind users. The section starts by introducing deep learning methods for representing images and text using unimodal representations, then it discusses how to learn joint multimodal representations for images and text together.</p>
<p class="h2"><a id="ch4_4_1"/><b><span class="bg2">4.4.1</span>&#160;&#160;&#160;&#160;Image Representation</b></p>
<p class="noindent">As previously mentioned in <a href="#ch4_2_1">Section 4.2.1</a>, the most common deep learning approach to represent images is the CNN. The image is passed as input through multiple convolutional layers to compute a hidden representation <i>h</i> = <i>CNN</i>(<i>x</i>), where <i>CNN</i>(<i>x</i>) feeds the input <i>x</i> through the network and extracts a representation of the image in a top layer. The representation <i>h</i> is considered as the image embedding.</p>
<p class="indent">In some more advanced studies, like in Girshick et al. [2014] and Karpathy and Fei-Fei [2015], a so-called Regional Convolutional Neural Network (RCNN) is used to detect objects at different locations of the image. Then, a representation <i>v</i> is calculated from each detected location based on the pixels <i>I<sub>b</sub></i> inside each bounding box as:</p>
<p class="eqn"><a id="eq4_6"/><img src="../images/eq4_6.png" alt="Image"/></p>
<p class="noindent">where <i>CNN</i>(<i>I<sub>b</sub></i>) is the transformation of the pixels inside the bounding box <i>I<sub>b</sub></i> found at the top hidden activation of the CNN. <i>W<sub>m</sub></i> is another transformation matrix, and <i>b<sub>m</sub></i> is a <i>bias vector</i>. The result of this final transformation is the final embedding for the given region inside the image. The whole image is thus represented as a set of vectors {<i>v<sub>i</sub></i>&#124;<i>i</i> = 1 &#8230;}, where <i>d</i> is the number of detected locations.</p>
<p class="h2"><a id="page_114"/><a id="ch4_4_2"/><b><span class="bg2">4.4.2</span>&#160;&#160;&#160;&#160;Text Representation</b></p>
<p class="noindent">In this section, we discuss different neural network-based methods used for learning vector representations of words and sentences from large text data. To give initial understanding of the topic, we start by discussing models that aim at representing text from the unimodal point of view. In other words, the training objectives do not correlate the text with the image modality, rather they seek to represent text in vector space using only text material. In this context, the most common methodology is to train a neural network-based language model which provides a conditional probability distribution over words given some number of previous and possibly future words. Later in this chapter we will see how text representations can be trained jointly with image modality using training objectives that correlate both modalities together.</p>
<p class="indent">In order to obtain high-quality vector representation of text, it is required that similar words become close to each other in the resulting vector space. For example, in the context of inflectional languages, nouns can have multiple word endings, and if similar words are searched in a subspace of the original vector space, it is possible to find words that have similar endings [Mikolov et al. 2009]. Nevertheless, Mikolov et al. [2013a] recently showed that similarity of word representations goes beyond simple syntactic regularities. For example, using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(&#8220;King&#8221;) - vector(&#8220;Man&#8221;) + vector(&#8220;Woman&#8221;) results in a vector that is closest to the vector representation of the word &#8220;Queen&#8221; [Mikolov et al. 2013b].</p>
<p class="indent">Several model architectures have been introduced to learn these vectors whilst preserving syntactic and semantic regularities among words. In addition to the model architecture, the accuracy of such a model depends heavily on the dimensionality of the word vectors and on the amount of the training data [Mikolov et al.2013&#176;].</p>
<p class="indent">Usually, it is possible to go from word-level representations to sentence-level representation by simply averaging the word vectors of a given sentence. For RNN-based models, an alternative approach is to use the representation of the last word in the sentence to represent the whole sentence. For a bidirectional RNN, alternatively, the average of the hidden states at all time steps can represent the whole sentence. This will be illustrated in the following sections.</p>
<p class="h3"><a id="ch4_4_2_1"/><b>4.4.2.1&#160;&#160;&#160;&#160;Feed-Forward Neural Networks</b></p>
<p class="noindent">The probabilistic feed-forward neural network language model (FFNN-LM) was proposed in Bengio and Ducharme [2001]. It consists of standard dense feed-forward <a id="page_115"/>layers preceded by a linear projection layer (with linear activation function). For an <i>m</i>-gram FFNN-LM, <i>m</i> &#8722; 1 previous words are encoded at the input layer of the network using the 1-of-<i>N</i> encoding, where <i>N</i> is the vocabulary size. This is a simple binary encoding, also well known as the <i>1-hot</i> encoding, where every word is represented by a binary sparse vector of size <i>N</i> with a single value of <i>one</i> at the index of the corresponding word and <i>zeros</i> at all other positions. The input layer is then connected to a projection layer, using a shared projection matrix that projects each of the <i>m</i> &#8722; 1 words into a <i>P</i> dimensional space. The projection layer is connected to one or more standard feed-forward layers. The final hidden state is used to compute a probability distribution over all the words in the vocabulary via an output softmax layer of size <i>N</i>. The network is trained to predict the following word that comes directly after the <i>m</i> &#8722; 1 history words [Arisoy et al. 2012, Mousa et al. 2013]. <a href="#fig4_8">Figure 4.8</a> shows the architecture of the model. Note that it is not necessary that all hidden layers have the same dimension.</p>
<div class="cap" id="fig4_8">
<p class="image"><img src="../images/fig4_8.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 4.8</b>&#160;&#160;&#160;&#160;Architecture of a deep FFNN-LM that estimates the probability distribution <i>p</i>(<i>w<sub>n</sub></i>|<i>h<sub>n</sub></i>), where <i>h<sub>n</sub></i> is the history: <i>w</i><sub><i>n</i>&#8722;<i>m</i>+1</sub><i>w</i><sub><i>n</i>&#8722;<i>m</i>+2</sub> &#8230; <i>w</i><sub><i>n</i>&#8722;1</sub> (from Mousa [2014]).</p>
</div>
<p class="indent">In principle, this model is able to predict the probability of any word given any history, where the history words are viewed together as an interpolated point into a higher dimensional space. The softmax normalization guarantees the generation of a valid (properly normalized) probability distribution from the neural network. In fact, performing the softmax normalization for each input history is very time <a id="page_116"/>consuming due to the large size of the output layer that is usually needed in practice. Therefore, a hierarchical version of the softmax is introduced in Morin and Bengio [2005], where a binary tree representation of the vocabulary is used. Thus, the number of the output units that need to be evaluated can drop down to around log<sub>2</sub>(<i>N</i>). Another proposed methods try to use unnormalized models during training [Collobert and Weston 2008, Huang et al. 2012].</p>
<p class="indent">As seen above, the FFNN-LM is designed to estimate a probability distribution over word sequences that make up the natural language sentences. Therefore, word embeddings can be taken as the projected vectors found in the rows of the projection matrix.</p>
<p class="h3"><a id="ch4_4_2_2"/><b>4.4.2.2&#160;&#160;&#160;&#160;Continuous Bag-of-Words Model</b></p>
<p class="noindent">As illustrated in Mikolov et al. [2013a], the continuous bag-of-words (CBOW) model architecture is similar to the FFNN-LM, however the nonlinear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged). This is called a bag-of-words model because the order of words in the history does not influence the projection. Furthermore, also words from the future are used. The model is trained to predict the middle word of a window given some limited number of history and future words. <a href="#fig4_9">Figure 4.9</a> shows an example of this model that predicts a word given a continuous bag of two history and two future words. After training the model, word embeddings reside in the shared projection matrix.</p>
<p class="h3"><a id="ch4_4_2_3"/><b>4.4.2.3&#160;&#160;&#160;&#160;Continuous Skip-Gram Model</b></p>
<p class="noindent">The continuous skip-gram model [Mikolov et al. 2013a] is a reversed version of the CBOW model. Instead of predicting the current word based on the context, the context is predicted based on the current word. Thus, the current word is used as an input to a network with a continuous projection layer that predicts words within a certain window before and after the current word. It is found that increasing the range improves the quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, less weights usually are given to the distant words by sampling less from those words in the training examples. The model is depicted in <a href="#fig4_10">Figure 4.10</a>. The final word embeddings reside in the projection matrix.</p>
<div class="cap" id="fig4_9">
<p class="image"><a id="page_117"/><img src="../images/fig4_9.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 4.9</b>&#160;&#160;&#160;&#160;A CBOW model predicting a word given two history and two future words.</p>
</div>
<p class="h3"><a id="ch4_4_2_4"/><b>4.4.2.4&#160;&#160;&#160;&#160;Recurrent Neural Networks</b></p>
<p class="noindent">The recurrent neural network language model (RNN-LM) was proposed in Mikolov et al. [2010] to overcome certain limitations of the FFNN-LM. For example, RNN-LM does not need to specify a limited context length. On the contrary, RNNs can effectively account for a full context length that starts at the beginning of the sentence or even across sentences. The standard RNN-LM does not have a projection layer&#8212;only input, hidden and output layers. The history look-up is performed by the recurrent connections [Mikolov et al. 2013a].</p>
<p class="indent">In a RNN-LM, the time steps correspond to the word positions in a training sentence. At every time step, the network takes as input the word at the current position encoded as a 1-hot binary vector. The input vector is then passed to one or more recurrent hidden layers. The output of the final hidden layer is passed to an output layer with a softmax activation function to produce a correctly normalized probability distribution. The target output at each word position is the next word in the sentence. At the end, the network can predict the long-span conditional probability <i>p</i>(<i>w<sub>m</sub></i>|<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, &#8230;, <i>w</i><sub><i>m</i>&#8722;1</sub>) for any word <i>w<sub>m</sub></i> &#8712; <i>V</i> and a given history <i>w</i><sub>1</sub><i>w</i><sub>2</sub> &#8230; <i>w</i><sub><i>m</i>&#8722;1</sub>, where <i>V</i> is the vocabulary. <a href="#fig4_11">Figure 4.11</a> shows an unfolded example of an RNN-LM over a sentence &#60; <i>s</i> &#62; <i>w</i><sub>1</sub> <i>w</i><sub>2</sub> <i>w</i><sub>3</sub> &#60; /<i>s</i> &#62;, where &#60; <i>s</i> &#62; and &#60; /<i>s</i> &#62; are the sentence start and end symbols.</p>
<div class="cap" id="fig4_10">
<p class="image"><img src="../images/fig4_10.png" alt="Image"/></p>
<p class="figcaption"><a id="page_118"/><b>Figure 4.10</b>&#160;&#160;A continuous skip-gram model predicting a context of two history and two future words given the current word.</p>
</div>
<div class="cap" id="fig4_11">
<p class="image"><img src="../images/fig4_11.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 4.11</b>&#160;&#160;Architecture of an RNN-LM predicting a word given its full previous history.</p>
</div>
<p class="indent">The traditional RNN neurons can also be replaced by more advanced units, like the memory cells proposed in Hochreiter and Schmidhuber [1997a]. In this case, the network is called Long Short-Term Memory (LSTM) RNN. Deeper models can also be constructed by stacking as many recurrent hidden layers as required. The <a id="page_119"/>word embeddings are preserved at the weight matrix between the input and hidden layer. To get a sentence-level embedding, the hidden activation vector of the top most hidden layer at the last word position can be used since it depends on the whole sentence and represents the most abstract features about the input word sequence.</p>
<p class="h2"><a id="ch4_4_3"/><b><span class="bg2">4.4.3</span>&#160;&#160;&#160;&#160;Multimodal Joint Representation</b></p>
<p class="noindent">In the previous sections, we described the transformations that map every image and sentence into embedding vectors in a common vector space. This is true because now both modalities can be represented in the same vector space with the same embedding dimension. However, the main issue now is how to correlate them together. Since the supervision is at the level of entire images and sentences, we need to formulate an image-sentence score as a function of the representing vectors [Karpathy and Fei-Fei 2015]. A sentence-image pair should have a high matching score if the words of the sentence have a more confident support in the image. The model of Karpathy et al. [2014], uses the dot product <img src="../images/inline119_1.png" alt="Image"/> between the <i>i</i>-th image region and the <i>t</i>-th word as a measure of similarity and use it to define the score between image <i>k</i> and sentence <i>l</i> as:</p>
<p class="eqn"><a id="eq4_7"/><img src="../images/eq4_7.png" alt="Image"/></p>
<p class="noindent">where <i>g<sub>k</sub></i> is the set of image regions in image <i>k</i> and <i>g<sub>l</sub></i> is the set of sentence fragments in sentence <i>l</i>. The indices <i>k, l</i> range over the images and sentences in the training set. This score carries the interpretation that a sentence fragment aligns to a subset of image regions whenever the dot product is positive. The following reformulation in Karpathy and Fei-Fei [2015] simplifies the model:</p>
<p class="eqn"><a id="eq4_8"/><img src="../images/eq4_8.png" alt="Image"/></p>
<p class="noindent">where every word <i>s<sub>t</sub></i> aligns to the single best image region. Assuming that <i>k</i> = <i>l</i> denotes the correspondence between image and sentence, the final max-margin structured loss function can be formulated as:</p>
<p class="eqn"><a id="eq4_9"/><img src="../images/eq4_9.png" alt="Image"/></p>
<p class="noindent">This objective encourages aligned image-sentence pairs to have a higher score than misaligned pairs, by a margin [Karpathy and Fei-Fei 2015]. The model is shown in <a href="#fig4_12">Figure 4.12</a>. In this model, word embeddings are generated via a bidirectional <a id="page_120"/>RNN which scans the sentence in both directions rather than only from left to right. Since the cost function is differentiable, the model can be learned end-to-end using a gradient-descent optimizing all parameters of the model simultaneously to find the relevant shared representations.</p>
<div class="cap" id="fig4_12">
<p class="image"><img src="../images/fig4_12.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 4.12</b>&#160;&#160;Learning a scoring function between image regions and text descriptions (from Karpathy and Fei-Fei [2015]).</p>
</div>
<p class="indent">Another multimodal embedding model is proposed in Reed et al. [2016] for solving the visual recognition problem, where images are classified using both visual features and natural language descriptions. The proposed model learns a compatibility function using inner product of features generated by deep neural encoders. An illustration of the model using a CNN for processing images and a word-level RNN for processing text is given in <a href="#fig4_13">Figure 4.13</a>. The objective is to maximize the compatibility between a description and its matching image, and minimize the compatibility with images from other classes. Thus, given data <span class="f1">S</span> = {(<i>v<sub>n</sub>, t<sub>n</sub>, y<sub>n</sub></i>), <i>n</i> = 1, &#8230;, <i>N</i>} containing visual information <i>v</i> &#8712; <span class="f1">V</span>, text description <a id="page_121"/><i>t</i> &#8712; <span class="f1">T</span> and class labels <i>y</i> &#8712; <span class="f1">Y</span>, the model seeks to learn functions <i>f<sub>v</sub></i> : <span class="f1">V</span> &#8594; <span class="f1">Y</span> and <i>f<sub>t</sub></i> : <span class="f1">T</span> &#8594; <span class="f1">Y</span> that minimize the empirical risk:</p>
<p class="eqn"><a id="eq4_10"/><img src="../images/eq4_10.png" alt="Image"/></p>
<p class="noindent">where &#916; : <span class="f1">Y</span> &#215; <span class="f1">Y</span> &#8594; <span class="f2">&#8477;</span> is the 0&#8211;1 loss, <i>N</i> is the number of image and text pairs in the training set. This objective is called <i>deep structured joint embedding</i> which is symmetric with respect to images and text. As described by Reed et al. [2016], it is possible to use just one of the two terms of this objective. For example, the first term is used to train only image classifier, i.e., only image encoder <i>f<sub>v</sub></i> is trained. In this case it is called <i>deep asymmetric structured joint embedding</i>. It is also possible to build an asymmetric model in opposite direction, i.e. only train <i>f<sub>t</sub></i> in order to perform image retrieval Reed et al. [2016].</p>
<p class="indent">A compatibility function <i>F</i> : <span class="f1">V</span> &#8594; <span class="f1">T</span> &#8594; <span class="f2">&#8477;</span> is defined that uses features from encoder functions <i>&#981;</i>(<i>v</i>) for images and <i>&#966;</i>(<i>t</i>) for text:</p>
<p class="eqn"><a id="eq4_11"/><img src="../images/eq4_11.png" alt="Image"/></p>
<p class="noindent">The image and text classifiers are formulated as follows:</p>
<p class="eqn"><a id="eq4_12"/><img src="../images/eq4_12.png" alt="Image"/></p>
<p class="eqn"><a id="eq4_13"/><img src="../images/eq4_13.png" alt="Image"/></p>
<p class="noindent">(4.11)where <span class="f1">T</span>(<i>y</i>) is the subset of <span class="f1">T</span> from class <i>y</i>, <span class="f1">V</span>(<i>y</i>) is the subset of <span class="f1">V</span> from class <i>y</i>, and the expectation is over text descriptions sampled uniformly from these subsets.</p>
<p class="indent">Since the compatibility function is shared by <i>f<sub>t</sub></i> and <i>f<sub>v</sub></i>, in the symmetric objective, it must learn to yield accurate predictions for both classifiers. From the perspective of the text encoder, this means that text features must produce a higher compatibility score to a matching image compared to both the score of that image with any mismatched text, and the score of that text with any mismatched image.</p>
<p class="indent">Since the 0-1 loss is discontinuous, a surrogate objective function that is continuous and convex is optimized instead:</p>
<p class="eqn"><a id="eq4_14"/><img src="../images/eq4_14.png" alt="Image"/></p>
<p class="noindent">where the misclassification losses are written as:</p>
<div class="cap" id="fig4_13">
<p class="image"><img src="../images/fig4_13.png" alt="Image"/></p>
<p class="figcaption"><a id="page_122"/><b>Figure 4.13</b>&#160;&#160;Learning a scoring function between full images and text descriptions (from Reed et al. [2016]).</p>
</div>
<p class="eqn"><a id="eq4_15"/><img src="../images/eq4_15.png" alt="Image"/></p>
<p class="eqn"><a id="eq4_16"/><img src="../images/eq4_16.png" alt="Image"/></p>
<p class="noindent">Since now all encoders are differentiable, the network parameters can be trained end-to-end using back-propagation.</p>
<p class="h1"><a id="ch4_5"/><b><span class="bg1">4.5</span>&#160;&#160;&#160;&#160;Perspectives</b></p>
<p class="noindent">As described earlier in this chapter, deep learning has been successfully applied to learning representations for solving supervised tasks in complex domains such as speech, image, or language processing. Yet, multimodal interactive systems are dealing with even more complex tasks, requiring to integrate interactive contexts over several time steps and behaving consistently over multiple turns of interaction. Errors might accumulate and major deviations in the course of an interaction can be observed if time dependency between successive decisions is not explicitly accounted for in the learning process. For this reason, Reinforcement Learning (RL) [Sutton and Barto 1998] was introduced in interactive systems two decades ago [Levin et al. 1998]. Until very recently, the combination of Reinforcement Learning and Deep Learning was considered as a hard problem because of incompatibility of theoretical assumptions (essentially the i.i.d. hypothesis is of course not met when dealing with sequential decision making). Nevertheless, Deep Reinforcement <a id="page_123"/>Learning have succeeded in solving major AI challenges such as reaching superhuman performance at playing Atari games from raw pixels [Mnih et al. 2015]or defeating the Go game world champion from basic low-level descriptions of the board [Silver et al. 2016]. It is therefore clear that Deep RL will have to play a key role in the perspective of training end-to-end multimodal interactive systems. Not only to embed sequential decision-making algorithms into the learning process but to drive the representation learning process so as to extract meaningful features from low-level signals in terms of their ability to ensure goal achievement. A major obstacle remains the difficulty of generating enough in-domain data. Unlike for games where simulation and self-play can artificially produce as much data as required for learning, human-machine interaction involves a costly data collection process which is still a bottleneck for Deep RL to apply.</p>
<p class="h1n"><a id="ch4_6"/><b>Focus Questions</b></p>
<p class="noindent"><b>4.1.</b> What is the difference between early, intermediate, and late fusion models?</p>
<p class="noindentt"><b>4.2.</b> Name three different modality combinations and the different possibilities for fusion of these combinations.</p>
<p class="noindentt"><b>4.3.</b> Name one advantage of intermediate fusion models over early fusion models and one over late fusion models.</p>
<p class="noindentt"><b>4.4.</b> How do sequence-to-sequence models encode data from one modality?</p>
<p class="noindentt"><b>4.5.</b> How do sequence-to-sequence models decode the encoder representation into data of another modality?</p>
<p class="noindentt"><b>4.6.</b> What are the advantages of incorporating the attention mechanism into encoder-decoder models?</p>
<p class="h1n"><a id="ch4_7"/><b>References</b></p>
<p class="ref">M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. <i>arXiv preprint arXiv:1603.04467</i>. 99</p>
<p class="ref">R. Al-Rfou, G. Alain, A. Almahairi, C. Angermueller, D. Bahdanau, N. Ballas, F. Bastien, J. Bayer, A. Belikov, A. Belopolsky, Y. Bengio, A. Bergeron, J. Bergstra, V. Bisson, J. Bleecher Snyder, N. Bouchard, N. Boulanger-Lewandowski, X. Bouthillier, A. de Br&#233;bisson, O. Breuleux, P.-L. Carrier, K. Cho, J. Chorowski, P. Christiano, T. Cooijmans, M.-A. C&#244;t&#233;, M. C&#244;t&#233;, A. Courville, Y. N. Dauphin, O. Delalleau, J. Demouth, G. Desjardins, S. Dieleman, L. Dinh, M. Ducoffe, V. Dumoulin, S. Ebrahimi Kahou, D. Erhan, Z. Fan, O. Firat, M. Germain, X. Glorot, I. Goodfellow, <a id="page_124"/>M. Graham, C. Gulcehre, P. Hamel, I. Harlouchet, J.-P. Heng, B. Hidasi, S. Honari, A. Jain, S. Jean, K. Jia, M. Korobov, V. Kulkarni, A. Lamb, P. Lamblin, E. Larsen, C. Laurent, S. Lee, S. Lefrancois, S. Lemieux, N. L&#233;onard, Z. Lin, J. A. Livezey, C. Lorenz, J. Lowin, Q. Ma, P.-A. Manzagol, O. Mastropietro, R. T. McGibbon, R. Memisevic, B. van Merri&#235;nboer, V. Michalski, M. Mirza, A. Orlandi, C. Pal, R. Pascanu, M. Pezeshki, C. Raffel, D. Renshaw, M. Rocklin, A. Romero, M. Roth, P. Sadowski, J. Salvatier, F. Savard, J. Schl&#252;ter, J. Schulman, G. Schwartz, I. V. Serban, D. Serdyuk, S. Shabanian, E. Simon, S. Spieckermann, S. R. Subramanyam, J. Sygnowski, J. Tanguay, G. van Tulder, J. Turian, S. Urban, P. Vincent, F. Visin, H. de Vries, D. Warde-Farley, D. J. Webb, M. Willson, K. Xu, L. Xue, L. Yao, S. Zhang, and Y. Zhang. 2016. Theano: A Python framework for fast computation of mathematical expressions. <i>arXiv preprint arXiv:1605.02688</i>. 99</p>
<p class="ref">D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos, E. Elsen, J. Engel, L. Fan, C. Fougner, A. Y. Hannun, B. Jun, T. Han, P. LeGresley, X. Li, L. Lin, S. Narang, A. Y. Ng, S. Ozair, R. Prenger, S. Qian, J. Raiman, S. Satheesh, D. Seetapun, S. Sengupta, C. Wang, Y. Wang, Z. Wang, B. Xiao, Y. Xie, D. Yogatama, J. Zhan, and Z. Zhu. 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. In <i>Proc. Int. Conf. on Machine Learning</i>, pp. 173&#8211;182. New York, NY. 99</p>
<p class="ref">E. Arisoy, T. Sainath, B. Kingsbury, and B. Ramabhadran. 2012. Deep neural network language models. In <i>Proc. NAACL-HLT Workshop</i>, pp. 20&#8211;28. Montreal, Canada. 115</p>
<p class="ref">P. K. Atrey, M. A. Hossain, A. El Saddik, and M. S. Kankanhalli. 2010. Multimodal fusion for multimedia analysis: a survey. <i>Multimedia Systems</i>, 16(6): 345&#8211;379. DOI: 10.1007/s00530-010-0182-0. 100, 104</p>
<p class="ref">D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural machine translation by jointly learning to align and translate. In <i>Proc. International Conference on Learning Representations</i>. Banff, Canada. 99, 110, 111, 112</p>
<p class="ref">Y. Bengio and R. Ducharme. 2001. A neural probabilistic language model. In <i>Proc. Advances in Neural Information Processing Systems</i>, vol. 13, pp. 932&#8211;938. Denver, CO. 114</p>
<p class="ref">L. Bottou. 2010. Large-scale machine learning with stochastic gradient descent. In <i>Proc. COMPSTAT&#8217;2010</i>, pp. 177&#8211;186. Springer, Paris, France. DOI: 10.1007/978-3-7908-2604-3_16. 105</p>
<p class="ref">Y. Cheng, X. Zhao, R. Cai, Z. Li, K. Huang, and Y. Rui. 2016. Semi-supervised multimodal deep learning for RGB-D object recognition. In <i>Proc. Int. Joint Conf. on AI</i>, pp. 3345&#8211;3351. New York, NY. 105</p>
<p class="ref">K. Cho, B. van Merrienboer, &#199;. G&#252;l&#231;ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In <i>Proc. Empirical Methods in Natural Language Processing, EMNLP 2014</i>, pp. 1724&#8211;1734. Doha, Qatar. 105</p>
<p class="ref"><a id="page_125"/>K. Cho, A. Courville, and Y. Bengio. 2015. Describing multimedia content using attention-based encoder-decoder networks. <i>IEEE Transactions on Multimedia</i>, 17(11): 1875&#8211;1886. DOI: 10.1109/TMM.2015.2477044. 110</p>
<p class="ref">J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio. 2015. Attention-based models for speech recognition. In <i>Proc. Advances in Neural Information Processing Systems</i>, pp. 577&#8211;585. Montreal, Canada. 111</p>
<p class="ref">R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In <i>Proc. Int. Conf. on Machine Learning</i>, pp. 160&#8211;167. Helsinki, Finland. DOI: 10.1145/1390156.1390177. 116</p>
<p class="ref">L. Deng and D. Yu. 2014. Deep learning: Methods and applications. <i>Foundations and Trends in Signal Processing</i>, 7(3&#8211;4): 197&#8211;387. 99</p>
<p class="ref">A. Eitel, J. T. Springenberg, L. Spinello, M. Riedmiller, and W. Burgard. 2015. Multimodal deep learning for robust rgb-d object recognition. In <i>Proc. Intelligent Robots and Systems (IROS)</i>, pp. 681&#8211;687. IEEE, Hamburg, Germany. 105</p>
<p class="ref">G. Erdogan, I. Yildirim, and R. A. Jacobs. 2014. Transfer of object shape knowledge across visual and haptic modalities. In <i>Proc. 36th Annual Conference of the Cognitive Science Society</i>. Quebec City, Canada. 112</p>
<p class="ref">F. Eyben, M. W&#246;llmer, A. Graves, B. Schuller, E. Douglas-Cowie, and R. Cowie. 2010. Online emotion recognition in a 3-d activation-valence-time continuum using acoustic and linguistic cues. <i>Journal on Multimodal User Interfaces</i>, 3(1&#8211;2): 7&#8211;19. DOI: 10.1007/s12193-009-0032-6. 103</p>
<p class="ref">R. Girshick, J. Donahue, T. Darrell, and J. Malik. 2014. Rich feature hierarchies for accurate object detection and semantic segmentation. In <i>Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition</i>, pp. 580&#8211;587. Columbus, OH. DOI: 10.1109/CVPR.2014.81. 113</p>
<p class="ref">I. Goodfellow, Y. Bengio, and A. Courville. 2016. <i>Deep Learning</i>. MIT Press. <a href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a>. 100</p>
<p class="ref">G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. 2012. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. <i>Signal Processing Magazine, IEEE</i>, 29(6): 82&#8211;97. DOI: 10.1109/MSP.2012.2205597. 99</p>
<p class="ref">S. Hochreiter and J. Schmidhuber. 1997a. Long short-term memory. <i>Neural Computation</i>, 9(8): 1735&#8211;1780. DOI: 10.1162/neco.1997.9.8.1735. 118</p>
<p class="ref">S. Hochreiter and J. Schmidhuber. 1997b. Long short-term memory. <i>Neural Computation</i>, 9(8): 1735&#8211;1780. 105</p>
<p class="ref">E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In <i>Proc. 50th Annual Meeting Assoc. for Computational Linguistics</i>, pp. 873&#8211;882. Jeju Island, Korea. 116</p>
<p class="ref">S. E. Kahou, X. Bouthillier, P. Lamblin, C. Gulcehre, V. Michalski, K. Konda, S. Jean, P. Froumenty, Y. Dauphin, N. Boulanger-Lewandowski, et al. 2016. Emonets: <a id="page_126"/>Multimodal deep learning approaches for emotion recognition in video. <i>Journal on Multimodal User Interfaces</i>, 10(2): 99&#8211;111. DOI: 10.1007/s12193-015-0195-2. 103</p>
<p class="ref">A. Karpathy and L. Fei-Fei. 2015. Deep visual-semantic alignments for generating image descriptions. In <i>Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</i>, pp. 3128&#8211;3137. Boston, MA. 113, 119, 120</p>
<p class="ref">A. Karpathy, A. Joulin, and F. Li. 2014. Deep fragment embeddings for bidirectional image sentence mapping. In <i>Proc. Advances in Neural Information Processing Systems</i>, pp. 1889&#8211;1897. Montreal, Canada. 119</p>
<p class="ref">G. Keren, J. Deng, J. Pohjalainen, and B. Schuller. 2016. Convolutional neural networks with data augmentation for classifying speakers native language. In <i>Proc. INTERSPEECH, Annual Conference of the International Speech Communication Association</i>. San Francisco, CA. DOI: 10.21437/Interspeech.2016-261. 103</p>
<p class="ref">G. Keren, S. Sabato, and B. W. Schuller. 2017a. Tunable sensitivity to large errors in neural network training. In <i>Proc. Conference on Artificial Intelligence (AAAI)</i>, pp. 2087&#8211;2093. San Francisco, CA. 105</p>
<p class="ref">G. Keren, S. Sabato, and B. W. Schuller. 2017b. Fast single-class classification and the principle of logit separation. <i>arXiv preprint arXiv:1705.10246</i>. 105</p>
<p class="ref">G. Keren and B. W. Schuller. 2016. Convolutional RNN: an enhanced model for extracting features from sequential data. In <i>Proc. International Joint Conference on Neural Networks, IJCNN</i>, pp. 3412&#8211;3419. Vancouver, Canada. DOI: 10.1109/IJCNN.2016.7727636. 105</p>
<p class="ref">D. Kingma and J. Ba. 2015. Adam: A method for stochastic optimization. In <i>Proc. International Conference on Learning Representations</i>. Banff, Canada. 105</p>
<p class="ref">A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012. Imagenet classification with deep convolutional neural networks. In <i>Proc. Advances in Neural Information Processing Systems</i>, pp. 1097&#8211;1105. Lake Tahoe, NV. 99</p>
<p class="ref">Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. 1989. Backpropagation applied to handwritten zip code recognition. <i>Neural Computation</i>, 1(4): 541&#8211;551. DOI: 10.1162/neco.1989.1.4.541. 108</p>
<p class="ref">E. Levin, R. Pieraccini, and W. Eckert. 1998. Using markov decision process for learning dialogue strategies. In <i>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</i>, vol. 1, pp. 201&#8211;204. IEEE, Seattle, WA. DOI: 10.1109/ICASSP.1998.674402. 122</p>
<p class="ref">E. Mansimov, E. Parisotto, J. L. Ba, and R. Salakhutdinov. 2015. Generating images from captions with attention. In <i>Proc. International Conference on Learning Representations</i>. Banff, Canada. 108, 109, 111</p>
<p class="ref">H. Mei, M. Bansal, and M. R. Walter. 2016. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In <i>Proc. Conference on Artificial Intelligence (AAAI)</i>, pp. 2772&#8211;2778. Phoenix, AZ. 107, 111</p>
<p class="ref"><a id="page_127"/>T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a. Efficient estimation of word representations in vector space. <i>arXiv preprint arXiv:1301.3781</i>. 114, 116, 117</p>
<p class="ref">T. Mikolov, W. Yih, and G. Zweig. 2013b. Linguistic regularities in continuous space word representations. In <i>Proc. NAACL-HLT</i>, pp. 746&#8211;751. Atlanta, Georgia. 114</p>
<p class="ref">T. Mikolov, M. Karafi&#225;t, L. Burget, J. H. &#268;ernock&#253;, and S. Khudanpur. 2010. Recurrent neural network based language model. In <i>Proceedings of the INTERSPEECH, Annual Conference of the International Speech Communication Association</i>, pp. 1045&#8211;1048. Makuhari, Chiba, Japan. 117</p>
<p class="ref">T. Mikolov, J. Kopeck&#253;, L. Burget, O. Glembek, and J. Cernock&#253;. 2009. Neural network based language models for highly inflective languages. In <i>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</i>, pp. 4725&#8211;4728. Taipei, Taiwan. 114</p>
<p class="ref">V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. <i>Nature</i>, 518(7540): 529&#8211;533. DOI: 10.1038/nature14236. 123</p>
<p class="ref">F. Morin and Y. Bengio. 2005. Hierarchical probabilistic neural network language model. In <i>Proc. International Workshop on Artificial Intelligence and Statistics</i>, pp. 246&#8211;252. Barbados. DOI: 10.1.1.88.9794. 116</p>
<p class="ref">A. E. Mousa. 2014. <i>Sub-Word Based Language Modeling of Morphologically Rich Languages for LVCSR</i>. Ph.D. thesis, Computer Science Department, RWTH Aachen University, Aachen, Germany. 115</p>
<p class="ref">A. E. Mousa, H.-K. J. Kuo, L. Mangu, and H. Soltau. 2013. Morpheme-based featurerich language models using deep neural networks for LVCSR of Egyptian Arabic. In <i>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</i>. Vancouver, Canada. DOI: 10.1109/ICASSP.2013.6639311. 115</p>
<p class="ref">M. Paleari and B. Huet. 2008. Toward emotion indexing of multimedia excerpts. In <i>Proc. 2008 International Workshop on Content-Based Multimedia Indexing</i>, pp. 425&#8211;432. IEEE, London, UK. DOI: 10.1109/CBMI.2008.4564978. 102</p>
<p class="ref">E. Park, X. Han, T. L. Berg, and A. C. Berg. 2016. Combining multiple sources of knowledge in deep CNNs for action recognition. In <i>Proc. 2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</i>, pp. 1&#8211;8. IEEE, Lake Placid, NY. DOI: 10.1109/WACV.2016.7477589. 105</p>
<p class="ref">B. T. Polyak. 1964. Some methods of speeding up the convergence of iteration methods. <i>USSR Computational Mathematics and Mathematical Physics</i>, 4(5): 1&#8211;17. 105</p>
<p class="ref">S. Reed, Z. Akata, H. Lee, and B. Schiele. 2016. Learning deep representations of fine-grained visual descriptions. In <i>Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</i>. Las Vegas, NV. DOI: 10.1109/CVPR.2016.13. 120, 121, 122</p>
<p class="ref">M. Schuster and K. K. Paliwal. 1997. Bidirectional recurrent neural networks. <i>IEEE Transactions on Signal Processing</i>, 45(11): 2673&#8211;2681. DOI: 10.1109/78.650093. 111</p>
<p class="ref"><a id="page_128"/>D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. 2016. Mastering the game of Go with deep neural networks and tree search. <i>Nature</i>, 529(7587): 484&#8211;489. 123</p>
<p class="ref">J. Sung, I. Lenz, and A. Saxena. 2015. Deep multimodal embedding: Manipulating novel objects with point-clouds, language and trajectories. <i>arXiv preprint arXiv:1509.07831</i>. 113</p>
<p class="ref">I. Sutskever, O. Vinyals, and Q. V. Le. 2014. Sequence to sequence learning with neural networks. In <i>Proc. Advances in Neural Information Processing Systems</i>, pp. 3104&#8211;3112. Montreal, Canada. 105</p>
<p class="ref">R. S. Sutton and A. G. Barto. 1998. <i>Introduction to Reinforcement Learning</i>. MIT Press. 122</p>
<p class="ref">C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. 2015. Going deeper with convolutions. In <i>Proc. IEEE International Conference on Computer Vision and Pattern Recognition</i>, pp. 1&#8211;9. Boston, MA. DOI: 10.1109/CVPR.2015.7298594. 99</p>
<p class="ref">G. Trigeorgis, M. Nicolaou, S. Zafeiriou, and B. W. Schuller. 2016. Deep canonical time warping. In <i>Proc. IEEE International Conference on Computer Vision and Pattern Recognition</i>, pp. 5110&#8211;5118. Las Vegas, NV. DOI: 10.1109/CVPR.2016.552. 100</p>
<p class="ref">S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and K. Saenko. 2015a. Sequence to sequence-video to text. In <i>Proc. IEEE Conference Computer Vision</i>, pp. 4534&#8211;4542. Santiago, Chile. DOI: 10.1109/ICCV.2015.515. 108</p>
<p class="ref">S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. J. Mooney, and K. Saenko. 2015b. Translating videos to natural language using deep recurrent neural networks. In <i>Proc. NAACL-HLT</i>, pp. 1494&#8211;1504. Denver, CO. 108</p>
<p class="ref">O. Vinyals, M. Fortunato, and N. Jaitly. 2015a. Pointer networks. In <i>Proc. Advances in Neural Information Processing Systems</i>, pp. 2692&#8211;2700. Montreal, Canada. 111</p>
<p class="ref">O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. 2015b. Grammar as a foreign language. In <i>Proc. Advances in Neural Information Processing Systems</i>, pp. 2773&#8211;2781. Montreal, Canada. 107, 108, 111</p>
<p class="ref">O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 2015c. Show and tell: A neural image caption generator. In <i>Proc. IEEE International Conference on Computer Vision and Pattern Recognition</i>, pp. 3156&#8211;3164. Boston, MA. 107, 109</p>
<p class="ref">K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In <i>Proc. International Conference on Machine Learning</i>, pp. 2048&#8211;2057. Lille, France. 110, 111, 112</p>
<p class="ref">L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A. Courville. 2015. Describing videos by exploiting temporal structure. In <i>Proc. IEEE Conference Computer Vision</i>, pp. 4507&#8211;4515. Santiago, Chile. DOI: 10.1109/ICCV.2015.512. 111</p>
</body>
</html>