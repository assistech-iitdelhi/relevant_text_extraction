<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_331"/>11</p>
<p class="chtitle"><b>Multimodal Learning Analytics: Assessing Learners&#8217; Mental State During the Process of Learning</b></p>
<p class="chauthor"><b>Sharon Oviatt, Joseph Grafsgaard, Lei Chen, Xavier Ochoa</b></p>
<p class="h1-1"><a id="ch11_1"/><b><span class="bg1">11.1</span>&#160;&#160;&#160;&#160;Introduction</b></p>
<p class="noindent">Today, new technologies are making it possible to assess what students know at fine levels of detail, and to model human learning in ways that support entirely new insights about the process of learning. These developments are part of a general recent trend to understand users&#8217; mental state, which is illustrated in this volume&#8217;s chapters on reliably detecting <i><b>cognitive load</b></i> (<a href="23_Chapter10.xhtml">Chapter 10</a>), depression and related disorders (<a href="25_Chapter12.xhtml">Chapter 12</a>), and deception (<a href="26_Chapter13.xhtml">Chapter 13</a>). Progress in all of these areas has been directly enabled by new computational methods and strategies for collecting and analyzing multimodal-multisensor data.</p>
<p class="indent">In this chapter, we focus on students&#8217; mental state during the process of learning, which is a complex activity that only can be evaluated over time. We describe <i><b>multimodal learning analytics,</b></i> an emerging field that includes far more powerful predictive capabilities than preceding work on <i><b>learning analytics,</b></i> which has been based solely on click-stream and linguistic analysis of typed input.</p>
<p class="indent">The achievement of learning fundamentally involves consolidating new performance skills (e.g., ability to communicate orally), types of domain expertise (e.g., knowledge of math), and metacognitive awareness (e.g., ability to diagnose errors). <a id="page_332"/>In this chapter, we focus mainly on new research involving the first two areas: analytics for detecting <i>skilled performance</i> and <i>domain expertise</i>. We also briefly discuss related mental states that are <i>prerequisites for learning</i>, in particular attention, motivation, and engagement, as well as initial work on the <i>moment of insight</i> during problem solving. However, research on multimodal learning analytics has yet to investigate, or attempt to predict, deeper aspects of learning&#8212;such as the emergence of <i>metacognitive awareness</i>, or the ability to <i><b>transfer learning</b></i> to new contexts. The Glossary provides definitions of terminology used throughout this chapter. In addition, Focus Questions are available to aid readers in understanding important chapter content.</p>
<p class="indent">The purpose of this chapter is not to survey learning technologies more generally, including first-generation learning analytics based on keyboard-and-mouse computer interfaces. This chapter also does not cover the development of applications based on new multimodal learning analytic techniques, which would be premature, although new applications are discussed in <a href="#ch11_6">Section 11.6</a>. For a more in-depth discussion of the modalities typically included in multimodal learning analytics work, readers are referred to other handbook chapters on speech, writing, touch, gestures, gaze, image processing, and activity recognition using sensors [Cohen and Oviatt 2017, Hinckley 2017, Katsamanis et al. 2017, Kopp and Bergmann 2017, Potamianos et al. 2017, Qvarfordt 2017].</p>
<p class="h1-1"><a id="ch11_2"/><b><span class="bg1">11.2</span>&#160;&#160;&#160;&#160;What is Multimodal Learning Analytics?</b></p>
<p class="noindent">Multimodal learning analytics typically collects synchronized multi-stream data on students&#8217; natural communication patterns (e.g., speech, writing, gaze, gesturing, facial expressions, non-verbal behavior), activity patterns (e.g., number of problem solutions proposed), and sometimes also physiological and neural patterns (e.g., EEG, heart rate, fNIRS). These rich data are then analyzed with the aim of modeling, predicting, and supporting learning-oriented behaviors during educational activities. The dominant approaches to analysis include various types of empirical, machine learning, and hybrid techniques. These second-generation multimodal learning analytic techniques are capable of predicting mental states during the process of learning, in some cases automatically and in real time, as will be discussed later in this chapter.</p>
<p class="indent">In addition, these rich multimodal data can be analyzed across multiple levels, which is referred to as <i><b>multi-level multimodal learning analytics.</b></i> For example, a communication modality like speech can be analyzed at the signal level (e.g., amplitude, duration), activity level (e.g., number of speaking turns), representational level (e.g., linguistic content), metacognitive level (e.g., intentional use of face-to-face speech for persuasion), and others. In this sense, collecting speech provides very rich data beyond simply knowing the content of what students said. Paralinguistic information, such as speech amplitude, pitch, and accent, can clarify students&#8217; attitude, motivation, and attentional focus&#8212;important preconditions for learning. Multimodal learning analytics also supports multi-level data collection across behavioral and physiological/neural measures, which can be used to both corroborate and more fully understand the nature of any learning effects. The richness of multi-level data are further expanded when sensor data is collected, which can provide contextual information about a learning setting (e.g., GPS for location) or a learner&#8217;s activities and interactions with others who are part of a group (e.g., physical proximity).</p>
<div class="box">
<p class="bhead"><a id="page_333"/><b>Glossary</b></p>
<p class="hangbx"><b>Cognitive load</b> is a multidimensional construct that refers to the momentary working memory load experienced by a person while performing a cognitive task (Chapter 10). It can be increased by a wide variety of factors, including the difficulty of a task, the materials or tools used (e.g., computer interface), the situational context (e.g., distracting vs. quiet setting), the social context (e.g., working individually, vs. jointly with a group), a person&#8217;s expertise in the domain, a person&#8217;s physiological stress level, and so forth. For a detailed discussion of the dynamic and often non-linear interplay between cognitive load and domain expertise, including how cognitive load can either expand or minimize the performance gap between low- and high-performing students, see Oviatt [2013b].</p>
<p class="hangbx"><b>Domain expertise</b> refers to the level of working knowledge and problem-solving competence within a specific subject matter, such as algebra. It is a relatively stable state that influences how a person perceives and strategizes solving a problem. For the same task, a more expert student will group elements within it into higherlevel patterns, or perceive the problem in a more integrated manner. A person&#8217;s level of domain expertise influences a variety of problem-solving behaviors (e.g., fluency, effort expended, accuracy). A more expert person also will experience lower cognitive load when working on the same problem as a less expert person. Most people experience domain expertise in some subjects. This everyday experience of domain expertise is distinct from elite expert performance that occurs in a small minority of virtuosos or prodigies, which can take a decade or lifetime to achieve.</p>
<p class="hangbx"><b>Learning analytics</b> involves the collection, analysis, and reporting of data about learners, including their activities and contexts of learning, in order to understand and provide better support for effective learning. First-generation learning analytics focused exclusively on computer-mediated learning using keyboard-and-mouse computer interfaces. This limited analyses to click-stream activity patterns and linguistic content. Early learning analytic techniques mainly have been applied to managing educational systems (e.g., attendance tracking, tracking work completion), advising students based on their individual profiles (e.g., courses taken, grades received, time spent in learning activities), and improving educational technologies. Learning analytics data typically are summarized on dashboards for educators, such as teachers or administrators.</p>
<p class="hangbx"><b>Metacognitive awareness</b> involves higher-level self-regulatory behaviors that guide the learning process, such as an individual&#8217;s awareness of what type of problem they are working on and how to approach solving it, the ability to diagnose error states, or understanding what tools are best suited for a particular task.</p>
<p class="hangbx"><b>Moment of insight</b> refers to one of several phases during the process of problem solving. It involves the interval of time immediately before and after a person consciously realizes the solution to a problem they&#8217;ve been working on. This idea represents what the person believes is the solution, although it may or may not be correct.</p>
<p class="hangbx"><a id="page_334"/><b>Multi-level multimodal learning analytics</b> refers to the different levels of analysis enabled by multimodal data collection. For example, speech and handwriting can be analyzed at the signal, activity pattern, representational, or metacognitive levels. During research on learning, it frequently is valuable to analyze data across behavioral and physiological/neural levels for a deeper understanding of any learning effects. In this regard, multi-level multimodal learning analytics can support a more comprehensive systems-level view of the complex process of learning.</p>
<p class="hangbx"><b>Multimodal learning analytics</b> is an emerging area that analyzes students&#8217; natural communication patterns (e.g., speech, writing, gaze, non-verbal behavior), activity patterns (e.g., number of hours spent studying), and physiological and neural patterns (e.g., EEG, fNIRS) in order to predict learning-oriented behaviors during educational activities. These rich data can be analyzed at multiple levels, for example at the signal level (e.g., speech amplitude), activity level (e.g., frequency of speaking), representational level (e.g., linguistic content), and others. These second-generation learning analytic techniques are capable of predicting mental states during the process of learning, in some cases automatically and in real time.</p>
<p class="hangbx"><b>Prerequisites for learning</b> are precursors for successful learning to occur, which can provide early markers. They include attention to the learning materials, emotional and motivational predisposition to learn, and active engagement with the learning activitites.</p>
<p class="hangbx"><b>Skilled performance</b> involves the acquisition of skilled action patterns, for example when learning to become an expert athlete or musician. Skilled performance also can involve the acquisition of communication skills, as in developing expertise in writing compositions or giving oral presentations. The role of deliberate practice has been emphasized in acquiring skilled performance.</p>
<p class="hangbx"><b>Transfer of learning</b> refers to students&#8217; ability to recognize parallells and make use of learned information in new contexts, for example to apply learned information outside of the classroom, in contexts not resembling the original framing of the problem, with different people present, and so forth. This requires generalizing the learned information beyond its original concrete context.</p>
</div>
<p class="indent"><a id="page_335"/>One important impact of multi-level multimodal learning analytics is that it can support a more comprehensive systems-level view of the complex process of learning. We will return to this topic in <a href="#ch11_5_2">Section 11.5.2</a> of the chapter. A second impact is that multi-level multimodal learning analytics enables a fuller understanding of the trade-offs associated with evaluating new learning interventions or computer technologies. That is, rather than assessing an educational intervention or computer tool with a single metric, using multiple modalities potentially can expose a range of advantages and disadvantages that can provide: (1) more valuable collective information about the pros, cons, and overall impact on learning, and (2) better diagnostic information as a basis for improving the intervention or computer tool.</p>
<p class="h2-1"><a id="ch11_2_1"/><b><span class="bg2">11.2.1</span>&#160;&#160;Why is Multimodal Learning Analytics Emerging Now, and What is its History?</b></p>
<p class="noindent">As discussed in detail elsewhere [Oviatt and Cohen 2015], during the past decade multimodal-multisensor interfaces have become the dominant computer interface worldwide, surpassing keyboard-based interfaces. These interfaces have proliferated especially rapidly in support of small mobile devices, such as cell phones. As a result, the dominant educational technology platform worldwide now is mobile cell phones, which are no longer keyboard-centric. Instead, their input capabilities include speech, touch, gesturing, pen input, images, virtual keyboard, and an increasing array of sensors. In short, computer interfaces have become increasingly multimodal-multisensor and, as such, input is dominated by a collection of more expressively powerful communication modalities.</p>
<p class="indent">The impact of this paradigm shift is that learning analytics techniques that rely exclusively on click-stream analysis have become limited in their utility. They eventually will become obsolete as the trend toward small mobile devices and richer human-computer interfaces expands in developing regions and elsewhere. In their place, new techniques now are beginning to collect, analyze, model, and predict learning-oriented behaviors based on multimodal-multisensor input.</p>
<p class="indent"><a id="page_336"/>The <i>First International Workshop on Multimodal Learning Analytics</i> and <i>Second International Workshop and Data-Driven Grand Challenge on Multimodal Learning Analytics</i>, held in 2012 and 2013 respectively, represented initial multidisciplinary working meetings in this new field. These events were associated with the <i>ACM International Conference on Multimodal Interaction (ICMI)</i>, which provided valuable expertise in multimodal technology, infrastructure, and databases that was required to organize the new field of multimodal learning analytics. As efforts were made to broaden disciplinary participation to include more cognitive and learning scientists, subsequent multimodal learning analytic tutorials then were held in conjunction with the annual <i>Learning Analytics and Knowledge Conference (LAK)</i> in 2015, 2016, and 2017.</p>
<p class="h2-1"><a id="ch11_2_2"/><b><span class="bg2">11.2.2</span>&#160;&#160;What are the Main Objectives of Multimodal Learning Analytics?</b></p>
<p class="noindent">A primary objective of multimodal learning analytics is to analyze coherent signal, activity pattern, representational, and metacognitive data in order to uncover entirely new learning-oriented phenomena and to advance learning theory. Multimodal learning analytics aims to identify domain expertise and changes in domain expertise rapidly, reliably, and objectively. It also aims to identify change during the consolidation of skilled performance, transitions in metacognitive awareness, and prerequisites or precursors of learning that involve attention, emotion and motivation, and engagement in learning activities.</p>
<p class="indent">Advances in multimodal learning analytics are expected to contribute new empirical findings, theories, methods, and metrics for understanding how students learn. They also can contribute to improving pedagogical support for students&#8217; learning by assessing the impact of new digital tools, teaching materials, strategies, and curricula on students&#8217; learning outcomes. In addition, multimodal learning analytics is expected to advance new computational and engineering techniques related to machine learning and hybrid forms of analytics that co-process two or more data sources (e.g., speech and images).</p>
<p class="indent">Virtually all assessments of student learning rely critically on the availability of accurate metrics as forcing functions. Multimodal learning analytics is expected to yield a large array of more sensitive, reliable, objective, and richly contextualized metrics of learning-oriented behaviors and expertise. This includes ones that could be analyzed unobtrusively, continuously, automatically, in natural field settings, and more affordably than traditional educational assessments. Progress in multimodal learning analytics will transform our ability to accurately identify and stimulate effective learning, support more rapid feedback and responsive interventions, and facilitate learning in a more diverse range of students and contexts. One <a id="page_337"/>long-term objective of this research is to develop automated prediction of learning (e.g., expertise and skill level) that is more accurate and less affected by biases in social perception than human decision makers. For example, this includes accurately distinguishing domain experts from individuals who are simply socially dominant (e.g., speaking more frequently and loudly).</p>
<p class="h2-1"><a id="ch11_2_3"/><b><span class="bg2">11.2.3</span>&#160;&#160;How Does Multimodal Learning Analytics Differ from Earlier Work on Learning Analytics, and What are its Primary Advantages?</b></p>
<p class="noindent">Initial work on learning analytics aimed to collect and analyze data to better understand and support learners and educational processes. It focused on examining students&#8217; input during computer-mediated learning while they used keyboard-and-mouse interfaces. As a result, analytics were limited to activity patterns and linguistic content present in click-stream data. These analytics typically were summarized on dashboards for professional educators, such as teachers or school administrators. They mainly have been used to assist with managing educational systems (e.g., attendance monitoring, work completion). Later, they started to be used to predict and advise students on educational outcomes, often based on individual student profiles in comparison with a larger group. For example, the system might track the time spent by a student in different learning activities, her work completion, and grades received. This could be used to provide advisory information to the student on the likelihood that she will meet her learning goals, and what actions she could take to improve her expected educational outcome. One common aim of these capabilities, for example, has been to create early warning systems to detect and reduce student dropouts. Learning analytic techniques have been applied to a variety of technology-mediated learning environments, including learning management systems [Arnold and Pistilli 2012], intelligent tutoring systems [Baker et al. 2004], massive open online courses [Kizilcec et al. 2013], educational video games [Serrano-Laguna and Fern&#225;ndez-Manj&#243;n 2014], and so forth. In these contexts, learning analytic results also have been used to study and improve the impact of educational technologies.</p>
<p class="indent">Like multimodal learning analytics, earlier work on learning analytics relied heavily on data resources, which are expensive and time-consuming to collect. Both areas have been part of the broader data science and analytics movement. However, major differences between learning analytics and multimodal learning analytics include the following:</p>
<p class="bullt">&#8226;&#160;&#160;<i>Learning analytics has been limited to click-stream data, based on keyboard-and-mouse interfaces</i>. In contrast, the multi-level multimodal data sources <a id="page_338"/>described above provide a richer collection of data sources that offer more powerful predictive capabilities. Since they are based on natural communication modalities that mediate thought [Luria 1961, Vygotsky 1962], they provide a particularly informative window on human thinking and learning. In this regard, multimodal learning analytic data are well suited to the task of predicting students&#8217; mental state and learning.</p>
<p class="bullt">&#8226;&#160;&#160;<i>Learning analytics does not produce as large a dataset per unit of time</i> as multimodal learning analytics, because the latter typically records multiple streams of synchronized data during the same recorded time period (e.g., 12 data streams in the Math Data Corpus, described in <a href="#ch11_3_1">Section 11.3.1</a>). Analysis of these multimodal data streams can support a more comprehensive multilevel analysis of learning, including how different levels interact to produce emergent learning phenomena. This can generate a qualitatively different systems-level view of the process of learning, as illustrated in <a href="#ch11_5_2">Section 11.5.2</a>.</p>
<p class="bullt">&#8226;&#160;&#160;<i>Learning analytics has been limited to analyzing computer-mediated learning</i>. In contrast, multimodal learning analytic techniques can be used to evaluate both interpersonal and technology-mediated learning. This is an important distinction because learning occurs in both contexts. In addition, evaluation of interpersonal learning outcomes is needed to provide a &#8220;ground-truth&#8221; for assessing the impact of different types of technology-mediated learning.</p>
<p class="bullt">&#8226;&#160;&#160;<i>Learning analytics has been more limited to assessment in stationary settings</i>, compared with multimodal learning analytics, because it typically has been used on desktop and larger computers that support keyboard input. In contrast, multimodal learning analytic techniques originated to accommodate mobile devices and more natural interfaces, so their usage context extends to a wider range of field and mobile learning settings.</p>
<p class="bullt">&#8226;&#160;&#160;<i>Learning analytics in its original form mainly focused on supporting educational management systems, student profiling and advising to promote academic success, researching and improving educational technologies, and similar aims</i>. In contrast, multimodal learning analytic techniques aim to provide finer-grained models of student learners and the learning process, including tracking individual students&#8217; mental states while they learn. In this regard, it is learner-centric, with a particular focus on students&#8217; thoughts and mental states as learning progresses.</p>
<p class="bullt">&#8226;&#160;&#160;<i>Learning analytics has only been able to collect and analyze data when students are interacting with a computer (e.g., active typing)</i> and, if some students don&#8217;t <a id="page_339"/>type or interact much, then their learning-oriented behaviors potentially cannot be analyzed over time. In contrast, multimodal learning analytics can track multiple data sources, some active (e.g., speaking) and some passive (e.g., camera images, sensors), to jointly provide more continuous and lengthier periods of monitoring learners, compared with a single information source like typing.</p>
<p class="h1-1"><a id="ch11_3"/><b><span class="bg1">11.3</span>&#160;&#160;&#160;&#160;What Data Resources are Available on Multimodal Learning Analytics?</b></p>
<p class="noindent">In this section, we describe two examples of different data resources that have been available to the research community in this area: one that provides data on domain expertise and learning in mathematics high-school students (Math Data Corpus), and a second that provides data on skilled performance in undergraduate students at giving oral presentations (Oral Presentation Corpus). Both of these datasets have been used by participants in the series of data-driven grand challenge workshops on multimodal learning analytics that were described in <a href="#ch11_2_1">Section 11.2.1</a>.</p>
<p class="h2-1"><a id="ch11_3_1"/><b><span class="bg2">11.3.1</span>&#160;&#160;Math Data Corpus</b></p>
<p class="noindent">The Math Data Corpus was the first dataset that became available to multimodal learning analytics researchers participating in the <i>First International Data-Driven Grand Challenge Workshop on Multimodal Learning Analytics</i> in 2013 [Oviatt and Cohen 2013]. The primary goal of the challenge using this dataset was to analyze coherent signal, activity pattern, and representational content in an effort to predict each student&#8217;s level of math domain expertise and changes in students&#8217; expertise as rapidly, reliably, and objectively as possible. A secondary goal was to identify learning-oriented behaviors during these problem-solving interactions. Participants in this data-driven challenge could use empirical/statistical, machine learning, or hybrid methods as they pursued these objectives.</p>
<p class="indent">The Math Data Corpus includes high-fidelity time-synchronized multimodal data recordings on collaborating groups of high-school students as they worked together to solve mathematics problems varying in difficulty. Data were collected on students&#8217; multimodal communication and activity patterns, including (1) speech, (2) digital pen input on large sheets of writing paper, and (3) camera capture of their facial expressions, gestures, and physical movements (see <a href="#fig11_2">Figure 11.2</a>, left). The dataset includes six small student groups (i.e., three students apiece), who each met twice to solve math problems and peer tutor one another. In total, approximately 29 student-hours of recorded multimodal data are available during <a id="page_340"/>these collaborative problem-solving sessions. <a href="#fig11_1">Figure 11.1</a> illustrates image capture during data collection.</p>
<div class="cap" id="fig11_1">
<p class="image"><img src="../images/fig11_1.jpg" alt="Image"/></p>
<p class="figcaption"><b>Figure 11.1</b>&#160;&#160;Synchronized views from all five video-cameras at the same moment in time during data collection. Videos A, B and C show close-up views of the three individual students, video D a wide-angle view of all students, and video E a top-down view of students&#8217; writing and artifacts on the tabletop. (Used with permission from Oviatt [2013a])</p>
</div>
<p class="indent">Participants in this dataset included 18 high school students, 9 female and 9 male, who had recently completed Introductory Geometry. They ranged from low to high performers. During each session, students worked on 16 geometry and algebra problems, 4 apiece representing easy, moderate, hard, and very hard difficulty levels. A computer displayed the math problem sets and facilitated the interaction, which was controlled by a simulation environment detailed in Arthur et al. [2006]. All students could use pen and paper and a calculator as tools to draw diagrams, make calculations, etc. while working on each problem. Students worked on each problem in phases: (1) they asked to see the problem, read it and discussed what it meant; (2) if needed, they asked the computer to display related math terms or <a id="page_341"/>equations; (3) each student worked individually, writing equations and calculations on paper as they attempted to solve the problem; (4) one student proposed the solution, which the students then discussed together until they understood how the problem had been solved and agreed on it; (5) the group&#8217;s solution was submitted, after which the computer verified whether it was correct or not; (6) if correct, the computer randomly called upon one student to explain how they had arrived at the answer; (7) if not correct, the computer could display a worked example of how the solution had been calculated. <a href="#fig11_2">Figure 11.2</a> (right) illustrates a sample of students&#8217; written data while solving the math problems, which averaged 80% nonlinguistic content such as numbers, symbols, and diagrams [Oviatt and Cohen 2013].</p>
<div class="cap" id="fig11_2">
<p class="image"><img src="../images/fig11_2.jpg" alt="Image"/></p>
<p class="figcaption"><b>Figure 11.2</b>&#160;&#160;Synchronized multimodal data collection of students&#8217; images, writing, and speech as they solved math problems (left); Sample of students&#8217; handwritten work while solving math problems (right). (Used with permission from Oviatt [2013a])</p>
</div>
<p class="indent">This data resource includes original data files (high-resolution videos, speech .wav files, digital ink .jpegs and .cvs files), extensive coding of problem segmentation and phases, problem-solving correctness, student expertise ratings, representational content of students&#8217; writing, and verbatim speech transcriptions of lexical content. It also includes data on coding reliabilities, and appendices with math problem sets and other information required for those interested in conducting their own data analyses. Detailed documentation on this corpus is available in Oviatt et al. [2014], and in <a href="#tab11_1">Table 11.1</a>.</p>
<p class="indent">Overall, the Math Data Corpus is a well structured dataset, with authentic learning and documented improvement in students&#8217; performance over sessions [Oviatt 2013a]. The ground-truth coding of communication and performance during these sessions is documented in detail. One important feature of this dataset is high-fidelity data capture of the speech, writing, and images, which involves 12 simultaneous data streams with tight synchronization among them. Another strength is uniquely detailed coding of students&#8217; written representations, including the type of representation, its semantic content, and the presence of disfluencies. This coding, which involved over 10,000 representations, is unusual because relevant automated analysis tools are lacking. Perhaps the most important strength of this dataset is that the math problems were finely calibrated by difficulty level (easy to very hard), and all problems had canonical correct answers, such that students&#8217; expertise level could be rated on a scale from 0&#8211;100. This ensured a sensitive and solid ground-truth foundation for documenting predictors of domain expertise and learning, which is especially important in initial research. The limitations of this corpus are that it does not include long-term longitudinal data on individual students over time in a naturalistic setting like a classroom. It also is limited to the single content domain of mathematics. Future data collection could model the strengths of the Math Data Corpus, while planning a longer-term data collection covering more content domains and an expanded student population. While daunting, this task could be accomplished by a dedicated team of researchers and professional educators.</p>
<p class="tcaption" id="tab11_1"><a id="page_342"/><b>Table 11.1</b>&#160;&#160;Math data corpus summary</p>
<table class="table1">
<tr>
<td class="t1"><p class="tab1">Assessment</p></td>
<td class="t1"><p class="tab1">Math domain expertise: Correct solutions, learning over sessions</p></td>
</tr>
<tr>
<td><p class="tab1">Educational activity</p></td>
<td><p class="tab1">Math problem solving, peer tutoring within small group, authentic learning documented</p></td>
</tr>
<tr>
<td class="t1"><p class="tab1">Modalities</p></td>
<td class="t1"><p class="tab1">Speech, digital pen, images (face, gestures, gaze, upper body)</p></td>
</tr>
<tr>
<td><p class="tab1">Duration</p></td>
<td><p class="tab1">Short-term longitudinal</p></td>
</tr>
<tr>
<td class="t1"><p class="tab1">Data &#38; coding</p></td>
<td class="t1"><p class="tab1">29 hours recorded: Transcribed verbatim speech and 10,000 written representations (symbols, numbers, diagrams, words);</p>
<p class="tab1">Percent correct solutions on problems at 4 levels difficulty;</p>
<p class="tab1">Ground-truth coding of each student&#8217;s expertise level;</p>
<p class="tab1">Problem segmentation and phases</p></td>
</tr>
<tr>
<td><p class="tab1">Participants</p></td>
<td><p class="tab1">18 high school students (gender balanced, high &#38; low performers)</p></td>
</tr>
<tr>
<td class="t1"><p class="tab1">Language</p></td>
<td class="t1"><p class="tab1">English</p></td>
</tr>
</table>
<p class="h2-1"><a id="page_343"/><a id="ch11_3_2"/><b><span class="bg2">11.3.2</span>&#160;&#160;Oral Presentation Corpus</b></p>
<p class="noindent">The Oral Presentation Corpus was the second dataset to become available for multimodal learning analytics researchers participating in grand challenge workshops. This dataset documents students&#8217; competence in an area of skilled performance&#8212;that of giving effective oral presentations. It also differs from the Math Data Corpus in documenting native speakers of Spanish.</p>
<p class="indent">The Oral Presentation Corpus includes 441 oral presentations given by Spanish-speaking undergraduate students. The students worked in small groups of up to six members on projects, which varied in topics such as discussing entrepreneurship ideas or software design. Each group of students collaborated on making one slide presentation, but individual students all gave part of the oral presentation. During these presentations, data were collected for individual students on their natural multimodal communication in a regular classroom setting, including: (1) speech, (2) camera images of facial expressions and physical movements, and (3) skeletal full-body movement data gathered using Kinect (for data recording details, see Chen et al. [2015] and Nguyen et al. [2012]). The emergence of 3D motion tracking devices, such as the Microsoft Kinect used in this work [Zhang 2012], has greatly facilitated multimodal research. In addition, data was available on the group&#8217;s corresponding (4) slide presentation files. In total, approximately 19 hours of multimodal data is available for analysis from these presentations. <a href="#fig11_3">Figure 11.3</a> illustrates a sample of the type of data recorded in the Oral Presentation Corpus, based on very similar infrastructure developed at Educational Testing Service.</p>
<p class="indent">The oral presentation quality for each student was evaluated by two human experts, who used a rubric based on the following six performance criteria: (1) speech coherence and organization, (2) pronunciation and voice control (e.g., volume), (3) language (e.g., audience appropriate), (4) eye contact and body language (e.g., expressiveness), (5) confidence and enthusiasm, and (6) accompanying slide presentation (e.g., readability, grammar, visual design). These criteria were rated on a scale of 0&#8211;4 and totalled. Based on this assessment, each student received a grade for their oral presentation. The accompanying slides were assigned a separate group grade, which all students in a given group received. <a href="#tab11_2">Table 11.2</a> summarizes the main features of the Oral Presentation Corpus, Ochoa [unpubl].</p>
<div class="cap" id="fig11_3">
<p class="image"><img src="../images/fig11_3.jpg" alt="Image"/></p>
<p class="figcaption"><b>Figure 11.3</b>&#160;&#160;Sample recording of a speaker and talk slide (left), Kinect analysis of full-body movement (middle) and data recording instruments (right) from Educational Testing Service&#8217;s multimodal presentation corpus, which had the same basic features as the Oral Presentation Corpus infrastructure first developed by Ochoa and colleagues. (From Leong et al. [2015])</p>
</div>
<p class="indent"><a id="page_344"/>Overall, the Oral Presentation Corpus contains authentic information on students&#8217; skilled performance collected in actual classrooms. It expands the multimodal learning analytics community&#8217;s available data to include Spanish speakers. Another strength of this corpus is its synchronized information on each speaker&#8217;s speech and non-verbal movements&#8212;with the latter including facial expressions, gestures, and full-body articulated movements. These latter data were collected using a combination of camera images and Kinect. In addition, accompanying slide presentations were available for evaluation. On the other hand, since oral presentations do not have a canonical correct form, it was necessary to judge them on a more qualitative basis using grades.</p>
<p class="indent">From the viewpoint of advancing multimodal learning analytics research and related system development, the major limitation of this dataset is that it evaluated students&#8217; <i>competence</i> at this type of skilled performance in a single session, but it does not include any data on student learning over time. Another limitation of this corpus is that grading of oral presentations relied on six particular criteria, and it can be disputed whether these are the best metrics for judging presentation quality with respect to validity and reliability. One aim of conducting multimodal learning analytics research on corpora like this is to automate what are now qualitative judgment ratings in an effort to eventually produce more objective and reliable scoring of presentation quality. Although not publicly available, Educational Testing Service collected a very similar corpus, using infrastructure shown <a id="page_345"/>in <a href="#fig11_3">Figure 11.3</a>, in which the PC version of Kinect was used so multiple data streams could be recognized jointly on a PC. They used the Public Speaking Competence Rubric (PSCR) [Schreiber et al. 2012], which is well recognized and potentially more valid for ground-truth assessment [Chen et al. 2014a]. They also adopted more rigorous human rating procedures, with multiple raters systematically adjudicating presentation scores. These represent important methodological improvements for future researchers interested in collecting similar data.</p>
<p class="tcaption" id="tab11_2"><b>Table 11.2</b>&#160;&#160;Oral presentation corpus summary</p>
<table class="table1">
<tr>
<td class="t1"><p class="tab1">Assessment</p></td>
<td class="t1"><p class="tab1">Skilled oral presentation</p></td>
</tr>
<tr>
<td><p class="tab1">Educational activity</p></td>
<td><p class="tab1">Authentic in-classroom presentations (small groups)</p></td>
</tr>
<tr>
<td class="t1"><p class="tab1">Modalities</p></td>
<td class="t1"><p class="tab1">Speech, images (face, gestures, body), Kinect (skeletal movements), with accompanying slide presentations</p></td>
</tr>
<tr>
<td><p class="tab1">Duration</p></td>
<td><p class="tab1">Single session</p></td>
</tr>
<tr>
<td class="t1"><p class="tab1">Data &#38; coding</p></td>
<td class="t1"><p class="tab1">10 hours recorded, 130 oral presentations; Teacher grades for individual student&#8217;s oral presentation quality and group&#8217;s slide quality; Ratings based on 2 teachers and 6 performance criteria</p></td>
</tr>
<tr>
<td><p class="tab1">Participants</p></td>
<td><p class="tab1">Undergraduate students</p></td>
</tr>
<tr>
<td class="t1"><p class="tab1">Language</p></td>
<td class="t1"><p class="tab1">Spanish</p></td>
</tr>
</table>
<p class="h2-1"><a id="ch11_3_3"/><b><span class="bg2">11.3.3</span>&#160;&#160;Other Infrastructure for Recording Classroom and Computer-based Multimodal Data</b></p>
<p class="noindent">In the above datasets, infrastructure was developed for multimodal recording of individuals and small groups. However, multimodal recording also has focused on more conventional classroom settings and student-teacher interactions. As examples of different approaches, Raca and Dillenbourg [2013] used a relatively traditional approach with multiple cameras in a classroom to collect data on students&#8217; head positions, body movements (e.g., writing, postural shifts), question-answer exchanges, along with coordinated teacher slides and recordings. <a href="#fig11_4">Figure 11.4</a> illustrates students&#8217; gaze estimated from head pose data. Their primary aim was to track students&#8217; engagement and concentration during classes. The infrastruture included a visual analytic summary of students&#8217; spatial location in the classroom, which displayed each student&#8217;s level of attention and whether they wrote notes.</p>
<div class="cap" id="fig11_4">
<p class="image"><img src="../images/fig11_4.jpg" alt="Image"/></p>
<p class="figcaption"><b>Figure 11.4</b>&#160;&#160;Gaze estimation based on camera capture of students&#8217; head pose during classroom lecture. (Used with permission from Raca and Dillenbourg [2014])</p>
</div>
<p class="indent"><a id="page_346"/>Tabletop-enhanced classrooms also have been used to capture and analyze students&#8217; interactions. This infrastructure places more emphasis on collaborative activities [Martinez Maldonado et al. 2012]. They are capable of collecting students&#8217; touch, pen input during writing, and manual interaction with learning objects through table surfaces enabled with multi-touch and multi-stylus capabilities. Some tabletop environments also have been developed to understand speech commands to interact with the system [Jones et al. 2011]. Others have flexible configurations that support adapting displays from horizontal tables to vertical white boards [Leitner et al. 2009], and writing and interacting across different types of surface [Anoto 2016].</p>
<p class="indent">In order to improve on the quality of classroom data collection, other researchers have proposed the design of a personal Multimodal Recording Device (MRD), which currently is being tested. This MRD could be used for close-range recording of an individual&#8217;s audio, visual, and writing behaviors in a classroom using a desktop recording device [Dominguez et al. 2015]. <a href="#fig11_5">Figure 11.5</a> (left) illustrates that the camera inside the device would collect video-images of the students&#8217; face, upper torso, and hands when writing, gesturing, and so forth. A close-range microphone would collect the student&#8217;s speech. As shown in <a href="#fig11_5">Figure 11.5</a> (right) the student&#8217;s digital pen would be integrated with the recording device to collect digital stroke data while the student writes in a paper notebook on their desktop. The MRD would connect to classroom WiFi to upload and share data among students. Apart from producing higher-fidelity data, the MRD would give individual students control over whether and when their own data was collected to ensure acceptable privacy. In addition, video-recordings would capture the teacher&#8217;s classroom presentation and synchronized slides, which would be available for students in the class [Dominguez et al. 2015].</p>
<div class="cap" id="fig11_5">
<p class="image"><a id="page_347"/><img src="../images/fig11_5.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 11.5</b>&#160;&#160;Design of Multimodal Recording Device (MRD) for high-fidelity classroom data collection. (Used with permission from Dominguez et al. [2015])</p>
</div>
<p class="indent">Although most desktop computers are keyboard-and-mouse enabled, for multimodal learning analytics research student-computer interactions can be recorded using supplementary sensors, as shown in <a href="#fig11_6">Figure 11.6</a> (left). In this example, the desk is instrumented with audiovisual sensors: a webcam with microphone, an infrared depth sensor (for body posture and motion), and an infrared eye-tracking device. The student also wears a wireless bracelet on her wrist to track electrodermal activity. These sensors record directly onto the computer workstation, synchronizing multimodal data streams. Together, these devices provide non-invasive measures of eye gaze, nonverbal behavior, and physiology in a lab setting.</p>
<p class="h2-1"><a id="ch11_3_4"/><b><span class="bg2">11.3.4</span>&#160;&#160;Why Publicly Available Multimodal Learning Datasets are Limited</b></p>
<p class="noindent">Publicly available multimodal data resources, such as those described in <a href="#ch11_3_1">Sections 11.3.1</a> and <a href="#ch11_3_2">11.3.2</a>, are rare for several reasons. One is that ethics clearance has not been conducted in a manner that permits sharing the rich multimodal data for privacy reasons. In addition, since corpora are expensive to collect and ground-truth code, in some cases they have not been shared for commercial reasons. For example, Educational Testing Service has collected corpora related to oral presentation planning [Chen et al. 2014a, 2014b, 2015], but these activities were supported <a id="page_348"/>by corporate investments to produce future assessment products. Another reason is that collection and analysis of synchronized multi-stream multimodal data requires knowledge of the component technologies and an ability to customize software, since off-the-shelf tools are limited.</p>
<div class="cap" id="fig11_6">
<p class="image"><img src="../images/fig11_6.jpg" alt="Image"/></p>
<p class="figcaption"><b>Figure 11.6</b>&#160;&#160;Student facial expressions and upper-body movements captured with a camera during interaction with an intelligent tutoring system (left), and showing processing of pose information (right). (From Ezen-Can et al. [2015])</p>
</div>
<p class="indent">Many corpora on interactions involving educational technologies have been collected in the past, but the vast majority are limited to keyboard-and-mouse input or a single input modality like speech. For example, CMU LearnLab&#8217;s well-known data repository, DataShop (<a href="http://pslcdatashop.web.cmu.edu">http://pslcdatashop.web.cmu.edu</a>), has many publicly available datasets&#8212;but they are not multimodal ones. In specific areas of education like second language learning, digital databases are available involving continuous written or spoken input produced by foreign or second language learners of many world languages (for example, see <a href="http://www.uclouvain.be/en-cecl-lcworld.html">http://www.uclouvain.be/en-cecl-lcworld.html</a>). However, these also are not multimodal databases. Furthermore, when written composition skills in a foreign language are available for study, students&#8217; input typically has been typed, written with non-digital tools and then scanned in, or transcribed into digital form from non-digital writing materials. In these cases, digital databases that capture both the content and dynamic formation of students&#8217; handwriting are not available.</p>
<p class="h2-1"><a id="ch11_3_5"/><b><span class="bg2">11.3.5</span>&#160;&#160;Computational Methods and Techniques for Analyzing Multi-level Multimodal Data</b></p>
<p class="noindent">The data analytic techniques typically applied to multimodal learning analytics corpora range from standard empirical and statistical techniques, to machine learning <a id="page_349"/>techniques, and also new hybrid methods. In the area of statistical analysis, earlier education and cognitive science research on topics like domain expertise reported a large array of individual &#8220;markers&#8221; [Ericsson et al. 2006, Purandare and Litman 2008, Worsley and Blikstein 2010]. We refer readers to Ericsson et al. [2006] for a summary of this body of work, which preceded and has contributed to multimodal learning analytics. Markers of expert performance, such as the use of linguistic terms indicating certainty, typically were identified in empirical studies using correlations or standard statistical significance tests.</p>
<p class="indent">More recent multimodal learning analytics research has shifted to adopting statistical techniques that can contribute to <i>prediction</i> of expertise and learning, either alone or in combination with the use of machine learning techniques. When possible predictors are studied, researchers examine the proportion of variance accounted for to distinguish which factors are more powerful rather than weak (e.g., accounting for 60% rather than 1% of data variance). More powerful predictors then are good candidates to consider including in automated machine learning feature sets [Oviatt et al. 2016]. Researchers also conduct regressions, multiple regressions, stepwise linear regressions, and similar analyses to build models and go beyond correlational analysis&#8212;instead determining whether factors are causal ones [Oviatt et al. 2016]. Other research has extracted information about clusters of related multimodal features that have predictive power, for example using principal components analysis [Chen et al. 2014a]. These are examples of commonly used empirical/statistical techniques, but by no means an exhaustive list.</p>
<p class="indent">For a detailed technical overview of signal processing and machine learning methods currently used to model and analyze multimodal data, readers are referred to other related chapters in this volume, (<a href="12_Chapter01.xhtml">Chapters 1</a>&#8211;<a href="15_Chapter04.xhtml">4</a>). The development of new machine learning techniques for handling multimodal data is an active area of research now, especially for topics like audio-visual data processing. Another active research area is the development of hybrid analysis techniques that combine empirical and machine learning capabilities, which is promising since they have different strengths and weaknesses.</p>
<p class="h1-1"><a id="ch11_4"/><b><span class="bg1">11.4</span></b>&#160;&#160;&#160;&#160;<b>What are the Main Themes from Research Findings on Multimodal Learning Analytics?</b></p>
<p class="noindent">This section discusses findings and major themes from research on the prerequisites of learning (e.g., attention, engagement), skilled performance, domain expertise, and insight during problem solving.</p>
<p class="h2-1"><a id="page_350"/><a id="ch11_4_1"/><b><span class="bg2">11.4.1</span></b>&#160;&#160;<b>New Literature on Domain Expertise, Skilled Performance, and Insight During Problem Solving</b></p>
<p class="noindent">Within the past five years, several general themes have emerged from research on multimodal learning analytics, including from the two databases described in <a href="#ch11_3">Section 11.3</a>. The findings discussed in this section focus on the consolidation of domain expertise and skilled performance. They can be summarized as follows.</p>
<p class="h2a"><b>Analysis of information present in all modalities can yield fertile prediction results.</b></p>
<p class="noindent">Students&#8217; level of domain expertise in the Math Data Corpus has been predictable based on analyses involving a variety of different rich communication modalities, including speech, vision, and writing [Ochoa et al. 2013, Oviatt and Cohen 2013]. For example, the dominant math expert in a group of collaborating students can be identified through visual analysis of who controls the calculator, or by audio analysis of who speaks the most math terms [Ochoa et al. 2013]. Based on handwriting, expertise can be identified by dynamic signal-level parameters [Oviatt et al. 2016]. It also has been identified from the frequency of diagramming, and the structural elements and causal features in diagrams [Jee et al. 2009, 2014]. Using the Oral Presentation Corpus, student scores on presentation quality are predictable using either speech or visual analysis [Chen et al. 2014b].</p>
<p class="h2a"><b>Valuable predictive information is present at multiple levels of analysis, and does not necessarily require any linguistic content analysis.</b></p>
<p class="noindent">Multiple levels of analyzing communication modalities all contain surprisingly fertile information about domain expertise, including the signal, activity pattern, lexical/representational and transactional levels [Jee et al. 2009, 2014, Martin and Schwartz 2009, Ochoa et al. 2013, Oviatt et al. 2015, Oviatt and Cohen 2013, Worsley and Blikstein 2010]. At the activity pattern level, how frequently a student contributes problem solutions, irrespective of whether they are actually correct or not, can accurately predict the dominant expert in a group over 90% of the time. In fact, the most expert students in the Math Data Corpus initiated four-fold more problem solutions than non-experts [Oviatt and Cohen 2013]. This indicates that reliable identification of domain expertise can be accomplished by analyzing activity patterns, without any content analysis. At the representational level, higher-performing experts make more structured diagrams [Martin and Schwartz 2009], and they increase their diagramming more as tasks become more difficult [Oviatt et al. 2007].</p>
<p class="indent"><a id="page_351"/>When analyzing the Math Data Corpus at a transactional level, research demonstrates that patterns of overlapped speech can identify (1) phases of rapid problem-solving progress within a group, and (2) level of expertise of students in the group [Oviatt et al. 2015]. The frequency and duration of overlapped speech increased by 120&#8211;143% during the brief &#8220;moment-of-insight&#8221; phase of group problem solving, compared with matched intervals. In addition, more expert students interrupted non-experts 37% more often than non-experts interrupted them, in spite of the fact that they talked less overall. These group dynamics involved speech activity patterns, with no linguistic content analysis required, although linguistic content analysis can be combined to improve prediction accuracy [Oviatt et al. 2015].</p>
<p class="indent">Based on the Oral Presentation Corpus for predicting skilled performance, students&#8217; overall score for quality of presentation was predictable based on paralinguistic or signal-level features like pitch, pitch variability, speaking rate, speech fluency (i.e., minimized pauses), and postural and gestural movement energy [Chen et al. 2014b]. These features likewise did not require any analysis of speakers&#8217; language content. However, presentation scores also could be predicted at the representational level based on linguistic features like the use of first person pronouns.</p>
<p class="h2a"><b>Multimodal combination of information sources yields higher reliabilities than individual ones.</b></p>
<p class="noindent">Analysis of combined multimodal information provides more accurate identification of domain expertise than unimodal sources [Oviatt and Cohen 2013]. In one study based on the Math Data Corpus, the dominant domain expert in a group was correctly identifiable 100% of the time by co-processing speech and written information on who <i>initiated</i> the most problem solutions, irrespective of whether or not they were correct solutions. This multimodal prediction rate surpassed that for either speech or writing alone, which did not exceed 90% [Oviatt and Cohen 2013].</p>
<p class="indent">During prediction of skilled performance involving oral presentation quality, models based on multimodal speech and image processing likewise outperform unimodal models [Chen et al. 2014a, 2014b]. As an example, automated scores based on speech vs. visual data using SVM resulted in prediction performance (i.e., measured as Pearson correlation coefficient between machine-predicted scores and human-rated scores of 0.375 and 0.388, respectively&#8212;compared with 0.511 using combined multimodal features). This pattern of results was replicated using Generalized Boosted Regression Models (GBM) [Friedman 2002], with prediction correlation based on speech alone 0.457, improving to 0.546 based on multimodal features [Chen et al. 2014b].</p>
<p class="indent"><a id="page_352"/>With respect to prerequisites of learning, multimodal recognition models have been demonstrated to substantially outperform unimodal models at detecting students&#8217; interest (vs. disinterest) in problem-solving activities [Kapoor and Picard 2005]. On the more challenging task of identifying emotional state, past meta-analyses of emotion recognition based on 90 multimodal systems demonstrated that in 85% of these cases multimodal models produced more accurate results than the best unimodal models, with an average improvement of approximately 10% [D&#8217;Mello and Kory 2015]. In recent research that developed models to predict students&#8217; engagement, affect, and pre-to-post-test learning gains during computer-mediated tutoring sessions, prediction accuracy for all three types of models improved when shifting from unimodal to bimodal features, and again from bimodal to trimodal features [Grafsgaard et al. 2014].</p>
<p class="h2a"><b>Major findings that reflect change in expertise are evident across modalities and levels of analysis.</b></p>
<p class="noindent">As one example, domain expertise has consistently been associated with increased fluency of communication patterns. Based on the Math Data Corpus, non-expert students&#8217; writing and speech were 42% and 103% more disfluent than experts, respectively [Oviatt and Cohen 2014]. In addition to lexical evidence of greater fluency, at the signal level Cheng and colleagues showed that expert mathematicians produce fewer and briefer pauses when writing math formulas [Cheng and Rojas-Anaya 2007, 2008]. That is, the microstructure of pausing between written strokes within a math formula shifts to become more fluent. This transition reflects increased chunking of information in working memory as expertise develops in mathematicians (see <a href="#ch11_5_1">Section 11.5.1</a>).</p>
<p class="h2a"><b>Early multimodal learning analytics research already has achieved predictions of 90% and above.</b></p>
<p class="noindent">Analysis of activity pattern data alone, with no analysis of lexical content, has correctly classified students by domain expertise at levels exceeding 90%. As discussed above, more expert students in mathematics have been correctly identified simply by analyzing their frequency of proposing problem solutions, irrespective of whether or not they were correct. When unimodal speech data vs. combined speech and writing data provided the basis for analyses, correct identification was 90% and 100%, respectively [Oviatt and Cohen 2013].</p>
<p class="indent">As another example, students who were domain experts in math were reliably distinguishable from non-experts 90% of the time simply based on the total amount they wrote when solving problems at different difficulty levels. Whereas non-experts <a id="page_353"/>increased their total writing most between easy and moderate tasks, experts increased their writing most between hard and very hard tasks [Oviatt and Cohen 2014]. Both of these examples demonstrate accurate prediction based on a single activity pattern metric, with no content analysis required.</p>
<p class="indent">As a third example, recent work has shown that domain experts can be distinguished from non-experts with 92% accuracy based simply on signal-level writing dynamics, like the average distance and pressure of their written strokes [Zhou et al. 2014, Oviatt et al. 2016] (see <a href="#ch11_5_2">Section 11.5.2</a>). Again, this prediction performance was based exclusively on low-level signal features, and did not rely on any content analysis of what students wrote.</p>
<p class="indent">Also based on the Math Data Corpus, a predictive index of expertise that analyzed speech activity patterns and linguistic content during interruptions within a group correctly distinguished expert from non-expert students 95% of the time within 3 min of interaction [Oviatt et al. 2015], surpassing reliabilities of either single data source. Not only did expert students interrupt others more frequently, the linguistic content of their interruptions more often disagreed, corrected, and directed others on problem-solving actions. In contrast, inexpert students expressed more uncertainty and apologized more often for errors [Oviatt et al. 2015]. This example leverages both speech activity patterns and linguistic content analysis during small group problem solving.</p>
<p class="indent">The multi-level multimodal results outlined above emphasize that there is an opportunity to strategically combine information sources to achieve far higher reliabilities in the future. From a pragmatic perspective, these early findings on the robustness of multimodal prediction also highlight that this is a fertile area to begin developing analytics systems. From a theoretical viewpoint, they indicate that new multi-level multimodal findings potentially can lead to more coherent systems-level views of the learning process, as illustrated in <a href="#ch11_5_2">Section 11.5.2</a></p>
<p class="h2-1"><a id="ch11_4_2"/><b><span class="bg2">11.4.2</span>&#160;&#160;Prior Literature on Prerequisites of Learning: Attention, Engagement, Emotional State</b></p>
<p class="noindent">A large and multidisciplinary body of research has investigated learning-oriented behaviors that are pre-requisites for learning, including attention, engagement, and emotional state. This research has been based on a wide range of behavioral, physiological, sensor-based, and in some cases neuroimaging technologies. During learning activities, early evidence of these prerequisite behaviors increases the likelihood that subsequent learning will occur. Furthermore, combining predictive features based on attention and emotional state potentially can yield faster prediction and improved reliability. In this section, we discuss a sample of recent <a id="page_354"/>findings in this area, which is by no means exhaustive. For a current summary of the state-of-the-art on emotion recognition using multimodal-multisensor information sources, including emotion recognition during learning-oriented activities, readers are referred to other chapters in this volume (<a href="17_Chapter05.xhtml">Chapters 5</a>&#8211;<a href="20_Chapter08.xhtml">8</a>). In addition, a recent survey article provides a meta-analysis of multimodal affect detection systems, including a comparison of their multimodal vs. unimodal accuracies and other information [D&#8217;Mello and Kory 2015].</p>
<p class="indent">In past research, multimodal systems have been built that aim to detect individual students&#8217; attention and interest (vs. disinterest) while engaging in problemsolving activities. This very basic prediction goal can yield relatively high reliabilities. For example, one system reported 86% accuracy in unconstrained naturalistic contexts, based on using a multimodal combination of facial expressions, postural movements, and human-computer activity patterns. The multimodal system accuracy outperformed alternative unimodal recognition models [Kapoor and Picard 2005]. In other research, children&#8217;s engagement with a robotic partner was assessed while playing a game of chess in a naturalistic setting. Postural cues and body motion were evaluated, with various models achieving accuracies of 70&#8211;79% [Sanghvi et al. 2011].</p>
<p class="indent">Others have aimed to develop infrastructure and assess learners&#8217; attention in classrooms, in particular to detect decreased concentration using a multiple-camera approach [Raca and Dillenbourg 2013]. For greater accuracy in assessing engagement, gaze direction, duration, scan patterns, and similar metrics are other sources for tracking learners&#8217; attention to objects and tasks, as well as their level of engagement with human, virtual, or robotic interlocutors [Nakano and Ishii 2010, Peters et al. 2010, Rich et al 2010]. However, in naturalistic contexts like classrooms head position often is used as a proxy instead of assessing a learner&#8217;s actual gaze [Raca and Dillenbourg 2013]. In addition, direct information about on-task behavior, or activity patterns compatible with an educational activity (e.g., touching and manipulating an object), presents more definitive follow-on information that can be combined with gaze to improve prediction. This would require a more temporally-cascaded approach to prediction, in which brief behavioral slices encompassing gaze + subsequent manual activity are extracted to more reliably calibrate a student&#8217;s level of engagement.</p>
<p class="indent">The large body of research on students&#8217; attention and engagement has too often studied these precursors of learning as an end goal, without pursuing a more meaningful analysis of their relation to actual learning outcomes. As a counter-example to this limitation in the literature, Grafsgaard et al. [2014] built multimodal models that aim to more comprehensively predict students&#8217; affect, engagement, and also pre-to-post-test learning gains during computer-mediated tutoring sessions. <a id="page_355"/>They analyzed dialogue content, facial and postural movements, hand gestures, and task actions as information sources. For these interrelated modeling tasks, the results showed improvement in prediction accuracy when shifting from unimodal to bimodal features, and from a bimodal to trimodal collection of features [Grafsgaard et al. 2014]. The trimodal model for predicting affect far exceeded uni-and bimodal models, with an <i>R</i><sup>2</sup> of 0.52, compared with a best bimodal <i>R</i><sup>2</sup> of 0.14. Most importantly, the trimodal model for predicting students&#8217; actual learning gains exceeded both uni- and bimodal models, with an <i>R</i><sup>2</sup> of 0.54. This compared with a best dialogue-only <i>R</i><sup>2</sup> of 0.37, and a best bimodal (dialogue and nonverbal behavior) <i>R</i><sup>2</sup> of 0.47. Now that multimodal learning analytic techniques enable collecting larger, richer, and more longitudinal datasets, the community will be in a better position to: (1) examine interrelations among emotion, engagement, and learning outcomes in different contexts and (2) strategically select the most informative modalities to assess in order to achieve high enough reliabilities to develop fieldable applications.</p>
<p class="indent">In their meta-analysis of multimodal affect detection systems, D&#8217;Mello and Kory [2015] summarized that most existing systems rely on person-dependent models that fuse audio and visual information to detect acted (rather than natural spontaneous) expressions of basic emotions. Their meta-analysis of 90 multimodal systems revealed that multimodal emotion recognition is consistently more accurate than unimodal approaches to identifying emotions, by an average magnitude of 10%. To further improve their robustness advantage, future multimodal approaches to emotion recognition need to more carefully strategize which specific information sources to include so their complementarity and ability to dampen major sources of noise is optimized. It should certainly not always be assumed that the same audio-visual sources will be the best solution. This is an emerging field, and recognition of more spontaneous emotions, complex emotional blends, timing and intensity of emotions, and cultural variability in emotional expression remain difficult challenges yet to be fully addressed. Two major trends in recent emotion recognition research have been an increase in: (1) assessment and modeling of multimodal information sources and (2) recognition of naturalistic emotional expressions, rather than acted ones [Zeng et al. 2009].</p>
<p class="indent">To facilitate research on emotion recognition, a variety of software toolkits are available, such as the Computer Expression Recognition Toolbox (CERT) for tracking fine-grained facial movements (e.g., eyebrow raising, eyelid tightening, mouth dimpling; [Littlewort et al. 2011]). This toolkit has been used to analyze video recordings of naturalistic learning interactions [Grafsgaard et al. 2013], and is based on Ekman and Friesen&#8217;s [1978] Facial Action Coding System. Other off-the-shelf computer vision toolkits for analyzing faces include Visage [Visage <a id="page_356"/>Technologies 2016], SHORE [Fraunhofer 2016], GAVAM (Generalized Adaptive View-based Appearance Model [Morency et al. 2010], and newly released Open-Face ([Baltrusaitis et al. 2016]. These tools provide solutions for measuring head orientation, eye gaze direction, and facial expressions.</p>
<p class="indent">During learning activities, researchers have attempted to understand the relation between emotional state, on- and off-task activity patterns, and constructive learning [Woolf et al. 2009]. To learn effectively, students&#8217; activity level in relevant tasks clearly needs to increase, which is facilitated by positive affect and curiousity [D&#8217;Mello et al. 2008]. The approach of current systems more often is to focus on detecting negative emotional states that impede learning, in particular boredom, confusion, and frustration&#8212;and then to use this information to adapt a learning system&#8217;s presentation of educational content in order to facilitate positive flow and problem-solving progress. For example, Grafsgaard et al. [2014] singled out frustration during their emotion modeling and recognition work. They pointed out that reliable identification of different specific emotions can require different information sources. Baker and colleagues emphasized recognition of boredom and confusion, and advocated pedagogical interventions to disrupt vicious cycles that occur when students become bored and stay bored for long periods of time [Baker et al. 2010]. In previous work on interaction with intelligent tutoring systems, multiple regression analyses confirmed that learner&#8217;s dialogue features are predictive of affective states such as boredom, confusion, flow, and frustration, with up to 54% accuracy [D&#8217;Mello et al. 2008]. During recognition of spontaneous emotional expressions, as would be the case for students in an actual classroom using an educational system, facial features provide additional valuable predictive information [D&#8217;Mello and Graesser 2010].</p>
<p class="h1-1"><a id="ch11_5"/><b><span class="bg1">11.5</span>&#160;&#160;&#160;&#160;What is the Theoretical Basis of Multimodal Learning Analytics?</b></p>
<p class="noindent">This section discusses the explanatory power of existing learning theories in accounting for multimodal learning analytics data, and also how multimodal learning analytics data can generate more coherent systems-level learning theories in the future.</p>
<p class="h2-1"><a id="ch11_5_1"/><b><span class="bg2">11.5.1</span>&#160;&#160;How Existing Learning Theories Explain Data Observed in Multimodal Learning Analytics</b></p>
<p class="noindent">Previous work has discussed theories of learning that are most relevant to guiding the design of effective educational interfaces [Oviatt 2013b]. In this handbook, Oviatt [2017] provides background on theories that are specifically relevant to de <a id="page_357"/>signing successful multimodal-multisensor interfaces, which includes <i>limited resource theories</i> (e.g., Working Memory and Cognitive Load theories) and <i>perception-action dynamic theories</i> (e.g., Activity and Embodied Cognition theories). Both of these theoretical perspectives are relevant to the present discussion on multimodal learning analytics.</p>
<p class="h2a"><b>Limited-resource Theories</b></p>
<p class="noindent">The aim of this section is to explain why the multimodal data we observe from student learners contain certain characteristics when they are novices, but quite different ones after they develop expertise in a domain, or a higher level of skilled performance. Cognitive limited-resource theories (Working Memory theory, Cognitive Load theory) have been used to account for the general finding that as people learn their behavior becomes more fluent, which occurs across modalities (e.g., speech, writing). In Cheng&#8217;s research, expert mathematicians produced fewer and shorter pauses when handwriting math formulas, a signal-level adaptation that resulted in greater fluency [Cheng and Rojas-Anaya 2007, 2008]. In Oviatt&#8217;s research, students who were more expert in math also produced fewer disfluencies at the lexical level, resulting in more fluent communication when speaking and writing [Oviatt and Cohen 2014]. Limited-resource theories claim these changes occur through repeated engagement in activity procedures (e.g., writing calculations for math formulas), which causes people to perceive and group isolated lower-level units of information into higher-level organized wholes [Baddeley and Hitch 1974]. That is, through repeated engagement in activity procedures, more expert students learn to apprehend larger patterns and strategies. As a result, domain experts do not need to retain and retrieve as many units of information from working memory during a task, which frees up their memory reserves for focusing on more difficult tasks or acquiring new knowledge.</p>
<p class="indent">From a more linguistic perspective, limited-resource theories have characterized people as adaptively conserving energy at the signal, lexical, and dialogue levels. For example, Lindblom&#8217;sH&#38;H Theory [1990] maintains that speakers strive for articulatory economy by using relaxed hypo-clear speech, unless the intelligibility of their communication is threatened. In this case, they will expend more effort to produce hyper-clear speech. Likewise, at the lexical and dialogue levels the Theory of Least Collaborative Effort states that people reduce their descriptions as they achieve common ground with a dialogue partner [Clark and Brennan 1991, Clark and Schaefer 1989]. During learning activities, research shows that expert students&#8217; likewise adapt by becoming more concise. They use more apt and succinct technical terms, and more concise noun phrase descriptions with <a id="page_358"/>a lower ratio of words to concepts [Purandare and Litman 2008]. Based on the Math Data Corpus, experts&#8217; explanations of their math answers were 15% more concise in number of words, compared with non-experts [Oviatt et al. 2015]. When writing, expert math students likewise wrote more compact domain-specific symbols [Oviatt et al. 2007], as well as 13% fewer total written representations. One important impact of this adaptation is to minimize communicators&#8217; cognitive load, as well as the total energy they expend, so they can focus on more difficult tasks.</p>
<p class="indent">During interpersonal or human-computer interactions, the literature demonstrates that human performance improves when people combine different modalities to express complementary information, for example sketching a Punnett square diagram while speaking a genetics explanation, because this multimodal information can be processed in separate brain regions simultaneously. This likewise effectively circumvents working memory limitations related to any single region. Furthermore, this multimodal advantage can accrue whether simultaneous information processing involves two input streams, an input and output stream, or two output streams [Oviatt 2017]. For example, numerous education studies have shown that a multimodal presentation format supports students&#8217; learning more successfully than unimodal presentation [Mayer and Moreno 1998, Mousavi et al. 1995]. When using a multimodal format, larger performance advantages also have been demonstrated on more difficult tasks, compared with simpler ones [Tindall-Ford et al. 1997]. Flexible multimodal interaction and interfaces are effective partly because they support students&#8217; ability to self-manage their own working memory in a way that reduces cognitive load [Oviatt et al. 2004]. For example, students prefer to interact unimodally when working on easy problems, but upshift to interacting multimodally on harder ones. From a limited-resource theory viewpoint, when students upshift to multimodal processing on hard problems they are distributing information processing across different brain regions. This frees up working memory and minimizes their cognitive load so performance can be enhanced on these harder tasks [Oviatt et al. 2004].</p>
<p class="h2a"><b>Perception-action Dynamic Theories</b></p>
<p class="noindent">Perception-action dynamic theories provide a holistic systems view of interaction between humans and their environment, including feedback processes as part of a dynamic loop. Currently, Embodied Cognition theory is most actively being researched, often in the context of human learning or neuroscience research. It claims that representations involve activating neural processes that recreate a related action-perception experiential loop, which is based on multisensory perceptual <a id="page_359"/>and multimodal motor neural circuits in the brain [James et al. 2017b, Nakamura et al. 2012]. During this feedback loop, multisensory perception of an action (e.g., the visual, haptic, and auditory cues involved in writing a letter shape) primes motor neurons in the observer&#8217;s brain (e.g., corresponding finger movements), which facilitates related comprehension and learning (e.g., letter recognition and reading). Neuroscience findings have confirmed that actively writing symbolic representations like letter shapes, compared with passively viewing or typing them, stimulates greater brain activation, more durable long-term sensorimotor memories in the reading neural circuit, and more accurate letter recognition [James and Engelhardt 2012, Longcamp et al. 2008].</p>
<p class="indent">Both Activity and Embodied Cognition theories support multimodal learning analytic findings that students&#8217; increased physical and communicative activity facilitates learning and the consolidation of expertise. While Activity theorists like Luria and Vygotsky were particularly interested in how speech activity facilitates self-regulation and cognition [Luria 1961, Vygotsky 1962], subsequent research has shown that increased activity in all communication modalities stimulates thought. As tasks become more difficult, speech, gesture, and writing all increase in frequency, reducing cognitive load and improving performance [Comblain 1994, Goldin-Meadow et al. 2001, Xiao et al. 2003].</p>
<p class="indent">Furthermore, multisensory perception and multimodal action patterns facilitate learning to a greater extent than unimodal ones. They stimulate more total neural activity across a range of modalities, more intense bursts of neural activity, more widely distributed activity across the brain&#8217;s neurological substrates, and longer distance connections (see [Oviatt 2017]). As discussed above, research has demonstrated that this results in demonstrably improved performance and learning. See [Oviatt 2017] for a more extended discussion of how increased multisensory-multimodal activity influences human brain structure and processing.</p>
<p class="h2-1"><a id="ch11_5_2"/><b><span class="bg2">11.5.2</span>&#160;&#160;How Multimodal Learning Analytics Can Generate New Systems-level Learning Theory</b></p>
<p class="noindent">Rich multi-level multimodal data analytics research has the potential to support more coherent systems-level theories of the learning process. As this field continues to develop, such theories could explain how multiple levels interact (e.g., neural, signal, activity pattern, representational) to produce emergent learning phenomena. In what follows we describe recent research findings which, combined with an analysis of past literature, produced a new systems-level theory about the process of learning.</p>
<p class="indent"><a id="page_360"/>As introduced in <a href="#ch11_4_1">Section 11.4.1</a>, recent multimodal learning analytics work has revealed that math domain experts can be distinguished from non-experts with 92% accuracy based simply on signal-level handwriting dynamics, like the average distance, duration, and pressure of their written strokes [Zhou et al. 2014, Oviatt et al. 2016]. When more expert math students write math formulas, their average strokes are shorter, briefer, and lighter in pressure as shown in <a href="#fig11_7">Figure 11.7</a>. Modeling of these multiple signal features revealed that students who are domain experts in mathematics expend 46% <i>less total energy</i> forming written strokes, compared with non-expert students. When within-subject longitudinal data were analyzed over time, students who demonstrated learning also showed a decrease in written total energy expended, but no change was evident in students who did not learn.</p>
<p class="indent">In summary, students dynamically co-adapt the total writing energy they expend as a function of their level of domain expertise when thinking and reasoning in that domain. Specifically, there is an <i>inverse relation</i> between level of expertise and metrics reflecting communicative energy expended&#8212;low expertise/high energy, high expertise/low energy. This inverse relation is evident between individuals (i.e., non-experts vs. experts), and also within individuals over time as they learn (i.e., beginning student, mastery student). Domain novices expend more energy during initial stages of learning, but they adapt by reducing it as they gradually acquire more expertise.</p>
<p class="indent">While these signal-level predictive results are perhaps surprising, parallel findings have been obtained across multiple levels of analysis&#8212;including neural, signal, activity pattern, and linguistic. For example, related literature has shown that neural activation is reduced up to 90% when performing a task after becoming a domain expert [Hill and Schneider 2006]. As summarized in <a href="#ch11_5_1">Section 11.5.1</a>, domain experts also reduce the total amount they communicate, for example speaking or writing fewer words when they explain how a math problem is solved. The research described above on dynamic adaptation of signal-level writing energy, combined with this and similar related literature, has generated a unique view of what occurs during the process of learning.</p>
<p class="indent">From a theoretical viewpoint, one premise is that the total energy an organism expends is a limited resource that they adapt in a homeostatic manner automatically and continuously in order to establish equilibrium. A major systems-level aim of organisms is to conserve energy resources for vital functions across all levels, which is an evolutionary advantage. During learning, students will reduce energy expenditure on communication so they can flexibly allocate it to master harder tasks or prepare for learning new content [Oviatt et al. 2016]. Briefer and less complex communication while solving a problem frees up working memory resources for thinking and reasoning about it, which can facilitate further integration of <a id="page_361"/>memory and increased learning. In short, the emergent outcome of this limited-resource systems-level process is that experts&#8217; reduced energy baseline enables a larger energy reserve and greater adaptive range for expending energy on initiating new learning activities. This initiative further expands their expertise, generating a self-sustaining process of learning facilitating preparation and engagement in further learning [Oviatt et al. 2016]. In fact, multimodal learning analytics research conducted on the Math Data Corpus confirmed that experts were four-fold more active initiating solutions, worked more on harder problems, and learned more between sessions than non-experts [Oviatt et al. 2016]. For a more extended discussion of this limited-resource view of cross-level interactions between expertise status, energy expended on cognitive processing and related language, total energy reserves, and emergent initiative to engage in more learning activities, see Oviatt et al. [2016].</p>
<div class="cap" id="fig11_7">
<p class="image"><img src="../images/fig11_7.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 11.7</b>&#160;&#160;Pen stroke data for a domain expert (top) vs. non-expert (bottom) based on writing a formula in two consecutive sessions (left). The expert shows a 32% relative reduction in total energy expended (TE), compared with the non-expert. The expert also shows a 17% relative reduction in total energy expended over sessions, which corresponded with a 10% learning gain (right top). (CDE=cumulative domain expertise, based on ground-truth coding in Oviatt et al. [2014])</p>
</div>
<p class="h1-1"><a id="ch11_6"/><b><span class="bg1">11.6</span>&#160;&#160;&#160;&#160;What are the Main Challenges and Limitations of Multimodal Learning Analytics?</b></p>
<p class="noindent">The field of multimodal learning analytics is data-intensive and demanding in unique ways. Perhaps the most demanding aspect of conducting research in this area is the requirement to obtain a relevant dataset, or else to collect it yourself. <a id="page_362"/>Often multimodal datasets collected in learning contexts cannot be shared with others due to IRB restrictions. The need to collect and analyze data involving multiple high-fidelity input streams in a tightly time-synchronized manner can require technical expertise spanning multiple component technologies (e.g., speech, image, and ink processing), fine-grained time-stamping of data, and custom design of retrieval and analysis tools. For some modalities, well-developed tools are available, such as Praat for speech data analysis [Boersma 2002]. However, for other modalities like writing and manipulation of ink data, researchers often must develop their own analysis tools.</p>
<p class="indent">After the initial data collection, transcription and coding of multimodal learning analytics data can be extremely time-consuming, with few automated tools available. All such datasets require high-quality ground-truth coding, in particular to solidly document students&#8217; level of performance and learning progress. These data collection and coding burdens are compounded by the fact that multimodal learning analytics ideally should be supported by extended longitudinal datasets, preferably sampling data over months, which can involve vast amounts of rich multimodal data. Once collected and coded, an excellent longitudinal dataset is exceedingly valuable, and could support extensive exploratory analysis by an entire community of researchers for years.</p>
<p class="indent">Another major challenge is that, without stakeholder approval and control, the best learning analytics technology will not be adopted by the education system and society. The opaque nature of machine learning creates a problem for social-organization control and the likelihood that valuable new prediction technologies will be adopted. For technology adoption, different stakeholders need to be involved in designing and vetting applications involving the technology. For example, learning analytic technology stands to play a definitive role in guiding the future of educational practice. Students, teachers, parents, government funding agencies, and society at large all will need to provide input on the goals, metrics, applications, privacy protections, and other issues involved in adopting learning analytics. To support community discussions, learning analytic techniques must be transparent, conceptually clear, and easy to understand and communicate in order to support consensus building and social policy formation. Participatory design involving stakeholder input will be required to initially design, and then continually modify, learning analytic techniques for educational purposes. The long-term aim is end-user appropriation of learning analytic tools by the education community itself.</p>
<p class="indent">An additional major challenge to the widespread use of multimodal learning analytics in schools is concern about protecting student and teacher privacy. As data <a id="page_363"/>sources become more richly multimodal, and learning analytic tools more mobile, privacy violations risk becoming intrusive, pervasive, and potentially damaging to basic civil liberties. It will be essential that technology designers, educators, and citizens engage in participatory design of learning analytic tools to ensure that policy formation protects the privacy, employment, and human rights of students and teachers from whom data are collected. In the U.S., California already has passed legislation limiting collection, use, and sale of students&#8217; data by third-party groups [Singer 2014]. Among the lessons for designers of learning analytic systems are that: (1) users must be able to understand and control any system developed, which requires transparency and continuous involvement; (2) a learning analytic system cannot be gameable, or users will discover contingencies and game it to their advantage; and (3) users of analytic systems must participate in their design to protect their own privacy.</p>
<p class="indent">This chapter has not focused on multimodal learning analytic applications, which would be premature given the emergent status of this new field. However, systems are indeed beginning to be designed and developed to provide real-time feedback to the learner or instructor about learning processes. As one early example, Batrinca et el. [2013] developed a public speaking skill training system based on advanced multimodal sensing and virtual human technologies. In this work, a virtual audience of software personas responds to the quality of a speaker&#8217;s oral presentation in real time by providing feedback. Similar systems have been developed using speech and image processing [Kurihara et al. 2007, Nguyen et al. 2012]. Other researchers are contributing to improving these new systems. For example, research has examined how different types of multimodal system feedback can improve students&#8217; learning of presentation skills [Schneider et al. 2015]. Another project is extending this work to applications like personnel hiring [Nguyen et al. 2014]. Eventually, this type of multimodal technology could transform previous human-scored assessment of presentations, making them more reliable, objective, and cost-efficient. A major challenge that lies ahead is the development of other new systems that use multimodal learning analytic findings to adaptively support learners in more sensitive and useful ways.</p>
<p class="h1-1"><a id="ch11_7"/><b><span class="bg1">11.7</span>&#160;&#160;&#160;&#160;Conclusions and Future Directions</b></p>
<p class="noindent">Learning is a complex human activity, and it cannot be understood fully without collaboration between learning scientists, cognitive and linguistic scientists, and computer scientists. Over the last five years, researchers with expertise in handling multimodal data and infrastructure have pioneered new computational <a id="page_364"/>methods, strategic prediction techniques, and fertile findings in the emerging area of multimodal learning analytics. With new methods now available, cognitive, linguistic, and learning scientists can participate in organizing the collection of meaningful datasets, and analyzing them to improve the modeling and prediction of students&#8217; mental states during educational activities. Eventually, practicing educators and students themselves will be able to participate by tailoring valuable aspects of the technology for use in classrooms and on their own mobile devices, which will improve the likelihood of adoption and valuable utility to end users.</p>
<p class="indent">Major efforts will be required to explore the vast exploratory space of multi-level multimodal learning analytics, and then to combine the most fertile predictors into ultra-reliable learning analytic systems. The collection of new multimodal datasets, and research studies based on them, will need to focus on tracking learners in situated contexts longitudinally over time. Accomplishing this will require major investments in new computational tools and software to automate this data-intensive work. The days of investigating single &#8220;markers&#8221; of expertise and learning during one-shot testing are long past. To advance learning analytics now, researchers must engage in teamwork to transparently and efficiently collect multi-stream data over extended periods of time in classrooms and other natural contexts. These data will be valuable in supporting continuous assessment of a person&#8217;s mental status, and real-time support with responsive intervention by teachers and adaptive educational systems.</p>
<p class="indent">While multimodal learning analytics has progressed quickly toward predicting students&#8217; level of domain expertise and skilled performance, future work must be initiated to examine the more challenging topics of how students gain insight during problem solving, metacognitive awareness for self-regulating learning, and transfer of learned information. To develop a deeper understanding of these processes, multimodal learning analytics offers more powerful data and context-sensitive metrics for revealing the multi-level process of learning, which is especially needed across behavioral and neuropsychological indices. New research also must clarify the relation between students&#8217; level of domain expertise and the transient cognitive load they experience when solving problems. Other topics that need to be addressed are how we can more objectively assess a student&#8217;s level of domain expertise during learning, independent of conflating personality attributes like extroversion and dominance. This raises the question of what multimodal learning analytic systems may be able to assess more accurately and objectively than experienced teachers who, like the rest of us, are limited by human biases in inferential reasoning [Kahneman et al. 1982]. To make progress <a id="page_365"/>on all of these topics, cognitive, learning and linguistic scientists will be especially indispensable members of future research teams. For further discussion by a multidisciplinary panel of experts on challenging topics in multimodal learning analytics and the design of multimodal educational technologies, see [James et al. 2017a].</p>
<p class="h1n"><a id="ch11_8"/><b>Focus Questions</b></p>
<p class="noindent"><b>11.1.</b> Why is multimodal learning analytics as a field emerging now?</p>
<p class="noindentt"><b>11.2.</b> What are the main objectives of multimodal learning analytics, and what are its main advantages compared with previous learning analytics?</p>
<p class="noindentt"><b>11.3.</b> How do the Math Data Corpus and Oral Presentation Corpus differ? What opportunities do they each provide for examining learning with multimodal data?</p>
<p class="noindentt"><b>11.4.</b> What main computational methods and techniques are currently being used to analyze large multi-level multimodal data sets? What are each of their strengths and limitations?</p>
<p class="noindentt"><b>11.5.</b> Existing learning analytics systems often are used for educational or classroom management, rather than for assessing learning behaviors or domain expertise in individual students. Why is multimodal learning analytics a more promising avenue for evaluating learners&#8217; mental state?</p>
<p class="noindentt"><b>11.6.</b> What different levels of analysis does multimodal learning analytics enable? In what ways do they provide a comprehensive and powerful window on students&#8217; learning?</p>
<p class="noindentt"><b>11.7.</b> What types of data could be collected and analyzed automatically to evaluate student learning using existing commercially available technology?</p>
<p class="noindentt"><b>11.8.</b> In the future, what educational assessment objectives might be accomplished more effectively using multimodal learning analytics, rather than an expert human teacher? Why?</p>
<p class="noindentt"><b>11.9.</b> What evidence exists that experts conserve communication energy? What are the implications for exploring further predictors of expertise using multimodal learning analytics? And for producing new theory?</p>
<p class="noindentt"><b>11.10.</b> What specific types of data will be needed to support next-step research on multimodal learning analytics?</p>
<p class="noindentt"><a id="page_366"/><b>11.11.</b> What are the main privacy concerns involved in using multimodal learning analytics, and how would you approach managing them?</p>
<p class="noindentt"><b>11.12.</b> Using either the Math Data Corpus or Oral Presentation Corpus, design a multimodal learning analytics study to investigate a question that you think is interesting. You can plan data analyses using any modality or level of analysis that would be supportive of your aims. State your hypotheses, and also the potential uses or impact of any findings.</p>
<p class="h1n"><a id="ch11_9"/><b>References</b></p>
<p class="ref">Anoto. 2016. <a href="http://pressreleases.triplepointpr.com/2016/02/07/anoto-announces-acquisition-of-we-inspire-destiny-wireless-and-pen-generations/">http://pressreleases.triplepointpr.com/2016/02/07/anoto-announces-acquisition-of-we-inspire-destiny-wireless-and-pen-generations/</a>. Accessed October 21, 2016. 346</p>
<p class="ref">K. E. Arnold and M. D. Pistilli. 2012. Course signals at purdue: using learning analytics to increase student success. <i>Proceedings of the 2nd International Conference on Learning Analytics and Knowledge</i>, ACM/SOLAR, 267&#8211;270. DOI: 10.1145/2330601.2330666. 337</p>
<p class="ref">A. Arthur, R. Lunsford, M. Wesson, and S. L. Oviatt. 2006. Prototyping novel collaborative multimodal systems: Simulation, data collection and analysis tools for the next decade. In <i>Eighth International Conference on Multimodal Interfaces (ICMI&#8217;06)</i>, pp. 209&#8211;226. ACM, New York. DOI: 10.1145/1180995.1181039. 340</p>
<p class="ref">A. D. Baddeley and G. J. Hitch. 1974. Working memory. G. A. Bower, editor, <i>The Psychology of Learning and Motivation: Advances in Research and Theory</i>, Volume 8, pp. 47&#8211;89. Academic Press, New York. 357</p>
<p class="ref">R. S. Baker, S. K. D&#8217;Mello, M. M. Rodrigo, and A. C. Graesser. 2010. Better to be frustrated than bored: The incidence, persistence, and impact of learners&#8217; cognitive-affective states during interactions with three different computer-based learning environments. <i>International Journal of Human-Computer Studies</i>, 68(4):223&#8211;241. DOI: 10.1016/j.ijhcs.2009.12.003. 356</p>
<p class="ref">R. S. Baker, A. T. Corbett, K. R. Koedinger, and A Z. Wagner. 2004. Off-task behavior in the cognitive tutor classroom: When students game the system. <i>Proceedings of the SIGCHI Conference on Human factors in Computing Systems (CHI)</i>, pp. 383&#8211;390. ACM, New York. DOI: 10.1145/985692.985741. 337</p>
<p class="ref">T. Baltrusaitis, P. Robinson, and L. Morency. 2016. OpenFace: An open source facial behavior analysis toolkit. In <i>Proceedings of the 2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</i>, IEEE, New York. DOI: 10.1109/WACV.2016.7477553. 356</p>
<p class="ref">L. Batrinca, G. Stratou, A. Shapiro, L.-P. Morency, and S. Scherer. 2013. Cicero: Towards a multimodal virtual audience platform for public speaking training. <i>Intelligent Virtual Agents</i>, 116&#8211;128. DOI: 10.1007/978-3-642-40415-3_10. 363</p>
<p class="ref">P. Boersma. 2002. Praat, a system for doing phonetics by computer. <i>Glot International</i>, 5(9/10):341&#8211;345. 362</p>
<p class="ref"><a id="page_367"/>L. Chen, G. Feng, J. Joe, C. Leong, C. Kitchen, C. and Lee. 2014a. Toward automated assessment of public speaking skills using multimodal cues. In <i>Proceedings of the International Conference on Multimodal Interaction (ICMI)</i>, pp. 200&#8211;203. ACM, New York. DOI: 10.1145/2663204.2663265. 345, 347, 349, 351</p>
<p class="ref">L. Chen, C. Leong, G. Feng, and C. Lee. 2014b. Using multimodal cues to analyze MLA&#8217;14 Oral Presentation Quality Corpus: Presentation delivery and slides quality. In <i>Proceedings of the 2014 ACM Grand Challenge Workshop on Multimodal Learning Analytics</i>, pp. 45&#8211;52. ACM, New York. DOI: 10.1145/2666633.2666640. 347, 350, 351</p>
<p class="ref">L. Chen, C. Leong, G. Feng, C. Lee, and S. Somasundaran. 2015. Utilizing multimodal cues to automatically evaluate public speaking performance. In <i>Proceedings of the International Conference on Affective Computing and Intelligent Interaction</i>, pp. 394&#8211;400. DOI: 10.1109/ACII.2015.7344601. 343, 347</p>
<p class="ref">P. C. H. Cheng, and H. Rojas-Anaya. 2007. Measuring mathematical formula writing competence: An application of graphical protocol analysis. In D. S. McNamara and J. G. Trafton editors, <i>Proceedings of the 29th Conference of the Cognitive Science Society</i>, pp. 869&#8211;874. Cognitive Science Society, Austin, TX. 352, 357</p>
<p class="ref">P. C. H. Cheng, and H. Rojas-Anaya. 2008. A graphical chunk production model: Evaluation using graphical protocol analysis with artificial sentences. In B. C. Love, K. McRae and V. M. Sloutsky editors, <i>Proceedings of 30th Annual Conference of the Cognitive Science Society</i>, pp. 1972&#8211;1977. Cognitive Science Society, Austin, TX. 352, 357</p>
<p class="ref">H. Clark and S. Brennan. 1991. Grounding in communication. In Resnick, Levine and Teasley, editors, <i>Perspectives on Socially Shared Cognition</i>, pp. 127&#8211;149, APA, Washington. 357</p>
<p class="ref">H. Clark, and E. F. Schaefer. 1989. Contributing to discourse. <i>Cognitive Science</i>, 13: 259&#8211;294. DOI: 10.1016/0364-0213(89)90008-6. 357</p>
<p class="ref">P. Cohen and S. Oviatt. 2017. Multimodal speech and pen interfaces. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 1: Foundations, User Modeling and Common Modality Combinations</i>. Morgan &#38; Claypool Publishers, San Rafael, CA. 332</p>
<p class="ref">A. Comblain. 1994. Working memory in Down&#8217;s Syndrome: Training the rehearsal strategy. <i>Down&#8217;s Syndrome Research and Practice</i>, 2(3):123&#8211;126. DOI: 10.3104/reports.42. 359</p>
<p class="ref">S. D&#8217;Mello, S. Craig, A. Witherspoon, B. McDaniel, and A. Graesser 2008. Automatic detection of learner&#8217;s affect from conversational cues. <i>User Modeling and User Adapted Interaction</i>, 18(1-2):45&#8211;80. DOI: 10.1007/s11257-007-9037-6. 356</p>
<p class="ref">S. K. D&#8217;Mello and A. C. Graesser. 2010. Multimodal semi-automated affect detection from conversational cues, gross body language, and facial features. <i>User Modeling and User- Adapted Interaction</i>, 20(2):147&#8211;187. DOI: 10.1007/s11257-010-9074-4. 356</p>
<p class="ref">S. K. D&#8217;Mello and J. Kory. 2015. A Review and meta-analysis of multimodal affect detection systems. <i>ACM Computing Surveys</i>, 47(3), article 43, ACM, New York. DOI: 10.1145/2682899. 352, 354, 355</p>
<p class="ref"><a id="page_368"/>F. Dominguez, V. Echeverria, K. Chiluiza, and X. Ochoa. 2015. Multimodal selfies: Designing a multimodal recording device for students in traditional classrooms. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 567&#8211;574. DOI: 10.1145/2818346.2830606. 346, 347</p>
<p class="ref">P. Ekman and W. V. Friesen. 1978. <i>Facial Action Coding System</i>, Consulting Psychologists Press, Palo Alto, CA. 355</p>
<p class="ref">K. Ericsson, N. Charness, P. Feltovich, and R. Hoffman, editors. 2006. <i>The Cambridge Handbook of Expertise and Expert Performance</i>. Cambridge University Press, New York. 349</p>
<p class="ref">A. Ezen-Can, J. F. Grafsgaard, J. C. Lester, and K. E. Boyer. 2015. Classifying student dialogue acts with multimodal learning analytics. In <i>Proceedings of the Fifth International Conference on Learning Analytics And Knowledge</i>, pp. 280&#8211;289. ACM, New York. DOI: 10.1145/2723576.2723588. 348</p>
<p class="ref">Fraunhofer. 2016. <a href="http://www.iis.fraunhofer.de/en/ff/bsy/tech/bildanalyse/shore-gesichtsdetektion.html">http://www.iis.fraunhofer.de/en/ff/bsy/tech/bildanalyse/shore-gesichtsdetektion.html</a>. Accessed October 22, 2016. 356</p>
<p class="ref">J. H. Friedman. 2002. Stochastic gradient boosting. <i>Computational Statistics &#38; Data Analysis</i>, 38(4):367&#8211;378. DOI: 10.1016/S0167-9473(01)00065-2. 351</p>
<p class="ref">S. Goldin-Meadow, H. Nusbaum, S. Kelly, and S. Wagner. 2001. Explaining math: Gesturing lightens the load. <i>Psychological Science</i>, 12(6):516&#8211;522. DOI: 10.1111/1467-9280.00395. 359</p>
<p class="ref">J. F. Grafsgaard, J. B. Wiggins, K. E. Boyer, E. N. Wiebe, and J. C. Lester. 2013. Automatically recognizing facial expression: Predicting engagement and frustration. <i>Proceedings of the Sixth International Conference on Educational Data Mining</i>, pp. 43&#8211;50. DOI: 10.1109/ACII.2013.33. 355</p>
<p class="ref">J. F. Grafsgaard, J. B. Wiggins, A. K. Vail, K. E. Boyer, E. N. Wiebe, and J. C. Lester. 2014. The additive value of multimodal features for predicting engagement, frustration, and learning during tutoring. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 42&#8211;49. ACM, New York. DOI: 10.1145/2663204.2663264. 352, 354, 355, 356</p>
<p class="ref">N. Hill and W. Schneider. 2006. Brain changes in the development of expertise: Neuroanatomical and neurophysiological evidence about skill-based adaptations. In Ericsson, Charness, Feltovich &#38; Hoffman, editors, <i>The Cambridge Handbook of Expertise and Expert Performance</i>. Cambridge University Press, 37:653&#8211;682. DOI: 10.1017/CBO9780511816796.037. 360</p>
<p class="ref">K. Hinckley. 2017. A background perspective on touch as a multimodal (and multi-sensor) construct. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 1: Foundations, User Modeling, and Common Modality Combinations</i>, Chap 4. Morgan &#38; Claypool Publishers, San Rafael, CA. 332</p>
<p class="ref"><a id="page_369"/>K. James and L. Engelhardt. 2012. The effects of handwriting experience on functional brain development in pre-literate children. <i>Trends in Neuroscience and Education</i>, 1: 32&#8211;42. DOI: 10.1016/j.tine.2012.08.001. 359</p>
<p class="ref">K. James, J. Lester, S. Oviatt, K. Cheng, and D. Schwartz. 2017a. Perspectives on learning with multimodal technologies. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 1: Foundations, User Modeling, and Common Modality Combinations</i>, Chapter 13. Morgan &#38; Claypool Publishers, San Rafael, CA. 365</p>
<p class="ref">K. James, S. Vinci-Booher, and F. Munoz-Rubke. 2017b. The impact of multimodalmultisensory learning on human performance and brain activation patterns. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 1: Foundations, User Modeling, and Common Modality Combinations</i>, Chapter 2. Morgan &#38; Claypool Publishers, San Rafael, CA. DOI: 10.1145/3015783.3015787. 359</p>
<p class="ref">B. Jee, D. Gentner, K. Forbus, B. Sageman, and D. Uttal. 2009. Drawing on experience: Use of sketching to evaluate knowledge of spatial scientific concepts. In <i>Proceedings of the 31st Conference of the Cognitive Science Society</i>, Amsterdam. 350</p>
<p class="ref">B. Jee, D. Gentner, D. Uttal, B. Sageman, K. Forbus, C. Manduca, C. Ormand, T. Shipley, and B. Tikoff. 2014. Drawing on experience: How domain knowledge is reflected in sketches of scientific structures and processes. <i>Research in Science Education</i>, 44(6), 859&#8211;883 Springer, Dordrecht. DOI: 10.1007/s11165-014-9405-2. 350</p>
<p class="ref">A. Jones, A. Kendira, D. Lenne, T. Gidel, and C. Moulin. 2011. The tatin-pic project: A multimodal collaborative work environment for preliminary design. In <i>Proceedings of the Conference on Computer Supported Cooperative Work in Design (CSCWD)</i>, pp. 154&#8211;161. DOI: 10.1109/CSCWD.2011.5960069. 346</p>
<p class="ref">D. Kahneman, P. Slovic, and A. Tversky, editors. 1982. <i>Judgment under Uncertainty: Heuristics and Biases</i>. Cambridge University Press, New York. 364</p>
<p class="ref">M. Kapoor and R. Picard. 2005. Multimodal affect recognition in learning environments. In <i>Proceedings of the ACM Multimedia Conference</i>, ACM, New York. DOI: 10.1145/1101149.1101300. 352, 354</p>
<p class="ref">A. Katsamanis, V. Pitsikalis, S. Theodorakis, and P. Maragos. 2017. Multimodal gesture recognition. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 1: Foundations, User Modeling, and Common Modality Combinations</i>, Chapter 11. Morgan &#38; Claypool Publishers, San Rafael, CA. DOI: 10.1145/3015783.3015796. 332</p>
<p class="ref">R. F. Kizilcec, C. Piech, and E. Schneider. 2013. Deconstructing disengagement: analyzing learner subpopulations in massive open online courses. In <i>Proceedings of the Third International Conference on Learning Analytics and Knowledge</i>, ACM, New York, pp. 170&#8211;179. DOI: 10.1145/2460296.2460330. 337</p>
<p class="ref">S. Kopp and K. Bergmann. 2017. Using cognitive models to understand multimodal processes: The case for speech and gesture production. In S. Oviatt, B. Schuller, <a id="page_370"/>P. Cohen, D. Sonntag, G. Potamianos and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 1: Foundations, User Modeling, and Common Modality Combinations</i>, Chapter 6. Morgan &#38; Claypool Publishers, San Rafael, CA. DOI: 10.1145/3015783.3015791. 332</p>
<p class="ref">K. Kurihara, M. Goto, J. Ogata, Y. Matsusaka, and T. Igarashi. 2007. Presentation sensei: A presentation training system using speech and image processing. In <i>Proceedings of the ACM International Conference on Multimodal Interfaces</i>, pp. 358&#8211;365. ACM, New York. DOI: 10.1145/1322192.1322256. 363</p>
<p class="ref">J. Leitner, J. Powell, P. Brandl, T. Seifried, M. Haller, B, Doray, and P. To. 2009. FLUX- A tilting multi-touch and pen-based surface. In <i>Proceedings of the International Conference on Human Factors in Computing (CHI)</i>. ACM, New York. DOI: 10.1145/1520340.1520459. 346</p>
<p class="ref">C. Leong, L. Chen, G. Feng, C. Lee, and M. Mulholland. 2015. Utilizing depth sensors for analyzing multimodal presentations: Hardware, software and toolkits. <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 547&#8211;556. ACM, New York. DOI: 10.1145/2818346.2830605. 344</p>
<p class="ref">B. Lindblom. 1990. Explaining phonetic variation: A sketch of the H and H theory. In W. Hardcastle and A. Marchal, editors, <i>Speech Production and Speech Modeling</i>, pp. 403&#8211;439. Kluwer, Dordrecht. DOI: 10.1007/978-94-009-2037-8_16. 357</p>
<p class="ref">G. Littlewort, J. Whitehill, T. Wu, I. Fasel, M. Frank, J. Movellan, and M. Bartlett. 2011. The Computer Expression Recognition Toolbox (CERT). In <i>Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition</i>, pp. 298&#8211;305. 355</p>
<p class="ref">M. Longcamp, C. Boucard, J.-C. Gilhodes, J.-L. Anton, M. Roth, B. Nazarian, and J.-L. Velay. 2008. Learning through hand or typewriting influences visual recognition of new graphic shapes: Behavioral and functional imaging evidence. <i>Journal of Cognitive Neuroscience</i>, 20(5):802&#8211;815. DOI: 10.1162/jocn.2008.20504. 359</p>
<p class="ref">A. R. Luria. 1961. <i>The Role of Speech in the Regulation of Normal and Abnormal Behavior</i>. Oxford, Liveright. 338, 359</p>
<p class="ref">L. Martin and D. Schwartz. 2009. Prospective adaptation in the use of external representations. <i>Cognition and Instruction</i>, 27(4):370&#8211;400. DOI: 10.1080/07370000903221775. 350</p>
<p class="ref">R. Martinez Maldonado, Y. Dimitriadis, J. Kay, K. Yacef, and M.-T. Edbauer. 2012. Orchestrating a multi-tabletop classroom: From activity design to enactment and reflection. In <i>Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces</i>, pp. 119&#8211;128. ACM, New York. DOI: 10.1145/2396636.2396655. 346</p>
<p class="ref">R. E. Mayer and R. Moreno. 1998. A split-attention effect in multimedia learning: Evidence for dual processing systems in working memory. <i>Journal of Educational Psychology</i>, 90(2):312&#8211;320. DOI: 10.1037/0022-0663.90.2.312. 358</p>
<p class="ref">L. Morency, J. Whitehill, and J. Movellan. 2010. Monocular head pose estimation using generalized adaptive view-based appearance model. <i>Image and Vision Computing</i>, 28(5):754&#8211;761. DOI: 10.1016/j.imavis.2009.08.004. 356</p>
<p class="ref"><a id="page_371"/>S. Y. Mousavi, R. Low, and J. Sweller. 1995. Reducing cognitive load by mixing auditory and visual presentation modes. <i>Journal of Educational Psychology</i>, 87(2):319&#8211;334. DOI: 10.1037/0022-0663.87.2.319. 358</p>
<p class="ref">K. Nakamura, W-J. Kuo, F. Pegado, L. Cohen, O. Tzeng, and S. Dehaene. 2012. Universal brain systems for recognizing word shapes and handwriting gestures during reading. In <i>Proceedings of the National Academy of Science</i>, 109(50):20762&#8211;20767. DOI: 10.1073/pnas.1217749109. 359</p>
<p class="ref">Y. I. Nakano and R. Ishii. 2010. Estimating user&#8217;s engagement from eye-gaze behaviors in human-agent conversations. In <i>Proceedings of the International Intelligent User Interface Conference</i>, pp. 139&#8211;148. ACM, New York. DOI: 10.1145/1719970.1719990. 354</p>
<p class="ref">A.-T. Nguyen, W. Chen, and M. Rauterberg. 2012. Online feedback system for public speakers. <i>IEEE Symposium on e-Learning, e-Management and e-Services</i>. Citeseer, Malaysia, 46&#8211;51. DOI: 10.1109/IS3e.2012.6414963. 343, 363</p>
<p class="ref">L. S. Nguyen, D. Frauendorfer, M. S. Mast, and D. Gatica-Perez. 2014. Hire me: Computational inference of hirability in employment interviews based on nonverbal behavior. <i>IEEE Transactions on Multimedia</i>, 16(4):1018&#8211;1031. 363</p>
<p class="ref">X. Ochoa. Description of the presentation quality dataset and coded documents, unpublished manuscript. 343</p>
<p class="ref">X. Ochoa, K. Chiluiza, G. M&#233;ndez, G. Luzardo, B. Guam&#225;n, and J. Castells. 2013. Expertise estimation based on simple multimodal features. In <i>Proceedings of the 15th ACM International Conference on Multimodal Interaction</i>, pp. 583&#8211;590. ACM, New York, NY. DOI: 10.1145/2522848.2533789. 350</p>
<p class="ref">S. L. Oviatt. 2013a. Problem solving, domain expertise, and learning: Ground-truth performance results for Math Data Corpus. In <i>International Data-Driven Grand Challenge Workshop on Multimodal Learning Analytics</i>. ACM, New York. DOI: 10.1145/2522848.2533791. 340, 341</p>
<p class="ref">S. Oviatt. 2013b. <i>The Design of Future Educational Interfaces</i>. Routledge Press, New York, NY. DOI: 10.4324/9780203366202. 333, 356</p>
<p class="ref">S. Oviatt. 2017. Theoretical foundations of multimodal interfaces and systems. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 1: Foundations, User Modeling and Common Modality Combinations</i>, Chapter 1. Morgan &#38; Claypool Publishers, San Rafael, CA. 356, 358, 359</p>
<p class="ref">S. L. Oviatt, A. Arthur, Y. Brock, and J. Cohen. 2007. Expressive pen-based interfaces for math education. In Chinn, Erkens and Puntambekar, editors, <i>Proceedings of the Conference on Computer-Supported Collaborative Learning</i>, International Society of the Learning Sciences, 8(2):569&#8211;578. 350, 358</p>
<p class="ref">S. Oviatt and A. Cohen. 2013. Written and multimodal representations as predictors of expertise and problem-solving success in mathematics. In <i>Proceedings of the</i> 15th <a id="page_372"/><i>ACM International Conference on Multimodal Interaction</i>, pp. 599&#8211;606. ACM, New York. DOI: 10.1145/2522848.2533793. 339, 341, 350, 351, 352</p>
<p class="ref">S. L. Oviatt and A. Cohen. 2014. Written activity, representations, and fluency as predictors of domain expertise in mathematics. In <i>Proceedings of the 16th ACM International Conference on Multimodal Interaction</i>. ACM, New York. DOI: 10.1145/2663204.2663245. 352, 353, 357</p>
<p class="ref">S. L. Oviatt, A. O. Cohen, N. Weibel, K. Hang, and K. Thompson. 2014. Multimodal learning analytics data resources: Description of math data corpus and coded documents. In <i>Third International Data-Driven Grand Challenge Workshop on Multimodal Learning Analytics</i>. ACM Press, New York. 341, 361</p>
<p class="ref">S. L. Oviatt and P. R. Cohen. 2015. <i>The Paradigm Shift to Multimodality in Contemporary Computer Interfaces</i>. Human-Centered Interfaces Synthesis series J. Carrol, editor. Morgan &#38; Claypool, San Rafael, CA. DOI: 10.2200/S00636ED1V01Y201503HCI030. 335</p>
<p class="ref">S. L. Oviatt, R. Coulston, and R. Lunsford. 2004. When do we interact multimodally? Cognitive load and multimodal communication patterns. In <i>Proceedings of the International Conference on Multimodal Interfaces</i>, pp. 129&#8211;136. ACM, New York. DOI: 10.1145/1027933.1027957. 358</p>
<p class="ref">S. L. Oviatt, K. Hang, J. Zhou, and F. Chen. 2015. Spoken interruptions signal productive problem solving and domain expertise in mathematics. In <i>Proceedings of the 17th ACM International Conference on Multimodal Interaction</i>. ACM, New York. DOI: 10.1145/2818346.2820743. 350, 351, 353, 358</p>
<p class="ref">S. L. Oviatt, K. Hang, J. Zhou, K. Yu, and F. Chen. Aptil 2016. Dynamic handwriting signal features predict domain expertise. <i>Incaa Designs Technical Report 52016</i>. 349, 350, 353, 360, 361</p>
<p class="ref">C. Peters, S. Asteriadis, and K. Karpouzis. 2010. Investigating shared attention with a virtual agent using a gaze-based interface. <i>Journal on Multimodal User Interfaces</i>, 3(1-2):119&#8211;130. DOI: 10.1007/s12193-009-0029-1. 354</p>
<p class="ref">G. Potamianos, E. Marcheret, Y. Mroueh, V. Goel, A. Koumbaroulis, A. Vartholomaios, and S. Thermos. 2017. Audio and visual modality combination in speech processing applications. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 1: Foundations, User Modeling, and Common Modality Combinations</i>, Chapter 12. Morgan &#38; Claypool Publishers, San Rafael, CA. DOI: 10.1145/3015783.3015797. 332</p>
<p class="ref">A. Purandare and D. Litman. 2008. Content-learning correlations in spoken tutoring dialogs at word, turn, and discourse levels. In <i>Proceedings of the 21st International FLAIRS Conference</i>, Coconut Grove, FL. 349, 358</p>
<p class="ref">P. Qvarfordt. 2017. Gaze-informed multimodal interaction. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 1: Foundations, User Modeling, and Common Modality</i> <a id="page_373"/><i>Combinations</i>, Chapter 9. Morgan &#38; Claypool Publishers, San Rafael, CA. DOI: 10.1145/3015783.3015794. 332</p>
<p class="ref">M. Raca and P. Dillenbourg. 2013. System for assessing classroom attention. In <i>Proceedings of the Third International Conference on Learning Analytics and Knowledge</i>, pp. 265&#8211;269. ACM, New York. DOI: 10.1145/2460296.2460351. 345, 354</p>
<p class="ref">M. Raca and P. Dillenbourg. 2014. Holistic analysis of the classroom. In <i>Proceedings of the ACM International Data-Driven Grand Challenge workshop on Multimodal Learning Analytics</i>, pp. 13&#8211;20. ACM, New York. DOI: 10.1145/2666633.2666636. 346</p>
<p class="ref">C. Rich, B. Ponsler, A. Holroyd, and C. Sidner. 2010. Recognizing engagement in humanrobot interaction. In <i>Proceedings of the International Conference on Human-Robot Interaction</i>, pp, 375&#8211;382. ACM, New York. 354</p>
<p class="ref">J. Sanghvi, A. Pereira, G. Castellano, P. McOwan, I. Leite, and A. Paiva. 2011. Automatic analysis of affective postures and body motion to detect engagement with a game companion. In <i>Proceedings of the International Conference on Human-Robot Interaction</i>. ACM, New York. DOI: 10.1145/1957656.1957781. 354</p>
<p class="ref">J. Schneider, D. B&#246;rner, P. Van Rosmalen, and M. Specht. 2015. Presentation trainer, your public speaking multimodal coach. In <i>Proceedings of the ACM on International Conference on Multimodal Interaction</i>, pp. 539&#8211;546. ACM, New York. DOI: 10.1145/2818346.2830603. 363</p>
<p class="ref">L. M. Schreiber, G. D. Paul, and L. R. Shibley. 2012. The development and test of the public speaking competence rubric. <i>Communication Education</i>. 61(3):205&#8211;233. DOI: 10.1080/03634523.2012.670709. 345</p>
<p class="ref">A. Serrano-Laguna and B. Fern&#225;ndez-Manj&#243;n. 2014. Applying learning analytics to simplify serious games deployment in the classroom. <i>Global Engineering Education Conference (EDUCON)</i>, pp. 872&#8211;877. IEEE, New York. DOI: 10.1109/EDUCON.2014.6826199. 337</p>
<p class="ref">N. Singer. 2014. With tech taking over schools, worries rise. <i>New York Times</i>. September 15, 2014. 363</p>
<p class="ref">S. Tindall-Ford, P. Chandler, and J. Sweller. 1997. When two sensory modes are better than one. <i>Journal of Experimental Psychology: Applied</i>, 3(3):257&#8211;287. DOI: 10.1037/1076-898X.3.4.257. 358</p>
<p class="ref">Visage Technologies. 2016. <a href="http://visagetechnologies.com">http://visagetechnologies.com</a>. Accessed October 22, 2016. 356</p>
<p class="ref">L. Vygotsky. 1962. <i>Thought and Language</i>. MIT Press, Cambridge, MA. (Translated by E. Hanfmann and G. Vakar from 1934 original). 338, 359</p>
<p class="ref">B. Woolf, W. Burleson, I. Arroyo, T. Dragon, D. Cooper, and R. Picard. 2009. Affect-aware tutors: recognising and responding to student affect. <i>International Journal of Learning Technology</i>, 4(3&#8211;4):129&#8211;164. DOI: 10.1504/IJLT.2009.028804. 356</p>
<p class="ref">M. Worsley and P. Blikstein. 2010. Toward the development of learning analytics: Student speech as an automatic and natural form of assessment. <i>Annual Educational Research Association Conference</i>. 349, 350</p>
<p class="ref"><a id="page_374"/>B. Xiao, R. Lunsford, R. Coulston, M. Wesson, and S. L. Oviatt. 2003. Modeling multimodal integration patterns and performance in seniors: Toward adaptive processing of individual differences. In <i>Proceedings of the Fifth International Conference on Multimodal Interfaces (ICMI)</i>. ACM, New York. DOI: 10.1145/958432.958480. 359</p>
<p class="ref">Z. Zeng, M. Pantic, G. Roisman, and T. Huang. 2009. A survey of affect recognition methods: Audio, visual, and spontaneous expressions. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 31:39&#8211;58. DOI: 10.1109/TPAMI.2008.52. 355</p>
<p class="ref">Z. Zhang. 2012. Microsoft Kinect sensor and its effect. <i>IEEE Multimedia</i>, 19(2):4&#8211;10. 343</p>
<p class="ref">J. Zhou, K. Hang, S. Oviatt, K. Yu, and F. Chen. 2014. Combining empirical and machine learning techniques to predict math expertise using pen signal features. In <i>Proceedings of the Second International Data-Driven Grand Challenge Workshop on Multimodal Learning Analytics</i>, pp. 29&#8211;36. ACM, New York. DOI: 10.1145/2666633.2666638. 353, 360</p>
</body>
</html>