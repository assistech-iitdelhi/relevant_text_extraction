<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_457"/>14</p>
<p class="chtitle"><b>Perspectives on Predictive Power of Multimodal Deep Learning: Surprises and Future Directions</b></p>
<p class="chauthor"><b>Samy Bengio, Li Deng, Louis-Philippe Morency, Bj&#246;rn Schuller<sup><a id="rfn1" href="#fn1">1</a></sup></b></p>
<p class="noindent">In this multidisciplinary discussion among experts on the challenge topic of deep learning for multimodal technology, we ask some basic questions. The focus of this discussion is to better understand where Deep Learning will take us in the field of Multimodal-Multisensor Interfaces. Specifically, it will deal with the perspectives on the predictive power of Multimodal Deep Learning, including &#8220;surprises&#8221; and&#8212;as the challenge topic&#8217;s title has it&#8212;future directions on multimodal deep learning.</p>
<p class="indent">In fact, deep learning has sky-rocketed recently all over in Computer Perception and many other Pattern Recognition tasks. Some of the first and most popular deep learning fields of application are closely linked to interaction, such as speech recognition, handwriting recognition, diverse computer vision tasks related to &#8220;seeing&#8221; the user, or user emotion recognition, to name but a few. The experts participating range from the more core machine learning side to audio, speech, and vision or more general multimodal intelligent signal analysis experts in the application domain of interaction.</p>
<p class="indent"><a id="page_458"/>Below are questions that served to focus the discussion by the expert participants, who included:</p>
<p class="hangt">Samy Bengio. Research Scientist in the Google Brain team, Google, USA; Expertise in deep architectures for sequences and other structured objects, understanding training and generalization of deep architectures, and adversarial training with application in image captioning.</p>
<p class="hangt">Li Deng. Chief Scientist of AI, Microsoft, USA (while participating in the panel; moved afterward to be Chief AI Officer at Citadel, USA); Expertise in artificial intelligence, machine learning, mathematical modeling, deep learning, neural networks, big data analytics (with application in speech recognition/understanding), natural language processing, image captioning, and financial and statistical modeling.</p>
<p class="hangt">Louis-Philippe Morency. Associate Professor and director of the Multimodal Communication and Machine Learning Laboratory (MultiComp Lab) at Carnegie Mellon University (CMU), USA; Expertise in multimodal machine learning for modeling of acoustic, visual, and verbal modalities, with an interest in human communication dynamics, and health behavior informatics.</p>
<p class="hangt">Bjorn Schuller. Associate Professor in Machine Learning, Imperial College London, UK and Full Professor and Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany, and CEO audEERING, Germany; Expertise in deep learning, recurrent networks, and end-2-end learning with application in multimodal and multisensorial affective computing and mHealth.</p>
<p class="indentt">Deep Learning is increasingly being used for Multisensorial and Multimodal Interaction. <a href="15_Chapter04.xhtml">Chapter 4</a> by Gil Keren, Amr El-Desoky Mousa, Olivier Pietquin, Stefanos Zafeiriou, and Bj&#246;rn Schuller shows this and presents different deep learning-based fusion mechanisms and representations for different modalities.</p>
<p class="h1-1"><a id="ch14_1"/><b><span class="bg1">14.1</span>&#160;&#160;&#160;&#160;Deep Learning as Catalyst for Scientific Discovery</b></p>
<p class="noindent">A hope coming with advanced machine learning techniques is that they can benefit science on a larger scale as by automated analysis of big data, natural language understanding for automated literature analysis or even curious systems that have intrinsic motivation to discover novel findings.</p>
<p class="h5"><a id="page_459"/><b>Question: In what ways have deep learning techniques shown that they can be a catalyst for scientific discovery?</b></p>
<p class="noindentt"><b>Samy Bengio:</b> Deep learning is mostly a very powerful machine learning approach which has been very successful in the last few years at solving several key problems such as computer vision and speech recognition. This has attracted many more research scientists to explore and learn to use deep learning in their domain of expertise. As more and more people learn to use deep learning and develop novel architectures and algorithms, we see deep learning successfully used in more and more domains. Some examples related to scientific discovery where deep learning has already had an impact in the last few years are: the search for the Higgs boson, better tools for identifying cancers and other diseases in humans, discoveries of new planets, including some earth-sized ones, exploration of new chemicals and pharmaceutical compounds, etc.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Thank you, Samy&#8212;in fact, the number of deep-learning skilled experts is growing impressively. And, indeed, we are witnessing tremendous steps forward in a vast selection of applications thanks to the advances in deep learning and related techniques. It is, however, also in particular interesting to notice that it has been adopted quite early on in interaction-relevant data analysis such as speech recognition [W&#246;llmer et al. 2010], emotion recognition [W&#246;llmer et al. 2008], or multimodal information processing [Ngiam et al. 2011] such as meeting analysis with several users [Reiter et al. 2006]. So, it seems there is also a particular relevance and opportunity for multimodal interfaces.</p>
<p class="noindentt"><b>Louis-Philippe Morency:</b> Deep neural representations were specifically important for multimodal research. They allowed to bring in a similar representation space for modalities that were previously seen as disparate. The natural representation of language is symbolic since each word can be seen as a token or symbol. This is a discrete representation which is quite different from visual images or a speech signal which are usually represented continuously. New neural representations based on the earlier work of distributed hypotheses allowed to create continuous representations of language, in a vectorial form. These word embeddings are an approximation of the semantic content represented the words.</p>
<p class="noindentt"><b>Li Deng:</b> One rising trend of deep learning research is machine reading and comprehension. This technology is expected to become more mature within the next several years, a goal aimed by Microsoft&#8217;s recent push in releasing a comprehensive dataset consisting of 100,000+ real-life questions and answers to bridge human parity in question answering by community efforts. Once successful, greater and <a id="page_460"/>greater scientific literature can be analyzed by deep learning techniques in a way that can help interpret and integrate vast different pieces of scientific discovery that earlier scientists were not able to do. For example, each medical researcher is typically specialized in one or two areas of medicine. Researching, reading, and writing medical literature have been typically confined within his/her limited area(s). AI methods based on deep learning for machine reading and comprehension have no such limitations.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> So computers can soon read and comprehend all papers on deep learning to suggest novel ways or&#8212;write a book like the present one distilling the most important aspects&#8212;exciting futures ahead. Hopefully, they will indeed also understand users in interaction as well as we do&#8212;likely empowered by the current advances in social and emotional intelligence. But how about non-textual information?</p>
<p class="noindentt"><b>Louis-Philippe Morency:</b> A second place where deep neural networks had a significant impact is to represent visual images and objects. Previous visual descriptors were representing low-level aspects of visual images such as edges and gradients. The renewal of Convolutional Neural Networks applied to visual object representations allowed to create higher-level representations which are closer to the semantic levels found in language representations. These new representations for language and vision allowed the expansion of new research topics such as image captions, visual question answering and video description. It is an exciting time for multimodal research.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> You both surely have a point, Li and Louis-Philippe. I find also it particularly interesting in the audio domain, where user profiling just recently went to end-to-end learning from raw data such as speech waveforms thanks to convolutional neural networks [Trigeorgis et al. 2016], and also the physiology domain [Keren et al. 2017], where user state monitoring has been successfully realized by such learning from end-to-end. In fact, one also finds increasingly larger networks to combine these modalities in one larger network with shared layers [Tzirakis et al. 2018]. It will be exciting to see what observations and findings can be found also for other related HCI disciplines, such as psychology, once deep learning empowered human data analysis becomes even more robust.&#8221;</p>
<p class="h1-1"><a id="ch14_2"/><b><span class="bg1">14.2</span></b>&#160;&#160;&#160;&#160;<b>Deep Learning in Relation to Conventional Machine Learning</b></p>
<p class="noindent">Deep learning has increasingly substituted other methods of machine learning such as statistical learning or kernel-based approaches in multimodal and multisensorial interaction tasks and beyond. Naturally, this raises the question how it compares to alternatives. The discussion includes the aspects whether it surpasses <a id="page_461"/>conventional machine learning, and if so, in which ways. Further, there could be potential new opportunities beyond former machine learning algorithms. Finally, one has to carefully consider also potential downsides such as the usual requirement for large amounts of training data or low transparency.</p>
<p class="h5"><b>Question: What are the perspectives on the predictive power of Multimodal Deep Learning, and what new avenues might it open up? How will it potentially surpass other, &#8220;conventional&#8221; methods of Machine Learning for Multimodal Fusion? And, what are the potential downsides of Multimodal Deep Learning in comparison?</b></p>
<p class="noindentt"><b>Louis-Philippe Morency:</b> One of the main strengths of deep neural networks, and by extension for the multimodal deep networks, are their capacities to incorporate information from a large amount of training samples. This is particularly important given the huge amount of multimodal and multimedia content currently available on the Internet. These multimodal deep architectures enable the modeling of multimodal content at large scale. New research will need to focus on learning with weak and noisy labels since most of the content found on the internet is only partially or weakly labeled. An important research direction will be on transfer learning for domains with limited resources. In many applications where we want to model human multimodal interactions, the amount of labeled data is often limited. We need new algorithms able to better deal with partial input observations and noisy output labels.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Yes, labeled multimodal user data is often quite limited, thinking for example of the field of affective computing. Transfer learning is also often easier with (deep) neural networks than with many other conventional learning algorithms. In addition, active learning taking the user herself into the loop seems promising. In addition, mechanisms to map between different representation forms used in different heterogeneous data sources will be crucial. In neural networks, one can co-learn these as different output nodes for different databases, such as an output node set for emotion categories and another set for emotion dimensions. Again, this is often more difficult to realize with conventional learning algorithms.</p>
<p class="noindentt"><b>Louis-Philippe Morency:</b> Multimodal fusion is the process of merging heterogeneous data with the purpose of performing a prediction [Baltru&#353;aitis et al. 2018]. This output prediction can be for discrete labels, as in multi-label classification, or for continuous outputs, as in regression. One of the main advantages of deep neural architecture is their ability to learn better representations. When it comes to making the decision, most recent deep learning approaches simply use a softmax function or similar loss function. We can definitely improve on these simple prediction <a id="page_462"/>mechanisms to enable modeling of the direct interactions between output labels, as is done in structure learning problems. Also, kernel-based approaches, such as multi-kernel learning, allow to model very complex similarity metrics using a relatively small amount of training data. This advantage has not yet been replicated with deep neural networks.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Good point. The ease of choosing task-adapted cost functions is a strong point for deep learning. In our Audio/Visual Emotion Challenge series, we have seen clear gains by participants using the evaluation measure as cost function (concordance correlation coefficient rather than, for example, mean square error or Pearson&#8217;s correlation). But of course, there are more advantages.</p>
<p class="noindentt"><b>Samy Bengio:</b> One important outcome of deep learning is the idea that one can learn to better represent information in a compact, yet useful, way by applying various successive non-linear transformations to the original data (this is also called &#8220;representation learning&#8221;). One crucial aspect of our environment is that concepts appear to us through several modalities (visual, auditory, textual, etc.) but still can be thought of as the same concepts. Deep learning has already shown promises for finding common representations of concepts, irrespective of how they are &#8220;sensed&#8221;: so an image of a dog, the written word &#8220;dog,&#8221; or the audio signal of someone saying &#8220;dog&#8221; should all have a common internal representation related to the concept of dog. Several recent algorithms have shown how deep learning can be used to obtain such representation. Several more recent approaches in generative deep models, such as Generative Adversarial Networks (GANs) [Goodfellow et al. 2014] or deep Autoregressive networks such as PixelCNNs [Oord et al. 2016] can then be used to generate actual data from such representations, which can, for instance, be used to transform the voice of the word &#8220;dog&#8221; into an image of a dog, which opens up a lot of novel applications.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Yes, Samy&#8212;I fully agree that GANs and other ways of fusing analysis and synthesis abilities will be a major further leap forward. And you have also shown that adversarial topologies can scale up [Kurakin et al. 2016]. After all, as humans, we also learn already from few examples, as we can imagine further examples. This seems a great opportunity for deep learning to surpass our former ways of multimodal fusion, as in particular multimodal examples are usually sparser than their unimodal counterparts. In fact, some work such as in [Petridis et al. 2010] has shown how to imagine the &#8220;missing&#8221; modality or features of it. So, will deep learning prevail?</p>
<p class="noindentt"><b>Samy Bengio:</b> As the amount of data and compute power grows, deep architectures promise to become better than most other approaches which cannot use such data and compute power efficiently.</p>
<p class="noindentt"><a id="page_463"/><b>Li Deng:</b> I gave a keynote at the NIPS-2016 Multimodal Machine Learning Workshop on these topics, with the main theme that multimodal signals can be effectively exploited to enhance the predictive power by enriching the supervision signals across different modalities. This type of learning scheme may be called distant-supervised learning. This proposed cross-modality supervision is closer to how a child learns to recognize and understand human speech than the current deep learning approaches to speech recognition. One new avenue that may open up is to change the current paradigm of speech recognition so that we no longer require labeling large amounts of speech data with their corresponding text. Instead, the correspondence can be made between speech and the corresponding images or videos that may be naturally available with abundance. Then, the cost of building high-quality speech recognition systems will be drastically reduced while the performance may be improved as well.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Absolutely, Li. One can see, for example, encoder-decoder architectures being used more and more outside of matching from the same representation to the same, such as in neuro-machine-translation (NMT), where one language is mapped onto another language [Cho et al. 2014]. An impressive example is learning about sounds from &#8220;watching&#8221; unlabeled video [Aytar et al. 2016]. So, what do you think about conventional approaches vs. deep solutions?</p>
<p class="noindentt"><b>Li Deng:</b> &#8220;Conventional&#8221; methods of machine learning based on &#8220;shallow&#8221; models are not powerful enough (that is, lacking sufficient learning capacity and representation power) to enable multimodal fusion and cross-modality supervision. For example, in our earlier work [Lin et al. 2009], we tried very hard to use HMMs (shallow model) to fuse diverse acoustic data from different languages. Very minor improvement was made then. After we introduced deep learning, we used hidden layers in the deep neural net to code the common internal representations of acoustic data from different languages. This way of fusing inverse sets of information with common underlying causes gave us much greater accuracy improvement in multilingual speech recognition (see [Deng et al. 2013, 2014, Huang et al. 2013]). This advantage inherent in deep representations of data can be generalized from multiple languages (with the same acoustic modality in this successful example) to multiple modalities.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Right. However, there are of course also downsides beyond, for example, the requirement for enough training data mentioned above and often longer training times and needed hardware resources.</p>
<p class="noindentt"><b>Li Deng:</b> Apotential downside of multimodal deep learning, as in most (not all) deep learning paradigms, is the nature of &#8220;black box&#8221; or non-interpretability of powerful <a id="page_464"/>prediction results. Much of the on-going work in the deep learning research community is devoted to overcoming this difficulty (e.g., [Lee et al. 2016, Palangi et al. 2018]).</p>
<p class="noindentt"><b>Louis-Philippe Morency:</b> Yes, one of the biggest downside of deep neural networks is their lack of interpretability. Probabilistic graphical models such as Bayesian Networks were praised for their interpretability since they allow for easy analysis of the intermediate states during the decision process. Current methods to interpret deep neural architectures are still quite primitive. Techniques such as T-SNE [Maaten and Hinton 2008] allow you to plot a 2D visualization of a neural layer, but in many cases these visualizations are not sufficient to understand the nature of the intermediate representations. There is a great opportunity for new representation learning approaches that can learn from large amounts of data while at the same time having interpretable intermediate representations.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Yes, &#8220;white-boxing&#8221;, that is, turning black box into white box models is on the way. At the same time, realizing &#8220;responsible&#8221; and &#8220;accountable&#8221; deep learning will be increasingly dominating our efforts&#8212;certainly also for multimodal interaction solutions empowered by deep neural networks. Whether, however, one can ever fully explain a decision made by such a network in a space with potentially millions of free learning parameters and highly nonlinear decision functions learned will remain to be seen. It seems most important to also include the data used for learning a model into the process of shedding light on decisions made by a network&#8212;after all, it learned from that data.</p>
<p class="h1-1"><a id="ch14_3"/><b><span class="bg1">14.3</span>&#160;&#160;&#160;&#160;Expected Surprises of Deep Learning</b></p>
<p class="noindent">While a surprise would not be one if expected, yet, some educated guesses certainly can be made concerning in which ways Deep Learning may be surprising beyond the surprises already seen such as the often enormous boost in recognition performances or opening up of new possibilities such as learning directly from raw data.</p>
<p class="h5"><b>Question: What &#8220;surprises&#8221; might Multimodal Deep Learning bring?</b></p>
<p class="noindentt"><b>Louis-Philippe Morency:</b> Multimodal deep learning has already brought many surprises. One of the most surprising results was the seminal work of Kiros et al. [2014] where they learned coordinated representations between visual objects and words. They showed that by coordinating the language and vision representations it is possible to interact between them. For example, the representation of a red car image can be subtracted with the word embedding of &#8220;red&#8221; and added the embedding <a id="page_465"/>of &#8220;blue&#8221;. The resulting visual representation is closest to blue cars. I expect more surprises in this direction. How can deep multimodal representations model the cross-modal relationships? How can we take advantages of these multimodal representations?</p>
<p class="noindentt"><b>Samy Bengio:</b> Problems that were thought to be hard, like image captioning, were actually surprisingly easy to tackle, at least on the surface. This is most likely because it is actually easier than previously thought to generate sentences that are grammatically correct.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Yes&#8212;very much, and indeed often in surprising leaps forward in terms of accuracy. However, some surprises included also how easily networks can be fooled [Nguyen et al. 2015]. Any surprises to be expected for multimodality in this context?</p>
<p class="noindentt"><b>Samy Bengio:</b> The hope of multimodal deep learning is that having simultaneous access to many modalities should make learning representations more robust. That said, I will be really surprised the day a deep learning model can generate things like humor!</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Well, deep models seem to have improved on recognizing humor [Bertero and Fung 2016], but I am sure we will soon experience more sophisticated humor by computers&#8212;best presented by some multimodal virtual agent? But there are also &#8220;smaller&#8221; and more technical surprises.</p>
<p class="noindentt"><b>Li Deng:</b> Yes, for example, drastic reduction of the labeling cost in large-scale prediction tasks using the current deep learning approaches for supervised learning (see previous discussions).</p>
<p class="h1-1"><a id="ch14_4"/><b><span class="bg1">14.4</span>&#160;&#160;&#160;&#160;The Future of Deep Learning</b></p>
<p class="h5"><b>Question: What are future directions for Multimodal Deep Learning?</b></p>
<p class="noindentt"><b>Samy Bengio:</b> Unsupervised learning is said to be the next frontier in deep learning in general, and probably in multimodal deep learning as well. Can we learn to generate one modality from another when we do not have access to so-called &#8220;parallel corpora&#8221;? For instance, we may have access to a large dataset of unlabeled images, as well as a large dataset of unlabeled speech. Can we still learn a useful joint representation of these concepts?</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Unsupervised learning is one of the major differences to our current and earlier mainly supervised learning approaches in multimodal data analysis. But, in fact, humans learn mostly reinforced rather than entirely unsupervised, that is, some more or less directly related contextual information serves as guidance for <a id="page_466"/>learning. At the moment, it seems that active learning and semi-supervised learning or their combination [Zhang et al. 2015, Wagner et al. 2018] are first more &#8220;timid&#8221; steps before going entirely unsupervised or only reinforced in the field. How do you see this trend?</p>
<p class="noindentt"><b>Louis-Philippe Morency:</b> One future direction for multimodal deep learning is co-learning. How can we use knowledge from one modality to help perform a task in a second modality? This is specifically useful if the first modality has more data than the second modality. An example of this new line of research is zero-shot learning applied to multimodal settings. Researchers started using language datasets as a first modality to help recognize visual objects (second modality) that were not part of the vision training set. I foresee new research pushing this work in a bidirectional collaboration between language and vision where each modality helps the other modality during learning.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Absolutely&#8212;as above, multimodality clearly offers the opportunity to learn from one modality about another. The bottleneck being, obviously, robust processing ability of the guiding modality in the first place. This often requires also reliable measures of confidence&#8212;unfortunately, however, only few works are found on this topic up to now in the field such as in Han et al. [2017]. So, what is needed most importantly to foster higher synergy in deep multimodal learning?</p>
<p class="noindentt"><b>Li Deng:</b> Explore more effective architectures of deep neural nets for fusing multimodal information. The cross-modality supervision discussed above can be viewed as fusion at the output layer; the multilingual speech recognition discussed above can be viewed as fusion at the hidden layers, etc. Many other potentially more effective ways of doing multimodal fusion are possible. Some of them can be inspired by traditional machine learning architectures [Deng and Li 2013].</p>
<p class="noindentt"><b>Samy Bengio:</b> The advent of GANs also opens up many more research directions, where one can now design a training approach without having access to a proper objective function, as most machine learning approaches need, but can go around it by using a game theoretical setting where two deep learning models compete with each other.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> So, learning from one modality about another while being able to synthesize missing information seems to be the most crucial step, but we need to continue finding even more efficient ways of fusion&#8212;including perhaps also transfer learning once new modalities that have never before been seen in combination appear. And, of course, being able to cope well with potential temporal misalignment [Graves et al. 2006, Trigeorgis et al. 2017] and many further ever-present challenges in better ways.</p>
<p class="h1-1"><a id="ch14_5"/><a id="page_467"/><b><span class="bg1">14.5</span>&#160;&#160;&#160;&#160;Responsibility in Deep Learning</b></p>
<p class="noindent">While deep learning helped improve accuracies in manifold if not most tasks, it is frequently blamed as a black-box approach. With millions or even billions of learning parameters in today&#8217;s networks, it seems clear that it is not a trivial task to provide full insight into the often high-dimensional and complex decision spaces. Further, with the new opportunities deep learning opens up, also ethical implications can be expected.</p>
<p class="h5"><b>Question: How transparent is what has been learned by Multimodal Deep Learning? How can we potentially improve on the transparency? What are potential ethical and societal implications?</b></p>
<p class="noindentt"><b>Li Deng:</b> Currently, deep learning results are generally not transparent to human users [Eldar et al. 2017, deng 2018]. One way of improving the transparency is to design special neural cell architectures which explicitly represent some known properties of the input signals. My research group at Microsoft is currently exploring this direction for natural language data in machine reading and comprehension tasks. I imagine similar approaches can be applied to multimodal data where the explicit properties to be made transparent may be even richer than natural language data [He and Deng 2017].</p>
<p class="noindentt"><b>Samy Bengio:</b> Deep learning is a very powerful machine learning approach which relies on multiple successive nonlinear transformations of the data. Even though the result is a powerful representation (very often the best we have found so far, compared to previous approaches), these transformations are often very hard to analyze. The risk is thus that such models may make wrong decisions that could then be hard to justify or even explain. Indeed, these wrong decisions are often simply the result of bad (or incomplete) training data, which can be hard to identify. Examples of such problems are biases in the training data favoring one category of objects (gender bias, ethnic bias, economic bias, etc.).</p>
<p class="noindentt"><b>Louis-Philippe Morency:</b> As multimodal sensing technologies are improving, it becomes important that we think about how they will be used in real-world applications. One important aspect is privacy. I believe that people should be able to own their multimodal data. In other words, multimodal behaviors recognized through multimedia content should only be shared after getting approval from the user. It is true for augmented reality devices (for example, Google Glasses) as well as online multimedia content. By properly managing privacy, we will allow multimodal sensing technologies to help people in many domains, including education and <a id="page_468"/>healthcare. A doctor will be able to teleconference with a patient and gather important multimodal behavior information automatically during the interactions. This has the potential to improve care and reduce treatment costs. These technologies also have potential to change the online learning experience for students from rural regions.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Privacy and ownership of data has clearly moved more into public concern, looking at the recent discussions and trials around social media. However, as mentioned above under 14.2, I think it is the data that a network has been trained on that also can support explanations about why a network made a certain decision. If we want to understand why humans behave a certain way, one indeed also often looks at their education, cultural upbringing or alike which can help better understand.</p>
<p class="noindentt"><b>Samy Bengio:</b> An additional problem is related to adversarial examples: complex models such as deep architectures can be fooled by crafting examples that look like a given class of object to any human eye, but are seen by the deep architecture as a different class. A simple example is a stop sign that has been slightly modified such that a self-driving car would see another indication (&#8220;no right turn on red&#8221;, for instance), while all humans would still see the stop sign. Research into making deep architectures more robust are undergoing, but it remains a difficult problem.</p>
<p class="noindentt"><b>Bj&#246;rn Schuller:</b> Yes, like the example I mentioned in 14.3. So clearly, it is not a trivial task to improve on transparency. At the same time, ethical responsibility will need to be addressed increasingly.</p>
<p class="h1-1"><a id="ch14_6"/><b><span class="bg1">14.6</span>&#160;&#160;&#160;&#160;Conclusion</b></p>
<p class="noindent">This discussion was centered on the predictive power of multimodal deep learning. It inquired on the power of deep architectures, some surprises along the way, and future avenues for even higher efficiency. A major concern, however, was also the transparency of learned models.</p>
<p class="indent">All discussants outlined the power of deep learning techniques. In particular, some see this as also given for multimodal and multisensorial data analysis such as in interaction. Examples of its power were, in particular, named the potential for comprehension (such as of text) and for representation (such as of audio and video data).</p>
<p class="indent">The need of large amounts of training data was repeatedly mentioned by the discussants. Solutions to ease this included transfer learning to exploit knowledge obtained in related tasks and adversarial topologies that are able to self-generate <a id="page_469"/>training data. Another option is unsupervised learning from data without labels. Solutions in between fully supervised and unsupervised learning (as in representation learning, for example, in an end-to-end manner) are active learning, semi-supervised learning (in particular co-training), and reinforced learning. The discussants all consider deep learning superior to conventional methods in several ways. For example, &#8220;shallow&#8221; methods are not efficient in terms of parameter handling, transfer learning is well feasible for neural networks, and learning of related representation forms across heterogeneous databases can be easily approached by multi-target learning. Further, adversarial topologies allow for the generation of training data by a network itself. The major concern named is the lack of transparency.</p>
<p class="indent">As surprises coming from deep learning in the context of multimodal data handling, the participants in the discussion named the ability to learn coordinated representations between visual objects and words as a case giving hope for several further multimodal tasks; further, that problems that were thought to be hard, such as for example image captioning, became surprisingly easy to handle with deep learning; and, finally, the drastic reduction of labeling cost by modern means of deep learning. On the negative end of surprises, how easily networks can be fooled was named.</p>
<p class="indent">The future directions include first unsupervised learning as the next frontier not only for deep learning in general, but also in multimodal deep learning. There, a question would be whether one can learn to generate one modality from another even without &#8220;parallel corpora,&#8221; that is, corpora per modality that are linked in some way. Along the line of unsupervised learning, weakly supervised learning such as by efficient combinations of active and semi-supervised, or even reinforced learning were named. As to semi-supervised learning, co-learning was highlighted, that uses knowledge from modality one to improve on a task in modality two. And, the search for more effective architectures of deep neural nets for fusion of modalities such as by cross-modality supervision seems crucial.</p>
<p class="indent">For improved transparency, avenues exist in the eyes of the discussants, such as designing special neural cell architectures explicitly representing selected known properties of the input signals. However, as deep learning relies on multiple successive, often difficult to analyze nonlinear transformations of the learning data, there is a clear risk that wrong decisions may occur which will not trivially be explained. Several discussants stress the connection to the training data, for example if it is biased or incomplete or even partially erroneous. As to the data, privacy is outlined as an important aspect, and one discussant stresses that users should own their multimodal data. Concluding, the significance of targeting responsibility is emphasized.</p>
<p class="h1"><a id="page_470"/><a id="ch14_7"/><b>References</b></p>
<p class="ref">Y. Aytar, C. Vondrick, and A. Torralba. 2016. Soundnet: Learning sound representations from unlabeled video. In <i>Proceedings Advances in Neural Information Processing Systems</i>, pp. 892&#8211;900. NIPS, Barcelona, Spain. 463</p>
<p class="ref">T. Baltru&#353;aitis, C. Ahuja, and L. P. Morency. 2018. Multimodal machine learning: A survey and taxonomy. In <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. DOI: 10.1109/TPAMI.2018.2798607. 461</p>
<p class="ref">D. Bertero and P. Fung. 2016. A long short-term memory framework for predicting humor in dialogues. In <i>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</i>, pp. 130&#8211;135. San Diego, CA. 465</p>
<p class="ref">K. Cho, B. Van Merri&#235;nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078. 463</p>
<p class="ref">L. Deng, J. Li, J.-T. Huang, K. Yao, D. Yu, F. Seide, M. Seltzer, G. Zweig, X. He, J. Williams, Y. Gong, and A. Acero. 2013. Recent advances in deep learning for speech research at Microsoft. In <i>Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, IEEE, Vancouver, Canada. 463</p>
<p class="ref">L. Deng and X. Li. 2013. Machine learning paradigms for speech recognition: An overview. <i>IEEE Transactions on Audio, Speech, and Language Processing</i>, 21(5): 1060&#8211;1089. DOI: 10.1109/TASL.2013.2244083. 466</p>
<p class="ref">L. Deng and D. Yu. 2014. <i>Deep Learning &#8211; Methods and Applications</i>, NOW Publishers. DOI: 10.1561/2000000039. 463</p>
<p class="ref">L. Deng. 2018. Artificial intelligence in the rising wave of deep learning: The historical path and future outlook. <i>IEEE Signal Processing Magazine</i>, 35(1): 177&#8211;180. 467</p>
<p class="ref">Y. Eldar, A. Hero, L. Deng, J. Fessler, J. Kovacevic, H.V. Poor, and S. Young. 2017. Challenges and open problems in signal processing: Panel Discussion summary from ICASSP 2017. <i>IEEE Signal Processing Magazine</i>, 34(6): 8&#8211;23. DOI: 10.1109/MSP.2017.2743842. 467</p>
<p class="ref">I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, and Y. Bengio. 2014. Generative adversarial nets. In <i>Proceedings of Advances in Neural Information Processing Systems</i>, pp. 2672&#8211;2680. NIPS, Montr&#233;al, Canada. 462</p>
<p class="ref">A. Graves, S. Fern&#225;ndez, F. Gomez, and J. Schmidhuber. 2006. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In <i>Proceedings of the 23rd International Conference on Machine Learning</i>, pp. 369&#8211;376. Pittsburgh, PA. DOI: 10.1145/1143844.1143891. 466</p>
<p class="ref">J. Han, Z. Zhang, M. Schmitt, M. Pantic, and B. Schuller. 2017. From hard to soft: Towards more human-like emotion recognition by modelling the perception uncertainty. In <i>Proceedings of the 2017 ACM Multimedia Conference</i>, pp. 890&#8211;897. ACM, Mountain View, CA. DOI: 10.1145/3123266.3123383. 466</p>
<p class="ref"><a id="page_471"/>X. He and L. Deng. 2017. Deep Learning for image-to-text generation: A technical overview. <i>IEEE Signal Processing Magazine</i>, 34(6): 109&#8211;116. DOI: 10.1109/MSP.2017.2741510. 467</p>
<p class="ref">J.-T. Huang, J. Li, D. Yu, L. Deng, and Y. Gong. 2013. Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In <i>Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, IEEE, Vancouver, Canada. DOI: 10.1109/ICASSP.2013.6639081. 463</p>
<p class="ref">G. Keren, T. Kirschstein, E. Marchi, F. Ringeval, and B. Schuller. 2017. End-to-end learning for dimensional emotion recognition from physiological signals. In <i>Proceedings of the International Conference on Multimedia and Expo (ICME)</i>, pp. 985&#8211;990. IEEE, Hong Kong. DOI: 10.1109/ICME.2017.8019533. 460</p>
<p class="ref">R. Kiros, R. Salakhutdinov, and R. S. Zemel. 2014. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539. 464</p>
<p class="ref">A. Kurakin, I. Goodfellow, and S. Bengio. 2016. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236. 462</p>
<p class="ref">M. Lee, X. He, S. W. Yih, J. Gao, L. Deng, and P. Smolensky. 2016. Reasoning in Vector Space: An Exploratory Study of Question Answering. In <i>Proceedings of the International Conference on Learning Representations (ICLR)</i>, San Juan, Puerto Rico. 464</p>
<p class="ref">H. Lin, L. Deng, D. Yu, Y. Gong, A. Acero, and C.-H. Lee. 2009. A study on multilingual acoustic modeling for large vocabulary ASR. In <i>Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing(ICASSP)</i>, IEEE, Taipei, Taiwan. DOI: 10.1109/ICASSP.2009.4960588. 463</p>
<p class="ref">L. V. D. Maaten and G. Hinton. 2008 Visualizing data using t-SNE. <i>Journal of Machine Learning Research</i>, 9: 2579&#8211;2605. 464</p>
<p class="ref">J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. 2011. Multimodal deep learning. In <i>Proceedings of the 28th international conference on machine learning (ICML)</i>, pp. 689&#8211;696. Bellevue, WA. 459</p>
<p class="ref">A. Nguyen, J. Yosinski, and J. Clune. 2015. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pp. 427&#8211;436. IEEE, Boston, MA. 465</p>
<p class="ref">A. V. D. Oord, N. Kalchbrenner, and K. Kavukcuoglu. 2016. Pixel recurrent neural networks. arXiv preprint arXiv:1601. 06759. 462</p>
<p class="ref">H. Palangi, P. Smolensky, X. He, and L. Deng. 2018. Question-answering with grammatically-interpretable representations. In <i>Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI)</i>, New Orleans, LA. 464</p>
<p class="ref">S. Petridis, A. Asghar, and M. Pantic. 2010. Classifying laughter and speech using audiovisual feature prediction. In <i>Proceedings of the International Conference on Acoustics Speech and Signal Processing (ICASSP)</i>, pp. 5254&#8211;5257. IEEE, Dallas, TX. DOI: 10.1109/ICASSP.2010.5494992. 462</p>
<p class="ref"><a id="page_472"/>S. Reiter, B. Schuller, and G. Rigoll. 2006. A combined LSTM-RNN-HMM-approach for meeting event segmentation and recognition. In <i>Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, IEEE, Toulouse, France. DOI: 10.1109/ICASSP.2006.1660362. 459</p>
<p class="ref">G. Trigeorgis, M. Nicolaou, S. Zafeiriou, and B. Schuller. 2017. Deep canonical time warping for simultaneous alignment and representation learning of sequences. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 40(5): 1128&#8211;1138. DOI: 10.1109/TPAMI.2017.2710047. 466</p>
<p class="ref">G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nicolaou, B. Schuller, and S. Zafeiriou. 2016. Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network. In <i>Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, pp. 5200&#8211;5204. IEEE, Shanghai, P.R. China. DOI: 10.1109/ICASSP.2016.7472669. 460</p>
<p class="ref">P. Tzirakis, S. Zafeiriou, and B. W. Schuller. 2018. End2You &#8211; The imperial toolkit for multimodal profiling by end-to-end learning. arXiv preprint arXiv:1802.01115. 460</p>
<p class="ref">J. Wagner, T. Baur, Y. Zhang, M. F. Valstar, B. Schuller, and E. Andr&#233;. 2018. Applying cooperative machine learning to speed up the annotation of social signals in large multi-modal corpora. arXiv preprint arXiv:1802.02565. 466</p>
<p class="ref">M. W&#246;llmer, F. Eyben, B. Schuller, and G. Rigoll. 2010. Recognition of spontaneous conversational speech using long short-term memory phoneme predictions. In <i>Proceedings of Interspeech</i>, pp. 1946&#8211;1949. ISCA, Makuhari, Japan. 459</p>
<p class="ref">M. W&#246;llmer, F. Eyben, S. Reiter, B. Schuller, C. Cox, E. Douglas-Cowie, and R. Cowie. 2008. Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies. In <i>Proceedings of Interspeech</i>, pp. 597&#8211;600. ISCA, Brisbane, Australia. 459</p>
<p class="ref">Z. Zhang, E. Coutinho, J. Deng, and B. Schuller. 2015. Cooperative learning and its application to emotion recognition from speech. <i>IEEE/ACM Transactions on Audio, Speech and Language Processing</i>, 23(1): 115&#8211;126. DOI: 10.1109/TASLP.2014.2375558.466</p>
<p class="line"/>
<p class="note"><a id="fn1" href="#rfn1">1</a>. All authors contributed equally to this challenge discussion.</p>
</body>
</html>