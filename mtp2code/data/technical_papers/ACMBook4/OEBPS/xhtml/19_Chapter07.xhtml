<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_203"/>7</p>
<p class="chtitle"><b>Multimodal Analysis of Social Signals</b></p>
<p class="chauthor"><b>Alessandro Vinciarelli, Anna Esposito</b></p>
<p class="h1"><a id="ch7_1"/><b><span class="bg1">7.1</span>&#160;&#160;&#160;&#160;Introduction</b></p>
<p class="noindent">One of the earliest books dedicated to the communication between living beings is <i>The Expression of Emotion in Animals and Man</i> by Charles Darwin. The text includes a large number of accurate and vivid descriptions of the way living beings express emotions: &#8220;many kinds of monkeys, when pleased, utter a reiterated sound, clearly analogous to our laughter, often accompanied by vibratory movements of their jaws or lips, with the corners of the mouth drawn backwards and upwards, by the wrinkling of the cheeks, and even by the brightening of the eyes&#8221; [Darwin 1872]. Darwin never uses the word <i>multimodal</i>, but the example above&#8212;and the many similar others the book contains&#8212;makes it clear that the expression of emotions often involves the simultaneous use of multiple communication channels and, correspondingly, the simultaneous stimulation of different senses.</p>
<p class="indent">Approximately 100 years after the seminal insights by Darwin, research in life and human sciences started to adopt the expression <i>multimodal communication</i> to denote the phenomenon above and to investigate its underlying principles and laws [Partan and Marler 1999, Scheffer et al. 1996, Rowe and Guilford 1996]. In parallel, it was observed that multimodality is not a peculiar expression or perception of emotions, but it concerns any interaction between living beings (human-human, animal-animal, or human-animal) [Poggi 2007]. In other words, multimodality plays a major role in the exchange of <i>social signals</i>, i.e., &#8220;acts or structures that influence the behavior or internal state of other individuals&#8221; [Mehu and Scherer 2012], &#8220;communicative or informative signals which &#8230; provide information about social facts&#8221; [Poggi and D&#8217;Errico 2012], or &#8220;actions whose function is to bring about some reaction or to engage in some process&#8221; [Brunet and Cowie 2012].</p>
<div class="box">
<p class="bhead"><a id="page_204"/><b>Glossary</b></p>
<p class="hangbx"><b>Classifier</b>: in pattern recognition and machine learning, it is a function that maps an object of interest (represented through a set of physical measurements called features) into one of the classes or categories such an object can belong to (the number of classes or categories is finite).</p>
<p class="hangbx"><b>Classifier Combination</b>: in pattern recognition and machine learning, it is a body of methodologies aimed at jointly using multiple classifiers to achieve a collective performance higher&#8212;to a statistically significant extent&#8212;than the individual performance of any individual classifier.</p>
<p class="hangbx"><b>Classifier Diversity</b>: in a set of classifiers that are being combined combined, the diversity is the tendency of different classifiers to have different performance in different regions of the input space.</p>
<p class="hangbx"><b>Communication</b>: process between two or more agents aimed at the exchange of information or at the mutual modification of beliefs, shared or individual.</p>
<p class="hangbx"><b>Redundancy</b>: tendency of multiple signals or communication channels to carry the same or widely overlapping information.</p>
<p class="hangbx"><b>Social Signals</b>: constellations of nonverbal behavioural cues aimed at conveying socially relevant information such as attitudes, personality, intentions, etc.</p>
<p class="hangbx"><b>Social Signal Processing</b>: computing domain aimed at modeling, analysis and synthesis of social signals in human-human and human-machine interactions.</p>
</div>
<p class="indent">At the moment, now that machines are powerful enough to deal with human behavior and its subtleties, the interest for multimodal communication has reacheed the computing community [Vinciarelli et al. 2009, 2012]. <a href="#fig7_1">Figure 7.1</a> shows the distribution of the number of computing oriented papers containing the word <i>multimodal</i> in the ACM Digital Library, one of the most important repositories of computing literature. The chart shows that the interest for the topic has been continuously increasing for the last 15 years. According to the latest technology forecasts,<sup><a id="rfn1" href="#fn1">1</a></sup>. socially intelligent technologies&#8212;in particular humanoid robots&#8212;will become a ubiquitous feature of everyday life in the next 20 years. This suggests that the trend of <a href="#fig7_1">Figure 7.1</a> will continue in the foreseeable future.</p>
<div class="cap" id="fig7_1">
<p class="image"><a id="page_205"/><img src="../images/fig7_1.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 7.1</b>&#160;&#160;&#160;&#160;The chart shows the number of papers that the ACM Digital Library returns after submitting the query <i>multimodal</i>. Overall, the number grows continuously since 2000.</p>
</div>
<p class="indent">Overall, the brief outline above shows that multimodality and multimodal communication are concepts that attract attention in communities as diverse as life sciences, computing, and human sciences. However, the exact meaning of the term <i>multimodal</i> is not necessarily the same in all fields. The goal of this chapter is to address, at least in part, such an issue and to show differences and commonalities (if any) behind the use of the word <i>multimodal</i> in the various areas. In particular, this chapter tries to show if concepts originally elaborated in life sciences can be &#8220;translated&#8221; into computational methodologies and, if yes, how.</p>
<p class="indent">The rest of the chapter is organized as follows. <a href="#ch7_2">Section 7.2</a> provides a brief introduction to the concept of multimodality in life and human sciences. <a href="#ch7_3">Section 7.3</a> describes the key methodological issues of multimodal approaches for the analysis of social signals. <a href="#ch7_4">Section 7.4</a> highlights some future perspectives and, finally, <a href="#ch7_5">Section 7.5</a> draws some conclusions.</p>
<p class="h1"><a id="ch7_2"/><b><span class="bg1">7.2</span>&#160;&#160;&#160;&#160;Multimodal Communication in Life and Human Sciences</b></p>
<p class="noindent">According to life sciences, &#8220;Animals communicate with their entire bodies and perceive signals with all available faculties (vision, audition, chemoreception, etc.). To best understand communication, therefore, we must consider the whole animal and all of its sensory emissions and percepts&#8221; [Partan and Marler 2005]. However, what makes communication truly multimodal is not the joint use of multiple modalities, but their integration to achieve communicative effects that cannot be achieved individually by the various modalities involved: &#8220;[the use of multimodal signals] produces unexpected psychological responses &#8230; which remain hidden when the components are presented alone, with the clear implication that the full significance of multicomponent animal signals cannot be understood by investigating components independently&#8221; [Rowe and Guilford 1996]. In other words, communication is multimodal when the effect of multiple modalities is not just the sum of the individual effects achieved with individual modalities.</p>
<div class="cap" id="fig7_2">
<p class="image"><a id="page_206"/><img src="../images/fig7_2.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 7.2</b>&#160;&#160;&#160;&#160;A reproduction of the scheme proposed in [Partan and Marler 2005, Partan and Marler 1999] and it shows the multimodal communication patterns observed in nature.</p>
</div>
<p class="indent"><a href="#fig7_2">Figure 7.2</a> shows a taxonomy of multimodal communication patterns observed in nature. The first important distinction is between patterns based on <i>redundant</i> and <i>non-redundant</i> signals, corresponding to the upper and lower half of the figure, respectively [Partan and Marler 1999]. In the case of redundant signals, the different modalities carry the same information and the main function of multimodality is to ensure that the message reaches the agent who is supposed to perceive it. A typical case in nature is the combination of acoustic signals and movements. The latter ensures that the message can be received even when there is loud noise in the environment, a condition that is frequent in natural settings. Conversely, the acoustic signals ensure that the message can be received even in the absence <a id="page_207"/>of light or in the case of visual occlusion. In the case of non-redundant signals, the main function of multimodality is to transmit a larger amount of information per unit of time. For example, the appropriate combination of pigmentation and chemical signals can discourage predators or attract sexual mates, two functions that require the relevant information to be communicated quickly in order to survive and transmit genes, respectively [Scheffer et al. 1996].</p>
<p class="indent">The second important criterion that informs the taxonomy of <a href="#fig7_2">Figure 7.2</a> is the response of the subject that perceives the multimodal communication pattern [Partan and Marler 2005], whether the response corresponds to a behavioral display or to a correct understanding of the message conveyed by the pattern. In the case of redundant signals, two scenarios are observed, namely <i>equivalence</i> and <i>enhancement</i>. The former corresponds to the case in which the response to the multimodal signal is the same as the responses observed when the unimodal signals are perceived individually. The latter corresponds to the case in which the response is the enhanced version of the one observed when the unimodal signals are perceived individually.</p>
<p class="indent">When the signals are non-redundant, the scenarios observed in nature are four, i.e., <i>independence, dominance, modulation</i>, and <i>emergence</i> (see <a href="#fig7_2">Figure 7.2</a>). Independence means that the response to a multimodal pattern is the mere sum of the responses to the individual signals. According to the principle outlined at the beginning of this section, such a scenario does not even qualify as an example of multimodal communication. The dominance scenario covers those cases where the response is the same as the one that would be observed for one of the unimodal signals in the multimodal pattern. The modulation scenario is similar, but the magnitude of the response changes with respect to the unimodal case. Finally, the emergence scenario corresponds to those cases in which the response is different from those that can be observed when each of the unimodal signals is perceived individually.</p>
<p class="indent">It is not surprising to observe that the first observations on multimodal communication were done by life scientists studying interactions between animals. The reason is that these use a much wider variety of sensory channels than how humans do, including hearing, sight, radar-like receptors of acoustic waves, olfaction, chemoreceptors, etc. [Rowe and Guilford 1996, Scheffer et al. 1996]. When it comes to human-human communication, interaction takes place mainly via speech and visual signals (facial expressions, gestures, posture, mutual distances, etc.). The other channels&#8212;touch, smell and taste&#8212;are used only rarely and only in very specific contexts (e.g., sexual intercourse). For this reason, in the case of humans, multimodality typically means <i>bimodality</i>. In particular, it is possible to distinguish <a id="page_208"/>between <i>micro-bimodality</i> (the combination between speech and movements like those of the lips that are necessary for the very emission of voice and articulation of phonemes) and <i>macro-bimodality</i> (the combination of speech and movements like facial expressions that are not strictly necessary for the emission of speech) [Poggi 2007].</p>
<p class="indent">To the best of our knowledge, no systematic attempts have been done to verify whether the taxonomy of <a href="#fig7_2">Figure 7.2</a> applies to human-human communication. However, a few examples indicate that this is actually the case. A person that attracts the attention of others by shouting and waiving arms is a case of equivalence (the visual signal tries to reach distances that the voice cannot). Similarly, people that say &#8220;No&#8221; while shaking their head enhance their message through the use of two redundant signals. In the case of non-redundant signals (lower half of <a href="#fig7_2">Figure 7.2</a>), a person can manifest aggressiveness through the tone of her voice while showing fear through a defensive body posture, thus fitting the independence scenario. The dominance case applies, e.g., to someone that says to be comfortable while blushing and, hence, is perceived as someone that is actually not comfortable. For what concerns the modulation case, prosody helps to stress and emphasize certain parts of a verbal message, thus achieving a modulation effect. Finally, irony can be considered a case of emergence where, e.g., the verbal and non-verbal components of a message are opposite to one another.</p>
<p class="h1"><a id="ch7_3"/><b><span class="bg1">7.3</span>&#160;&#160;&#160;&#160;Multimodal Analysis of Social Signals</b></p>
<p class="noindent">The taxonomy of <a href="#fig7_2">Figure 7.2</a> does not explain how multimodal communication patterns result into a given response, it simply provides criteria and terminology to describe multimodal communication in rigorous terms. Approaches for multimodal analysis of social signals do not explain the way multimodal signals produce a response either [Vinciarelli et al. 2009, Vinciarelli et al. 2012]. However, computer analysis requires one to express the process in operational terms. If <img src="../images/inline208.png" alt="Image"/> is a multimodal pattern, <img src="../images/inline208_1.png" alt="Image"/> is a vector of physical measurements extracted from modality <i>i</i> (meaning from the data captured with sensor <i>i</i>), and <i>R</i> is the total number of sensors adopted (every modality corresponds to a sensor), there are two approaches to deal with it. The first approach is called <i>early fusion</i> and it consists in concatenating the unimodal <img src="../images/inline208_2.png" alt="Image"/> vectors to obtain a multimodal vector <img src="../images/inline208_3.png" alt="Image"/>. The latter approach can then be fed to any pattern recognition approach to perform classification or regression depending on the problem. In this case, the response to <img src="../images/inline208_3.png" alt="Image"/> will be the output of the particular approach being used (e.g., a probability, a score, a distance, etc.). The second is called <i>late fusion</i> and consists of <a id="page_209"/>adopting a <i>Multiple Classifiers System</i> (MCS), i.e., a combination of several classifiers that deal separately with the various modalities. The output of an MCS is a probability distribution <img src="../images/inline208_4.png" alt="Image"/> is the set of all possible responses. The probability distribution <i>P</i>(<i>&#x03C9;<sub>k</sub>|X</i>) allows one to make a decision about the response to <i>X</i> according to the following rule:</p>
<p class="eqn"><a id="eq7_1"/><img src="../images/eq7_1.png" alt="Image"/></p>
<p class="indent">Early and late fusion are not expected to explain the way living beings respond to multimodal communication patterns. They are simply methodologies that allow a machine to map a multimodal input pattern <i>X</i> into a suitable response <img src="../images/inline209_6.png" alt="Image"/>. In other words, early and late fusion can perhaps reproduce the observations summarized in <a href="#fig7_2">Figure 7.2</a>, but they cannot be considered an explanation or a model of the processes that lead from stimulus to response in living beings.</p>
<p class="indent">The rest of this section focuses on the probabilistic framework underlying MCS and, in particular, it closely follows the approach proposed in Kittler et al. [1998]to estimate <img src="../images/inline209_1.png" alt="Image"/> (the literature does not provide similar frameworks for the early fusion). Such a distribution is difficult to estimate because this requires to knowing the distribution <img src="../images/inline209_2.png" alt="Image"/> which is typically difficult to infer:</p>
<p class="eqn"><a id="eq7_2"/><img src="../images/eq7_2.png" alt="Image"/></p>
<p class="noindent">For this reason, it is common to make independence assumptions that, while making the problem tractable, lead to intuitive combination rules. If the vectors <img src="../images/inline209_3.png" alt="Image"/> are assumed to be statistically independent given <i>&#969;<sub>j</sub></i>, then <img src="../images/inline209_2.png" alt="Image"/> boils down to the following:</p>
<p class="eqn"><a id="eq7_3"/><img src="../images/eq7_3.png" alt="Image"/></p>
<p class="noindent">The main problem with such an approximation is that the posterior probability becomes low even if just one of the terms <img src="../images/inline209_4.png" alt="Image"/> is low. For this reason, the adoption of MCS requires the assumption that the posteriors <img src="../images/inline209_5.png" alt="Image"/> are similar to the a-priori probabilities <i>P</i>(<i>&#969;<sub>k</sub></i>):</p>
<p class="eqn"><a id="eq7_4"/><img src="../images/eq7_4.png" alt="Image"/></p>
<p class="noindent">with &#124;<i>&#948;<sub>ji</sub></i>&#124; &#x003C; &#x003C; 1. This leads to the following:</p>
<p class="eqn"><a id="page_210"/><img src="../images/eq7_5.png" alt="Image"/></p>
<p class="noindent">Given that &#124;<i>&#948;<sub>ji</sub></i>&#124; &#x003C; &#x003C; 1, it is possible to neglect the nonlinear terms of the product appearing in the rightmost term:</p>
<p class="eqn"><a id="eq7_6"/><img src="../images/eq7_6.png" alt="Image"/></p>
<p class="noindent">Following <a href="#eq7_4">Equation (7.4)</a>, the expression of the <i>&#948;j<sub>k</sub></i> is as follows:</p>
<p class="eqn"><a id="eq7_7"/><img src="../images/eq7_7.png" alt="Image"/></p>
<p class="noindent">and by replacing the expression above in <a href="#eq7_6">Equation 7.6</a>, the result is:</p>
<p class="eqn"><a id="eq7_8"/><img src="../images/eq7_8.png" alt="Image"/></p>
<p class="noindent">The expression above is known as <i>Sum Rule</i> and it has the major advantage that <img src="../images/inline210_1.png" alt="Image"/> can be significantly different from zero even if one or more of the probabilities <img src="../images/inline210_1.png" alt="Image"/> are not.</p>
<p class="indent">Consider the following relationship:</p>
<p class="eqn"><a id="eq7_9"/><img src="../images/eq7_9.png" alt="Image"/></p>
<p class="noindent">The indication is that the product rule can be approximated with the minimum of the posteriors, while the sum rule can be approximated with the their maximum. This gives rise to some of the most common practices adopted in the late fusion of classifiers. The first example is the following:</p>
<p class="eqn"><a id="eq7_10"/><img src="../images/eq7_10.png" alt="Image"/></p>
<p class="noindent">When the priors are uniform (a frequent case in practical applications), the above boils down to the following:</p>
<p class="eqn"><a id="eq7_11"/><img src="../images/eq7_11.png" alt="Image"/></p>
<p class="noindent"><a id="page_211"/>also known as the <i>Maximum Rule</i>. If the signals are non-redundant, this rule appears to fit the dominance case in <a href="#fig7_2">Figure 7.2</a>. The response is fully determined by the only signal for which the last equation is satisfied. If the signals are redundant, the rule corresponds to the equivalence scenario when the value of <img src="../images/inline211_1.png" alt="Image"/> is the same for all <i>k</i>. The rule cannot reproduce the enhancement scenario because the response will never be greater than the maximum of <img src="../images/inline211_1.png" alt="Image"/> across all possible <i>j</i> and <i>k</i> values.</p>
<p class="indent">Consider the following:</p>
<p class="eqn"><a id="eq7_12"/><img src="../images/eq7_12.png" alt="Image"/></p>
<p class="noindent">where the last step is possible when the priors <i>P</i>(<i>&#969;<sub>j</sub></i>) are uniform and the <img src="../images/inline209_3.png" alt="Image"/> are assumed to be statistically independent given <i>&#969;<sub>j</sub></i>. According to <a href="#eq7_9">Equation (7.9)</a> the product is bound by the minimum and the decision rule becomes as follows:</p>
<p class="eqn"><a id="eq7_13"/><img src="../images/eq7_13.png" alt="Image"/></p>
<p class="noindent">meaning that the response corresponds to the class for which the minimum of <img src="../images/inline211_1.png" alt="Image"/> across the various modalities is the highest. In this case as well, the combination rule, known as the <i>Minimum Rule</i>, corresponds to the dominance and equivalence scenarii of <a href="#fig7_2">Figure 7.2</a>. In both cases, it is one of the modalities that determines the response.</p>
<p class="indent">When the priors are uniform, the sum rule:</p>
<p class="eqn"><a id="eq7_14"/><img src="../images/eq7_14.png" alt="Image"/></p>
<p class="noindent">boils down to:</p>
<p class="eqn"><a id="eq7_15"/><img src="../images/eq7_15.png" alt="Image"/></p>
<p class="noindent">meaning that the response corresponds to the class <img src="../images/inline209_6.png" alt="Image"/> for which the average <img src="../images/inline211_2.png" alt="Image"/> over the modalities <img src="../images/inline209_3.png" alt="Image"/> is the highest. However, given that the number of modalities is typically limited, it is better to use the median to avoid the effect of outliers:</p>
<p class="eqn"><a id="eq7_16"/><img src="../images/eq7_16.png" alt="Image"/></p>
<p class="noindent">where <i>M<sub>k</sub></i>[.] is the median over <i>k</i>. The above is called the <i>Median Rule</i> and the response is determined by one of the modalities, but it is not possible to know a <a id="page_212"/>priori which one. In this respect, the median rule appears to fit the dominance scenario in the case of non-redundant signals.</p>
<p class="indent">One last rule, the <i>Majority Vote Rule</i>, can be obtained by hardening the expression of the <img src="../images/inline212_1.png" alt="Image"/> as follows:</p>
<p class="eqn"><a id="eq7_17"/><img src="../images/eq7_17.png" alt="Image"/></p>
<p class="noindent">In this case, starting from the Sum Rule with the assumption that the priors are uniform, it is possible to write that:</p>
<p class="eqn"><a id="eq7_18"/><img src="../images/eq7_18.png" alt="Image"/></p>
<p class="noindent">This means that the response is the class that maximizes <img src="../images/inline212_1.png" alt="Image"/> for the largest number of modalities. In other words, the response corresponds to the class with respect to which the input modalities are most redundant.</p>
<p class="h2"><a id="ch7_3_1"/><b><span class="bg2">7.3.1</span>&#160;&#160;&#160;&#160;Classifier Diversity</b></p>
<p class="noindent"><a href="#ch7_3">Section 7.3</a> shows the principles underlying early and late fusion. However, it is not sufficient to combine multiple modalities to improve the effectiveness of a multimodal approach. Unlike in human and life sciences, where all observed multimodal communication patterns appear to have some advantages (see <a href="#ch7_2">Section 7.2</a>), in multimodal Social Signal Processing not all multimodal approaches are effective. In the case of the early fusion, many works show that not all concatenations of features extracted from different modalities lead to performance improvements, e.g., Ramanarayanan et al. [2015] and Subramanian et al. [2013]. Furthermore, feature selection approaches applied to concatenations of unimodal feature vectors do not always retain features coming from all modalities involved. In the case of the late fusion, the combination of classifiers works only when the inputs are <i>diverse</i> [Kuncheva and Whitaker 2003]. However, the literature does not provide clear and consensual definitions of the diversity and no approach for its measurement appears to clearly outperform the others: &#8220;despite the popularity of the term diversity, there is no single definition and measure of it &#8230; none of [its] measures is proven superior to the others and why these diversity measures are useful is still unclear&#8221; [Tang et al. 2006]. Still, the discussions about the topic echo the considerations of life and human sciences about redundancy across signals (see <a href="#ch7_2">Section 7.2</a>). In particular, the diversity is proposed as a criterion to identify the effective classifiers combinations, i.e., those in which the performance of the combination is higher <a id="page_213"/>than the performance of the best individual classifier to a statistically significant extent [Kuncheva and Whitaker 2003, Tang et al. 2006].</p>
<p class="indent">The only point that attracts consensus is that the diversity is a trade-off between two conflicting needs. On the one hand, it is intuitive that the performance of an MCS is higher when the performance of the individual classifiers tends to be higher. On the other hand, the performance of the individual classifiers should not be too high because diverse classifiers are expected to give different responses for the same input (otherwise they are not diverse) and this means that a fraction of the classifiers involved in the same MCS should make mistakes. In other words, MCS and, hence, multimodal approaches are useful only when unimodal approaches are in a certain performance range.</p>
<p class="indent">A few diversity measures are <i>pairwise</i>, i.e., they can be applied to pairs of classifiers (one of the most common cases in the literature). In case an MCS includes more than two classifiers, then the overall diversity is simply the average of all pairwise diversities. When two classifiers are combined and tested over a dataset, they are both right in a fraction <i>r</i><sub>1</sub><i>r</i><sub>2</sub> of the cases, both wrong in a fraction <i>w</i><sub>1</sub><i>w</i><sub>2</sub> of cases, and the first is right while the second is wrong in a fraction <i>r</i><sub>1</sub><i>w</i><sub>2</sub> of the cases and vice versa in a fraction <i>w</i><sub>1</sub><i>r</i><sub>2</sub> of the cases. The four fractions add up to 1: <img src="../images/inline213_1.png" alt="Image"/></p>
<p class="indent">The <i>Q</i> statistic is as follows:</p>
<p class="eqn"><a id="eq7_19"/><img src="../images/eq7_19.png" alt="Image"/></p>
<p class="noindent">it ranges between&#8722;1 (when at least one between <i>r</i><sub>1</sub><i>r</i><sub>2</sub> and <i>w</i><sub>1</sub><i>w</i><sub>2</sub> are null) and 1 (when at least one between <i>r</i><sub>1</sub><i>w</i><sub>2</sub> and <i>w</i><sub>1</sub><i>r</i><sub>2</sub> is null). The diversity is maximum when <i>Q</i> is close to zero, i.e., when the probability of both classifiers giving the same answer or a different answer is roughly the same. In other words, <i>Q</i> tends to be close to zero when the probability of the two classifiers giving a different response is roughly the same as the probability giving the same response. This means that there is equilibrium between the classifiers being redundant (potentially both right) and non-redundant (at least one of them must be wrong).</p>
<p class="indent">A similar principle can be observed in another pairwise measure of the diversity, namely the correlation:</p>
<p class="eqn"><a id="eq7_20"/><img src="../images/eq7_20.png" alt="Image"/></p>
<p class="noindent">It is possible to prove that <i>&#961;</i> and <i>Q</i> always have the same sign and that &#124;<i>&#961;</i>&#124; &#x2264; &#124;<i>Q</i>&#124;. In this case as well, the diversity is maximum when there is equilibrium between the <a id="page_214"/>probability of both classifiers giving the same answer and the two classifiers giving a different answer.</p>
<p class="indent">When it comes to diversity measures that apply to a whole set of classifiers, the <i>Kohavi-Wolpert variance</i> [Kohavi and Wolpert 1996] is one of the most commonly applied. If a MCS includes <i>L</i> classifiers, and <img src="../images/inline214_1.png" alt="Image"/> is the number of these that classify correctly an input <img src="../images/inline214_2.png" alt="Image"/>, then the probabilities of the output being correct or incorrect are as follows:</p>
<p class="eqn"><a id="eq7_21"/><img src="../images/eq7_21.png" alt="Image"/></p>
<p class="noindent">where <i>&#969;<sub>r</sub></i> and <i>&#969;<sub>w</sub></i> correspond to the right and wrong outcome for <img src="../images/inline214_2.png" alt="Image"/>, respectively, and <img src="../images/inline214_3.png" alt="Image"/> is the number of classifiers that give <i>&#x03C9;<sub>r</sub></i> as response. If a training set contains <i>N</i> patterns <img src="../images/inline208_2.png" alt="Image"/>, then the KW variance of the MCS is as follows:</p>
<p class="eqn"><a id="eq7_22"/><img src="../images/eq7_22.png" alt="Image"/></p>
<p class="noindent">If the expression above is maximized with respect a generic <img src="../images/inline214_5.png" alt="Image"/>, the result is that the maximum is achieved when <img src="../images/inline214_5.png" alt="Image"/> = <i>L</i>/2, i.e., when only half of the classifiers map <img src="../images/inline214_4.png" alt="Image"/> into the right response. In this case as well, the maximum diversity is achieved when there is equilibrium between the probability of a classifier in the MCS being right and the probability of it being wrong.</p>
<p class="indent">The literature provides mixed evidence about the relationship between diversity and performance of an MCS [Tang et al. 2006]. The possible explanations are that the current measurements available in the literature are not fully suitable and that the diversity does not depend only on the MCS, but also on the data at disposition. However, there is consensus that the diversity is desirable for an MCS and that there is a difference between diverse and non-diverse ensembles of classifiers. In this respect, the technical literature appears to echo the considerations of human and life sciences about redundancy and non-redundancy in multimodal communication. However, while these latter consider that redundant signals can still be useful in certain cases (see <a href="#ch7_2">Section 7.2</a>), the technical literature indicates that the lack of diversity tends to make the combination of multiple classifiers unuseful.</p>
<p class="h2"><a id="ch7_3_2"/><b><span class="bg2">7.3.2</span>&#160;&#160;&#160;&#160;State-of-the-Art in Multimodal Analysis of Social Signals</b></p>
<p class="noindent">This section surveys the works on multimodal analysis of social signals that were presented at the <i>ACM International Conference on Multimodal Interaction</i> between 2011 and 2015. In particular, the section takes into account works that have been published in the main proceedings of the conference (works presented in workshops, <a id="page_215"/>challenges, demo sessions, and doctoral consortium have not been taken into account). The <i>rationale</i> behind this choice is that the ICMI articles are likely to be representative of the current <i>state-of-the-art</i> as well as of the most recent and emerging trends.</p>
<p class="indent">Overall, an approach for the multimodal analysis of social signals can be described using the following three dimensions.</p>
<p class="bullt">&#8226;&#160;&#160;<b>Extraction of behavioral cues.</b> This dimension includes both the sensors adopted to record people and the cues actually targeted in the work. The two aspects are considered jointly because the choice of the cues determines the choice of the sensors and, conversely, the choice of the sensors makes certain cues detectable and others not.</p>
<p class="bullt">&#8226;&#160;&#160;<b>Inference of social/psychological phenomena or constructs.</b> This dimension includes the methodology adopted to map the cues into the phenomenon or construct of interest (typically based on machine learning or pattern recognition) and the phenomenon or construct itself. The two aspects are considered jointly because the construct determines the choice of the methodology. In particular, dimensional constructs (e.g., personality traits) require the use of regression techniques while cathegorical constructs (e.g., the Bales social roles) require the use of classifiers.</p>
<p class="bullt">&#8226;&#160;&#160;<b>Fusion of multiple modalities.</b> This dimension corresponds to the methodology adopted to combine the multiple modalities and, in particular, whether the methodology is an <i>early</i> or <i>late</i> fusion approach.</p>
<p class="noindentt"><a href="#tab7_1">Table 7.1</a> provides a synopsis of the articles based on the three dimensions above. Given the wide variety of cues targeted in the various works, the table adopts the sensors as a proxy for the type of behavioral cues that are actually extracted in the works. These are available in <a href="#tab7_2">Table 7.2</a> that shows that the cues most commonly detected are prosody (the way people speak) and face or head behavior (gaze contact, facial expressions, and head moevements). Such a distribution confirms the primacy of speech and face in social interactions [Vinciarelli et al. 2009, Vinciarelli et al. 2012]. As a consequence, cameras and microphones are the two sensors most commonly adopted. It is important to observe that speech processing and computer vision have developed a large number of approaches that, while having been designed for other tasks, are suitable for detecting verbal and nonverbal behavioral cues [Vinciarelli et al. 2009, Vinciarelli et al. 2012]. Thus, the use of microphones and cameras allows one to rely on a solid and extensive body of knowledge.</p>
<div class="cap">
<p class="tcaption" id="tab7_1"><a id="page_216"/><b>Table 7.1</b>&#160;&#160;&#160;&#160;The table includes the main works on multimodal analysis of social signals between 2011 and 2015<a id="rt7_1" href="#t7_1"><sup><i>a</i></sup></a></p>
<p class="image"><img src="../images/tab7_1.png" alt="Image"/></p>
</div>
<p class="source"><a id="t7_1" href="#rt7_1"><sup><i>a</i></sup></a> For every article, the table shows whether it uses microphones (A), cameras (V), depth cameras (K), or wearable sensors (W). Furthermore, the table shows whether the fusion is late or early (FA) and whether the use of a multimodal approach improves the performance of the best individual modality (M). Finally, N stands for no and Y stands for yes.</p>
<div class="cap">
<p class="tcaption" id="tab7_2"><a id="page_217"/><b>Table 7.2</b>&#160;&#160;&#160;&#160;The cues most commonly detected in the articles considered in this section</p>
<p class="image"><img src="../images/tab7_2.png" alt="Image"/></p>
</div>
<p class="indent"><a id="page_218"/>The construct most frequently addressed in ICMI works is the personality, at least when it comes to multimodal analysis of social signals (7 works out of the 28 considered in this section). The main probable reason is that personality is predictive of a large number of life aspects, including the ability to establish statisfactory relationships, professional success, tendency to criminal or antisocial behavior, etc. [Ozer and Benet-Martinez 2006]. In this respect, personality appears to be an important target for any technology expected to interact with people. Furthermore, the most important models available in the psychology literature represent personality as a vector in a low-dimensional space, a representation particularly suitable for computer processing.</p>
<p class="indent">When it comes to the combination approach, early fusion appears to be by far the most common and effective approach (22 works out of the 28 considered). This might be surprising because, unlike the case of late fusion (see <a href="#ch7_3">Section 7.3</a>), the literature does not provide a theoretic framework explaining why such a methodology works and how. Furthermore, the concatenation of multiple <img src="../images/inline214_4.png" alt="Image"/> often leads to high-dimensional feature vectors that require the application of dimensionality reduction techniques. A possible explanation is that late fusion requires classifiers that estimate <img src="../images/inline218_1.png" alt="Image"/>, but some of the most effective classification approaches give as output scores that cannot be interpreted as probabilities (e.g., the Support Vector Machines). This can make it difficult to apply late fusion approaches.</p>
<p class="h1"><a id="ch7_4"/><b><span class="bg1">7.4</span>&#160;&#160;&#160;&#160;Next Steps</b></p>
<p class="noindent">Early and late fusion are the established state-of-the-art in the analysis of multimodal social signals, i.e., the two methodologies that are applied in the large majority of the works presented in the literature (see above). Both approaches are known to work well, but more sophisticated approaches are emerging that promise to better account for the properties of multimodal data and, in particular, to better exploit the relationships between multiple modalities, whether these correspond to redundancy or complementarity in conveying a given social signal. The extensive survey proposed by Baltrusaitis et al. [2017] identifies five major issues when it comes to the analysis of multimodal data, namely <i>representation</i> (&#8220;how to represent and summarize multimodal data in a way that exploits the complementarity and redundancy of multiple modalities&#8221;, according to the definition provided in the survey), <i>translation</i> (&#8220;how to translate (map) data from one modality the other&#8221; in the words of the survey), <i>alignment</i> (&#8220;to identify the direct relations between (sub)elements from two or more different modalities&#8221; following the description of the survey), <i>fusion</i> (&#8220;to join information from two or more modalities to perform <a id="page_219"/>a prediction&#8221; in the survey&#8217;s terms) and <i>co-learning</i> (&#8220;transfer knowledge between modalities&#8221; in the definition available in the survey).</p>
<p class="indent">The two aspects more relevant to this chapter are representation and fusion (not to be confused with the meaning that the latter word has in the expressions of early and late fusion), the two main issues that have to be addressed in analyzing automatically social signals. For what concerns the representation, the survey by Baltrusaitis et al. [2017] proposes a distinction between <i>joint</i> and <i>coordinated</i> representations. In the first case, the multimodal representation <img src="../images/inline219_1.png" alt="Image"/> can be expressed as a function of unimodal representations <img src="../images/inline219_2.png" alt="Image"/>, where <i>i</i> = 1,&#8230;, <i>R</i> and <i>R</i> is total number of modalities involved: <img src="../images/inline219_3.png" alt="Image"/>. The early fusion described earlier in the chapter&#8212;the simple concatenation of the unimodal feature vectors&#8212;is a simple form of joint representation. In the second case&#8212;the coordinated representation&#8212;the approach is to use an individual mapping function <img src="../images/inline219_4.png" alt="Image"/> for every individual unimodal vector with the constraint that <img src="../images/inline219_5.png" alt="Image"/>, where the symbol ~ accounts for a relationship like, e.g., the maximization of the correlation. For what concerns the fusion, the survey describes a large number of approaches that range from graphical models&#8212;joint probability distributions defined over sets of random variables&#8212;to deep neural networks (the interested reader can refer to Baltrusaitis et al. [2017] for more details). The adoption of these most recent approaches promises to change the way multimodal analysis of social signals has been performed so far and, hopefully, to improve the performances achieved so far.</p>
<p class="indent">Another important issue that, to the best of our knowledge, has been addressed only to a limited extent is whether it is possible to detect constellations of nonverbal behavioral cues through unsupervised approaches, i.e., whether it is possible to understand the way people convey social and psychological information by combining multiple cues&#8212;typically belonging to different modalities&#8212;into one individual social signal. One possible approach to address such a task is the application of topic models or co-clustering approaches capable to detect and model frequent associations of multiple cues. Topic models (see a recent survey by Blei [2012]) have been developed for texts, but they can be applied to any type of data that can be represented with vectors where every component accounts for how frequently a given <i>word</i> (a cue in the case of behavior) has been observed. Co-clustering &#8220;is the simultaneous clustering of both points and their attributes&#8221; [Berkhin 2006] and it can help in addressing those application domains where people with different characteristics are expected to display different behavioral cues. A typical example is psychiatry, where pathologies are often represented in terms of <i>Homeostatic Property Clusters</i> [Zachar 2014], i.e., groups of individuals that tend to manifest the same <a id="page_220"/>behavioral symptoms (e.g., the tendency to avoid direct gaze in the case of autistic individuals). The main advantage of unsupervised approaches in these cases is that they might be capable to detect combinations of cues that are not easy to grasp for human observers (in the case of psychiatry, clinical observation is still the main approach toward the identification of the behavioral cues that characterize a pathology).</p>
<p class="indent">A last important issue to take into account is the explanatory power of the approaches adopted in the multimodal analysis of social signals. As it is a multidisciplinary field, Social Signal Processing does not simply target the performance of an approach&#8212;e.g., in terms of correct percentage of times a given social signal is interpreted correctly&#8212;it also tries to provide an insight about the way a given social signal is used and why. In particular, Social Signal Processing tries to provide human sciences researchers with alternative approaches to formulate their findings about the way people behave. One common approach is to use feature selection approaches to identify the features that perform better in inferring a given psychological phenomenon from behavioral observations. Compared with the methodologies that the psychologists adopt, such an approach considers subsets of features rather than individual features and, hence, it better accounts for interactions between multiple features. Another approach is to use the parameters of the models to obtain insights about what are the cues other low-level behavioral observations that better explain the outcome of a classification and, indirectly, better explain the phenomenon that is the target of the classification.</p>
<p class="h1"><a id="ch7_5"/><b><span class="bg1">7.5</span>&#160;&#160;&#160;&#160;Conclusions</b></p>
<p class="noindent">This chapter revolves around multimodal communication between living beings and shows how such a phenomenon is addressed in life sciences and computing technology. <a href="#ch7_2">Section 7.2</a> shows how life sciences have organized the multimodal communication patterns observed in nature into a coherent taxonomy. <a href="#ch7_3">Section 7.3</a> introduces early and late fusion, the two main approaches aimed at dealing with multimodal stimuli. In addition, the section includes a brief meta-analysis of the works on multimodal analysis of social signals that have been presented at the ICMI between 2011 and 2015. Finally, <a href="#ch7_4">Section 7.4</a> outlines some future perspectives.</p>
<p class="indent">Studies on multimodal communication in life sciences and computing have been conducted, presumably, with limited or no mutual influence at all. However, there are interesting parallels between the two areas. The first is that both areas focus on the response that a system gives to multiple stimuli known to account <a id="page_221"/>for the same input information. The second is that both areas recognize the redundancy across multiple modalities as a crucial issue and as a criterion to distinguish between different cases. The main difference is that nature, the subject of life sciences, appears to take advantage of all possible patterns of multimodal communication&#8212;whether they involve redundancy across modalities or not and whether the response is different from the one obtained with individual modalities or not&#8212;while technology appears to benefit only of those cases where the redundancy is limited and the most desirable response is enhanced with respect to the use of individual modalities. However, it is important to observe that parallels and similarities do not mean that the methodologies dealing with multimodal data are a model or an explanation of the way living beings deal with multisensory inputs.</p>
<p class="indent">An interesting aspect of computing technologies is the need to express every process in operational terms, i.e., in terms suitable for the implementation of a computer program. This has led to the development of the computational frameworks described in <a href="#ch7_3">Section 7.3</a> and, more extensively, in Kittler et al. [1998]. Furthermore, it has led to the definition of the <i>diversity</i>, a property that a multimodal approach is expected to have in order to work properly. Intuitively, the diversity is a measure of the lack of redundancy across modalities and this leads to an interesting difference between multimodality in nature and multimodality in machines. In the former case, redundancy can still be useful when one of the modalities can face obstacles in conveying its message (e.g., acoustic signals in a noisy environment). In the latter case, redundant modalities are never useful unless it is possible to know which one should be trusted in case the others fail due to noise or other technical problems. Last, but not least, a multimodal approach is considered to be successful when its performance is higher, to a statistically significant extent, than the performance achieved with the best individual modality. This means that, unlike in nature, if there is a modality that performs above a certain threshold, multimodality becomes unnecessary. In other words, multimodality appears to be an advantage only when individual modalities do not work well enough.</p>
<p class="indent">The meta-analysis of the works presented at ICMI between 2011 and 2015 shows that the large majority of the proposed approaches adopts an early fusion approach. In most cases, multimodal approaches based on different modalities lead to an improvement with respect to the best individual modality. In other words, multimodal approaches appear to be particularly suitable for the analysis of social signals that, both for their inherent ambiguity and the technical difficulties involved in their detection, cannot be detected and interpreted individually with high performance.</p>
<p class="h1n"><a id="page_222"/><a id="ch7_6"/><b>Focus Questions</b></p>
<p class="noindent"><b>7.1.</b> What does it mean that communication is multimodal in both human-human and human-machine interactions?</p>
<p class="noindentt"><b>7.2.</b> What does it mean that an automatic system for the analysis of social signals is multimodal?</p>
<p class="noindentt"><b>7.3.</b> Is redundancy across multiple modalities a problem? And does the answer depend on whether the subject is the communication between living beings or the automatic analysis of social signals?</p>
<p class="noindentt"><b>7.4.</b> What are the conditions for a combination of classifiers being effective?</p>
<p class="noindentt"><b>7.5.</b> Is it an advantage to adopt multimodal approaches for the automatic analysis of social signals? If yes, when?</p>
<p class="noindentt"><b>7.6.</b> What is the diversity in a set of classifiers being combined? And is it possible to measure the diversity?</p>
<p class="noindentt"><b>7.7.</b> What are the main modalities adopted in the automatic analysis of social signals?</p>
<p class="h1n"><a id="ch7_7"/><b>References</b></p>
<p class="ref">O. Aran and D. Gatica-Perez. 2013. One of a kind: Inferring personality impressions in meetings. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 11&#8211;18. DOI: 10.1145/2522848.2522859. 216, 217</p>
<p class="ref">T. Baltrusaitis, C. Ahuja, and L.-P. Morency. 2017. Multimodal machine learning: A survey and taxonomy. Technical report, arXiv. <a href="http://arxiv.org/abs/1705.09406">http://arxiv.org/abs/1705.09406</a>. 218, 219</p>
<p class="ref">L. Batrinca, B. Lepri, N. Mana, and F. Pianesi. 2012. Multimodal recognition of personality traits in human-computer collaborative tasks. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 39&#8211;46. DOI: 10.1145/2388676.2388687. 216, 217</p>
<p class="ref">L. M. Batrinca, N. Mana, B. Lepri, F. Pianesi, and N. Sebe. 2011. Please, tell me about yourself: Automatic personality assessment using short self-presentations. In <i>Proceedings of the 13th International Conference on Multimodal Interfaces</i>, pp. 255&#8211;262. 216, 217</p>
<p class="ref">P. Berkhin. 2006. A survey of clustering data mining techniques. In J. Kogan, C. Nicholas, and M. Teboulle, editors, <i>Grouping Multidimensional Data</i>, pp. 25&#8211;72. Springer Verlag. DOI: 10.1007/3-540-28349-8_2. 219</p>
<p class="ref">J.-I.-Isaac Biel, V. Tsiminaki, J. Dines, and D. Gatica-Perez. 2013. Hi youtube!: Personality impressions and verbal content in social video. In <i>Proceedings of ACM International Conference on Multimodal Interaction</i>, pp. 119&#8211;126, 2013. DOI: 10.1145/2522848.2522877. 216, 217</p>
<p class="ref"><a id="page_223"/>D.M. Blei. 2012. Probabilistic topic models. <i>Communications of ACM</i>, 55(4): 77&#8211;84. DOI: 10.1145/2133806.2133826. 219</p>
<p class="ref">P. Brunet and R. Cowie. 2012. Towards a conceptual framework of research on Social Signal Processing. <i>Journal of Multimodal User Interfaces</i>, 6(3&#8211;4): 101&#8211;115. DOI: 10.1007/s12193-012-0092-x. 203</p>
<p class="ref">M. Chatterjee, S. Park, L.-P. Morency, and S. Scherer. 2015. Combining two perspectives on classifying multimodal data for recognizing speaker traits. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 7&#8211;14. DOI: 10.1145/2818346.2820747. 216, 217</p>
<p class="ref">L. Chen, G. Feng, J. Joe, C. W. Wee Leong, C. Kitchen, and C. M. Lee. 2014. Towards automated assessment of public speaking skills using multimodal cues. In <i>Proceedings of the 16th International Conference on Multimodal Interaction</i>, pp. 200&#8211;203. DOI: 10.1145/2663204.2663265. 216, 217</p>
<p class="ref">K. Curtis, G. J. F. Jones, and N. Campbell. 2015. Effects of good speaking techniques on audience engagement. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 35&#8211;42. DOI: 10.1145/2818346.2820766. 216, 217</p>
<p class="ref">C. Darwin. 1872 <i>The expression of emotion in animals and man</i>. John Murray. 203</p>
<p class="ref">E. Delaherche and M. Chetouani. 2011. Characterization of coordination in an imitation task: Human evaluation and automatically computable cues. In <i>Proceedings of the 13th International Conference on Multimodal Interfaces</i>, pp. 343&#8211;350. DOI: 10.1145/2070481.2070546. 216, 217</p>
<p class="ref">S. Demyanov, J. Bailey, K. Ramamohanarao, and C. Leckie. 2015. Detection of deception in the mafia party game. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 335&#8211;342. DOI: 10.1145/2818346.2820745. 216, 217</p>
<p class="ref">H. Dibeklio&#287;lu, Z. Hammal, Y. Yang, and J. F. Cohn. 2015. Multimodal detection of depression in clinical interviews. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 307&#8211;310. DOI: 10.1145/2818346.2820776. 216, 217</p>
<p class="ref">S. Ghosh, M. Chatterjee, and L.-P. Morency. 2014. A multimodal context-based approach for distress assessment. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 240&#8211;246. DOI: 10.1145/2663204.2663274. 216, 217</p>
<p class="ref">J. F. Grafsgaard, J. B. Wiggins, A. K. Vail, K. E. Boyer, E. N. Wiebe, and J. C. Lester. 2014. The additive value of multimodal features for predicting engagement, frustration, and learning during tutoring. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 42&#8211;49. DOI: 0.1145/2663204.2663264. 216, 217</p>
<p class="ref">H. Hung and B. Kr&#246;se. 2011. Detecting F-formations as dominant sets. In <i>Proceedings of the International Conference on Multimodal Interfaces</i>, pp. 231&#8211;238. DOI: 10.1145/2070481.2070525. 216, 217</p>
<p class="ref">K. Kalimeri, B. Lepri, and F. Pianesi. 2013. Going beyond traits: Multimodal classification of personality states in the wild. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 27&#8211;34. DOI: 10.1145/2522848.2522878. 216, 217</p>
<p class="ref"><a id="page_224"/>J. Kittler, M. Hatef, R. P. W. Duin, and J. Matas. 1998. On combining classifiers. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 20(3): 226&#8211;239. DOI: 10.1109/34.667881. 209, 221</p>
<p class="ref">R. Kohavi and D. Wolpert. 1996. Bias plus variance decomposition for zero-one loss functions. In <i>Proceedings of the International Conference on Machine Learning</i>, pp. 275&#8211;83. 214</p>
<p class="ref">L. I. Kuncheva and C. J. Whitaker. 2003. Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy. <i>Machine Learning</i>, 51(2): 181&#8211;207. DOI: 10.1023/A:1022859003006. 212, 213</p>
<p class="ref">M. Mehu and K. Scherer. 2012. A psycho-ethological approach to Social Signal Processing. <i>Cognitive Processing</i>, 13(2): 397&#8211;414. DOI: 10.1007/s10339-012-0435-2. 203</p>
<p class="ref">G. Mohammadi, S. Park, K. Sagae, A. Vinciarelli, and L.-P. Morency. 2013. Who is persuasive?: The role of perceived personality and communication modality in social multimedia. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 19&#8211;26. DOI: 10.1145/2522848.2522857. 216, 217</p>
<p class="ref">Y. Nakano and Y. Fukuhara. 2012. Estimating conversational dominance in multiparty interaction. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 77&#8211;84. DOI: 10.1145/2388676.2388699. 216, 217</p>
<p class="ref">L. S. Nguyen and D. Gatica-Perez. 2015. I would hire you in a minute: Thin slices of nonverbal behavior in job interviews. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 51&#8211;58. DOI: 10.1145/2818346.2820760. 216, 217</p>
<p class="ref">F. Nihei, Y. I. Nakano, Y. Hayashi, H.-H. Hung, and S. Okada. 2014. Predicting influential statements in group discussions using speech and head motion information. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 136&#8211;143. DOI: 10.1145/2663204.2663248. 216, 217</p>
<p class="ref">D. J. Ozer and V. Benet-Martinez. 2006. Personality and the prediction of consequential outcomes. <i>Annual Reviews of Psychology</i>, 57:401&#8211;421. DOI: 10.1146/annurev.psych.57.102904.190127. 218</p>
<p class="ref">S. Park, H. S. Shim, M. Chatterjee, K. Sagae, and L.-P. Morency. 2014. Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 50&#8211;57. DOI: 10.1145/2663204.2663260. 216, 217</p>
<p class="ref">S. R. Partan and P. Marler. 1999. Communication goes multimodal. <i>Science</i>, 283(5406): 1272&#8211;1273. DOI: 10.1126/science.283.5406.1272. 203, 206</p>
<p class="ref">S. R. Partan and P. Marler. 2005. Issues in the classification of multimodal communication signals. <i>The American Naturalist</i>, 166(2): 231&#8211;245. DOI: 10.1086/431246. 205, 206, 207</p>
<p class="ref">I. Poggi. 2007. <i>Mind, Hands, Face and Body. A Goal and Belief View of Multimodal Communication</i>. Weidler. 203, 208</p>
<p class="ref"><a id="page_225"/>I. Poggi and F. D&#8217;Errico. 2012. Social Signals: a framework in terms of goals and beliefs. <i>Cognitive Processing</i>, 13(2): 427&#8211;445. DOI: 10.1007/s10339-012-0512-6. 203</p>
<p class="ref">N. Raiman, H. Hung, and G. Englebienne. 2011. Move, and i will tell you who you are: Detecting deceptive roles in low-quality data. In <i>Proceedings of the 13th International Conference on Multimodal Interfaces</i>, pp. 201&#8211;204. 216, 217</p>
<p class="ref">V. Ramanarayanan, C. W. Leong, L. Chen, G. Feng, and D. Suendermann-Oeft. 2015. Evaluating speech, face, emotion and body movement time-series features for automated multimodal presentation scoring. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 23&#8211;30. DOI: 10.1145/2818346.2820765. 212, 216, 217</p>
<p class="ref">C. Rowe and T. Guilford. 1996 Hidden colour aversions in domestic chicks triggered by pyrazine odours of insect warning displays. <i>Nature</i>, 383(6600): 520&#8211;522. DOI: 10.1038/383520a0. 203, 206, 207</p>
<p class="ref">F. A. Salim, F. Haider, O. Conlan, S. Luz, and N. Campbell. 2015. Analyzing multimodality of video for user engagement assessment. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 287&#8211;290. DOI: 10.1145/2818346.2820775. 216, 217</p>
<p class="ref">S. J. Scheffer, G. W. Uetz, and G. E. Stratton. 1996. Sexual selection, male morphology, and the efficacy of courtship signalling in two wolf spiders (araneae: Lycosidae). <i>Behavioral Ecology and Sociobiology</i>, 38(1): 17&#8211;23. DOI: 10.1007/s002650050212. 203, 207</p>
<p class="ref">S. Scherer, G. Stratou, and L.-P. Morency. 2013. Audiovisual behavior descriptors for depression assessment. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 135&#8211;140. DOI: 10.1145/2522848.2522886. 216, 217</p>
<p class="ref">B. Siddiquie, D. Chisholm, and A. Divakaran. 2015. Exploiting multimodal affect and semantics to identify politically persuasive web videos. In <i>Proceedings of the 2015 ACM International Conference on Multimodal Interaction</i>, pp. 203&#8211;210. DOI: 10.1145/2818346.2820732. 216, 217</p>
<p class="ref">S. Strohkorb, I. Leite, N. Warren, and B. Scassellati. 2015. Classification of children&#8217;s social dominance in group interactions with robots. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 227&#8211;234. DOI: 10.1145/2818346.2820735. 216, 217</p>
<p class="ref">R. Subramanian, Y. Yan, J. Staiano, O. Lanz, and N. Sebe. 2013. On the relationship between head pose, social attention and personality prediction for unstructured and dynamic group interactions. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 3&#8211;10. DOI: 10.1145/2522848.2522862. 212, 216, 217</p>
<p class="ref">E. K. Tang, P. N. Sugnathan, and X. Yao. 2006. An analysis of diversity measures. <i>Machine Learning</i>, 65(5): 247&#8211;271. DOI: 10.1007/s10994-006-9449-2. 212, 213, 214</p>
<p class="ref">A.K. Vail, J. F. Grafsgaard, J. B. Wiggins, J. C. Lester, and K. E. Boyer. 2014. Predicting learning and engagement in tutorial dialogue: A personality-based model. In <i>Proceedings of <a id="page_226"/>the ACM International Conference on Multimodal Interaction</i>, pp. 255&#8211;262. DOI: 10.1145/2663204.2663276. 216, 217</p>
<p class="ref">A. Vinciarelli, M. Pantic, and H. Bourlard. 2009. Social Signal Processing: Survey of an emerging domain. <i>Image and Vision Computing Journal</i>, 27(12): 1743&#8211;1759. DOI: 10.1016/j.imavis.2008.11.007. 204, 208, 215, 229, 517, 518</p>
<p class="ref">A. Vinciarelli, M. Pantic, D. Heylen, C. Pelachaud, I. Poggi, F. D&#8217;Errico, and M. Schroeder. 2012. Bridging the gap between social animal and unsocial machine: A survey of Social Signal Processing. <i>IEEE Transactions on Affective Computing</i>, 3(1): 69&#8211;87. DOI: 10.1109/T-AFFC.2011.27. 204, 208, 215</p>
<p class="ref">T. W&#246;rtwein, M. Chollet, B. Schauerte, L.-P. Morency, R. Stiefelhagen, and S. Scherer. 2015. Multimodal public speaking performance assessment. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, pp. 43&#8211;50. DOI: 10.1145/2818346.2820762. 216, 217</p>
<p class="ref">P. Zachar. 2014. Beyond natural kinds: Toward a &#8220;relevant&#8221; &#8220;scientific&#8221; taxonomy in psychiatry. In H. Kincaid and J. A. Sullivan, editors, <i>Classifying Psychopathology</i>, pp. 75&#8211;104. MIT Press. DOI: 10.1002/wps.20240. 219</p>
<p class="line"/>
<p class="note"><a id="fn1" href="#rfn1">1</a>. According to Tractica, &#8220;annual robot unit shipments will increase from 8.8 million in 2015 to 61.4 million by 2020, with more than half the volume in that year coming from consumer robots,&#8221; i.e., robots that integrate the everyday life of their users (<a href="http://www.tractica.com/newsroom/press-releases/global-robotics-industry-to-surpass-151-billion-by-2020/">http://www.tractica.com/newsroom/press-releases/global-robotics-industry-to-surpass-151-billion-by-2020/</a>)</p>
</body>
</html>