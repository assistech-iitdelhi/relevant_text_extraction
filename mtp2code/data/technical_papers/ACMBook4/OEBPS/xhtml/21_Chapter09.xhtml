<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_263"/>9</p>
<p class="chtitle"><b>How Do Users Perceive Multimodal Expressions of Affects?</b></p>
<p class="chauthor"><b>Jean-Claude Martin, C&#233;line Clavel, Matthieu Courgeon, Mehdi Ammi, Michel-Ange Amorim, Yacine Tsalamlal, Yoren Gaffary</b></p>
<p class="h1"><a id="ch9_1"/><b><span class="bg1">9.1</span>&#160;&#160;&#160;&#160;Introduction</b></p>
<p class="noindent">In their guidelines for multimodal user interface design [Reeves et al. 2004], researchers point that designers of multimodal interfaces need to determine how to support intuitive, streamlined interactions based on users&#8217; human information processing abilities. One main issue in multimodal interface design is to select appropriate combinations of modalities (e.g., complementarity, redundancy) [Martin 1998]. For example, in their study about the effects of speech-gesture cooperation in animated agents&#8217; behavior in multimedia presentations, Buisine and Martin [2007] explored the impact of different types of speech&#8211;gesture cooperation in agents&#8217; behavior: redundancy (gestures duplicate pieces of information conveyed by speech), complementarity (distribution of information across speech and gestures), and a control condition in which gesture does not convey semantic information. Fifty-four male and 54 female users attended 3 short presentations performed by different animated agents, recalled the content of presentations, and evaluated both the presentations and the agents. Although speech&#8211;gesture cooperation was not consciously perceived, it proved to influence users&#8217; recall performance and subjective evaluations: redundancy increased verbal information recall, ratings of the quality of explanation, and expressiveness of agents. Redundancy also resulted in higher <a id="page_264"/>likeability scores for the agents and a more positive perception of their personality. The same authors also analyzed the impact of the personality of the participants on their perception of the animated agent [Buisine and Martin 2010]. They observed a subtle interaction between personality and gender: all users but female introverts were influenced by the virtual tutor&#8217;s behavioral strategy. Speech-gesture redundancy in the tutor&#8217;s behavior improved recall of the lesson as well as subjective evaluation from the tutees (likeability, expressiveness and personality of the animated agent). Redundancy also improved both the effectiveness of the system (higher performance from users) and the social perception of animated agents, since those with redundant behavior appeared more likeable and their personality more positive. The benefits of redundancy were found on extraverts and male introverts, but female introverts were not influenced by animated agents&#8217; multimodal behavior.</p>
<p class="indent">Due to the emergence of Affective Computing, such animated agents are now able to express affects via human-like modalities. For example, they can express affects using expressive speech, as well as through their facial expressions, hand gestures, body postures, and movements [Courgeon et al. 2014, Niewiadomski et al. 2013]. Even the background graphics (e.g., a violent scene eliciting fear) displayed on the screen can be used to convey affective signals that can be in/congruent with the facial expressions of a virtual character displayed in the foreground. Advanced mechanical systems are even able to express affects using haptics [Tsalamlal et al. 2017]. Yet, only a few systems are able to intelligently combine these expressions of affects across these modalities, since little is known as to how users will perceive these complex multimodal signals. A user&#8217;s perception of multimodal expressions of emotions displayed by the computer will impact a user&#8217;s state and thus all these questions and their answers are of importance for researchers working on Multimodal User State recognition (see <a href="17_Chapter05.xhtml">Chapter 5</a>.</p>
<p class="indent">Every day, we perceive affects that our human peers express using multiple modalities such as their speech, facial expressions, posture, body movements, and touch [Scherer and Elgring 2007]. These signals coming from different modalities are sometimes <i><b>congruent,</b></i> and redundantly express the same affect. For example, your friend may express joy with both her facial expressions and her hand gestures. Some other times, these multimodal signals are complementary, or even <i><b>incongruent</b></i> and convey different affects. For example, your kid watching a horror movie may display a facial expression of joy, but his body posture may convey an expression of fear. Finally, sometimes only one modality conveys an affective state, the other modalities being either absent (e.g., during a phone conversation) or conveying a neutral expression.</p>
<p class="indent"><a id="page_265"/>Several researchers in psychology and neurosciences are exploring how these in/congruent expressions of affects coming from our human peers are processed by our brain [Kret et al. 2013]. For example, a study revealed that audiovisual presentation of nonverbal emotional information resulted in a significant increase in correctly classified stimuli when compared with visual and auditory stimulation [Kreifelts et al. 2007]. This behavioral gain was paralleled by enhanced activation (as observed via event-related fMRI) of multimodal brain areas. Such studies often use virtual characters to design stimuli since signals in each modality can easily be turned on/off for controlled experiments [Creed and Beale 2008].</p>
<p class="indent">When research is focused on the perception of affective expressions in a given modality (e.g., facial expressions), some researchers consider the other signals conveyed by other modalities as contextual information [de Gelder and Stock 2012], especially when one of these other modalities conveys a <i>neutral expression</i>. Researchers question whether a neutral affective state and neutral expressions exist and they have concerns about how to measure and manipulate them. The design and theoretical support for neutral expressions of emotions has been addressed by several researchers, some of them believing that neutral affective states minimize affective biases, some of them believing that they may actually create them, just like other affective states [Gasper and Danube 2016]. Designers of affective interactions and virtual characters do not always want or cannot always design expressions of emotions in all modalities. Thus some of the modalities are left with an expression of emotion which is intended to be neutral. Designers need to know how the perception of these &#8220;neutral&#8221; expressions is impacted by the expressions of emotions in other modalities.</p>
<p class="indent">It thus becomes of major importance for HCI designers and researchers to know how users will perceive and integrate these multiple signals of affects displayed by computers. Since virtual agents are now used in multiple application areas such as job interview training [Hoque et al. 2013], the mechanisms underlying users&#8217; perception of the emotional expressions displayed by virtual characters are of importance for designers. Indeed, this issue raises multiple questions. Do users bring into play the same fusion processes that they use when perceiving multimodal expressions of affects coming from a human peer? How does congruence and incongruence of multimodal expressions impact the perception and recognition of the intended affect? Do in/congruent multimodal signals speed up/down these affective perceptive processes? When should an affect expressive system generate congruent expressions of affects, and when should this system generate more subtle or incongruent expressions of affects? How will the user perceive these sophisticated combinations, and how will that impact task performance and subjective <a id="page_266"/>experience? How can we model this multisensory integration process in users&#8217; perception? How are the intensities of the expressions of emotions in the different modalities perceptually combined by users? What happens if one of the modalities conveys a neutral expression?</p>
<p class="indent">In this chapter, an overview of some answers to these questions is given. We summarize major experimental studies about the perception of multimodal expressions of affects in psychology and neurosciences. We also describe the evaluation studies that investigate these questions using a computer generated stimuli (e.g., a virtual character). We start by a section about emotions and their expressions. Next, we explain how humans perceive combinations of expressions of affects in several modalities (i.e., facial expressions, bodily expressions, speech, and haptics). We also point the impact of context on the perception of expressions of affects. In the conclusion, we explain how such knowledge should be used by designers of multimodal human-computer interaction for specifying multimodal affective displays or for defining studies evaluating how users perceive these displays. The Glossary lists the main concepts that are explained in the chapter. <a href="#tab9_1">Table 9.1</a> provides a synthetic view of some of the experiments described in this chapter.</p>
<p class="h1"><a id="ch9_2"/><b><span class="bg1">9.2</span>&#160;&#160;&#160;&#160;Emotions and Their Expressions</b></p>
<p class="noindent">There are multiple approaches to emotions and the associated theories [Gross and Barrett 2011]. Discrete emotion theories claim a limited number of emotions. Six basic emotions are usually considered: fear, happiness, anger, sadness, surprise, and disgust [Ekman and Friesen 1975]. Several researchers claim that basic emotions are seldom observed in everyday life and that a wider list of affect categories need to be considered. Baron-Cohen proposes a list of 416 so-called mental states (e.g., Relaxation). The MindReading database includes six audio-visual performances for each of these 416 mental states [Golan et al. 2006] including several modalities (speech, facial expressions, gaze, head movements).</p>
<p class="indent">Dimensional theorists define dimensional spaces that account for the similarities and differences in emotional experience. Three dimensions are often used [Russell and Mahrabian 1977]. Pleasure is a continuum ranging from extreme pain or unhappiness at one end to extreme happiness or ecstasy at the other end. Arousal ranges from sleep to frenzied excitement. Dominance ranges from feelings of total lack of control or influence on events and surroundings to the opposite extreme of feeling influential and in control.</p>
<p class="indent">Regarding the expressions of the emotions, Ekman suggests that a single emotion is expressed using a family of facial expressions. He lists distinctive clues to <a id="page_267"/>six basic emotions and the multiple ways to express each emotion [Ekman and Friesen 1975]. For example, anger can be expressed by pressing the lips together (thus masking the teeth). Anger can also be expressed with an open mouth exposing the teeth.</p>
<div class="box">
<p class="bhead"><b>Glossary</b></p>
<p class="hangbx"><b>Congruent expressions of affects</b>. A multimodal combination is said to involve congruent expressions of affects if each modality is conveying the same affect in terms of the category or the dimension (e.g., the facial expressions express anger and the hand gestures also express anger).</p>
<p class="hangbx"><b>Incongruent expressions of affects</b>. A multimodal combination is said to involve incongruent expressions of affects if the combined modalities are conveying different affects in terms of the category or the dimension (e.g., the facial expressions express joy, while hand gestures express anger). Such combinations are also called <b>blends of emotions</b>. They might occur even in a single modality such as the facial expressions (e.g., the upper part of the face may express a certain emotion, while the bottom part of the face conveys a different emotion).</p>
<p class="indentbx">Researchers exploring the perception of human (or computer-generated) multimodal expressions of affects usually consider the following attributes related to perception and affects that are impacted by or do impact multimodal perception:</p>
<p class="bullbx">&#9632;&#160;&#160;Recognition rate</p>
<p class="bullbx">&#9632;&#160;&#160;Reaction time</p>
<p class="bullbx">&#9632;&#160;&#160;Affect categories</p>
<p class="bullbx">&#9632;&#160;&#160;Affect dimensions</p>
<p class="bullbx">&#9632;&#160;&#160;Multimodal integration patterns</p>
<p class="bullbx">&#9632;&#160;&#160;Inter-individual differences and personality</p>
<p class="bullbx">&#9632;&#160;&#160;Timing: synchrony vs. sequential presentation of the signals in different modalities</p>
<p class="bullbx">&#9632;&#160;&#160;Modality dominance</p>
<p class="bullbx">&#9632;&#160;&#160;Context: environment and task related information (e.g., food or violent scenes, and associated applications for specific users with food disorders or PTSD)</p>
<p class="bullbx">&#9632;&#160;&#160;Task difficulty</p>
</div>
<p class="indent">In everyday life, it is frequent to express <i><b>blends of several emotions</b></i> at the same time [Ekman and Friesen 1975, Scherer 1998]. Indeed, recent research suggests that emotion categories are overlapping and probabilistic, rather than fixed and discrete [Hoemann et al. 2017]: it is based on the hypothesis that the brain creates predictions that are generative combinations of prior experiences constructed as specific emotions.</p>
<div class="cap">
<p class="tcaption"><a id="page_268"/><a id="tab9_1"/><b>Table 9.1</b>&#160;&#160;&#160;&#160;Research on the perception by humans of multimodal combinations of affects</p>
<p class="image"><img src="../images/tab9_1.png" alt="Image"/></p>
</div>
<p class="indent"><a id="page_269"/>Models of blends of face-only expressions of emotions have been proposed by researchers working on virtual characters. These models use either interpolations between predefined basic emotions&#8217; expressions for the whole face, spatial facial region decomposition, dimensional models of emotions, or sequenced expression models [Hyniewska et al. 2010]. Corpus-based approaches have also been considered with samples of facial and gestural expressions of emotions being used to manually design animations of expressive virtual characters [Buisine et al. 2006].</p>
<p class="indent">Producing congruent expressions in different modalities requires important skills given the difficulty of monitoring and controlling different cues at the same time [Scherer 2010]. Several studies explore how emotions are expressed by the face or by the body. Whereas Ekman suggests that posture might provide information regarding the intensity of emotion [Ekman and Friesen 1974], Wallbott observes discriminative features of emotion categories in posture and movement quality [Wallbott 1998]. Basic gestural form features of the hands might also be related somehow to emotions (e.g., the left hand was observed to be used more frequently when in a relaxed and positive mood) [Kipp and Martin 2009].</p>
<p class="h1"><a id="ch9_3"/><b><span class="bg1">9.3</span>&#160;&#160;&#160;&#160;How Humans Perceive Combinations of Expressions of Affects in Several Modalities</b></p>
<p class="noindent">In this section, we survey several studies which explore how participants perceive combinations of expressions of affects across at least two modalities. Although often targeting experimental results, these studies also provide insight on how the multimodal systems should display combinations of several modalities to convey expressions of affects to users during an interactive task.</p>
<p class="h2"><a id="ch9_3_1"/><b><span class="bg2">9.3.1</span>&#160;&#160;&#160;&#160;Perception of Facial and Bodily Expressions of Affects</b></p>
<p class="noindent">Face and body appear together in daily experience. Several studies observe that congruent combinations of facial and postural expressions of emotions improve emotion recognition [Gunes and Piccardi 2005, Hietanen and Lepp&#228;nen 2008, Meeren et al. 2005]. de Gelder observes that, when a face is accompanied by a body expressing the same emotion, the judgment accuracy and speed increase [de Gelder and Stock 2012]. These studies in psychology and neurosciences make use of pictures of human participants expressing emotions in different modalities.</p>
<p class="indent">In Clavel et al. [2009], the authors describe two experiments about the perception by a user, of combinations of facial and postural expressions of basic emotions <a id="page_270"/>displayed with an animated agent on a computer screen. The first study evaluates the contribution of congruent facial and postural expressions to the overall perception of basic emotion categories, as well as to the valence and activation dimensions. The second study explores the impact of incongruent expressions on the perception of blends of emotions. The results suggest that the congruence of facial and bodily expression facilitates the recognition of emotion categories. Yet, judgments were mainly based on the emotion expressed in the face, but were nevertheless impacted by the body postures in terms of perception of the activation dimension.</p>
<p class="indent">In the two above-mentioned studies, the full body of an animated agent from a front view was displayed. In 3D virtual environments and social scenes, virtual characters are not always visible from a front view. Nonverbal expressions of emotions should be designed so as to be properly perceived when viewed from multiple angles. Furthermore, the facial expression displayed on the face might get better recognized when displayed during a close view of the head, compared to when displayed over the full body. A study considered the impact of front vs. side views on the perception of blends of facial and postural expressions of emotions [Courgeon et al. 2011]. Results observed in previous studies for incongruent front-view images are confirmed for side views. Yet, subjects were less confident in their ratings for side-view images than for front view images.</p>
<p class="indent">Whereas the above-mentioned studies considered only multimodal expressions of affects displayed by a graphical virtual character displayed on a computer screen, other studies considered also expressions of emotions displayed by physical robots. [Chevalier et al. 2016] designed human-robot interactions for social learning for individuals with Autism Spectrum Disorders (ASD). People with ASD display interindividual differences in terms of sensory processing. Proprioception is the sense of the relative position of one&#8217;s own parts of the body and strength of effort being employed in movement. In order to define an individual&#8217;s profile, these authors posit that the individual&#8217;s reliance on proprioceptive and kinematic visual cues would affect the way the individual suffering from ASD interacts with a social agent (would it be a human, a robot, or a graphical virtual agent). They collected the EMBODI-EMO database containing videos of monomodal or congruent combinations of bodily/facial expressions of emotions. These expressions are displayed on various platforms: an animated agent, a Nao robot, and a Zeno robot endowed with capacities to express emotions via facial expressions. Different types of expressions were recorded: (1) monomodal: the emotion is expressed only through one communication modality (e.g., on the face or on the body) while the other conveys a neutral expression, and (2) multimodal congruence: the body and face express the <a id="page_271"/>same emotion in a synchronous manner. They considered four basic emotions: anger, happiness, fear, and sadness. The database includes 96 videos: 12 videos of facial expressions (4 emotions &#215; 3 platforms), 48 videos of body expressions (3 variations &#215; 4 emotions &#215; 4 platforms), and 36 videos of body and facial expressions (3 variations &#215; 4 emotions &#215; 3 platforms). They validated this database with typically developed (TD) individuals. The authors investigated the relationship between emotion recognition and proprioceptive and visual profiles of individuals with ASD. Participants with ASD relying more heavily on proprioceptive cues had lower emotion recognition scores on all conditions than participants relying on visual cues.</p>
<p class="indent">Combinations or facial and bodily expressions of emotions were also investigated by Buisine et al. [2014] with a focus on the impact of different types of postural expressions. These authors tested three conditions: in the &#8220;still&#8221; condition, a narrative content was accompanied by emotional facial expressions without any body movements; in the &#8220;idle&#8221; condition, emotionally neutral body movements were introduced; and in the &#8220;congruent&#8221; condition, emotional body postures congruent with the character&#8217;s facial expressions were displayed. The results of a study with 27 participants highlight the importance of the contextual information to emotion recognition and irony interpretation. They observed that both idle and emotional postures improved the recognition of emotional expressions. Moreover, emotional postures increased the perceived intensity of emotions and the perceived realism of the animations.</p>
<p class="indent">Dynamics of expressions is known to have a huge impact on emotion perception. Martinez et al. [2016] designed a set of dynamic stimuli to determine the relative contributions of the face and body to the accurate perception of basic emotions. Their findings suggest that even a short exposure time (250 ms) provided enough information to correctly identify an emotion above the chance level. Their study highlights the role of the body in emotion perception and suggests an advantage for angry bodies, which, in contrast to all other emotions, were comparable to the recognition rates from the face and may be advantageous for perceiving imminent threat from a distance.</p>
<p class="indent">In order to design expressive virtual characters, researchers often adopt a corpus-based approach and inspire from databases of human behaviors. The collection of ecological data for studying how human jointly express emotions via their face and their body rises challenges in terms of experimental method and equipment. Although recent movies are able to make use of advanced synchronized facial and bodily motion capture, such equipment is not always available for researchers conducting experimental studies. For example, researchers analyzed <a id="page_272"/>facial and bodily expressions of Bharatnatyam dancers but collected separate data for the facial expressions [Venkatesh and Jayagopi 2016a] and the body movements [Venkatesh and Jayagopi 2016b].</p>
<p class="h2"><a id="ch9_3_2"/><b><span class="bg2">9.3.2</span>&#160;&#160;&#160;&#160;Perception of Combinations of Speech and Other Modalities</b></p>
<p class="noindent">It is often the case that expressive speech is combined with other modalities. For example, Creed and Beale [2008] investigated how incongruent facial and audio expressions were perceived (e.g. a happy face with a concerned voice). They observed that incongruent animations were perceived as more engaging, warm, concerned, and happy when a happy or warm face was in the animation (as opposed to a neutral or concerned face) and when a happy or warm voice was in the animation (as opposed to a neutral or concerned voice). These results appear to follow cognitive dissonance theory as participants attempted to make mismatched expressions consistent on both the visual and audio dimensions of animations, resulting in confused perceptions of the emotional expressions.</p>
<p class="indent">Another study was conducted to explore how the intensity level (including neutral) of an expression in one modality affects the perception of the overall multimodal combination [Liebold and Ohler 2013]. The authors presented static pictures facial expressions displayed by virtual agents and vocal signals with expressive prosody and expressive content. They found that multimodal expressions of emotions yielded higher recognition rates than monomodal expressions of emotions. Additionally, emotionally neutral cues in one modality, when presented together with emotionally relevant cues in the other modality, impaired the recognition of the correct emotion category as well as its intensity level. They also found that participants required more time to classify unimodal emotional cues that were presented together with neutral cues (possibly due to increased cognitive effort).</p>
<p class="indent">Few studies considered how combinations of three modalities were perceived in terms of affects. In Clavel et al. [2012], the authors explored how in/congruent combinations of expressions in speech (semantic content and prosody), facial expressions and bodily expressions of basic emotion displayed on a virtual agent are perceived by participants. When the speech did not include any emotional cues, participants relied on the facial expression for reporting the perceived emotion. They observed that congruent expressions of an emotion lead to a better recognition of the emotion. Instead, when the speech content was incongruent with the non-verbal expressions, the participants reported the emotion expressed in the nonverbal modalities. These authors also recommend to check how the expressions intended to convey a neutral message are indeed perceived by users. In the study that they report, the virtual agent they used was a web agent designed by a <a id="page_273"/>company which had to convey positive messages to web users and thus had a subtle positive expression and design, even in the facial expression which was intended to be neutral. This slightly positive neutral expression revealed to impact the perception of users during incongruent combinations. Finally, these authors also point that they used the same dynamics of animations and multimodal combination for all the emotions. It might be the case that participants expect different dynamics of stimuli across different emotions.</p>
<p class="indent">The perception of intensity incongruence between auditory and visual modalities of synthesized expressions of laughter was also considered [Niewiadomski et al. 2015]. In particular, they investigated whether the incongruent expressions were perceived as (1) regulated or unregulated, and (2) successful or unsuccessful in terms of animation design. For this purpose, they conducted a perceptive study with a virtual agent. In/congruent multimodal expressions of laughter were synthesized from natural audiovisual laughter episodes, using machine learning algorithms. Next, the intensity of facial expressions and body movements were systematically manipulated, in order to check whether the resulting incongruent expressions were perceived differently compared to the corresponding congruent expressions. Results showed that (1) intensity incongruence lowered the ratings of believability and plausibility, and (2) the incongruent laughter expressions displaying high intensity in the audio modality and low intensity in the body movement and facial expression were perceived as more fake than the corresponding congruent expressions.</p>
<p class="h2"><a id="ch9_3_3"/><b><span class="bg2">9.3.3</span>&#160;&#160;&#160;&#160;Perception of Combinations of Haptic Expressions of Affects with Other Modalities</b></p>
<p class="noindent">Haptics has also received interest in terms of how it can be used to convey expressions of affects to the user. In this section, we describe how these affective haptics signals have been combined with other modalities. We also address the question of the modelling of the integration pattern used by participants during their perception of such multimodal expressions of affects.</p>
<p class="indent">A study explored how users perceive affective haptic signals when they are combined with facial expressions of affects [Tsalamlal et al. 2017]. In this study, the authors used an air jet device to project some air on the forearm of the user. The speed and the flow rate of the airjet have been observed to impact the valence of the affect perceived by the user (e.g., a low flow rate at slow movement velocity is perceived as pleasant).</p>
<p class="indent">In order to model how participants perceive combinations of haptic affective stimuli and facial expressions of emotions, Information Integration Theory (IIT) was used by the authors to model the algebraic rule (adding, multiplying, averaging, <a id="page_274"/>etc.) underlying multimodal perception of the emotional valence. We will briefly explain below this general technique used for studying integration of signals before describing how these authors applied it to multimodal expressions of affects.</p>
<p class="indent">Applying Information Integration Theory starts with a visual inspection of the collected data, especially with factorial design, looking for specific graphical and statistical signatures of integration models [Anderson 1982, 1989]. Patterns in the factorial plots provide a direct picture of the combination of multimodal sources of information. For instance, when studying the integration of two sources of information, an additive combination is suggested in the graph by parallelism of curves, and confirmed by a non-significant statistical interaction with an ANOVA. The signature of a multiplicative integration is instead a fan pattern observed in the graph confirmed by a significant linear &#215; interaction. Finally, averaging is suggested by parallel lines in the graph for bimodal conditions together with a crossover line for the unimodal condition, etc. IIT has been used to model the combination of multiple sources of information in the course of emotion perception, mainly within the visual modality [Oliveira et al. 2006, Prigent et al. 2014, Courbalay et al. 2016].</p>
<p class="indent">In their study about perception of combined facial and haptic expressions of emotions [Tsalamlal et al. 2017], the authors presented facial expressions and haptic stimuli separately, and then together. Visual stimuli were pictures of facial expressions with different emotional intensity. Touch stimuli consisted of air jet tactile stimulation on the arm. 20 participants were asked to evaluate the emotional valence they perceived on a continuous scale (from &#8722;100 =very negative to +100 =very positive). Analyses showed that the participants generally integrated both visual and touch information to evaluate emotional valence. The analysis of the ratings of valence by the participants was conducted using the IIT framework. The main integration rule used by participants revealed to be an average of the valence in the two modalities with non-systematic predominance of a modality over the other modality. Inter-individual variability in the algebraic rule was also evidenced (<a href="#fig9_1">Figure 9.1</a>) but the individual factors explaining this variability remains to be explored. Future work should investigate the link between the integration model displayed by the participants and individual characteristics such as age, gender, and personality traits, or even touch sensitivity.</p>
<p class="indent">Human-robot interaction, especially with humanoid robots, encourages the use of multimodal communication in a way similar to human-human communication such as affective handshaking [Ammi et al. 2015]. Some humanoid robots feature physical plastic hands and enable the display of simple facial expressions. These robots enable to study how users perceive combinations of expressions of emotions conveyed by haptics via the hands of the robot, and via facial expressions displayed on the face of the robot [Tsalamlal et al. 2015]. These authors conducted a study about how the haptic feedback displayed during the human-robot handshake could convey emotions, and more precisely, how it could influence the perception of emotions expressed through the facial expressions of the robot. Moreover, they examined the benefits of multimodality (i.e., visuo-haptic) over monomodality (i.e., visual-only and haptic-only). The results suggest that the multimodal (i.e., visuohaptic) stimuli presenting high values for grasping force and stiffness of movement were evaluated with higher values for the arousal and dominance dimensions than the visual-only stimuli. Furthermore, the analysis of the results corresponding to the monomodal haptic condition showed that participants discriminated well the dominance and the arousal dimensions of the haptic behaviors presenting low and high values for grasping force and stiffness of movement.</p>
<div class="cap" id="fig9_1">
<p class="image"><a id="page_275"/><img src="../images/fig9_1.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.1</b>&#160;&#160;&#160;&#160;Illustrative examples of mean participants&#8217; Emotional Valence ratings of facial expression intensities for unimodal (visual stimulus alone) and bimodal stimuli presentation (visuo-tactile stimuli, with tactile stimulation TS of varying intensity, LO=low, ME=medium, HI=high) (adapted from Tsalamlal et al. [2017]; participant A: near parallel lines plot (adding rule); participant B: fan pattern (multiplying rule); participant C: bimodal near parallel lines with unimodal visual (no-TS) crossover line plot, indicating equal-weight averaging; participant D: overlapped lines plot, indicating no integration but an effect of visual stimulation only.</p>
</div>
<p class="indent"><a id="page_276"/>Using another kind of haptic device (SensAble PHANTOM Desktop arm), [Gaffary et al. 2014] investigated the improvement of the recognition rate of emotions using visuo-haptic combinations compared to facial and haptic expressions alone. Four experiments were conducted in which the recognition rates of emotions using facial, haptic, and visuo-haptic expressions were compared. The first experiment evaluated the recognition rate of emotions using only facial expressions. The second experiment collected a large corpus of haptic expressions of emotions (e.g., 3D movements of the PHANTOM arm performed by participants) and subsequently identified the relevant haptic expression for each emotion. The third experiment evaluated the selected haptic expressions through statistical and perceptive tests to retain the signals that resulted in the most accurate identification of the corresponding emotion. Finally, the fourth experiment studied the impact of congruent visu&#8211;haptic combinations on the recognition of the investigated emotions. Generally, emotions rating high on the valence dimension were better recognized in the visual modality. However, emotions with high activation were better recognized in the haptic modality. These results highlighted the finding that participants are not equally aided by each modality when recognizing emotions efficiently. Beyond the recognition rate, multimodal expressions also improved the sensation of presence and expressivity.</p>
<p class="h1"><a id="ch9_4"/><b><span class="bg1">9.4</span>&#160;&#160;&#160;&#160;Impact of Context on the Perception of Expressions of Affects</b></p>
<p class="noindent">Context refers not only to the external surroundings in which expressions of emotion take place but also to parallel brain processes that dynamically constrain or shape how structural information from a modality is processed [Barrett et al. 2007]. For example, emotion words (implicitly or explicitly) may serve as an internal context <a id="page_277"/>to constrain the meaning of an expression of emotion during an instance of emotion perception. In this section, we review several types of such contextual influence that are investigated in research on perception of expressions of affects.</p>
<p class="indent">Gaze is considered an important contextual cue that can be exploited by designers to impact user&#8217;s perception and experience. For example, expressions of joy and anger appear to be considerably more intense when combined with direct gaze compared to the same expressions combined with averted gaze [Adams and Kleck 2003, 2005]. Postures and other body language (e.g., hand gestures) are also considered as context elements for the emotional facial perception. Thus the concomitant emotional body language influences the perception of facial expressions [Meeren et al. 2005]. Acoustic information may also serve as context for the perception, recognition and evaluation of facial expressions. Indeed, the identification of the emotion expressed in the face is biased in the direction of simultaneously presented affective speech prosody [de Gelder and Vroomen 2000].</p>
<p class="indent">In order to study how people perceive affective stimuli displayed by a computer, different experimental methods are available for in-lab studies. For example, people react with Rapid Facial Reactions (RFRs) when presented with facial expressions of emotions. An electromyography (EMG) system enables to measure such spontaneous muscular facial activities during the perception of facial expressions of emotions. EMG studies indicate that looking at static facial expressions causes spontaneous RFRs that are congruent with the presented facial expressions [Dimberg 1982]. Rapid facial reactions are considered automatic facial responses and occur rather quickly when facial expressions are presented to participants (500 ms after the display of the facial expression) [Dimberg 1982, Dimberg et al. 2000]. Congruent RFRs have been observed in response to subliminally presented facial displays [Dimberg et al. 2000]. Recent studies show that RFRs are not always congruent with emotional cues. In Philip et al. [2017], the authors manipulated the context of perception and studied its influence on RFRs. They used a subliminal affective priming task with emotional labels displayed on a computer screen. Facial electromyography (frontalis, corrugator, zygomaticus and depressor) was recorded on the face of participants while they observed static facial expressions (joy, fear, anger, sadness, and neutral expression) preceded/not preceded by a subliminal word (JOY, FEAR, ANGER, SADNESS or NEUTRAL). For the negative facial expressions, when the priming word was congruent with the facial expression, participants displayed congruent RFRs (mimicry). When the priming word was incongruent, they observed a suppression of mimicry. Happiness was not affected by the priming word. RFRs thus appear to be modulated by the congruency between emotional labels and facial expressions.</p>
<p class="indent"><a id="page_278"/>As already mentioned above, multimodal interaction is often considered in the case of users with special needs. In Tell and Davidson [2015], the emotion recognition abilities of children with ASD and typically developing children were compared in how they perceived in/congruent expressions of emotions displayed by a virtual character and the background graphics (i.e. a smiling boy surrounded by a swarm of bees). When facial expressions and situational cues of emotion were congruent, accuracy in recognizing emotions was decent for both children with ASD and typically developing children. When presented with facial expressions incongruent with situational cues, children with ASD relied more on the facial cues than the situational cues, whereas typically developing children relied more on the situational cues. The exception was fear. When presented with incongruent information, most children based their response on the situation, and indicated that the boy felt scared. While the majority of typically developing children commented on the disparity between the facial expressions and the situational cues, children with ASD did not mention the conflicting cues. Although typically developing children were more accurate in recognizing emotion with the situational cues, children with ASD were still adequate at identifying emotion from the situational cues alone. These findings suggest that children with ASD show an understanding of simple emotions in prototypical situations, but may prefer facial expressions when the facial expressions and the situational cues are incongruent.</p>
<p class="h1"><a id="ch9_5"/><b>9.5&#160;&#160;&#160;&#160;Conclusion</b></p>
<p class="noindent">In this chapter, we provided an overview of questions and experimental results related to the human perception of in/congruent expressions of affects across several modalities. Designers of human-computer interactions that need to convey expressions of emotions to users should know that:</p>
<p class="bullt">&#8226;&#160;&#160;generating and controlling synchronized expressions of affects across several modalities is challenging;</p>
<p class="bullt">&#8226;&#160;&#160;multiple studies have already considered the different combinations of modalities (possibly those that they intend to use in their own project);</p>
<p class="bullt">&#8226;&#160;&#160;emotion category and intensity recognition rates and speed usually benefit from emotional congruent expressions across several modalities;</p>
<p class="bullt">&#8226;&#160;&#160;different modalities may impact different attributes of emotion perception (e.g., category or arousal dimension);</p>
<p class="bullt"><a id="page_279"/>&#8226;&#160;&#160;they should test how their stimuli are perceived by users and might expect differences between some modalities, as well as differences between some emotions (e.g., anger);</p>
<p class="bullt">&#8226;&#160;&#160;they should pay attention to the temporal dynamics of the stimuli, the possible positive/negative impact of expressions that are intended to be neutral (e.g., idle posture or 3D model of a positive looking web agent);</p>
<p class="bullt">&#8226;&#160;&#160;users might be more confident in the emotion they recognized in front-viewed virtual agents than in side-viewed virtual agents;</p>
<p class="bullt">&#8226;&#160;&#160;expressions emotions might be best recognized with front-views;</p>
<p class="bullt">&#8226;&#160;&#160;expressions of emotions and their combinations may impact subjective perception (e.g., presence, realism, expressiveness, likeability) as well as user&#8217;s expressions and state;</p>
<p class="bullt">&#8226;&#160;&#160;there are several methodological challenges for designing multimodal expressions of emotions and the possibility of using existing databases should be considered; and</p>
<p class="bullt">&#8226;&#160;&#160;different user sensory profile may impact how these users process multimodal expressions of emotions.</p>
<p class="indentt">Applications such as video games, interactive storytelling, or social skills training might benefit from this knowledge since they often make use of virtual characters that should express complex and realistic affects across multiple modalities, and with a graphical and audio background. For example, since stress is expressed via multiple modalities [Gomez et al. 2017], a virtual patient simulating stress for training medics should display these coordinated signs in multiple modalities.</p>
<p class="indent">In the future, affective systems might also inspire from such knowledge for integrating other advanced expressive modalities (e.g., display of physiological signals by a virtual character, production of olfactory signals by an automated system). Neuroscientists are now also considering how the brain perceives congruent facial and bodily expressions of emotions displayed by several humans presented to participants [de Borst and de Gelder 2016]. This question is also relevant to HCI since several kinds of application display multiple interactive virtual characters at the same time on the screen (e.g., storytelling, serious games, etc.).</p>
<p class="h1n"><a id="ch9_6"/><b>Focus Questions</b></p>
<p class="noindent"><b>9.1.</b> Several studies explored how humans perceive multimodal expressions of affects. Why different modalities were considered?</p>
<p class="noindentt"><a id="page_280"/><b>9.2.</b> How can several expressions of affects be combined across several modalities?</p>
<p class="noindentt"><b>9.3.</b> When designing a system that displays combinations of expressions of affects in different modalities, which variables regarding user perception and interaction may be impacted?</p>
<p class="noindentt"><b>9.4.</b> What were the main results of such studies in terms of how in/congruence of combined affective expressions impact emotion recognition, response time and user experience?</p>
<p class="noindentt"><b>9.5.</b> What are the main integration patterns that users are likely to apply when they are presented with expressions of affects across several output modalities?</p>
<p class="h1n"><a id="ch9_7"/><b>References</b></p>
<p class="ref">R. B. Adams and R. E. Kleck. 2003. Perceived gaze direction and the processing of facial displays of emotion. <i>Psychological Science</i>, 14: 644&#8211;647. DOI: 10.1046/j.0956-7976.2003.psci_1479.x. 277</p>
<p class="ref">R. B. Adams and R. E. Kleck. 2005. The effects of direct and averted gaze on the perception of facially communicated emotion. <i>Emotion</i>, 5: 3&#8211;11. DOI: 10.1037/1528-3542.5.1.3.277</p>
<p class="ref">M. Ammi, V. Demulier, S. Caillou, Y. Gaffary, Y. Tsalamlal, J.-C. Martin, and A. Tapus. 2015. Haptic Human-Robot Interaction in a Handshaking Social Protocol. In <i>Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI &#8217;15)</i>, ACM, New York, pp. 263&#8211;270. DOI: 10.1145/2696454.2696485. 274</p>
<p class="ref">N. H. Anderson. 1982. <i>Methods of information integration theory</i>, vol. 2. Academic Press New York. 274</p>
<p class="ref">N. H. Anderson. 1989. Information integration approach to emotions and their measurement. In R. P. Kellerman, editors, <i>The Measurement of Emotions</i>, pp. 133&#8211;186. Academic Press. 274</p>
<p class="ref">L. F. Barrett, K. A. Lindquist, and M. Gendron. 2007. Language as context for the perception of emotion. <i>Trends in Cognitive Sciences</i>, 11(8): 327&#8211;332. DOI: 10.1016/j.tics.2007.06.003. 276</p>
<p class="ref">A. W. de Borst and B. de Gelder. 2016. Clear signals or mixed messages: Inter-individual emotion congruency modulates brain activity underlying affective body perception. <i>Social Cognitive and Affective Neuroscience</i>, 11(8): 1299&#8211;1309. DOI: 10.1093/scan/nsw039. 279</p>
<p class="ref">S. Buisine, S. Abrilian, R. Niewiadomski, J.-C. Martin, L. Devillers, and C. Pelachaud. 2006. Perception of blended emotions: From video corpus to expressive agent. In <i>Proceedings of the International Conference on Intelligent Virtual Agents (IVA&#8217;2006)</i>, pp. 93&#8211;106. Lecture Notes in Computer Science, vol 4133. Springer, Berlin, Heidelberg. DOI: 10.1007/11821830_8. 269</p>
<p class="ref"><a id="page_281"/>S. Buisine and J. C. Martin. 2007. The effects of speech-gesture cooperation in animated agents&#8217; behavior in multimedia presentations. <i>Interacting with Computers</i>, 19: 484&#8211;493. DOI: 10.1016/j.intcom.2007.04.002. 263</p>
<p class="ref">S. Buisine and J.-C. Martin. 2010. The influence of user&#8217;s personality and gender on the processing of virtual agents&#8217; multimodal behavior. In A. M. Colombus (editor), <i>The Psychology of Extraversion</i>, vol. 65, pp. 289&#8211;302. Nova Science Publishers New York. 264</p>
<p class="ref">S. Buisine, M. Courgeon, A. Charles, C. Clavel, J-C. Martin, N. Tan, and O. Grynszpan. 2014. The role of body posture in the recognition of emotion in contextually-rich scenarios. <i>International Journal of Human-Computer Interaction</i>, 30(1): 52&#8211;62. DOI: 10.1080/10447318.2013.802200. 271</p>
<p class="ref">P. Chevalier, J.-C. Martin, B. Isableu, C. Bazile, and A. Tapus. 2016. Impact of sensory preferences of individuals with autism on the recognition of emotions expressed by two robots, an avatar, and a human. <i>Autonomous Robots</i>, 41(3): 613&#8211;635. DOI: 10.1007/s10514-016-9575-z. 268, 270</p>
<p class="ref">C. Clavel, J. Plessier, J.-C. Martin, L. Ach, and B. Morel. 2009. Combining facial and postural expressions of emotions in a virtual character. In <i>Proceedings of the International Conference on Intelligent Virtual Agents (IVA&#8217;2009)</i>, pp. 287&#8211;300. Lecture Notes in Computer Science, vol. 5773. Springer, Berlin, Heidelberg. DOI: 10.1007/978-3-642-04380-2_31. 268, 269</p>
<p class="ref">C. Clavel, L. Devillers, J. Plessier, L. Ach, B. Morel, and J.-C. Martin. 2012. Combinaisons d&#8217;expressions vocales, faciales et posturales des &#233;motions chez un agent anim&#233; - perception par les utilisateurs. <i>Techniques et Sciences Informatiques (TSI)</i>, 31(4): 533&#8211;564. DOI: 10.3166/tsi.31.533-564. 268, 272</p>
<p class="ref">A. Courbalay, T. Deroche, M. Descarreaux, E. Prigent, J. O&#8217;Shaughnessy, and M-A. Amorim. 2016. Facial expression overrides lumbopelvic kinematics, for clinical judgments about low back pain intensity. <i>Pain Research &#38; Management</i>, vol. 2016, Article ID 7134825. DOI: 10.1155/2016/7134825. 274</p>
<p class="ref">M. Courgeon, C. Clavel, N. Tan, and J.-C. Martin. 2011. Front View vs. Side View of Facial and Postural Expressions of Emotions in a Virtual Character. In Z. Pan, A. D. Cheok, W. M&#252;ller, editors, <i>Transactions on Edutainment VI</i>. Lecture Notes in Computer Science, vol. 6758. Springer, Berlin, Heidelberg. DOI: 10.1007/978-3-642-22639-7_14. 268, 270</p>
<p class="ref">M. Courgeon, C. Clavel, N. Tan, and J.-C. Martin. 2014. Modeling Facial Signs of Appraisal During Interaction; Impact on Users&#8217; Perception and Behavior. In <i>Proceedings of the 13th International Conference on Autonomous Agents and Multiagent Systems (AAMAS&#8217;2014)</i>, pp. 765&#8211;772. Paris, France. 264</p>
<p class="ref">C. Creed and R. Beale. 2008. Psychological responses to simulated displays of mismatched emotional expressions. <i>Interacting with Computers</i>, 20(2): 225&#8211;239. DOI: 10.1016/j.intcom.2007.11.004. 265, 272</p>
<p class="ref">U. Dimberg. 1982. Facial reactions to facial expressions. Psychophysiology, 19(6):643&#8211;647. DOI: 10.1111/j.1469-8986.1982.tb02516. 277</p>
<p class="ref"><a id="page_282"/>U. Dimberg, M. Thunberg, and K. Elmehed. 2000. Unconscious facial reactions to emotional facial expressions. <i>Psychological Science</i>, 11(1): 86&#8211;89. DOI: 10.1111/1467-9280.00221. 277</p>
<p class="ref">P. Ekman and W. V. Friesen. 1974. Nonverbal behavior and psychopathology. In R. J. Friedman and M. M. Datz, editors, <i>The psychology of depression: contemporary theory and research</i>, pp. 203&#8211;232. Winston &#38; Sons, Washington, D.C. 269</p>
<p class="ref">P. Ekman and W. V. Friesen. 1975. U<i>nmasking the face. A guide to recognizing emotions from facial clues</i>. Prentice-Hall Inc., Englewood Cliffs, New Jersey. 266, 267</p>
<p class="ref">Y. Gaffary, V. Eyharabide, J.-C. Martin, and M. Ammi. 2014. <i>The Impact of Combining Kinesthetic and Facial Expression Displays on Emotion Recognition by Users. International Journal on Human-Computer Interaction</i>, 30: 904&#8211;920. DOI: 10.1080/10447318.2014.941276. 268, 276</p>
<p class="ref">K. Gasper and C. L. Danube. 2016. The scope of our affective influences when and how naturally occurring positive, negative, and neutral affects alter judgment. <i>Personality and Social Psychology Bulletin</i>, 42(3): 385&#8211;399. DOI: 10.1177/0146167216629131. 265</p>
<p class="ref">B. de Gelder and J. Vroomen. 2000. The perception of emotions by ear and by eye. <i>Cognition and Emotion</i>, 14(3): 289&#8211;311. DOI: 10.1080/026999300378824. 277</p>
<p class="ref">B. de Gelder and J. Van den Stock. 2012. Real faces, real emotions: perceiving facial expressions in naturalistic contexts of voices, bodies and scenes. In A. J. Calder, G. Rhodes, J. V. Haxby and M. H. Johnson, editors, <i>The Handbook of Face Perception</i>. Oxford University Press, Oxford. DOI: 10.1093/oxfordhb/9780199559053.013.0027.265, 269</p>
<p class="ref">O. Golan, S. Baron-Cohen, and J. Hill. 2006. The Cambridge Mindreading (CAM) face-voice battery: testing complex emotion recognition in adults with and without asperger syndrome. <i>Journal of Autism and Developmental Disorders</i>, 36(2): 169&#8211;183. DOI: 10.1007/s10803-005-0057-y 266</p>
<p class="ref">D. Gomez, C. Castanier, B. Chang, M. Val, M., F. Cottin, C. Le Scanff, and J.-C. Martin. 2017. Toward Automatic Detection of Acute Stress: Relevant Nonverbal Behaviors and Impact of Personality Traits. In <i>Proceedings on the 7th International Conference on Affective Computing and Intelligent Interaction (ACII2017)</i>, San Antonio, Texas, October 23&#8211;26. 279</p>
<p class="ref">J. J. Gross and L. F. Barrett. 2011. Emotion generation and emotion regulation: One or two depends on your point of view. <i>Emotion Review</i>, 3(1): 8&#8211;16. DOI: 10.1177/1754073910380974. 266</p>
<p class="ref">H. Gunes and M. Piccardi. 2005. Fusing face and body display for bi-modal emotion recognition: single frame analysis and multi-frame post integration. In <i>Proceedings of the International Conference Affective Computing and Intelligent Interaction (ACII&#8217;2005)</i>, pp. 102&#8211;110. Springer. DOI: 10.1007/11573548_14. 269</p>
<p class="ref">J.K. Hietanen and J.M. Lepp&#228;nen. 2008. Judgment of other people&#8217;s facial expressions of emotions is influenced by their concurrent affective hand movements. <i>Scandinavian Journal of Psychology</i>, 49(3): 221&#8211;230. DOI: 10.1111/j.1467-9450.2008.00644.x.. 269</p>
<p class="ref"><a id="page_283"/>K. Hoemann, M. Gendron, and L. Feldman Barrett. 2017. Mixed emotions in the predictive brain. <i>Current Opinion in Behavioral Sciences</i>, 15: 51&#8211;57. DOI: 10.1016/j.cobeha.2017.05.013. 267</p>
<p class="ref">M. E. Hoque, M. Courgeon, B. Mutlu, J-C. Martin, and R. W. Picard. 2013. MACH: My Automated Conversation coacH. In <i>Proceedings of the ACM International Joint Conference on Pervasive and Ubiquitous Computing (UBICOMP 2013)</i>. pp. 697&#8211;706. ACM, New York. 265</p>
<p class="ref">S. Hyniewska, R. Niewiadomski, M. Mancini, and C. Pelachaud. 2010. Expression of affects in embodied conversational agents. In K.R. Scherer, T. B&#228;nziger, and E. Roesch, editors, <i>Blueprint for Affective Computing</i>, pp. 213&#8211;221. Oxford University Press. 269</p>
<p class="ref">M. Kipp and J.-C. Martin. 2009. Gesture and emotion: can basic gestural form features discriminate emotions? In <i>Proceedings of the International Conference on Affective Computing and Intelligent Interaction (ACII&#8217;2009)</i>. IEEE Press. DOI: 10.1109/ACII.2009.5349544. 269</p>
<p class="ref">B. Kreifelts, T. Ethofer, W. Grodd, M. Erb, and D. Wildgruber. 2007. Audiovisual integration of emotional signals in voice and face: an event-related fMRI study. <i>Neuroimage</i>, 37(4): 1445&#8211;56. DOI: 10.1016/j.neuroimage.2007.06.020. 265</p>
<p class="ref">M. E. Kret, K. Roelofs, J. Stekelenburg and B. de Gelder. 2013. Emotional signals from faces, bodies and scenes influence observers&#8217; face expressions, fixations and pupil-size. <i>Frontiers in Human Neuroscience</i>, 7: 810. DOI: 10.3389/fnhum.2013.00810. 265</p>
<p class="ref">B. Liebold and P. Ohler. 2013. Multimodal emotion expressions of virtual agents - Mimic and vocal emotion expressions and their effects on emotion recognition. <i>Proceedings of the International Conference on Affective Computing and Intelligent Interaction (ACII&#8217;2013)</i>. DOI: 10.1109/ACII.2013.73. 272</p>
<p class="ref">J. C. Martin. 1998. TYCOON: theoretical and software tools for multimodal interfaces. In John Lee, editor, <i>Intelligence and Multimodality in Multimedia interfaces</i>. AAAI Press. 263</p>
<p class="ref">L. Martinez, V. B. Falvello, H. Aviezer, and A. Todorov. 2016. Contributions of facial expressions and body language to the rapid perception of dynamic emotions. <i>Cognition and Emotion</i>, 30(5): 939&#8211;952. DOI: 10.1080/02699931.2015.1035229. 271</p>
<p class="ref">H. K. Meeren, C. C. van Heijnsbergen, and B. de Gelder. 2005. Rapid perceptual integration of facial expression and emotional body language. In <i>Proceedings of the National Academy of Sciences of the United States of America</i>, 102: 16518&#8211;16523. DOI: 10.1073/pnas.0507650102. 269, 277</p>
<p class="ref">R. Niewiadomski, S. Hyniewska, and C. Pelachaud, 2013. Computational Models of Expressive Behaviors for a Virtual Agent. In Jonathan Gratch and Stacy Marsella, editors, <i>Social emotions in nature and artifact: Emotions in Human and Human-Computer Interaction</i>. Oxford University Press. 264</p>
<p class="ref">R. Niewiadomski, Y. Ding, M. Mancini, C. Pelachaud, G. Volpe, and A. Camurri. 2015. Perception of Intensity Incongruence in Synthesized Multimodal Expressions of <a id="page_284"/>Laughter. <i>Proceedings of the International Conference on Affective Computing and Intelligent Interaction (ACII&#8217;2015)</i>. DOI: 10.1109/ACII.2015.7344643. 273</p>
<p class="ref">A. M. Oliveira, M. P. Teixeira, I. B. Fonseca, E. R. Santos, and M. Oliveira. 2006. Interemotional comparisons of facially expressed emotion intensities: Dynamic ranges and general-purpose rules. In D. E. Kornbrot, R. M. Msefti, and A. W. MacRae, editors, <i>Proceedings of the 22nd Annual Meeting of the International Society for Psychophysics</i>, pp. 239&#8211;244. The International Society for Psychophysics, St. Albans UK. 274</p>
<p class="ref">L. Philip, J.-C. Martin, and C. Clavel. 2017. Suppression of Facial Mimicry of Negative Facial Expressions in an Incongruent Context. <i>International Journal of Psychophysiology</i>. DOI: 10.1027/0269-8803/a000191. 277</p>
<p class="ref">E. Prigent, M.-A. Amorim, P. Leconte, and D. Pradon. 2014. Perceptual weighting of pain behaviours of others, not information integration, varies with expertise. <i>European Journal of Pain</i>, 18(1): 110&#8211;119. 274</p>
<p class="ref">L. M., Reeves, J. Lai, J. A. Larson, S. Oviatt, T. S. Balaji, S. Buisine, P. Collings, P. Cohen, B. Kraal, J. C. Martin, M. McTear, T. V. Raman, K. M. Stanney, H. Su, and Q. Y. Wang. 2004. Guidelines for multimodal user interface design. <i>Communications of the ACM</i>, 47(1): 57&#8211;59. DOI: 10.1145/962081.962106. 263</p>
<p class="ref">J. A. Russell and A. Mehrabian. 1977. Evidence for a three-factor theory of emotions. <i>Journal of Research on Personality</i>, 11(3): 273&#8211;294. 266</p>
<p class="ref">K. R. Scherer. 1998. Analyzing emotion blends. In <i>Proceedings of the 10th Conference of the International Society for Research on Emotions</i>, pp. 142&#8211;148. 267</p>
<p class="ref">K. R. Scherer and H. Ellgring. 2007. Multimodal expression of emotion: affect programs or componential appraisal patterns? <i>Emotion</i>, (1): 158&#8211;171. 264</p>
<p class="ref">K. R. Scherer. 2010. Emotion and emotional competence: conceptual and theoretical issues for modelling agents. In K.R. Scherer, T. B&#228;nziger, and E. Roesch, editors, <i>Blueprint for Affective Computing</i>, pp. 3&#8211;20. Oxford University Press, Oxford. 269</p>
<p class="ref">B. Schuller. 2018. Multimodal user state &#38; trait recognition: an overview. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Chapter 5 Morgan &#38; Claypool Publishers, San Rafael, CA.</p>
<p class="ref">D. Tell and D. Davidson. 2015. Emotion recognition from congruent and incongruent emotional expressions and situational cues in children with autism spectrum disorder. <i>Autism</i>, 19(3): 375&#8211;379. DOI: 10.1177/1362361314535676. 268, 278</p>
<p class="ref">M. Y. Tsalamlal, M-A. Amorim, J-C. Martin, and M. Ammi. 2017. Combining facial expression and touch for perceiving emotional valence. <i>IEEE Transactions on Affective Computing</i>, 99. DOI: 10.1109/TAFFC.2016.2631469. 264, 268, 273, 274, 275</p>
<p class="ref">M. Tsalamlal, J-C. Martin, M. Ammi, A. Tapus, and M-A. Amorim. 2015. Affective Handshake with a Humanoid Robot: How do Participants Perceive and Combine its Facial and Haptic Expressions. In <i>Proceedings of the 6th International Conference on Affective <a id="page_285"/>Computing and Intelligent Interaction (ACII 2015 2015)</i>, pp. 334&#8211;340. Xi&#8217;an, China. 268, 276</p>
<p class="ref">P. Venkatesh and D. B. Jayagopi. 2016a. Automatic Expression Recognition and Expertise Prediction in Bharatnatyam. <i>Proceedings of the IEEE Conference on Advances in Computing, Communications and Informatics (ICACCI)</i>. 272</p>
<p class="ref">P. Venkatesh and D. B. Jayagopi. 2016b. Automatic bharatnatyam dance posture recognition and expertise prediction using depth cameras. In Y. Bi, S. Kapoor, and R. Bhatia, editors, <i>Proceedings of SAI Intelligent Systems Conference (IntelliSys)</i>. Lecture Notes in Networks and Systems, vol. 16. Springer. 272</p>
<p class="ref">H. G. Wallbott. 1998. Bodily expression of emotion. <i>European Journal of Social Psychology</i>, 28: 879&#8211;896. 269</p>
</body>
</html>