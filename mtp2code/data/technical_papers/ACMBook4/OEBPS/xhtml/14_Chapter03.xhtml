<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_71"/>3</p>
<p class="chtitle"><b>Learning for Multimodal and Affect-Sensitive Interfaces</b></p>
<p class="chauthor"><b>Yannis Panagakis, Ognjen Rudovic, Maja Pantic</b></p>
<p class="h1"><a id="ch3_1"/><b><span class="bg1">3.1</span>&#160;&#160;&#160;&#160;Introduction</b></p>
<p class="noindent">Humans perceive the world by combining acoustic, visual, and tactile stimuli from their senses and interact with each other using different modalities of communication, such as speech, gazes, and gestures. Therefore, it is beneficial to equip human-computer interaction interfaces with multisensory and multimodal signal processing and analysis abilities. For instance, among other applications, personal assistants such as Amazon Echo, Google Home, and browsers such as Opera and NetFront are currently offering multimodal interaction experience to users.</p>
<p class="indent"><i><b>Multimodal or multiview signals</b></i> are sets of heterogeneous data, captured by different sensors (including various types of cameras, microphones, and tactile sensors, to mention but a few) and in different contexts (where the context is defined in terms of who the subject is, where he/she is, what their task is, and so on). Such signals are expected to exhibit some mutual dependencies or <i><b>correlations</b></i>, since they usually represent the same physical phenomenon, and, thus, their simultaneous processing reveals information that is unavailable when considering the modalities independently. Therefore, a critical question here is how the different information sources should be modeled jointly and hence combined to achieve optimal responsive multimodal user interfaces. Indeed, the performance of multimodal interfaces is not only influenced by the different types of modalities to be integrated but also the abstraction level at which these modalities are integrated and fused as well as the machine learning method used for such multisensory data fusion. Regarding such machine learning methods, <a href="#ch3_2">Section 3.2</a> provides an overview of generic machine learning models, such as Canonical Correlation Analysis, its variants, and extensions, for extracting correlations from multimodal signals. A comprehensive review of multimodal machine learning methods can be found in <a href="12_Chapter01.xhtml">Chapter 1</a> of this volume.</p>
<div class="box">
<p class="bhead"><a id="page_72"/><b>Glossary</b></p>
<p class="hangbx">A <b>correlation</b> is a single number that describes the degree of relationship between two variables (signals). It most often refers to how close two variables are to having a linear relationship with each other.</p>
<p class="hangbx"><b>Domain adaptation</b> refers to machine learning methods that learn from a source data distribution a well performing model on a different (but related) target data distribution.</p>
<p class="hangbx">The <b>Facial Action Coding System (FACS)</b> refers to a set of facial muscle movements that correspond to a displayed emotion.</p>
<p class="hangbx"><b>Gross errors</b> refer to non-Gaussian noise of large magnitude. Gross errors are often in abundance in audio-visual data due to incorrect localisation and tracking, presence of partial occlusions, enviromental noise etc.</p>
<p class="hangbx"><b>Multimodal or multiview signals</b> are sets of heterogeneous data, captured by different sensors, such as various types of cameras, microphones, and tactile sensors and in different contexts.</p>
<p class="hangbx"><b>Temporal dynamics of facial expression</b>: rather than being like a single snapshot, facial appearance changes as a function of time. Two main factors affecting temporal dynamics of facial expression is the speed with which they unfold and the changes of their intensity over time.</p>
</div>
<p class="indent">Besides the multimodal nature of human interaction, human&#8211;human interaction is severely influenced by the affective states and responses of the interactants. Consequently, in the design of affect-sensitive multimodal interfaces, the goal is to ensure that they achieve the naturalness of human&#8211;human interaction by combining multimodal signals as visual (sight), audio (sound), and tactile (touch), and reasoning about users&#8217; affective state from them. In this context, the following challenges are identified.</p>
<p class="bullt">&#8226;&#160;&#160;How is the temporal dimension of multimodal data taken into account in order, for example, to capture the dynamics of human affect (e.g, temporal evolution of facial expressions intensity)?</p>
<p class="bulla"><a id="page_73"/>&#8226;&#160;&#160;What is the role of context within and between different modalities, and can this be used to improve the performance of reasoning systems to be embedded in target user-interfaces?</p>
<p class="indentt">To address the important challenges mentioned above, various models and machine learning methods have been proposed in the literature; please refer to <a href="18_Chapter06.xhtml">Chapters 6</a> and <a href="19_Chapter07.xhtml">7</a> in this volume for an overview. In this chapter, we mainly focus on providing an overview and discussion of recent trends in machine learning models for multimodal analysis of facial behavior, which is a cornerstone in the disigning of intelligent, affect-sensitive user interfaces. Specifically, we identify and discuss the following modeling directions:</p>
<p class="bullt">&#8226;&#160;&#160;temporal modeling,</p>
<p class="bulla">&#8226;&#160;&#160;context dependency, and</p>
<p class="bulla">&#8226;&#160;&#160;domain adaptation.</p>
<p class="h2"><a id="ch3_1_1"/><b><span class="bg2">3.1.1</span>&#160;&#160;&#160;&#160;The Importance of Temporal Information</b></p>
<p class="noindent">As mentioned earlier, a key factor in learning for affect-sensitive user interfaces is the modeling of facial behavior dynamics. In particular, temporal modeling of facial expressions is critical for several reasons. Different temporal segments and intensity levels of expressions never occur in isolation but vary smoothly in time. Furthermore, temporal segments and intensity levels of facial expressions differ in their duration (e.g., the higher intensity levels occur less frequently than the lower levels). Moreover, temporal segments of emotion expression occur in a specific temporal order, i.e., the onset of emotion expression is followed by its apex or offset segment. Accounting for this temporal structure of facial expressions is important for the models to be able, for instance, to discriminate between onset and offset temporal segments of facial expressions. In the context of affect-sensitive user-interfaces, the beginning and apex of an expression of happiness may signal the interest level by the user, while the expression offset may signal disinterest or user disengagement time. Apart from a few works that attempted multimodal learning of dynamics of facial affects, most of the existing approaches focus on learning from a single modality (i.e., face images).</p>
<p class="indent">Even when learning from visual modality, different fusion strategies are needed to address multimodality within the visual source of information due to different camera views, feature types, and so on. Thus, the same approaches that are used to address modeling challenges within a modality (e.g., multiple views of <a id="page_74"/>facial expressions) can easily be adapted to handle fusion of multimodal data collected by different sensors (e.g., audio and visual). Therefore, we review the learning approaches applicable to the modeling of facial affect from single and multiple modalities. We also point out the challenges typically encountered when performing fusion of multimodal data in temporal domains as the dynamics of facial affect can exhibit different patterns depending on the used modality. Therefore, it is critical how these potential discordances are efficiently handled in order to take the full advantage of incorporating the underlying dynamics within each modality and their relationships across modalities.</p>
<p class="h2"><a id="ch3_1_2"/><b><span class="bg2">3.1.2</span>&#160;&#160;&#160;&#160;The Importance of Context</b></p>
<p class="noindent">Context plays a crucial role in understanding the human behavioral signals that can otherwise be easily misinterpreted. For instance, a smile can be a display of politeness, contentedness, joy, irony, empathy, or a greeting, depending on the context. Yet, most existing methods to date focus on the simpler problem of detecting a smile as a prototypical and self-contained signal. To identify the smile as a social signal, one must simultaneously know <i>where</i> the subject is located (outside, at a reception, etc.), <i>what</i> his or her current task is, <i>when</i> the signal is displayed (timing), and <i>who</i> the expresser is (expresser&#8217;s identity, age and expressiveness). Vinciarelli et al. [2009] identify this as the W4 quadruplet (<i>where, what, when, who</i>) but quickly point out that comprehensive human behavior understanding requires the W5+ sextuplet (<i>where, what, when, who, why, how</i>), where the <i>why</i> and <i>how</i> factors identify both the stimulus that caused the social signal (e.g., funny video) as well as how the information is passed on (e.g, by means of facial expression intensity). However, most current methods used in the design of intelligent interfaces are unable to provide a satisfactory answer to W4, let alone W5+. Simultaneously, answering the W5+ is a key challenge of data driven design of affect-sensitive intelligent user-interfaces [Pantic et al. 2008].</p>
<p class="h2"><a id="ch3_1_3"/><b><span class="bg2">3.1.3</span>&#160;&#160;&#160;&#160;The Importance of Adaptability</b></p>
<p class="noindent">Ability to adapt existing models across different contextual factors as well as modalities is the ultimate goal of any learning system. To this end, we review existing trends in <i><b>domain adaptation</b></i>, and, in particular, adaptation models applicable to facial behavior modeling. While there is a tied relationship between context modeling and model adaptation, most of the existing works on the latter have focused on one specific contextual dimension: subject adaptation. This is understandable, as the ultimate goal of affect-sensitive user-interfaces is to use both external and (subject&#8217;s) internal signals so as to be able to fully adapt to the user. Yet, since this <a id="page_75"/>modeling direction is still in its early infancy, existing model adaptation techniques have placed a particular focus on the modeling of the variations within subjects, including their face physiognomy (i.e., facial geometry), facial appearance as well as subject-specific changes in expression intensity. This subject-specific adaptation is important, while in the same context the reaction of subjects to the same stimuli may differ, for example, when modeling facial expressions of pain, where a long-term exposure to pain may result in facial expressions not as intense as in the case of sudden and short-lived painful experience. Therefore, the ability of the machine learning models and, thus, the user interface, to successfully adapt to its user is one of the ultimate goals of its design. While, again, few existing models focus only on a single (and within), modality, there is no study that attempts model adaptation across different modalities (cross-modal adaptation). However, this is an important challenge in multimodal fusion for target user-interfaces, as it is expected to result in more robust and reliable interaction systems.</p>
<p class="indent">Finally, note that the modeling topics mentioned above cannot be easily seen in isolation from one another as they are largely intersected in terms of what type of learning they account for. For example, model adaptation techniques can be seen as a way of adjusting different contextual factors from W5+ within the model (subject as context question &#8220;who&#8221;), so that the resulting user-interfaces can achieve optimal performance. Furthermore, these modeling challenges are universal across different signal modalities (e.g., visual, auditory or tactile). As we mentioned above, the rest of this chapter focuses on one signal domain, that of facial signals, that most ubiquitously illustrates the new data-driven modeling directions. Specifically, we consider the problems of facial behavior modeling and describe the state-of-the-art in machine learning methods proposed for the challenges above, as they relate to multimodal context-sensitive modeling of user interfaces.</p>
<p class="h1"><a id="ch3_2"/><b><span class="bg1">3.2</span>&#160;&#160;&#160;&#160;Correlation Analysis Methods</b></p>
<p class="noindent">Finding correlations between two or more sets of data from different modalities is inherent to many tasks and applications pertaining to multimodal and multisensor computing. For instance, an image can be captured in both visible and infrared spectrum and may be represented via a variety of visual descriptors such as SIFTs, HoGs, IGOs [Lowe 2004, Tzimiropoulos et al. 2012, Simonyan et al. 2014] etc., which can be seen as distinct feature sets corresponding to the same object. Another prominent example of such a scenario lies in the task of face recognition: a face can be recognized by employing a normal image as captured in the visible spectrum, as well as infrared captures or even forensic sketches [Li et al. 2009, Wang and Tang <a id="page_76"/>2009]. Similarly, a particular human behavior can be identified by certain vocal, gestural, and facial features extracted from both the audio and visual modalities [Shan et al. 2007, Zhihong et al. 2009].</p>
<p class="indent">Since such sets of multimodal data (compromising of distinct feature sets) refer to the same object or behavior, it is anticipated that, part of the conveyed information is shared among all observation sets (i.e., correlated components). While the remaining information consists of individual information (individual components) which are particular only to a specific observation set. The correlation among different modalities provide useful information for tasks such as feature fusion [Correa et al. 2009, Atrey et al. 2010], multiview learning [Sun 2013], multilabel prediction [Sun et al. 2011], and multimodal behavior analysis [Shan et al. 2007, Zhihong et al. 2009, Nicolaou et al. 2014]. On the other hand, the individual components are deemed important for tasks such as clustering and signal separation [Zhou et al. 2012]. These individual features may interfere with finding the correlated components, just as the correlated components are likely to obscure the individual ones. Consequently, it is very important to <i>accurately</i> extract the <i>correlated</i> and the <i>individual components</i> among the multiple datasets.</p>
<p class="indent">The problem becomes rather challenging when dealing with data contaminated by <i><b>gross errors</b></i>, which are also <i>temporally misaligned</i>, i.e., temporal discrepancies manifest among the observation sequences. In practice, gross errors [Huber 1981] arise from either device artifacts (e.g., pixel corruptions, sonic artifacts), missing and incomplete data (e.g., partial image texture occlusions), or feature extraction failure (e.g., incorrect object localization, tracking errors). These errors <i>rarely</i> follow a Gaussian distribution [Candes et al. 2011]. Furthermore, asynchronous sensor measurements (e.g., lag between audio and visual sensors), view point changes, network lags, speech rate differences, and the speed of an action, behavior, or event result into temporally misaligned sets of data. Clearly, the accurate temporal alignment of noisy, temporally misaligned sets of data is a cornerstone in many computer vision [Junejo et al. 2011, Zhou and la Torre 2009], behavior analysis [Panagakis et al. 2013, Nicolaou et al. 2014], and speech processing [Sakoe and Chiba 1978] problems, to name a few.</p>
<p class="indent">Several methods have been proposed for the analysis of two sets of data. The Canonical Correlation Analysis (CCA) [Hotelling 1936] is a widely used method for finding linear correlated components among two data sets. Notable extensions of the CCA are the sparse CCA [Sun et al. 2011, Chu, et al. 2013], the kernel- [Akaho 2011] and deep-CCA [Andrew et al. 2013], as well as its probabilistic [Bach and Jordan 2005, Nicolaou et al. 2014] and Bayesian variants [Klami et al. 2013]. The Canonical Time Warping (CTW) [Zhou and la Torre 2009] and relevant methods <a id="page_77"/>[Trigeorgis et al. 2016, W&#246;llmer et al. 2009] extend the CCA to handle time warping in data. In order to extract correlated components among multiple data sets, generalizations of the CCA can be employed [Kettenring 1971, Li et al. 2009]. However, the aforementioned methods ignore the individual components of the data sets, a drawback which was alleviated by the Joint and Individual Variation Explained (JIVE) [Lock et al. 2013] and Common Orthogonal Basis Extraction (COBE) [Zhou et al. 2012]. Since most of the methods mentioned above rely on least squares error minimization, they are prone to gross errors and outliers [Huber 1981], causing the estimated components to be arbitrarily away from the true ones. This drawback is alleviated to some extent by the robust methods in Panagakis et al. [2013], Nicolaou et al. [2014], and Panagakis et al. [2016]</p>
<p class="indent">Next, a brief review of the CCA [Hotelling 1936], JIVE [Lock et al. 2013], DTW [Sakoe and Chiba 1978], CTW [Zhou and la Torre 2009], as well as the Robust Correlated and Independent Component Analysis (RCICA) [Panagakis et al. 2016] and RCICA with Time Warpings (RCITW) [Panagakis et al. 2016] is provided.</p>
<p class="h2"><a id="ch3_2_1"/><b><span class="bg2">3.2.1</span>&#160;&#160;&#160;&#160;Canonical Correlation Analysis</b></p>
<p class="noindent">The CCA extracts correlated features from a pair of multivariate data. In particular, given two data sets <img src="../images/inline77_1.png" alt="Image"/>, the CCA finds two matrices <img src="../images/inline77_2.png" alt="Image"/>, with <i>K</i> &#8804; min(<i>I</i><sub>1</sub>, <i>I</i><sub>2</sub>). These matrices define a common, low-dimensional latent subspace such that the linear combination of the variables in <b>X</b><sup>(1)</sup>, i.e., <img src="../images/inline77_7.png" alt="Image"/> are highly correlated with a linear combination of the variables in <b>X</b><sup>(2)</sup>, i.e., <img src="../images/inline77_8.png" alt="Image"/>. The CCA corresponds to the solution of the constrained least-squares minimization problem [Sun et al. 2011, la Torre 2012]:</p>
<p class="eqn"><a id="eq3_1"/><img src="../images/eq3_1.png" alt="Image"/></p>
<p class="h2"><a id="ch3_2_2"/><b><span class="bg2">3.2.2</span>&#160;&#160;&#160;&#160;Joint and Individual Variation Explained</b></p>
<p class="noindent">The JIVE recovers the joint and individual components among <i>N</i> &#8805; 2 data sets <img src="../images/inline77_3.png" alt="Image"/>. In particular, each matrix is decomposed into three terms: a low-rank matrix <img src="../images/inline77_4.png" alt="Image"/> capturing joint structure between data sets, a low-rank matrix <img src="../images/inline77_5.png" alt="Image"/> capturing individual structure to each data set, and a matrix <img src="../images/inline77_6.png" alt="Image"/> accounting for i.i.d. residual noise. That is,</p>
<p class="eqn"><a id="eq3_2"/><img src="../images/eq3_2.png" alt="Image"/></p>
<p class="noindent"><a id="page_78"/>Let <b>X</b>, <b>J</b>, and <b>R</b> be <img src="../images/inline78_1.png" alt="Image"/> matrices constructed by concatenation of the corresponding matrices,<sup><a id="rfn1" href="#fn1">1</a></sup> the JIVE solves the rank-constrained least-squares problem [Lock et al. 2013]:</p>
<p class="eqn"><a id="eq3_3"/><img src="../images/eq3_3.png" alt="Image"/></p>
<p class="noindent">Problem <a href="#eq3_3">(3.3)</a> imposes rank constraints on joint and individual components and requires the rows of <b>J</b> and <img src="../images/inline78_2.png" alt="Image"/> to be orthogonal. The intuition behind the orthogonality constraint is that sample patterns responsible for joint structure between data types are unrelated to sample patterns responsible for individual structure [Lock et al. 2013].</p>
<p class="indent">A closely related method to the JIVE is the COBE which extract the common and the individual components among <i>N</i> data sets of the same dimensions by solving a set of least-squares minimization problems [Zhou et al. 2012].</p>
<p class="h2"><a id="ch3_2_3"/><b><span class="bg2">3.2.3</span>&#160;&#160;&#160;&#160;Dynamic and Canonical Time Warping</b></p>
<p class="noindent">Given two temporally misaligned data sets with the same dimensionality I, namely <img src="../images/inline78_3.png" alt="Image"/>, the DTW aligns them along the time axis by solving [Sakoe and Chiba 1978]:</p>
<p class="eqn"><a id="eq3_4"/><img src="../images/eq3_4.png" alt="Image"/></p>
<p class="noindent">where <i><b>&#916;</b></i><sup>(<i>n</i>)</sup>, <i>n</i> = 1, 2 are binary selection matrices encoding the alignment path. Although the number of possible alignments is exponential in <i>J</i><sub>1</sub> &#183; <i>J</i><sub>2</sub>, the DTW recovers the optimal alignment path in <span class="f1">O</span>(<i>J</i><sub>1</sub> &#183; <i>J</i><sub>2</sub>) by employing dynamic programming. Clearly, the DTW can handle only data of the same dimensions. The CTW [Zhou and la Torre 2009] incorporates CCA into the DTW, allowing the alignment of data sequences of different dimensions by projecting them into a common latent subspace found by the CCA [Hotelling 1936]. Furthermore, the CCA-based projections perform feature selection by reducing the dimensionality of the data to that of the common latent subspace, handling the irrelevant or possibly noisy attributes.</p>
<p class="indent"><a id="page_79"/>More formally, let <img src="../images/inline79_1.png" alt="Image"/> be a set of temporally misaligned data of different dimensionality (i.e., <i>I</i><sub>1</sub> &#8800; <i>I</i><sub>2</sub>), the CCA is incorporated into the DTW by solving [Zhou and la Torre 2009]:</p>
<p class="eqn"><a id="eq3_5"/><img src="../images/eq3_5.png" alt="Image"/></p>
<p class="noindent"><img src="../images/inline79_2.png" alt="Image"/> project <b>X</b><sup>(1)</sup> and <b>X</b><sup>(2)</sup>, respectively, onto a common latent subspace of <i>K</i> &#8804; min(<i>I</i><sub>1</sub>, <i>I</i><sub>2</sub>) dimensions, where the correlation between the data sequences is maximized. <b>D</b> is a diagonal matrix of compatible dimensions. The set of constraints in <a href="#eq3_5">(3.5)</a> is imposed in order to make the CTW translation, rotation, and scaling invariant.</p>
<p class="h1r"><b>Remark 3.1</b></p>
<p class="noindent">By adopting the least squares error, the aforementioned methods assume Gaussian distributions with small variance [Huber 1981]. Such an assumption rarely holds in real-word multimodal data, where gross non-Gaussian corruptions are in abundance. Consequently, the components obtained by employing CCA, JIVE, DTW, and CTW in the analysis of grossly corrupted data may be arbitrarily away from the true ones, degenerating their performance.</p>
<p class="indent">A general framework to alleviate the aforementioned limitation and recover both the correlated and individual components is detailed next.</p>
<p class="h2"><a id="ch3_2_4"/><b><span class="bg2">3.2.4</span>&#160;&#160;&#160;&#160;Robust Correlated and Individual Components Analysis</b></p>
<p class="noindent">Consider two data sets from different modalities or feature sets possibly contaminated by gross but sparse errors. Without loss of generality these datasets are represented by two zero-mean matrices, namely <img src="../images/inline79_3.png" alt="Image"/> of different dimensions, i.e., <i>I</i><sub>1</sub> &#8800; <i>I</i><sub>2</sub>. The RCICA recovers the <i>correlated</i> and <i>individual</i> components of the data sets as well as the <i>sparse corruptions</i> by seeking a decomposition of each matrix into three terms:</p>
<p class="eqn"><a id="eq3_6"/><img src="../images/eq3_6.png" alt="Image"/></p>
<p class="noindent"><a id="page_80"/><img src="../images/inline80_1.png" alt="Image"/> are <i>low-rank</i> matrices with mutually independent column spaces, capturing the correlated and individual components, respectively, and <img src="../images/inline80_2.png" alt="Image"/> is a <i>sparse</i> matrix accounting for sparse non-Gaussian errors.</p>
<p class="indent">To ensure that the fundamental identifiability of the recovered components is guaranteed, the column spaces of <img src="../images/inline80_3.png" alt="Image"/> must be orthogonal to those of <img src="../images/inline80_4.png" alt="Image"/>. To facilitate this, the components are decomposed as:</p>
<p class="eqn"><a id="eq3_7"/><img src="../images/eq3_7.png" alt="Image"/></p>
<p class="eqn"><a id="eq3_8"/><img src="../images/eq3_8.png" alt="Image"/></p>
<p class="noindent">where<img src="../images/inline80_5.png" alt="Image"/> are column orthonormal matrices spanning the columns of <img src="../images/inline80_6.png" alt="Image"/>, respectively. <i>K</i> denotes the upper bound of unknown rank of <img src="../images/inline80_7.png" alt="Image"/> are the upper bounds of unknown rank of <img src="../images/inline80_8.png" alt="Image"/>. The mutual orthogonality of the column spaces is established by requiring <img src="../images/inline80_9.png" alt="Image"/>. In analogy to the CCA, <img src="../images/inline80_10.png" alt="Image"/> are required to be maximally correlated.</p>
<p class="indent">A natural estimator accounting for the low-rank components and the sparsity of <img src="../images/inline80_11.png" alt="Image"/> is to minimize the objective function of CCA, i.e., <img src="../images/inline80_12.png" alt="Image"/> as well as the rank of <img src="../images/inline80_13.png" alt="Image"/> and the number of nonzero entries of <img src="../images/inline80_14.png" alt="Image"/> measured by the &#8467;<sub>0</sub>-(quasi) norm, e.g., Candes et al. [2011], Liu and Yan [2012], Huang et al. [2012], and Panagakis et al. [2013]. Unfortunately, both rank and &#8467;<sub>0</sub>-norm minimization is NP-hard [Vandenberghe and Boyd 1996, Natarajan 1995]. The nuclear- and the &#8467;<sub>1</sub>- norms are typically adopted as convex surrogates to rank and &#8467;<sub>0</sub>- norm, respectively [Fazel 2002, Donoho 2006]. Accordingly, the objective function for the RCICA is defined as:</p>
<p class="eqn"><a id="eq3_9"/><img src="../images/eq3_9.png" alt="Image"/></p>
<p class="noindent">where the unknown variables are collected in <img src="../images/inline80_15.png" alt="Image"/> and are <img src="../images/inline80_16.png" alt="Image"/> positive parameters controlling the correlation, rank, and sparsity of the derived spaces.</p>
<p class="indent">Due to the unitary invariance of the nuclear-norm, e.g., <img src="../images/inline80_17.png" alt="Image"/>, <a href="#eq3_9">(3.9)</a> is simplified and thus the RCICA solves the constrained non-linear optimization problem:</p>
<p class="eqn"><a id="eq3_10"/><img src="../images/eq3_10.png" alt="Image"/></p>
<p class="noindent"><a id="page_81"/>Recall that the constraints (i) decompose each matrix into three terms capturing the correlated and the individual components as well as the sparse corruptions. The constraints (ii) are inherited by the CCA (cf. <a href="#eq3_1">(3.1)</a>) and are imposed in order to normalize the variance of the correlated components thus making them invariant to translation, rotation, and scaling (i.e., since data may have large-scale differences, this constraint normalizes them in order to facilitate the identification of correlated/shared components). The third set of constraints (iii) deem RCICA to be a <i>projective</i> method, a point which will be further clarified shortly in what follows. The constraints (iv) are imposed in order to ensure the identifiability of the model. That is, in order to perfectly disentangle the low-rank correlated and individual components, their column spaces should be mutually orthogonal. Otherwise, it would be impossible to guarantee the feasibility of the decomposition.</p>
<p class="indent">If we assume that there are no individual components (i.e., by setting <img src="../images/inline81_1.png" alt="Image"/>, and the dimensionality of the data is the same, i.e., <i>I</i><sub>1</sub> = <i>I</i><sub>2</sub>, and by setting <img src="../images/inline81_2.png" alt="Image"/>, then the RCICA is reduced to the Robust CCA [Panagakis et al. 2013]:</p>
<p class="eqn"><a id="eq3_11"/><img src="../images/eq3_11.png" alt="Image"/></p>
<p class="noindent">where <img src="../images/inline81_3.png" alt="Image"/> are low-rank matrices reconstructing correlated components and <img src="../images/inline81_4.png" alt="Image"/> are positive parameters controlling the sparsity in the error matrices.</p>
<p class="indent"><a id="page_82"/>Clearly, the RCICA has several appealing properties, deeming the technique advantageous in comparison to relevant methods. They are listed in what follows.</p>
<p class="numlistt">1.&#160;&#160;The RCICA is a more general approach, meaning that the CCA is also a special case of the RCICA. Indeed, if we assume that there are no gross errors in the data (i.e., <img src="../images/inline82_1.png" alt="Image"/>) and by letting <img src="../images/inline82_2.png" alt="Image"/>, i.e., there are no individual components, it is easy to verify that the solution of <a href="#eq3_10">(3.10)</a> is identical to that of <a href="#eq3_1">(3.1)</a>, while {<b>U</b><sup>(<i>n</i>)</sup> = <b>V</b><sup>(<i>n</i>)</sup>}.</p>
<p class="numlist">2.&#160;&#160;The RCICA can inherently handle data sets of different dimensionality.</p>
<p class="numlist">3.&#160;&#160;The RCICA is projective in the sense that the correlated and individual features of unseen (test) vectors can be extracted via the projection matrices <img src="../images/inline82_3.png" alt="Image"/>, respectively. Obviously, this is not the case for the RCCA in <a href="#eq3_11">(3.11)</a> where the reconstruction of the correlated components is recovered.</p>
<p class="numlist">4.&#160;&#160;The exact number of correlated and individual components needs not to be known in advance. Instead, an upper bound of the components&#8217; number is sufficient. The minimization of the nuclear-norms in <a href="#eq3_10">(3.10)</a> and <a href="#eq3_11">(3.11)</a> enables the actual number (i.e., rank) of the components to be determined automatically. Clearly, this is not the case in the CCA and the JIVE, where the number of components should be exactly determined.</p>
<p class="indentt">We finally note that the RCICA and the RCCA can handle data contaminated by Gaussian noise by vanishing the error term, that is by setting <img src="../images/inline82_4.png" alt="Image"/>.</p>
<p class="h2"><a id="ch3_2_5"/><b><span class="bg2">3.2.5</span>&#160;&#160;&#160;&#160;RCICA with Time Warpings (RCITW)</b></p>
<p class="noindent">Accurate temporal alignment of noisy data sequences is essential in several problems such as the alignment and the temporal segmentation of human motion [Zhou and De la Torre Frade 2012], the alignment of facial and motion capture data [Zhou and la Torre 2009, Panagakis et al. 2013], the alignment of multiple continuous annotations [Nicolaou et al. 2014] etc. The problem is defined as finding the temporal coordinate transformation that brings two given data sequences into alignment in time. To handle temporally misaligned, grossly corrupted data, the DTW is incorporated into the RCICA. Formally, given two sets <img src="../images/inline82_5.png" alt="Image"/> of different dimensionality and length, i.e., <i>I</i><sub>1</sub> &#8800; <i>I</i><sub>2</sub>, <i>J</i><sub>1</sub> &#8800; <i>J</i><sub>2</sub>, the RCITW enables their temporal alignment onto the subspace spanned by the robustly estimated correlated components. To this end, the RCITW solves:</p>
<p class="eqn"><a id="eq3_12"/><a id="page_83"/><img src="../images/eq3_12.png" alt="Image"/></p>
<p class="noindent">where <img src="../images/inline83_3.png" alt="Image"/>, <i>n</i> = 1, 2 are binary selection matrices encoding the warping path as in the CTW. The constraint <b>X</b><sup>(<i>n</i>)</sup><i><b>&#916;</b></i><sup>(<i>n</i>)</sup><b>1</b> = <b>0</b>, <i>n</i> = 1, 2 ensures that the temporally aligned data are zero-mean. By solving <a href="#eq3_12">(3.12)</a>, the temporally aligned correlated components of reduced dimensions are given by <img src="../images/inline83_1.png" alt="Image"/>. Moreover, one can obtain a reconstruction of the temporally aligned data in the original space by <img src="../images/inline83_2.png" alt="Image"/>.</p>
<p class="h1"><a id="ch3_3"/><b><span class="bg1">3.3</span>&#160;&#160;&#160;&#160;Temporal Modeling of Facial Expressions</b></p>
<p class="noindent">There are two main streams in the current research on automatic analysis of facial expressions. The first considers holistic facial expressions such as facial expressions of six basic emotions (fear, sadness, happiness, anger, disgust, surprise) proposed by [Ekman et al. 2002] or facial expressions of pain, for instance. The second considers local muscle activations producing facial expressions. These are described with a set of facial muscle actions named Action Units (AUs), as defined by the <i><b>Facial Action Coding System (FACS)</b></i> [Ekman et al. 2002]. In what follows, we review the existing approaches for temporal learning of facial expression dynamics.</p>
<p class="indent">Different methods have been proposed for classification of facial expressions from image sequences. Despite inherent dynamic information presented in target image sequences of facial expressions, the majority of existing works still rely on static methods for classification of facial expressions, such as those for recognition of six-basic emotion categories [Ekman et al. 2002]. These methods employ classifiers such as rule-based classifiers [Pantic and Rothkrantz 2004, Black and Yacoob 1997], Neural Networks (NN) [Padgett and Cottrell 1996, Tian 2004], Support Vector Machines (SVM) [Bartlett et al. 2005, Shan et al. 2009], and Bayesian Networks (BN) [Cohen et al. 2003]. Bi-directional Long-Short-Term Memory Neural Networks have been also applied to emmotions recognition [W&#246;llmer et al. 2013, Trigeorgis et al. <a id="page_84"/>2016]. For the static classification of AUs (i.e., by employing still images), where the goal is to assign to each AU a binary label indicating the presence of an AU, the classifiers based on NN [Bazzo and Lamar 2004, Fasel and Luettin 2000], Ensemble Learning techniques (such as AdaBoost [Yang et al. 2009] and GentleBoost [Hamm et al. 2011]), and SVM [Chew et al. 2012, Bartlett et al. 2006, Kapoor et al. n.d.], are commonly employed.</p>
<p class="indent">The common weakness of the frame-based classification methods is that they ignore dynamics of target facial expressions or AUs. Although some of the frame-based methods use the features extracted from several time frames in order to encode dynamics of facial expressions, models for dynamic classification provide a more principled way of doing so. With a few exceptions, most of the dynamic approaches to classification of facial expressions are based on the variants of Dynamic Bayesian Networks (DBN) (e.g., Hidden Markov Models (HMM) and Conditional Random Fields (CRF)). Discriminative models based on CRFs have been proposed in der Maaten and Hendriks [2012], Jain et al. [2011], and Chang et al. [2009]. For instance, der Maaten and Hendriks [2012] trained a linear-chain CRF per AU. The models&#8217; states are binary variables indicating the AU activations. Jain et al. [2011] proposed a generalization of this model, a Hidden Conditional Random Field (HCRF) [Wang et al. 2006], where an additional layer of hidden variables is used to model temporal dynamics of facial expressions. The training of the model was performed using image sequences, but classification of the expressions was done by selecting the most likely class (i.e., emotion category) at each time instance. Another modification of HCRF, named partially observed HCRF, was proposed in Chang et al. [2009]. In this method, classification of the emotion categories (sequence-based), and the AU combinations (frame-based), is accomplished simultaneously. This method outperformed the standard HCRF, which does not use a prior information about the AU combinations. Recently, Walecki et al. [2017] proposed a Variable-state Latent CRF (VSL-CRF) model for expresion and AU segmentation that also imposes ordinal relationships between the temporal states in the model, implicitly accounting for development of temporal phases of an expression (onset, apex, offset). Temporal consistency of AUs was also modeled in Simon et al. [2010] using the structured-output SVM framework for detecting the starting and ending frames of each AU. More complex graph structures within the DBN framework have been proposed in Zhang and Ji [2005], Tong et al. [2007] for dynamic classification of facial expressions. In Zhang and Ji [2005], the DBN was constructed from interconnected time slices of static Bayesian networks, where each static network was used to link the geometric features (i.e. locations of characteristic facial points) to the target emotion categories via a set of related AUs. Tong <a id="page_85"/>et al. [2007] modeled relationships between different AUs using another variant of a DBN.</p>
<p class="h2"><a id="ch3_3_1"/><b><span class="bg2">3.3.1</span>&#160;&#160;&#160;&#160;Temporal Segmentation of Facial Expressions</b></p>
<p class="noindent">Most of the works on facial expression analysis from image sequences focus only on classification of target expressions and/or AUs. Yet, these do not explicitly encode the dynamics (i.e., they do not perform classification of the temporal segments: neutral, onset, apex, offfset of an expression). Both the configuration, in terms of AUs constituting the observed expressions, and their dynamics are important for categorization of, e.g., complex psychological states, such as various types of pain and mood [Pantic and Bartlett 2007]. They also represent a critical factor in interpretation of social behaviors like social inhibition, embarrassment, amusement, and shame, and are a key parameter in differentiation between posed and spontaneous facial displays [Ekman et al. 2002].</p>
<p class="indent">The class of models that performs segmentation of the expression sequences into different temporal segments are presented in Pantic and Patras [2005], 2006. These are static rule-based classifiers based on the geometric features (i.e., facial points) that encode temporal segments of AUs in near-frontal and profile view faces, respectively. The works in Koelstra et al. [2010] and Valstar and Pantic [2012] proposed modifications of standard HMMs to encode temporal evolution of the AU segments. Specifically, Koelstra et al. [2010] proposed a combination of discriminative, frame-based GentleBoost ensemble learners, and HMMs for classification and temporal segmentation of AUs. Similarly, Valstar and Pantic [2012] combined SVMs and HMMs in a Hybrid SVM-HMM model based on the geometric features for the same task. A variant of the linear-chain CRF, named the Conditional Ordinal Random Field (CORF), was proposed in Kim and Pavlovic [2010] for temporal segmentation of six emotion categories. In this model, the node features of the linear-chain CRF model are set using the modeling strategy of the standard ordinal regression models, e.g. Chu and Ghahramani [2005], in order to enforce the ordering of the temporal segments (neutral&#60;onset&#60;apex). The authors emphasize the importance of modeling the ordinal constraints, as well as the temporal constraints imposed by a transition model defined on the segments. An extension of this model was proposed in Rudovic et al. [2012b], where the authors combined different emotion-specific CORF models in the HCRF framework. In contrast to CORF, this model performs simultaneous classification and temporal segmentation of the emotion categories. More recently, Rudovic et al. [2012a] introduced a kernel extension of the CORF model and applied it to the AU temporal segmentation. This model showed improved performance over the previous approaches, <a id="page_86"/>in the target task, which is mainly attributed to its modeling of nonlinear feature functions.</p>
<p class="h2"><a id="ch3_3_2"/><b><span class="bg2">3.3.2</span>&#160;&#160;&#160;&#160;Intensity Estimation of Facial Expressions</b></p>
<p class="noindent">Facial expression dynamics can also be described in terms of their intensity. Explicit analysis of the expression intensity is important for accurate interpretation of facial expressions, and is also essential for distinguishing between spontaneous and posed facial expressions [Pantic and Bartlett 2007]. For example, a full-blown smile and a smirk, both coded as AU12 but with different intensities, have very different meanings (e.g., enjoyment vs. sarcasm). However, discerning different intensities of facial expressions is a far more challenging task than the expression classification. This is mainly because the facial muscle contractions are combined with the individual&#8217;s physical characteristics, producing changes in appearance that can vary significantly between subjects [Ekman et al. 2002]. As a consequence, the methods that work for intense expressions may generalize poorly to subtle expressions with low intensity.</p>
<p class="indent">AU intensity estimation is a relatively recent problem within the field, and only a few works have addressed it so far. Based on the modeling approach, these can be divided into static methods [Mahoor et al. 2009, Mavadati et al. 2013, Savrana et al. 2012, Kaltwang et al. 2012, Jeni et al. 2013] and dynamic methods [Rudovic et al. 2013]. The static methods can further be divided into classification-based (e.g., [Mahoor et al. 2009, Mavadati et al. 2013]) and regression-based methods (e.g, [Savrana et al. 2012, Kaltwang et al. 2012, Jeni et al. 2013]). The static classification-based methods [Mahoor et al. 2009, Mavadati et al. 2013] perform multiclass classification of the intensity of AUs using the SVM classifier. For example, Mahoor et al. [2009] performed the intensity estimation of AU6 (cheek raiser) and AU12 (lip corner puller) from facial images of infants. The fusion of different input features was obtained by concatenation of the geometric and appearance features. On the other hand, the static regression-based methods model the intensity of AUs on a continuous scale, using either logistic regression [Savrana et al. 2012], RVM regression [Kaltwang et al. 2012], or Support Vector Regression (SVR) [Jeni et al. 2013]. For instance, Savrana et al. [2012] used Logistic Regression for AU intensity estimation, where the input features were selected by applying an AdaBoost-based method to the Gabor wavelet magnitudes of 2D luminance and 3D geometry extracted from the target images. Kaltwang et al. [2012] used the RVM model for intensity estimation of 11 AUs using image features such as Local Binary Patterns (LBPs), Discrete Cosine Transform (DCT), and the geometric features (i.e., facial points), and performed their fusion. Recently, the learning for AU intensity estimation started shifting toward <a id="page_87"/>joint modeling of AUs, with the goal of exploiting AU relationships to achieve more robust estimation. To this end, studies in Kaltwang et al. [2015] and Walecki et al. [2016a] employ the modeling based on Latent Trees and Copula functions, respectively, to capture the AU dependences, achieving superior performance compared to independent modeling of AUs. This is particularly important in the context of user interfaces where spontaneous facial expressions are captured, and, thus, AU activations occur simultaneously.</p>
<p class="h1"><a id="ch3_4"/><b><span class="bg1">3.4</span>&#160;&#160;&#160;&#160;Context Dependency</b></p>
<p class="noindent">When it comes to context modeling, as mentioned above, only few works addressed it so far. Existing models can be classified in terms of what context questions from W5+ design they model. The static models from the previous section focus on different feature modeling by answering the context question <i>how</i>. The other models reviewed, and which attempt temporal modeling of facial expressions and AUs, also answer the context question <i>when</i>. Recently, [Rudovic et al. 2013] proposed the Context-sensitive CORF (cs-CORF) model for dynamic estimation of intensity of AUs, and facial expressions of pain. This model is a generalization of the CORF models [Kim and Pavlovic 2010, Rudovic et al. 2012b], proposed for expression classification and temporal segmentation. The cs-CORF provides means of accounting for all six context questions from the W5+ context model. In [Rudovic et al. 2013], the authors demonstrate the influence of context on intensity estimation of facial expressions by modeling the context questions <i>who</i> (the observed person), <i>how</i> (the AU intensity-related changes in facial expressions), and <i>when</i> (the timing of the AU intensities). The context questions <i>who</i> and <i>how</i> are modeled by means of the newly introduced context and context-free covariate effects, while the context question <i>when</i> is modeled in terms of temporal correlation between the ordinal outputs, i.e., the AU intensity levels. The authors show that modeling of the context question <i>who</i> improves significantly the ability of the model to discriminate between the expression intensity levels of different subjects. Despite this, there is still a lot of room for improvement of existing models when it comes to context modeling. The basic limitation lies in the fact that context is a very broad concept and its formulation, when it comes to modeling of facial affect, is still unclear. For instance, answering the context questions such as <i>when</i> can be attempted at different levels: within the image sequences by modeling neighboring relationships between the frames, but also at a higher level by accounting for relationships between certain events and co-occurrence of AUs. While explicit models of context and their link to machine learning models is currently developing, we advocate that the context should <a id="page_88"/>not be taken as a rigid concept, but rather serve as a general approach with the aim of driving the development of target machine learning models. Also, context can have different meanings depending on modality that is used in the target user interfaces (e.g., answering context question <i>what</i> may refer to the verbal contact extracted from audio modality, which is not present within the tactile modality). Besides these context dependencies that can vary across modalities, it is of utmost importance for intelligent user interfaces, and employed machine learning models, to leverage the context in order to perform efficient fusion of multimodal data. Lastly, we argue that due to the broad definition of context, it should be application-tailored in order to focus on the most informative cues when modeling target affect.</p>
<p class="h1"><a id="ch3_5"/><b><span class="bg1">3.5</span>&#160;&#160;&#160;&#160;Model Adaptation</b></p>
<p class="noindent">Domain adaptation is currently a hot topic in machine learning, mainly because it allows generic models like SVMs to be adapted to a specific context. For instance, variation in head-pose and illumination (&#8220;where&#8221;) has been addressed by combining illumination invariant features with multiview learning techniques [Zhu and Ji 2006, Hu et al. 2008, Moore and Bowden 2011, Rudovicet al. 2013, Hesse et al. 2012, Eleftheriadis et al. 2015]. On the other hand, the individual differences among subjects (&#8220;who&#8221;) have mainly been tackled by accounting for the subject information at the training stage. Specifically, the original feature set is extended by adding the subject-specific features [Rudovic et al. 2015], or by building person-specific classifiers [Valstar et al. 2011]. Although these approaches showed improvement over generic classifiers, there is still a number of challenges to address. In particular, the multiview learning requires a large amount of images in various poses, which is typically not available. On the other hand, for building personalized classifiers, access to an adequate collection of images of the target person is essential. Consequently, existing approaches perform re-weighting previosly learned classifiers to fit the target data (e.g., Chu et al. [2013]), or training of new models using the additional target data. However, both of these are sub-optimal. Thus, the aim of domain adaptation approaches is to find an effective approach to adapt the already trained generic models for facial behavior analysis by using a small number of target data. In the case of the context question &#8220;where,&#8221; this boils down to adapting the frontal classifier to a non-frontal view using only a small number of expressive images from the target view/subject. This approach is expected to generalize better than generic classifiers learned from the available source and/or target (training) data. Below we review existing models and recent trends in (semi-supervised) domain adaptation and their relation to the affect modeling tasks.</p>
<p class="h2"><b>3.5.1</b> <a id="page_89"/><b>Domain Adaptation</b></p>
<p class="noindent">Domain adaptation is a well studied problem in machine learning (for an extensive survey, see Patel et al. [2015]). Here we review relevant (semi-)supervised adaptation approaches. For instance, Kulis et al. [2011] learns a transformation that maximizes similarity between data in the source and target domains by enforcing data pairs with the same labels to have high similarity, and pairs with different labels to be dissimilar. Then, a k-NN classifier is used to perform classification of target data. Hoffman et al. [2012] is an extension of this approach to multiple source domains. The input data are assumed to be generated from category-specific local domain mixtures, the mixing weights of which determine the underlying domain of the data, classified using an SVM classifier. Similarly, Hoffman et al. [2013] learns a linear asymmetric transformation to maximally align target features to the source domain. This is attained by introducing max-margin constraints that allow the learning of the transformation matrix and SVM classifier jointly. Donahue et al. [2013] extends the work in Hoffman et al. [2013] by introducing additional constraints to the max-margin formulation. More specifically, unlabeled data from the target domain are used to enforce the classifier to produce similar predictions for similar target-source data. While these methods attempt to directly align the target to source features, several works attempted this through a shared manifold. For instance, Duan et al. [2012] learns a nonlinear transformation from both source and target data to a shared latent space, along with the target classifier. Likewise, Yao et al. [2015] finds a low-dimensional subspace, which preserves the structure across the domains. The subspace is facilitated by projection functions that are learned jointly with the linear classifier. Again, the structure preservation constraints are used to ensure that similar data across domains are close in the subspace. Following the above line of research, in Zhang et al. [2016] the complementarity between audio-visual features is exploited to improve the performance of emotion recognition when model adaptation to a new database is required. Furthermore, methods for subspace learning within the same modality (audio) but different contexts (language) for emotion recognition have been recently proposed [Deng et al. 2014, Sagha et al. 2016].</p>
<p class="h2"><a id="ch3_5_2"/><b><span class="bg2">3.5.2</span>&#160;&#160;&#160;&#160;Domain Adaptation in Facial Behavior Analysis</b></p>
<p class="noindent">The majority of approaches for domain adaptation in the context of facial behavior analysis focus on building personalized classifiers for the test subjects. For instance, Miao et al. [2012] uses the supervised kernel mean matching (KMM) to align the source and target data distributions. This is achieved by re-weighting the source <a id="page_90"/>data, which, in combination with the target data, form the input features that are used to train the SVM [Cortes and Vapnik 1995] classifier for FER. Likewise, Chu et al. [2013] uses unsupervised KMM to learn person-specific AU detectors. This is attained by modifying the SVM cost function to account for the KMM between source and target data, adjusting the SVM&#8217;s hyperplane to the target test data. However, in this case the classifier has to be re-learned for each target subject. In Chen et al. [2013], a two-step learning approach is proposed for person-specific pain recognition and AU detection. First, data of each subject are regarded as different source domains, and are used to train weak Adaboost classifiers. Then, the weak classifiers are weighted based on their classification performance on the available target data. In Sangineto et al. [2014] and Chu et al. [2013], the Adaboost classifiers are replaced with the linear SVMs, and then the support vector regression (SVR) is employed to learn the mapping from the feature distribution to the parameters of the SVM classifier.</p>
<p class="indent">Note that, apart from Chen et al. [2013], all the works mentioned above perform in the unsupervised adaptation setting. While this requires less effort in terms of obtaining the labels for the target sub-sample, its underlying assumption is that target data can be well represented as a weighted combination of the source data. However, in real-world data, this assumption can easily be violated, resulting in poor performance of the adapted classifier. Nevertheless, both approaches have pros and cons, and ultimately it is a trade-off between the models accuracy and the labor-work needed to obtain annotated data. Finally, note that while majority of these models has been proposed for uni-modal learning, few extensions for multimodal approach have been proposed (see Patel et al. [2015] for details).</p>
<p class="h1"><a id="ch3_6"/><b><span class="bg1">3.6</span>&#160;&#160;&#160;&#160;Conclusion</b></p>
<p class="noindent">In this chapter, we provided an overview of machine learning models for affect-sensitive user interfaces, where multimodal signals are used to capture facial behavior. The major challenges in extracting shared information or correlations among signals of different type is that these signals should be noise and outliers free, while at the same time be synchronized in time. To address these challenges, several methods for correlation analysis have been presented in detail in <a href="#ch3_2">Section 3.2</a>. To extract human affect from multimodal signals, we identified the following three important modeling challenges. That is, we discussed the importance of temporal information in facial behavior and presented a set of machine learning methods capable of incorporating temporal information in the learning process. The second modeling challenge revolves around the context and the crucial role that it plays <a id="page_91"/>in understanding the human behavioral signals. To this end, machine learning methods incorporating context into the learning process have been discussed. Finally, we presented learning methods that are able to adapt existing models across different contextual factors as well as modalities.</p>
<p class="h1n"><a id="ch3_7"/><b>Focus Questions</b></p>
<p class="noindent"><b>3.1.</b> What are the three main factors affecting the performance of multimodal interfaces?</p>
<p class="noindentt"><b>3.2.</b> Why the temporal dimension of multimodal data is important in affect-sensitive interfaces?</p>
<p class="noindentt"><b>3.3.</b> What is &#8220;context&#8221; in affect-sensitive interfaces? What is the role of context within and between different modalities?</p>
<p class="noindentt"><b>3.4.</b> Why adaptability of affect-sensitive interfaces is important?</p>
<p class="noindentt"><b>3.5.</b> What is correlation among two signals? What is canonical correlation analysis?</p>
<p class="noindentt"><b>3.6.</b> What is correlated and the individual components among the multiple datasets?</p>
<p class="noindentt"><b>3.7.</b> How can we find correlation among multimodal signals of different length?</p>
<p class="noindentt"><b>3.8.</b> What are the main two approaches in the current research on automatic analysis of facial expressions?</p>
<p class="noindentt"><b>3.9.</b> How domain adaptation is applied in the context of facial behavior analysis?</p>
<p class="h1n"><a id="ch3_8"/><b>References</b></p>
<p class="ref">S. Akaho. 2011. A kernel method for canonical correlation analysis. In <i>Proceeedings of the 2011 International Meeting of the Psychometric Society</i>. Springer-Verlag. DOI: 10.1.1.108.6910&#38;rep=rep1. 76</p>
<p class="ref">G. Andrew, R. Arora, K. Livescu, and J. Bilmes. 2013. Deep canonical correlation analysis. In <i>Proceedings of the 2013 International Conference on Machine Learning</i>. Atlanta, Georgia. 76</p>
<p class="ref">P. K. Atrey, M. A. Hossain, A. El-Saddik, and M. S. Kankanhalli. 2010. Multimodal fusion for multimedia analysis: a survey. <i>Multimedia Systems</i>, 16(6): 345&#8211;379. DOI: 10.1007/s00530-010-0182-0. 76</p>
<p class="ref">F. R. Bach and M. I. Jordan. 2005. A probabilistic interpretation of canonical correlation analysis, Technical report. Department of Statistics University of California, Berkeley. 76</p>
<p class="ref"><a id="page_92"/>M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J. Movellan. 2005. Recognizing facial expression: machine learning and application to spontaneous behavior. In <i>CVPR</i>, vol. 2, pp. 568&#8211;573. DOI: 10.1109/CVPR.2005.297. 83</p>
<p class="ref">M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J. Movellan. 2006. Fully automatic facial action recognition in spontaneous behavior. In <i>IEEE FG</i>, pp. 223&#8211;230. DOI: 10.1109/FGR.2006.55. 84</p>
<p class="ref">J. Bazzo and M. Lamar. 2004. Recognizing facial actions using gabor wavelets with neutral face average difference. In <i>IEEE FG</i>, pp. 505&#8211;510. DOI: 10.1109/AFGR.2004.1301583. 84</p>
<p class="ref">M. J. Black and Y. Yacoob. 1997. Recognizing facial expressions in image sequences using local parameterized models of image motion. <i>International Journal of Computer Vision</i> 25, 23&#8211;48. DOI: 10.1023/A:1007977618277. 83</p>
<p class="ref">E. J. Candes, X. Li, Y. Ma, and J. Wright. 2011. Robust principal component analysis. <i>Journal of ACM</i> 58, 1&#8211;37. DOI: 10.1145/1970392.1970395. 76, 80</p>
<p class="ref">K.-Y. Chang, T.-L., Liu, and S.-H. Lai. 2009. Learning partially-observed hidden conditional random fields for facial expression recognition. In <i>IEEE CVPR</i>, pp. 533&#8211;540. DOI: 10.1109/CVPR.2009.5206612. 84</p>
<p class="ref">J. Chen, X. Liu, P. Tu, and A. Aragones. 2013. Learning person-specific models for facial expression and action unit recognition. <i>Pattern Recognition Letters</i> 34(15): 1964&#8211;1970. DOI: 10.1016/j.patrec.2013.02.002. 90</p>
<p class="ref">S. Chew, P. Lucey, S. Lucey, J. Saragih, J. Cohn, I. Matthews, and S. Sridharan. 2012. In the pursuit of effective affective computing: The relationship between features and registration. <i>Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on</i> 42(4): 1006&#8211;1016. DOI: 10.1109/TSMCB.2012.2194485. 84</p>
<p class="ref">D. Chu, L.-Z. Liao, M. K. Ng, and X. Zhang. 2013. Sparse canonical correlation analysis: New formulation and algorithm. <i>IEEE Transacrions on Pattern Analasis and Machine Intelligence</i>, 35(12): 3050&#8211;3065. DOI: 10.1109/TPAMI.2013.104. 76</p>
<p class="ref">W. Chu, and Z. Ghahramani. 2005. Gaussian processes for ordinal regression. <i>JMLR</i> 6, 1019&#8211;1041. 85</p>
<p class="ref">W.-S. Chu, F. D. L. Torre, and J. F. Cohn. 2013. Selective transfer machine for personalized facial action unit detection. In <i>CVPR</i>, pp. 3515&#8211;3522. DOI: 10.1109/CVPR.2013.451. 88, 90</p>
<p class="ref">I. Cohen, N. Sebe, L. Chen, A. Garg, and T. S. Huang. 2003. Facial expression recognition from video sequences: Temporal and static modelling. In <i>Computer Vision and Image Understanding</i>, pp. 160&#8211;187. DOI: 10.1016/S1077-3142(03)00081-X. 83</p>
<p class="ref">N. M. Correa, Y.-O. Li, T. Adali, and V. D. Calhoun. 2009. Fusion of fMRI, sMRI, and EEG data using canonical correlation analysis. In <i>IEEE ICASSP</i> pp. 385&#8211;388. DOI:10.1109/ICASSP.2009.4959601. 76</p>
<p class="ref">C. Cortes and V. Vapnik. 1995. Support-vector networks. <i>Machine learning</i>, 20(3): 273&#8211;297. DOI: 10.1023/A:1022627411411. 90</p>
<p class="ref"><a id="page_93"/>J. Deng, R. Xia, Z. Zhang, Y. Liu, and B. Schuller. 2014. Introducing shared-hidden-layer autoencoders for transfer learning and their application in acoustic emotion recognition. In <i>IEEE ICASSP</i>, pp. 4818&#8211;4822. DOI: 10.1109/ICASSP.2014.6854517. 89</p>
<p class="ref">L. V. der Maaten and E. Hendriks. 2012. Action unit classification using active appearance models and conditional random fields. <i>Cognitive Processing</i> 13(2): 507&#8211;518. DOI:10.1007/s10339-011-0419-7. 84</p>
<p class="ref">J. Donahue, J., Hoffman, E. Rodner, K. Saenko and T. Darrell. 2013. Semi-supervised domain adaptation with instance constraints. In <i>CVPR</i>, pp. 668&#8211;675. DOI: 10.1109/CVPR.2013.92. 89</p>
<p class="ref">D. Donoho. 2006. For most large underdetermined systems of equations, the minimal &#8467;<sub>1</sub>-norm near-solution approximates the sparsest near-solution. <i>Communications on Pure and Applied Mathematics</i>, 59(7): 907&#8211;934. DOI: 10.1002/cpa.20131/pdf. 80</p>
<p class="ref">L. Duan, D. Xu, and I. Tsang. 2012. Learning with augmented features for heterogeneous domain adaptation. In <i>ICML</i>. 89</p>
<p class="ref">P. Ekman, W. Friesen, and J. Hager. 2002. <i>Facial Action Coding System (FACS): Manual</i>. A Human Face. 83, 85, 86</p>
<p class="ref">S. Eleftheriadis, O. Rudovic, and M. Pantic. 2015. Discriminative shared gaussian processes for multiview and view-invariant facial expression recognition. <i>IEEE TIP</i>, 24(1): 189&#8211;204. DOI: 10.1109/TIP.2014.2375634. 88</p>
<p class="ref">B. Fasel and J. Luettin. 2000. Recognition of asymmetric facial action unit activities and intensities. In <i>ICPR</i>, vol. 1, pp. 1100&#8211;1103. DOI: 10.1109/ICPR.2000.905664. 84</p>
<p class="ref">M. Fazel. 2002. Matrix rank minimization with applications, Ph.D. thesis, Department of Electrical Engineering, Stanford University, CA. 80</p>
<p class="ref">J. Hamm, C. G. Kohler, R. C. Gur, and R. Verma. 2011. Automated facial action coding system for dynamic analysis of facial expressions in neuropsychiatric disorders. <i>Journal of Neuroscience Methods</i>, 200(2): 237&#8211;256. DOI: 10.1016/j.jneumeth.2011.06.023. 84</p>
<p class="ref">N. Hesse, T. Gehrig, H. Gao, and H. K. Ekenel. 2012. Multiview facial expression recognition using local appearance features. In <i>ICPR, IEEE</i>, pp. 3533&#8211;3536. 88</p>
<p class="ref">J. Hoffman, B. Kulis, T. Darrell, and K. Saenko. 2012. Discovering latent domains for multisource domain adaptation. In <i>ECCV</i>, pp. 702&#8211;715. DOI: 10.1.1.377.8406. 89</p>
<p class="ref">J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Darrell. 2013. Efficient learning of domain-invariant image representations. In <i>ICLR</i>. 89</p>
<p class="ref">H. Hotelling. 1936. Relations between two sets of variates. <i>Biometrika</i>, 28(3/4): 321&#8211;377. DOI: 10.1007/978-1-4612-4380-9_14.pdf. 76, 77, 78</p>
<p class="ref">Y. Hu, Z. Zeng, L. Yin, X. Wei, J. Tu, and T. Huang. 2008. A study of non-frontal-view facial expressions recognition. In <i>ICPR</i>, pp. 1-4. DOI: 10.1109/ICPR.2008.4761052. 88</p>
<p class="ref">D. Huang, R. S. Cabral, and F. De la Torre. 2012. Robust regression. In <i>ECCV</i>. 80</p>
<p class="ref">P. J. Huber. 1981. <i>Robust Statistics</i>. Wiley. 76, 77, 79</p>
<p class="ref"><a id="page_94"/>S. Jain, C. Hu, and T. Aggarwal. 2011. Facial expression recognition with temporal modeling of shapes. In <i>ICCV Workshops</i>, pp. 1642&#8211;1649. DOI: 10.1109/ICCVW.2011.6130446. 84</p>
<p class="ref">L. A. Jeni, J. M. Girard, J.T. Cohn, and F. D. L. Torre. 2013. Continuous au intensity estimation using localized, sparse facial feature space. <i>IEEE FG</i> pp. 1&#8211;7. DOI: 10.1109/FG.2013.6553808. 86</p>
<p class="ref">I. N. Junejo, E. Dexter, I. Laptev, and P. Perez. 2011. View-independent action recognition from temporal self-similarities. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 33, 172&#8211;185. DOI: 10.1109/TPAMI.2010.68. 76</p>
<p class="ref">S. Kaltwang, O. Rudovic, and M. Pantic. 2012. Continuous pain intensity estimation from facial expressions. <i>ISVC</i>, 7432, 368&#8211;377. DOI: 10.1007/978-3-642-33191-6_36. 86</p>
<p class="ref">S. Kaltwang, S. Todorovic, and M. Pantic. 2015. Latent trees for estimating intensity of facial action units. In <i>IEEE CVPR</i>. Boston, MA. DOI: 10.1109/CVPR.2015.7298626. 87</p>
<p class="ref">A. Kapoor, Y. A. Qi, and R. W. Picard. 2005. Hyperparameter and kernel learning for graph based semi-supervised classification. In <i>AMFG. IEEE Computer Society</i>, pp. 195&#8211;202. 84</p>
<p class="ref">J. R. Kettenring. 1971. Canonical analysis of several sets of variables. <i>Biometrika</i> 58(3): 433&#8211;451. DOI: 10.1.1.154.3872. 77</p>
<p class="ref">M. Kim, and V. Pavlovic. 2010. Structured output ordinal regression for dynamic facial emotion intensity prediction. <i>ECCV</i>, pp. 649&#8211;662. DOI: 10.1007/978-3-642-15558-1_47. 85, 87</p>
<p class="ref">A. Klami, S. Virtanen, and S. Kaski. 2013. Bayesian canonical correlation analysis. <i>JMLR</i>, 14, 965&#8211;1003. 76</p>
<p class="ref">S. Koelstra, M. Pantic, and I. Patras. 2010. A dynamic texture based approach to recognition of facial actions and their temporal models. <i>IEEE TPAMI</i>, 32, 1940&#8211;1954. DOI:10.1109/TPAMI.2010.50. 85</p>
<p class="ref">B. Kulis, K. Saenko, and T. Darrell. 2011. What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. In <i>CVPR</i>, pp. 1785&#8211;1792. DOI: 10.1109/CVPR.2011.5995702. 89</p>
<p class="ref">F. D. la Torre. 2012. A least-squares framework for component analysis. <i>IEEE TPAMI</i>, 34(6): 1041&#8211;1055. DOI: 10.1109/TPAMI.2011.184. 77</p>
<p class="ref">S. Z. Li, Z. Lei, and M. Ao. 2009. The HFB face database for heterogeneous face biometrics research. In <i>Proceedings 2009 IEEE Computer Vision and Pattern Recognition Workshops</i>, pp. 1&#8211;8. DOI: 10.1109/CVPRW.2009.5204149. 75, 77</p>
<p class="ref">G. Liu and S. Yan. 2012. Active subspace: Toward scalable low-rank learning. <i>Neural Computaion</i> 24(12): 3371&#8211;3394. DOI: 10.1162/NECO_a_00369. 80</p>
<p class="ref">E. F. Lock, K. A. Hoadley, J. Marron, and A. B. Nobel. 2013. Joint and individual variation explained (JIVE) for integrated analysis of multiple data types. <i>The Annals of Applied Statistics</i>, 7(1): 523. DOI: 10.1214/12-AOAS597. 77, 78</p>
<p class="ref"><a id="page_95"/>D. G. Lowe. 2004. Distinctive image features from scale-invariant keypoints. <i>International Journal of Computer Vision</i>, 60(2): 91&#8211;110. DOI: 10.1023/B:VISI.0000029664.99615.94. 75</p>
<p class="ref">M. Mahoor, S. Cadavid, D. Messinger, and J. Cohn. 2009. A framework for automated measurement of the intensity of non-posed facial action units. <i>IEEE CVPR&#8217;W</i>, pp. 74&#8211;80. DOI: 10.1109/CVPRW.2009.5204259. 86</p>
<p class="ref">S. Mavadati, M. Mahoor, K. Bartlett, P. Trinh, and J. Cohn. 2013. Disfa: A spontaneous facial action intensity database. <i>IEEE Transactions on Affective Computing</i>, 4(2): 151&#8211;160. DOI: 10.1109/T-AFFC.2013.4. 86</p>
<p class="ref">Y.-Q. Miao, R. Araujo, and M. S. Kamel. 2012. Cross-domain facial expression recognition using supervised kernel mean matching. In <i>ICMLA</i>, pp. 326&#8211;332. DOI: 10.1109/ICMLA.2012.178. 89</p>
<p class="ref">S. Moore and R. Bowden. 2011. Local binary patterns for multiview facial expression recognition. <i>Computer Vision and Image Understanding</i>, 115(4): 541&#8211;558. DOI: 10.1016/j.cviu.2010.12.001. 88</p>
<p class="ref">B. K. Natarajan. 1995. Sparse approximate solutions to linear systems. <i>SIAM Journal of Computing</i> 24(2): 227&#8211;234. DOI: 10.1137/S0097539792240406. 80</p>
<p class="ref">M. A. Nicolaou, Y. Panagakis, S. Zafeiriou, and M. Pantic. 2014. Robust canonical correlation analysis: Audio-visual fusion for learning continuous interest. In <i>IEEE ICASSP</i>, pp. 1522&#8211;1526. DOI: 10.1109/ICASSP.2014.6853852. 77</p>
<p class="ref">M. A. Nicolaou, V. Pavlovic, and M. Pantic. 2014. Dynamic probabilistic CCA for analysis of affective behavior and fusion of continuous annotations. <i>IEEE TPAMI</i> 36(7): 1299&#8211;1311. DOI: 10.1109/TPAMI.2014.16. 76, 82</p>
<p class="ref">C. Padgett and G. W. Cottrell. 1996. Representing face images for emotion classification. In <i>NIPS</i>, pp. 894&#8211;900. MIT Press. 83</p>
<p class="ref">Y. Panagakis, M. A. Nicolaou, S. Zafeiriou, and M. Pantic. 2013. Robust canonical time warping for the alignment of grossly corrupted sequences. In <i>IEEE CVPR</i>. Portland, Oregon. DOI: 10.1109/CVPR.2013.76. 76, 77, 80, 81, 82</p>
<p class="ref">Y. Panagakis, M. A. Nicolaou, S. Zafeiriou, and M. Pantic. 2016. Robust correlated and individual component analysis. <i>IEEE TPAMI</i>, 38(8): 1665&#8211;1678. DOI: 10.1109/TPAMI.2015.2497700. 77</p>
<p class="ref">M. Pantic and M. Bartlett. 2007. Machine Analysis of Facial Expressions. <i>I-Tech Education and Publishing</i>, pp. 377&#8211;416. DOI: 10.5772/4847. 85, 86</p>
<p class="ref">M. Pantic, A. Nijholt, A. Pentland, and T. Huanag. 2008. Human-centred intelligent human? computer interaction (hci<sup>2</sup>): how far are we from attaining it? <i>International Journal of Autonomous and Adaptive Communications Systems</i> 1(2): 168&#8211;187. DOI: 10.1504/IJAACS.2008.019799. 74</p>
<p class="ref">M. Pantic and I. Patras. 2005. Detecting facial actions and their temporal segments in nearly frontal-view face image sequences. <i>Proceedings of IEEE International Conference Systems, Man and Cybernetics</i>, pp. 3358&#8211;3363. DOI: 10.1109/ICSMC.2005.1571665. 85</p>
<p class="ref"><a id="page_96"/>M. Pantic and I. Patras. 2006. Dynamics of facial expression: Recognition of facial actions and their temporal segments from face profile image sequences. <i>IEEE Transactions on Systems, Man and Cybernetics Part B</i>, 36(2): 433&#8211;449. DOI: 10.1109/TSMCB.2005.859075. 85</p>
<p class="ref">M. Pantic and L. J. Rothkrantz. 2004. Facial action recognition for facial expression analysis from static face images. <i>Transactions on Systems Man, and Cybernetics Part B</i>, 34(3): 1449&#8211;1461. DOI: 10.1109/TSMCB.2004.825931. 83</p>
<p class="ref">V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. 2015. Visual domain adaptation: A survey of recent advances. <i>IEEE Signal Processing Magazine</i>, 32(3): 53&#8211;69. DOI: 10.1109/MSP.2014.2347059. 89, 90</p>
<p class="ref">O. Rudovic, M. Pantic, and I. Patras. 2013. Coupled gaussian processes for pose-invariant facial expression recognition. <i>IEEE TPAMI</i>, 35(6): 1357&#8211;1369. DOI: 10.1109/TPAMI.2012.233. 88</p>
<p class="ref">O. Rudovic, V. Pavlovic, and M. Pantic. 2012a. Kernel conditional ordinal random fields for temporal segmentation of facial action units. <i>IEEE ECCV&#8217;W</i>. DOI: 10.1007/978-3-642-33868-7_26. 85</p>
<p class="ref">O. Rudovic, V. Pavlovic, and M. Pantic. 2012b. Multioutput laplacian dynamic ordinal regression for facial expression recognition and intensity estimation. <i>IEEE CVPR</i>. pp. 2634&#8211;2641. DOI: 10.1109/CVPR.2012.6247983. 85, 87</p>
<p class="ref">O. Rudovic, V. Pavlovic, and M. Pantic. 2013. Context-sensitive conditional ordinal random fields for facial action intensity estimation. In <i>ICCV&#8217;W</i>, pp. 92&#8211;499. DOI: 10.1109/ICCVW.2013.70. 86, 87</p>
<p class="ref">O. Rudovic, V. Pavlovic, and M. Pantic. 2015. Context-sensitive dynamic ordinal regression for intensity estimation of facial action units. <i>IEEE TPAMI</i>, 37(5): 944&#8211;958. DOI: 10.1109/TPAMI.2014.2356192. 88</p>
<p class="ref">H. Sagha, J. Deng, M. Gavryukova, J. Han, and B. Schuller. 2016. Cross lingual speech emotion recognition using canonical correlation analysis on principal component subspace. In <i>IEEE ICASSP</i>, pp. 5800&#8211;5804. DOI: 10.1109/ICASSP.2016.7472789. 89</p>
<p class="ref">H. Sakoe and S. Chiba. 1978. Dynamic programming algorithm optimization for spoken word recognition. <i>IEEE Transactions on Acoustics, Speech, and Signal Processing</i>, (1): 43&#8211;49. DOI: 10.1109/TASSP.1978.1163055. 76, 77, 78</p>
<p class="ref">E. Sangineto, G. Zen, E. Ricci, and N. Sebe. 2014. We are not all equal: Personalizing models for facial expression analysis with transductive parameter transfer. In <i>ACM Multimedia</i>, pp. 357&#8211;366. DOI: 10.1145/2647868.2654916. 90</p>
<p class="ref">A. Savrana, B. Sankur, and M. Bilgeb. 2012. Regression-based intensity estimation of facial action units. <i>Image and Vision Computing</i>, 30(10): 774&#8211;784. DOI: 10.1016/j.imavis.2011.11.008. 86</p>
<p class="ref">C. Shan, S. Gong, and P, W. McOwan. 2007. Beyond facial expressions: Learning human emotion from body gestures. In <i>Proceedings 2007 British Machine Vision Conference</i>, pp. 1&#8211;10. DOI: 10.5244/C.21.43. 76</p>
<p class="ref"><a id="page_97"/>C. Shan, S. Gong, and P. W. McOwan. 2009. Facial expression recognition based on local binary patterns: A comprehensive study. <i>Image and Vision Computing</i>, 27(6): 803&#8211;816. DOI: 10.1016/j.imavis.2008.08.005. 83</p>
<p class="ref">T. Simon, M. H. Nguyen, F. De la Torre, and J. F. Cohn. 2010. Action unit detection with segment-based svms. In <i>IEEE CVPR</i>. DOI: 10.1109/CVPR.2010.5539998. 84</p>
<p class="ref">K. Simonyan, A. Vedaldi, and A. Zisserman. 2014. Learning local feature descriptors using convex optimisation. <i>IEEE TPAMI</i>, 36(8): 1573&#8211;1585. DOI: 10.1109/TPAMI.2014.2301163. 75</p>
<p class="ref">L. Sun, S. Ji, and J. Ye. 2011. Canonical correlation analysis for multilabel classification: A least-squares formulation, extensions, and analysis. <i>IEEE TPAMI</i>, 33(1): 194&#8211;200. DOI: 10.1109/TPAMI.2010.160. 76, 77</p>
<p class="ref">S. Sun. 2013. A survey of multiview machine learning. <i>Neural Computing and Applications</i>, 23, (7&#8211;8). DOI: 10.1007/s00521-013-1362-6. 76</p>
<p class="ref">Y.-L. Tian. 2004. Evaluation of face resolution for expression analysis. In <i>IEEE CVPR&#8217;W</i>, pp. 82&#8211;82. DOI: 10.1109/CVPR.2004.334. 83</p>
<p class="ref">Y. Tong, W. Liao, and, Q. Ji. 2007. Facial action unit recognition by exploiting their dynamic and semantic relationships. <i>IEEE TPAMI</i>, 29(10): 1683&#8211;1699. DOI: 10.1109/TPAMI.2007.1094. 84, 85</p>
<p class="ref">G. Trigeorgis, M. A. Nicolaou, S. Zafeiriou, and B. W. Schuller. 2016. Deep canonical time warping. In <i>IEEE CVPR</i>, pp. 5110&#8211;5118. DOI: 10.1109/CVPR.2016.552. 77, 83, 84</p>
<p class="ref">G. Tzimiropoulos, S. Zafeiriou, and M. Pantic. 2012. Subspace learning from image gradient orientations. <i>IEEE TPAMI</i>, 34(12): 2454&#8211;2466. DOI: 10.1109/TPAMI.2012.40. 75</p>
<p class="ref">M. F. Valstar, B. Jiang, M. Mehu, M. Pantic, and K. Scherer. 2011. The first facial expression recognition and analysis challenge. In <i>FG</i>, pp. 921&#8211;926. DOI: 10.1109/FG.2011.5771374. 88</p>
<p class="ref">M. F. Valstar and M. Pantic. 2012. Fully automatic recognition of the temporal phases of facial actions. <i>IEEE Transactions on Systems, Man, and Cybernetics</i> 42, 28&#8211;43. DOI: 10.1109/TSMCB.2011.2163710. 85</p>
<p class="ref">L. Vandenberghe and S. Boyd. 1996. Semidefinite programming. <i>SIAM Review</i>, 38(1): 49&#8211;95. 80</p>
<p class="ref">A. Vinciarelli, M. Pantic, and H. Bourlard. 2009. Social signal processing: Survey of an emerging domain. <i>Image and Vision Computing</i>, 27(12): 1743&#8211;1759. DOI: 10.1016/j.imavis.2008.11.007. 74</p>
<p class="ref">R. Walecki, O. Rudovic, M. Pantic, and V. Pavlovic. 2016a. Copula ordinal regression for joint estimation of facial action unit intensity. <i>IEEE CVPR</i>, Las Vegas, Nevada. DOI: 10.1109/CVPR.2016.530. 87</p>
<p class="ref">R. Walecki, O. Rudovic, V. Pavlovic, and M. Pantic. 2017. Variable-state latent conditional random field models for facial expression analysis. <i>IMAVIS</i>, vol. 58, pp. 25&#8211;37. DOI: 10.1016/j.imavis.2016.04.009. 84</p>
<p class="ref"><a id="page_98"/>S. Wang, A. Quattoni, L.-P. Morency, D. Demirdjian, and T. Darrell. 2006. Hidden conditional random fields for gesture recognition. <i>IEEE CVPR</i>, pp. 1097&#8211;1104. DOI: 10.1109/CVPR.2006.132. 84</p>
<p class="ref">X. Wang and X. Tang. 2009. Face photo-sketch synthesis and recognition. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 31(11): 1955&#8211;1967. DOI: 10.1109/TPAMI.2008.222. 75, 76</p>
<p class="ref">M. W&#246;llmer, M. Al-Hames, F. Eyben, B. Schuller, and G. Rigoll. 2009. A multidimensional dynamic time warping algorithm for efficient multimodal fusion of asynchronous data streams. <i>Neurocomputing</i>, 73(1): 366&#8211;380. DOI: 10.1016/j.neucom.2009.08.005. 77</p>
<p class="ref">M. W&#246;llmer, M., Kaiser, F. Eyben, B. Schuller, and G. Rigoll. 2013. Lstm-modeling of continuous emotions in an audiovisual affect recognition framework. <i>Image and Vision Computing</i>, 31(2): 53&#8211;163. DOI: 10.1016/j.imavis.2012.03.001. 83</p>
<p class="ref">P. Yang, Q. Liu, and D. N. Metaxas. 2009. Boosting encoded dynamic features for facial expression recognition. <i>Pattern Recognition Letters</i>, (2): 132&#8211;139. DOI: 10.1016/j.patrec.2008.03.014. 84</p>
<p class="ref">Y. Yao, Y. Pan, C-W. Ngo, H. Li, and T. Mei. 2015. Semi-supervised domain adaptation with subspace learning for visual recognition. In <i>CVPR</i>, pp. 2142&#8211;2150. DOI: 10.1109/CVPR.2015.7298826. 89</p>
<p class="ref">Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang. 2009. A survey of affect recognition methods: Audio, visual, and spontaneous expressions. <i>IEEE TPAMI</i>, 31(1): 39&#8211;58. DOI: 10.1109/TPAMI.2008.52. 76</p>
<p class="ref">Y. Zhang and Q. Ji. 2005. Active and dynamic information fusion for facial expression understanding from image sequences. <i>IEEE TPAMI</i>, 27(5): 699&#8211;714. DOI: 10.1109/TPAMI.2005.93. 84</p>
<p class="ref">Z. Zhang, F. Ringeval, B. Dong, E. Coutinho, E. Marchi, and B. Sch&#252;ller. 2016. Enhanced semi-supervised learning for multimodal emotion recognition. In <i>IEEE ICASSP</i>, pp. 5185&#8211;5189. DOI: 10.1109/ICASSP.2016.7472666. 89</p>
<p class="ref">F. Zhou and F. De la Torre Frade. 2012. Generalized time warping for multimodal alignment of human motion. In <i>IEEE CVPR</i>. 82</p>
<p class="ref">F. Zhou and F. D. la Torre. 2009. Canonical time warping for alignment of human behavior. In <i>Advances in Neural Information Processing Systems</i>, pp. 2286&#8211;2294. 76, 77, 78, 79, 82</p>
<p class="ref">G. Zhou, A. Cichocki, and S. Xie. 2012. Common and individual features analysis: beyond</p>
<p class="ref">canonical correlation analysis. <i>arXiv preprint arXiv:1212.3913</i>. 76, 77, 78</p>
<p class="ref">Z. Zhu and Q. Ji. 2006. Robust real-time face pose and facial expression recovery. In <i>CVPR</i>, vol. 1, pp. 681&#8211;688. DOI: 10.1109/CVPR.2006.259. 88</p>
<p class="line"/>
<p class="note"><a id="fn1" href="#rfn1">1</a>. <img src="../images/pg78_03.png" alt="Image"/></p>
</body>
</html>