<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_375"/>12</p>
<p class="chtitle"><b>Multimodal Assessment of Depression from Behavioral Signals</b></p>
<p class="chauthor"><b>Jeffrey F. Cohn, Nicholas Cummins, Julien Epps, Roland Goecke, Jyoti Joshi, Stefan Scherer</b></p>
<p class="h1-1"><a id="ch12_1"/><b><span class="bg1">12.1</span>&#160;&#160;&#160;&#160;Introduction</b></p>
<p class="noindent">Reliable, valid, and efficient assessment of depression is critical to identify individuals in need of treatment and to gauge treatment response. Current methods of assessment are limited to subjective measures of patient self-report and clinical interview. They fail to take into account observable measures of behavior that could better inform detection of the occurrence and severity of depression. Recent advances in computer vision, signal processing, and machine learning have potential to meet the need for improved depression screening, diagnosis, and ascertainment of severity (i.e., assessment). This chapter reviews these advances. We describe multimodal measures of behavior and physiology, how these measures can be processed to extract features sensitive to depression, and how classification or prediction may be used to provide automatic assessment of depression occurrence and severity.</p>
<p class="indent">Following an overview in <a href="#ch12_3">Section 12.3</a> of how behavioral and physiological sensor signals can be processed in a multimodal manner, <a href="#ch12_4">Sections 12.4</a>&#8211;<a href="#ch12_7">12.7</a> discuss in detail how to extract features sensitive to depression, based on insights from earlier scientific investigations and from studies of existing clinical assessment. These latter sections also touch on key machine learning methods that have been adopted to date. Of particular interest is the fusion of information from different modalities, and <a href="#ch12_8">Section 12.8</a> discusses fusion in the context of both classification and prediction.</p>
<p class="indent"><a id="page_376"/>For researchers investigating automatic analysis of depression, high-quality research data is a critical concern. Similarly important for the practical application of multimodal depression assessment systems is their likely context of use. Both concerns are discussed in <a href="#ch12_9">Section 12.9</a>, while <a href="#ch12_10">Section 12.10</a> provides an overview of key challenges in this research area.</p>
<p class="indent">In this chapter, the focus is primarily on depression. This emphasis reflects the emphasis in the field. Most work to date on automated, multimodal assessment of psychopathology has focused on depression. Other disorders, such as posttraumatic stress disorder, generalized anxiety disorder, traumatic brain injury, suicidality, dementia, Alzheimer&#8217;s disease, schizophrenia, Parkinson&#8217;s disease, and autism spectrum disorder, have received less attention and accordingly are not discussed in detail. The approaches and many of the methods we discuss, nevertheless, are applicable to disorders that share behavioral features, are comorbid with depression, or otherwise share variance with depression. Dimensional models of psychopathology [HiTOP Undated, Kotov et al. 2010] in particular group unipolar depression (Major Depressive Disorder and Dysthymia), Generalized Anxiety Disorder, PTSD, and in some models Borderline Personality Disorder together as distress orders. From this perspective, the research we review on depression is relevant to these disorders as well.</p>
<p class="h1-1"><a id="ch12_2"/><b><span class="bg1">12.2</span>&#160;&#160;&#160;&#160;Depression</b></p>
<p class="noindent"><i><b>Depression</b></i> is one of the most common mental disorders [Kessler et al. 2005] and a leading cause of disease burden worldwide [Mathers and Loncar 2011, World Health Organization 2014]. It severely impacts quality of life, is costly both for affected individuals and society, and increases risk for suicide [Blair-West et al. 1999].</p>
<p class="indent">Depression may be <i>uni</i>polar or <i>bi</i>polar. Unipolar is the more common kind and is typically what is meant by &#8220;depression.&#8221; Bipolar depression includes manic or hypomanic episodes in addition to depressive episodes over the course of the disorder. Bipolar depression has not been a focus of research in multimodal machine learning; it is a topic for future work.</p>
<p class="indent">Although there are a several variants of (unipolar) depression, the most frequent and severe form is Major Depressive Disorder (MDD), which has a 1-year prevalence of about 7% in adults [American Psychiatric Association 2015]. MDD and all forms of depression are distinct from sadness, a normal emotion that has adaptive cognitive and interpersonal benefits [Forgas 2014]. Symptoms of depression include: markedly depressed mood or markedly diminished interest or pleasure most of the day; significant change in weight or appetite; insomnia or hypersomnia; <i>psychomotor retardation</i> or agitation that is observable by other persons; diminished ability to think or concentrate; increased indecisiveness; fatigue or loss of energy; feelings of worthlessness or excessive or inappropriate guilt; and recurrent thoughts of death and suicidal ideation, intentions, or actions [American Psychiatric Association [2]2015]. To meet diagnostic criteria for MDD, markedly depressed mood or loss of interest and four or more of the symptoms are required over a period of two weeks or more, and these symptoms may not be due to another disorder, medical condition, or other factor [American Psychiatric Association 2015]. Because there are many possible combinations of these symptoms, depression is a heterogeneous disorder that varies in presentation and severity [&#216;stergaard et al. 2011]. It varies as well in etiology, functions, prognosis, and course.</p>
<div class="box">
<p class="bhead"><a id="page_377"/><b>Glossary</b></p>
<p class="hangbx"><b>Active Appearance Model</b> (AAM). An AAM is a statistical model of shape and grey-level appearance that can generalize to almost any face [Edwards, Taylor, &#38; Cootes, 1998; Matthews &#38; Baker, 2004]. An AAM seeks to find the model parameters that can generate a synthetic image as close as possible to a target image [Cootes &#38; Taylor, 2004]. AAMs are learned from hand-labelled training data.</p>
<p class="hangbx"><b>Action units</b> and action descriptors are the smallest visually discriminable facial movements. Action units are movements for which the anatomic basis is known [Cohn, Ambadar, &#38; Ekman, 2007]. They are represented as either binary events (presence vs. absence) or with respect to five levels of ordinal intensity. Action units individually or in combinations can represent nearly all possible facial expressions.</p>
<p class="hangbx"><b>Bag-of-words</b> (BoW) is a data-driven algorithm for summarizing large volumes of features. It can be thought of as a histogram whose bins are determined by partitions (or clusters) of the feature space.</p>
<p class="hangbx"><b>Depression</b> refers broadly to the persistence over an extended period of time of several of the following symptoms: lowered mood, interest, or pleasure; psychomotor retardation; psychomotor agitation; diminished ability to think/concentrate; increased indecisiveness; fatigue or loss of energy; insomnia; hypersomnia; significant weight loss or weight gain; feelings of worthlessness or excessive guilt; and recurrent thoughts of death or recurrent suicidal ideation. It is important to note that there are multiple definitions of depression (see references in <a href="#ch12_2">Section 12.2</a>).</p>
<p class="hangbx"><b>FACS</b> refers to the Facial Action Coding System [Ekman &#38; Friesen, 1978; Ekman, Friesen, &#38; Hager, 2002]. FACS describes facial activity in terms of anatomically based action units (AUs). Depending on the version of FACS, there are 33 to 44 AUs and a large number of additional &#8220;action descriptors&#8221; and other movements.</p>
<p class="hangbx"><b>Gaussian mixture models</b> (GMMs) are probability density functions comprising a weighted sum of individual Gaussian components, each with their own mean and covariance. They are commonly employed to compactly characterize arbitrary distributions (e.g. of features) that are not well-fitted by a single Gaussian.</p>
<p class="hangbx"><b>Hand-crafted features</b> refer to features developed to extract a specific type of information, usually as part of a hypothesis-driven research study. By contrast, data-driven features are those extracted automatically from raw signal data by algorithms (e.g., neural networks), whose physical interpretation often cannot easily be described.</p>
<p class="hangbx"><b>Longitudinal data</b> refers to multiple recordings of the same type from the same individual at different points in time, between which it is likely that the individual&#8217;s state (e.g., depression score) has changed.</p>
<p class="hangbx"><a id="page_378"/><b>Mel frequency cepstral coefficients</b> (MFCCs) are features that compactly represent the short-term speech spectrum, including formant information, and are widely used to characterize both spoken content (for automatic speech recognition) and speaker-specific qualities (for automatic speaker verification). Briefly, a mel-scale frequency-domain filterbank is applied to the spectrum to obtain mel filterbank energies, the log of which is transformed to a lower-dimensional representation using the discrete cosine transform.</p>
<p class="hangbx"><b>Overfitting</b> is a problem that occurs when the training or estimation of a machine learning method is performed on data with too few training examples relative to the number of parameters to be estimated. The resulting problem is that the method becomes too closely tuned to the training data, and generalizes poorly to unseen test data.</p>
<p class="hangbx"><b>Spatio-temporal</b> features are those which have both a spatial and a time dimension. For example, the intensity of pixels in a video vary both in terms of their position within a given frame (spatial dimension) and in terms of the frame number for a given pixel coordinate (temporal dimension).</p>
<p class="hangbx"><b>Support vector machine (SVM)</b> is a widely used discriminative classification method, which defines a separating hyperplane between two classes of features, which is defined in terms of particular feature instances that are close to the class boundaries, called support vectors.</p>
<p class="hangbx"><b>Support vector regression (SVR)</b> is a commonly used method for multivariate regression, which concentrates on fitting a model by considering only training features that are not very close to the model prediction.</p>
<p class="hangbx"><b>Voice quality</b> refers to the type of phonation during voiced speech. Depending on the physical movement of the vocal folds during phonation, the perceived quality of speech can change, even for the same speech sound uttered at the same pitch. Descriptors such as &#8220;creaky&#8221; and &#8220;breathy&#8221; are applied to specific modes of vocal fold vibration.</p>
<p class="hangbx"><b>Vowel space area</b> is a term given to the two dimensional area enclosed by lines connecting pairs of vowels in the formant (F1/F2) space.</p>
</div>
<p class="indent"><a id="page_379"/>Depression often is comorbid with other disorders and may be a prominent component of them as well. Generalized Anxiety Disorder (GAD) is often comorbid with depression, and depression is a strong component of Post-Traumatic Stress Disorder (PTSD). Commonalities among depression and other disorders have motivated calls for dimensional models of psychopathology [HiTOP Undated, Watson and Clark 2006, Watson et al. 2007] in place of the more traditional categorical models. Because most efforts in computer vision and signal processing relevant to depression have been informed by categorical models of depression, we pursue that perspective in the following. We wish to acknowledge, however, that the close relation between depression and other disorders, especially GAD and PTSD, implies that the approaches discussed herein are likely relevant to other disorders or dimensions of psychopathology as well.</p>
<p class="h2-1"><a id="ch12_2_1"/><b><span class="bg2">12.2.1</span>&#160;&#160;Assessment of Depression</b></p>
<p class="noindent">While laboratory tests for depression have been proposed (e.g., dexamethasone suppression test) [Carroll 1984], none had proven specific for depression. To date, screening and diagnosis depend primarily on subjective reports from patients, their families, or caregivers [Blais and Baer 2010]. Diagnostic interviews include the SCID-5 [First et al. 2015] and the M.I.N.I. [Sheehan et al. 1997].</p>
<p class="indent">These instruments can require an hour or more to administer, are highly dependent on the expertise of the interviewer, and may require additional measures to assess severity. Based on clinical trials, the Hamilton Rating Scale for Depression (HRSD) [Hamilton 1960] is the current standard for assessing severity and treatment response [Gotlib and Hammen 2002; Zimmerman et al. 2004].</p>
<p class="indent">For screening purposes, the Beck Depression Index (BDI) [Beck et al. 1996], the Center for Epidemiological Studies Depression Scale [Eaton et al. 2004, Radloff 1977], and the Patient Health Questionnaire (PHQ-9) [Kroenke et al. 2001] are widely used. These are all self-report inventories and lack <i>specificity</i> for depression (e.g., Campbell and Cohn 1991). Newer screening techniques, such as Computing Adaptive Testing, tailor a test paradigm to match a patient&#8217;s impairment level [Gibbons et al. 2012], but have yet to achieve clinical impact.</p>
<p class="indent"><a id="page_380"/>None of these methods explicitly take into account observable measures of behavior. Depression has marked observable influence on psychomotor functioning (retardation or agitation), expression of affect (reductions in positive affect and increases in negative), and interpersonal communication. Modalities include facial expression, gaze, head and body motion, and vocalization. The approaches reviewed herein quantify objective changes in these modalities to inform our understanding of depression and to detect depression and depression severity. They answer the need for reliable, valid, and efficient assessment of depression for research and clinical use.</p>
<p class="indent">Since 2009 the National Institute of Mental Health (NIMH), through the Research Domain Criteria (RDoC) project, has been encouraging research into new diagnostic tools for mental illness based on neurobiology and measurable behavioral signals [Cuthbert and Insel 2013]. RDoC and related dimensional paradigms [HiTOP Undated] emphasize broad, hierarchical constructs, such as distress, fear, and substance use disorders. For instance, both depression and generalized anxiety disorder are considered stress disorders by the hierarchical taxonomy of psychopathology [HiTOP Undated]. Multimodal assessment can inform the relation between disorders and contribute to new diagnostic understanding.</p>
<p class="indent">The goals for objective, multimodal assessment of depression include: (1) screening for depression, i.e., identifying from behavioral signals individuals who warrant referral; (2) aiding rather than substituting or replacing clinical diagnosis, since diagnostic criteria include information that is occult to multimodal assessment (e.g., duration of episode) and like self-report measures lack specificity for depression (i.e., cannot rule out other causes for symptoms); (3) measuring severity of depression in individuals who have been diagnosed; and (4) contributing to research on the validity of categorical vs. dimensional perspectives on psychopathology. Clinically, multimodal assessment may be conducted in diverse contexts over extended periods of time without incurring reactivity effects. The remainder of this chapter offers a concise overview of the use of behavioral signals for the objective assessment of depression using automatic methods of feature extraction and classification or prediction.</p>
<p class="h1-1"><a id="ch12_3"/><b><span class="bg1">12.3</span>&#160;&#160;&#160;&#160;Multimodal Behavioral Signal Processing Systems</b></p>
<p class="noindent">Behavioral signal processing research shows potential advantages for assessing depression [Girard and Cohn 2015, Cummins et al. 2015a] and related disorders by employing quantitative methods to capture and model key behavioral signals. The term behaviomedics has even been coined to describe this broader research area <a id="page_381"/>[Valstar 2014a]. Within the psychopathology literature alterations in facial movements, speech activity and body movement are well reported. Recently, significant advances have been made into the automatic detection of depression using one or more of these behavioral modalities [Cummins et al. 2013, Girard et al. 2013, Joshi et al. 2013a, Scherer et al. 2014, Williamson et al. 2014], building on the recent availability of larger (publicly available) datasets collected under controlled conditions (<a href="#ch12_9">Section 12.9</a>).</p>
<p class="indent">The general structure of an automatic multimodal system for assessing depression is shown in <a href="#fig12_1">Figure 12.1</a>, and begins with the acquisition of raw signal data from sensors (e.g., microphones, cameras, eye trackers, accelerometers). From these signals, features are extracted over regular intervals of time (frames or windows). The features characterize some aspect of the raw signal in a compact format, and are designed or chosen to be ideally both sensitive and specific to depression. Possibly after temporal alignment or synchronization across different modes, the features are then input to a machine-learning algorithm, which maps them to a depression state or score, based on prior knowledge in the form of labelled features from diverse levels of depression severity. To date, researchers have focused on three types of mappings: (i) depression detection, i.e., classification between non-depressed and depressed categories; (ii) severity recognition, i.e., classification between two or more ordinal categories (or score groups) of depression severity; and (iii) prediction, i.e., regression on the features to estimate a numerical depression score.</p>
<div class="cap" id="fig12_1">
<p class="image"><img src="../images/fig12_1.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 12.1</b>&#160;&#160;Overview of typical multimodal system for automatic assessment of depression, showing two modalities, with two of the more commonly employed different fusion possibilities shown dashed.</p>
</div>
<p class="h1"><a id="page_382"/><a id="ch12_4"/><b><span class="bg1">12.4</span>&#160;&#160;&#160;&#160;Facial Analysis</b></p>
<p class="noindent">Since well before the automatic analysis of facial images began, clinicians and clinical scientists using manual analysis methods have observed marked changes in facial expression related to depression. Decreased smiling and laughter have been reported frequently (for citations, see Girard et al. [2014b]). Findings for negative expressions have been more mixed, with some studies reporting increases and others decreases, which may reflect the heterogeneity of depression. A key development toward automated measures was work by Cohn et al. [2009]. They applied both Ekman&#8217;s manual Facial Action Coding System <i><b>(FACS)</b></i> [Ekman et al. 2002] and automated facial image analysis using <i><b>active appearance models (AAM)</b></i> to a large longitudinal database (more than 50 individuals) of depressed and no-longer depressed patients with the goal of comparing manual and automated methods. They found that automatic facial analysis closely approached manual coding in terms of two-class depressed/no-longer depressed classification accuracy. Together with follow-up studies using both automatic and manual FACS coding [Girard et al. 2013, Girard et al. 2014b], they found high consistency both between manual and automatic coding of action units and in the pattern of depression effects that were found. This work opened the door to significant new opportunities for facial expression analysis using computer vision methods.</p>
<p class="indent">FACS describes facial expressions taxonomically in terms of <i><b>action units</b></i> (AU), which are the fundamental actions of individual muscles or muscle groups [Cohn et al. 2007]. An overview of literature on facial expression changes due to depression by Scherer et al. [2014] found that decreases in smiling and mouth movement and emotional expressivity and an increase in frowns are common across multiple studies. In the facial visual behavior descriptors selected for their own studies, Scherer et al. [2014] list smile intensity and smile duration, with the latter being important because some papers have reported depressed individuals smiling often but briefly. Smiling relates in part to context. Reed et al. [2007] found that currently depressed subjects smiled as often as comparison subjects in response to comedy but more <a id="page_383"/>often used smile controls to dampen their smiles. Girard et al. [2013] found that among severely depressed individuals, both smiling and non-smiling expressions were more often associated with facial actions associated with contempt than for healthy controls. These kinds of changes are associated with lower AU 12 (lip corner puller/zygomaticus major) activity and higher AU 14 (dimpler/buccinator) activity. The influence of depression on negative affectivity also provides motivation for extracting facial information.</p>
<p class="indent">Most approaches to automated face analysis involve face detection, detection, and tracking of facial landmarks, or fiduciary points, feature extraction, and action unit or expression detection. For nearly frontal face images (plus/minus about 15&#8211;20&#176;), face detection and rough head pose estimation can be accomplished using the Viola and Jones (VJ) face detector (OpenCV and MATLAB implementations available). For the VJ face detector, the required training time is longer and the false positive rate is higher than another recently proposed face detector available in the dlib C++ library, based on Histogram of Oriented Gradients (HOG) and a structural <i><b>support vector machine (SVM)</b></i> based training algorithm.</p>
<p class="indent">To quantify the shape and appearance of a face, deformable models have often been used. These include the active appearance model (AAM) [Baker et al. 2004, Cootes et al. 2002] and constrained local model [Baltrusaitis et al. 2012, Saragih et al. 2009]. The active appearance model is a two-dimensional triangular mesh with vertices arranged near fiduciary points in the facial image (e.g., the lower facial boundary, eyebrows, eyes, and nose). In order to fit the model, some supervised learning is needed, which can be achieved by manually labeling a small proportion of video frames (5% in Cohn et al. [2009]) of a particular individual. Asthana et al. [2009] showed that a regression-based automatic face annotation and model building approach that only requires annotated frontal images, in the extreme case just a single annotated frontal image, drastically simplifies the deformable model building process. This is possible by posing the problem of learning the pattern of manual annotation between frontal face images, having arbitrary expressions, and corresponding face images at different poses in a data-driven regression framework.</p>
<p class="indent">In many cases, it is desirable that the initial active appearance model (before fitting) is a global model (e.g., across all sessions), so that a common basis for interpreting the model parameters for each individual or session can be established. Various features can be extracted from the positions, velocities, and statistics of the vertices across a particular recording, and dimensionality reduction methods may be used to find a more compact representative feature set, given the typically large number of vertices.</p>
<p class="indent"><a id="page_384"/>More recently, supervised descent based models [Xiong and De la Torre 2013] have made possible precise measurement of dense facial features that require no person-specific training. The approach of [Jeni et al. 2015] is capable of 3D alignment from 2D video over a radius of about plus/minus 60&#176;, which is more than sufficient for the range of head pose observed in spontaneous facial expression in interviews and small-group interactions.</p>
<p class="indent">A variety of approaches to action unit detection have been proposed (for reviews, see Cohn and De la Torre [2015], Corneanu et al. [2015]). They typically involve face and facial feature detection, extraction of shape, appearance, or <i>spatio-temporal</i> features and training a classifier for action unit detection. Recent approaches eschew the use of <i>hand-crafted features</i> (e.g., SIFT or Gabor) in favor of empirical approaches to such as convolutional neural networks. Initial efforts suggest that such approaches are competitive with conventional approaches for action unit detection and may generalize better across databases [Ghose et al. 2015]. A review is beyond the scope of the current chapter. Error analyses suggest that action unit detection is reliable within a range of about plus/minus 20&#176;from frontal and is comparable for men and women and robust to ethnic differences, although more work on this topic is needed [Girard et al. 2014a, Chu et al. 2016].</p>
<p class="indent">These approaches to depression measurement or detection all involve learning a mapping from the occurrence or intensity of facial actions to depression. An alternative or adjunct approach is to learn a mapping directly from the timing of facial movement to depression. At least three appear promising. One [Dibeklioglu et al. 2015] quantifies the dynamics of facial motion and head motion from dense 3D tracking [Jeni et al. 2015] irrespective of the configuration of facial expression. This approach samples a rich set of dynamic features (<a href="#fig12_2">Figure 12.2</a>). For the binary classification of depressed versus remitted, they obtained a correct classification rate of 86%. For the more challenging three-state classification of depressed, intermediate, and remitted, they obtained a correct classification rate of 78%.</p>
<p class="indent">A second unsupervised approach is to detect depression from spatio-temporal interest points (STIP) [Joshi et al. 2013b]. The STIP method finds salient &#8220;interest points&#8221; around the face where there is significant local variation both spatially and temporally. Around these points, histograms of gradient and flow can be calculated. The STIP features can easily grow to a massively high dimension, which can be handled through per-recording clustering (<i>k</i>-means) in the approach by Joshi et al. [2013b].</p>
<p class="indent">Last, a spatio-temporal descriptor, known as local binary patterns in three orthogonal planes (LBP-TOP) can be mapped to depression status. A spatio-temporal descriptor is a low-level representation of video/image data that describes basic characteristics such as texture and motion. LBP-TOP is computationally simple, <a id="page_385"/>relatively robust to variations in illumination, and has been shown to be effective for automatic facial analysis in depression without requiring any manual labeling. LBP operates at the pixel level, thresholding neighborhood pixels with the value of the central pixel to provide a binary pattern, whose histogram can be used as a texture descriptor. In facial analysis, spatial information can be retained by dividing the image up into local regions, and computing the LBP descriptors separately for each local region. LBP-TOP extends this spatial approach into the spatio-temporal domain, by concatenating LBP co-occurrence statistics in each of the three orthogonal directions. Like STIP features, LBP-TOP features are very high-dimensional and can be summarized using clustering methods, such as <i>Bag of Words</i> (BoW) approaches, before being used in depression classification [Joshi et al. 2013a]. STIP and LBP-TOP features can be combined using BoW for increased reliability (see <a href="#fig12_3">Figure 12.3</a>).</p>
<div class="cap" id="fig12_2">
<p class="image"><img src="../images/fig12_2.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 12.2</b>&#160;&#160;Dynamic features measured from pitch, yaw, and roll of head motion and motion of the first eigenvector of facial fiduciary points. From Dibeklioglu et al. [2015].</p>
</div>
<p class="indent">Both approaches, those that map depression from the configuration of facial expression and those that map depression from the dynamics of facial movement, have shown promising results for the task of discriminating between depressed and non-depressed subjects and change in severity of over the course of depression.</p>
<p class="h1-1"><a id="ch12_5"/><b><span class="bg1">12.5</span>&#160;&#160;&#160;&#160;Speech Analysis</b></p>
<p class="noindent">Speech is regarded as a key behavioral marker of depression [Cummins et al. 2015a]; cognitive and physiological alterations associated with depression influence speech production mechanisms and thereby the acoustic quality of speech. Indeed, prosodic abnormalities such as decreased verbal activity, slowed speech rate and monotonous pitch have long been associated with depression [Kraepelin 1921]. Further, clinicians often use these acoustic alterations, in a subjective manner, when diagnosing a patient [Hall et al. 1995, Sobin and Sackeim 1997]. Significant research has been undertaken to objectively identify potential relationships between changes in speech cues and depression [Cummins et al. 2015a]. However, many conflicting results, particularly for prosodic markers, are reported in the literature as to the exact nature of the observed effects.</p>
<div class="cap" id="fig12_3">
<p class="image"><a id="page_386"/><img src="../images/fig12_3.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 12.3</b>&#160;&#160;Combination of STIP and LBP-TOP features combined in a Bag of Words approach for action unit or depression detection. From Joshi et al. [2012].</p>
</div>
<p class="indent"><a id="page_387"/>The most commonly reported prosodic effects include: decreases in pitch variability [Nilsonne et al. 1987, Stassen et al. 1995] and decreases in energy variability [Quatieri and Malyska 2012, Horwitz et al. 2013]. While intuitively these alterations match with subjective clinical descriptions of speech affected by depression as &#8220;flat&#8221; and &#8220;monotonous,&#8221; there are also a substantial number of papers that do not support these findings [Alpert et al. 2001, Yang et al. 2013, Stassen et al. 1991]. Alterations in speech rate measures are more consistently reported: both decreases in overall speech rate and increases in pause rate [Alghowinem et al. 2012, Mundt et al. 2012]. Recently, work by Trevino et al. [2011] reported that depression-induced changes in speech rate are potentially stronger when extracted at the phoneme level. Their analysis also indicates that depression has similar effects on phonemes that have been grouped together by manner of articulation.</p>
<p class="indent">Depression has also been linked to changes in a patient&#8217;s <i><b>voice quality</b></i> [Cummins et al. 2015a]; a suprasegmental property of speech relating to phonation types. A range of voice quality measures/features such as increased spirantization [Flint et al. 1993], increased aspiration [Quatieri and Malyska 2012], and decreased spectral harmonic and spectral tilt [H&#246;nig et al. 2014] have been reported in speech affected by depression. These changes are all indicative of a breathier phonation in depressed speech. A tense voice quality has also been linked with speech affected by depression: work led by Scherer demonstrates that Normalized Amplitude Quotient (NAQ) and the Quasi-Open-Quotient (QOQ) have each independently exhibited strong statistical significance for discerning between speech heavily affected by depression and speech not affected by depression [Scherer et al. 2013a, 2013b, Scherer et al. 2013c]. NAQ is related to the speed of closure of the vocal fold periodic vibrations during voiced speech, while QOQ is related to the duration of vocal fold opening as a proportion of the pitch period.</p>
<p class="indent">Changes in <i>formant features</i>&#8212;dominant components in the speech spectrum&#8212;also highlight the link between a tense voice quality and depression. Increases in muscle tension dampen the vocal tract resonances and limit articulator movements, altering formant behavior [Cummins et al. 2015a]. Alterations in formant behavior are well reported in the literature [Flint et al. 1993, France et al. 2000, Mundt et al. 2007, Scherer et al. 2016] and formant features have been widely employed in many speech-based depression detection systems [Helfer et al. 2013, Low et al. 2011, Williamson et al. 2013].</p>
<p class="indent">Depression has also been shown to manifest in the speech spectrum, a high-dimensional representation of the frequency distribution of a signal over a short <a id="page_388"/>time interval. The two most frequently observed effects are shifts in energy distribution and decreases the variance of sub-band energy measured at the utterance level [Cummins et al. 2015a]. Shifts in energy distribution is the more widely reported of the two effects; however, there are disagreements in the literature as to the exact nature of the effect. While some papers report that increases in depression severity result in energy shifts from lower (below 500 Hz) to higher (500&#8211;1000 Hz) bands [France et al. 2000, Ozdas et al. 2004, Tolkmitt et al. 1982]; others report the opposite effect; more energy located in energy bands below 500 Hz in speech affected by depression [Yingthawornsuk et al. 2006]. More recently, it has been reported that speech affected by depression can potentially be characterized by a decrease in sub-band energy variability [Cummins et al. 2015c, Quatieri and Malyska 2012]; this effect matches well with clinical descriptions of speech affected by depression as flat and monotonous. Work by Cummins et al. [2015c] indicates, over two commonly used speech-depression corpora, consistent trends of negative correlations in sub-band energy variability with increasing levels of depression.</p>
<p class="indent">While there is a wide range of speech alterations attributed to depression, the clinical utility of many of the prosodic, source, formant, and spectral features previously discussed is potentially limited. Most features were not specifically designed for the task of capturing depression in speech, and are hence also sensitive to the phonetic variability within an utterance as well as differences in speaker characteristics&#8212;age, gender, ethnicity&#8212;and emotional and social signals [Cummins et al. 2015a]. This issue is further confounded by the heterogeneous clinical profile of depression, highlighted by the many conflicting results seen in the literature [Cummins et al. 2015a]. There can be no guarantees as to the presence and strength of depression on any <i>one</i> of the speech features discussed. One approach that has been adopted to mitigate this is to use large or very large feature sets, for example those generated by the COVAREP or openSMILE toolkits for Audio/Visual Emotion Challenge (AVEC) [Valstar et al. 2014b, Valstar et al. 2013] baseline systems, however this approach needs to be employed with care to avoid <i><b>overfitting.</b></i></p>
<p class="indent">Very recent research indicates that instead of attempting to analyze the effects of depression, there are advantages to analyzing the effects of depression on overall speech motor control [Cummins et al. 2015c, Scherer et al. 2015, 2016, Williamson et al. 2013]. Speech motor control refers to the muscular systems and strategies needed to control speech production [Kent and Kim 2003]; common speech motor control disorders include Dysarthria, Apraxia, and Fluency Disorders such as stuttering. Speech alterations commonly associated with speech motor control such as intensity decay, prosodic abnormalities, articulatory, and phonetic errors [Kent and Kim 2003] <a id="page_389"/>can be visualized in the spectrogram of speech affected by depression (<a href="#fig12_4">Figure 12.4</a>).</p>
<p class="indent">Links between depression related effects and speech motor control can be inferred from the previously discussed prosodic, source, formant, and spectral effects associated with depression. Prosodic effects relating to decreases in speech rate and phoneme rate observed in speech affected by depression provide evidence that depression can slow the movement of the articulatory muscles. Alterations to voice quality features provides evidence that depression can affect laryngeal coordination&#8212;also observable in the spectrogram of speech affected by depression (<a href="#fig12_4">Figure 12.4</a>). Finally, reductions in formant dynamics and sub-band energy variability indicate that depression can potentially increase articulatory effort.</p>
<div class="cap" id="fig12_4">
<p class="image"><img src="../images/fig12_4.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 12.4</b>&#160;&#160;Comparison of example spectrograms for the spoken digits 1&#8211;10 for (a) speech not affected by depression and (b) speech heavily affected by depression. The x-axes are time, the y-axes are amplitude (top plots) and frequency (lower plots), and the color intensity denotes regions of higher energy. Spectral features are well suited to capturing effects of depression: note the decreased intensity, flatter pitch contours, and decreases in harmonic to noise ratio comparing (b) with (a). From Cummins [2016].</p>
</div>
<p class="indent"><a id="page_390"/>Williamson et al. [2013] proposed and introduced the Vocal Tract Coordination (VTC) feature space for depression prediction tasks, designed specifically to characterize reduced articulatory coordination as reflected in either formant frequencies or the mel-spectrum. Specifically, VTCs capture changes in correlation that occur at different time scales across either feature space. VTC feature spaces were used in both the AVEC-2013 and AVEC-2014 top-ranked systems; for full details on their extraction the reader is referred to Williamson et al. [2013] and Williamson et al. [2014].</p>
<p class="indent">Recently, results presented in Scherer et al. [2015], 2016 indicate that there are significant reductions in <i><b>Vowel Space Area</b></i> (VSA) in speech affected by depression. VSA is measured using the Euclidean distances between the <i>F1</i> and <i>F2</i> coordinates of the corner vowels /i/, /u/, and /a/ and is typically manually extracted from an utterance [Skodda et al. 2012]. Scherer et al. [2016] present a method for automatically measuring VSA and evaluate this on a range of commonly used depression-speech corpora to highlight its suitability for characterizing speech affected by depression. Cummins et al. [2015c], using measures of acoustic variability, quantified the effects of depression on per-utterance Gaussian mixture models of <i><b>mel frequency cepstral coefficient</b></i> (MFCC) features. Results presented by the authors demonstrate that depression significantly reduces both the local and global spread of phonetic events. These findings indicate that speech affected by depression can be characterized by a reduction in the number of distinct speech sounds produced and a reduction in overall speaking effort and animation. Further, they provide strong evidence that depression should be considered a speech motor control disorder; similar characterizations are found in the appropriate literature [McRae et al. 2002, Orozco-Arroyave et al. 2014, Sapir et al. 2010].</p>
<p class="indent">A range of investigations have been undertaken into speech-based systems for the automatic classification of depression. Whilst a range of different classifiers have been explored, arguably SVM and <i><b>Gaussian Mixture Models</b></i> (GMMs) are the most typically used [Alghowinem et al. 2012, 2013c, Cohn et al. 2009, Cummins et al. 2011, 2013a, 2013, 2014, Helfer et al. 2013, Low et al. 2010, 2011, Ooi et al. 2013, Scherer et al. 2013a, 2013b, Sturim et al. 2011, Trevino et al. 2011, Valstar et al. 2013]. This is not surprising, as both techniques are suitable for robustly modeling small/sparse datasets and have a range of well-established software implementations (e.g., Weka, HTK, SVMTorch).</p>
<p class="indent">Typically, <i>detection</i> style classification papers have tended to adopt a more data-driven focus; a range of papers have investigated suitability of forming a classification <a id="page_391"/>system from combinations of groups of prosodic, source, formant and spectral features with binary classification accuracy ranges between 60&#8211;80% reported [Alghowinem et al. 2012, Cummins et al. 2011, Low et al. 2010, Moore et al. 2008, Ooi et al. 2013]. On the other hand <i>severity</i> style classification problems have tended to be more knowledge driven, for example, using single prosodic, voice quality, or formant-based features, respectively [Cohn et al. 2009, Scherer et al. 2013a, 2013c, Helfer et al. 2013]; typical accuracies of 60&#8211;80% (3&#8211;5 classes) have been reported for depression severity classification.</p>
<p class="indent">Due to the public availability of the AVEC datasets (<a href="#ch12_9">Section 12.9</a>), speech based depression prediction approaches have recently gained greater attention. The AVEC data-centric multimodal depression severity prediction challenge (discussed in further detail in <a href="#ch12_8">Section 12.8</a>) audio baseline performances were set using the popular brute-forced approach which combines a high dimensional (2268 features) multivariate acoustic feature space and a <i><b>Support Vector Regressor</b></i> (SVR) back-end. This configuration of features and regression achieved results, in terms of root mean square error (RMSE) between predicted and clinician-assessed BDI scores, of 14.12 and 12.57 on the AVEC 2013 and AVEC 2014 test sets respectively [Valstar et al. 2013, 2014b].</p>
<p class="indent">A range of regression approaches have been trialled on the AVEC data; as well as SVR [Cummins et al. 2013, 2014, Mitra et al. 2014, P&#233;rez et al. 2014, Sidorov and Minker 2014, Valstar et al. 2013, 2014b, 2013], Decision trees [Kaya et al. 2014b], Relevance Vector Machines [Cummins et al. 2015b], Gaussian Processes [P&#233;rez et al. 2014], Extreme Learning Machines [Kaya et al. 2014a], and Generalized Linear Models [Senoussaoui et al. 2014] have all shown reasonable performance&#8212;RMSEs between 9 and 10&#8212;when predicting depression. However, underlying these approaches is the assumption that there is a regular relationship between changes in speech features and changes in depression scores; given the ordinal nature of depression scores such a relationship may not exist [Cummins 2016].</p>
<p class="indent">To help account for the lack of regular relationship, less conventional regression approaches are starting to be explored for depression score prediction, the most promising of which are the two-stage regression systems [Williamson et al. 2013, Cummins 2016]. Williamson et al. [2013] proposed the Gaussian Staircase Regression (GSR) approach. The idea of this system was to capture the irregular relationship between speech features and depression scores by using a series of models&#8212;single Gaussians&#8212;along the depression score axis the test statistics from this pseudo GMM was then used as the basis for univariate depression prediction. As with VTCs, GSR prediction was a major system component in both the AVEC-2013 and 2014 winning entries, attaining RMSEs of 8.50 and 8.12 (not combined with video features, see <a href="#ch12_8">Section 12.8</a>), respectively [Williamson et al. 2014, 2013].</p>
<p class="h1"><a id="page_392"/><a id="ch12_6"/><b><span class="bg1">12.6</span>&#160;&#160;&#160;&#160;Body Movement and Other Behavior Analysis</b></p>
<p class="noindent">A large body of research has examined the relationship between nonverbal behavior and clinical conditions. Most of this research resides in clinical and social psychology and communication science and, until very recently, the vast majority relied on manual annotation of gestures and facial expressions. Despite at least 40 years of intensive research, there is still surprisingly little progress on identifying clear relationships between patient disorders and expressed behavior. In part, this is due to the difficulty in manually annotating data, inconsistencies in how clinical states and expressed behaviors are defined across studies, and the wide range of social contexts in which behavior is elicited and observed. Despite these complexities, there is general consensus on the relationship between some clinical conditions (especially depression and social anxiety) and associated nonverbal cues. In the following, we provide a review of key findings on non-verbal, non-facial behavior and its relationship to clinical conditions.</p>
<p class="indent">For computing body movement patterns, nine body parts relative to the torso center have been considered. These body parts consist of the upper and lower parts of left and right arm, head, and upper and lower parts of left and right leg. A state-of-the-art Mixture of Parts based human body detector was employed to obtain an approximate location of various body parts in a video frame by Yang and Ramana [2011]. The central location of nine parts can be transformed into polar coordinates. In order to compute the motion pattern in the body parts, the torso center can be considered as the reference point. Polar coordinates for each part representing orientation and distance from the torso center can be computed from the real pixel coordinate values. The motion patterns can be joined part-wise and a polar plot can be computed based on the values of distances and orientations depicting overall movement by all parts combined. In <a href="#fig12_5">Figure 12.5</a>, the plot represents the relative movement of the body parts with respect to the torso in a video.</p>
<p class="indent">The change in body movement patterns with changing severity of depression has been studied using within-subject <i>longitudinal data</i> [Joshi et al. 2013c]. It was observed that there is a significant drop in the frequency of overall body movements when the participants were diagnosed with severe depression (<a href="#fig12_5">Figure 12.5</a>), and this is supported by other studies (e.g., Girard et al. [2014b]).</p>
<p class="indent">Emotional expressivity over time and emotional movements are also indicative of an underlying clinical state. For example, depressed patients frequently display flattened or negative affect including less emotional expressivity [Perez and Riggio 2003, Bylsma et al. 2008], less intense and shorter smiles [Scherer et al. 2013b, 2014], fewer mouth movements [Fairbanks et al. 1982, Schelde 1998], more frowns <a id="page_393"/>[Fairbanks et al. 1982, Perez and Riggio 2003], and fewer gestures [Hall et al. 1995, Perez and Riggio 2003]. Some findings suggest it is not the total quantity of expressions that is important, but their dynamics. For example, depressed patients may frequently smile, but these are perceived as less genuine and often shorter in duration [Kirsch and Brunnhuber 2007] than in non-clinical populations. Social anxiety and PTSD share some of the features of depression [Scherer et al. 2013b, 2014, 2016] and also have a tendency toward heightened emotional sensitivity and more energetic responses including hypersensitivity to stimuli: for example, more startle responses, and greater tendency to display anger [Kirsch and Brunnhuber 2007] or shame [Menke 2011].</p>
<div class="cap" id="fig12_5">
<p class="image"><img src="../images/fig12_5.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 12.5</b>&#160;&#160;Body movement pattern for four individuals who were severely depressed at one point in time and have shown improvement over the course of treatment, from Joshi et al. [2013c]. Each column shows two plots depicting body motion patterns, which belong to the same participant in two different states. The blue lines in the polar plots represent the observed body movement pattern during the interview session. Note the denser blue region in the bottom row, depicting more movement under low severity of depression.</p>
</div>
<p class="indent">Certain gestures are seen with greater frequency in clinical populations. Fidgeting is often reported. This includes gestures such as tapping or rhythmically shaking hands or feet and is seen in both anxiety and depression [Fairbanks et al. 1982, 2013b, 2014]. Similarly, &#8220;self-adaptors&#8221;, such as rhythmically touching, hugging or stroking parts of the body or self-grooming, e.g., repeatedly stroking the hair [Fairbanks et al. 1982] or touching the face [Scherer et al. 2013b, 2014], have been identified to be of interest in this field of research [Ekman and Friesen 1969].</p>
<p class="indent">Gaze and mutual attention are critical behaviors for regulating conversations, so it is not surprising that a number of clinical conditions are associated with atypical <a id="page_394"/>patterns of gaze. Depressed patients have a tendency to maintain less mutual gaze [Waxer 1974], show nonspecific gaze, such as staring off into space [Schelde 1998], and avert their gaze, often together with a downward angling of the head [Perez and Riggio 2003, Scherer et al. 2013b, 2014], which may be sustained for longer than usual [Alghowinem et al. 2013b]. The pattern for depression, PTSD, and perhaps other distress disorders is similar, with patients often avoiding direct eye contact with the clinician [Scherer et al. 2013c].</p>
<p class="h1-1"><a id="ch12_7"/><b><span class="bg1">12.7</span>&#160;&#160;&#160;&#160;Analysis using Other Sensor Signals</b></p>
<p class="noindent">A number of other signal types that can be practically acquired out of the clinical context have been investigated as indicators of depression. One of these is eye movement, with Abel et al. [1991] finding that smooth pursuit (ability of eye gaze to track a moving visual target) and saccade movements were impaired for depressed patients. These findings were confirmed using computer vision analysis, which provided depression detection accuracies of 70&#8211;75% for a gender-balanced database of 30 depressed and 30 control participants [Alghowinem et al. 2013a], based on features extracted using ratios of horizontal and vertical eye position (relative to the eye corners and eyelid/eye fold) and eyelid opening ratio. It must be noted, however, that dysfunction in saccadic movement is likely to be non-specific to depression. They are reliably found in patients with schizophrenia and their first-degree relatives and are considered an endophenotype of schizophrenia [Calkins et al. 2008, Hong et al. 2008, Levy et al. 1993].</p>
<p class="indent">Pupil diameter has also been investigated in studies by Siegle et al. [2001], 2003 and Burkhouse et al. [2015], who found that depressed individuals were slower to respond to emotional stimuli and spent more time processing those stimuli, as gauged by pupillometry. The measure of pupil diameter and reactivity to emotional stimuli has been shown to correlate with depression risk [Burkhouse et al. 2015] and the propensity for depressive rumination [Siegle et al. 2003]. While pupil diameter seems to be a strong behavioral predictor of depression it has not yet found its application in a large number of studies involving automatic behavior analyses. With improving computer vision techniques and facial tracking algorithms it is only a matter of time until the automatic and robust assessment of pupil diameter will become a commodity. As with saccades, indeed more so, pupil dilation lacks specificity for depression. Pupil dilation is a measure of sympathetic arousal, which is a primary dimension of emotion.</p>
<p class="indent">Other autonomic measures have been investigated, including phasic skin conductance, heart rate, and vagal tone [Dawson et al. 1985]. While these lack specificity <a id="page_395"/>for depression, they may prove useful especially in detecting change in severity over time.</p>
<p class="indent">In recent years, there has been considerable interest in signals that can be acquired from mobile device sensors [Glenn and Monteith 2014], such as GPS and accelerometry [Schueller et al. 2014], real time self-monitoring of mood [Proudfoot et al. 2013], and text acquired from social media interaction [Nguyen et al. 2017, Milne et al. 2015]. Other studies have also revealed links between social connectivity and mental illnesses [Rosenquist et al. 2011, Pachucki et al. 2015]. It has been speculated that tracking changes in social networks using smart devices might reveal information on changes in at-risk individuals&#8217; mood state [Larsen et al. 2015].</p>
<p class="indent">There is also huge increase in the trend of using social networking sites, such as Facebook and Twitter, to engage and socialize with people. Recently, some studies have leveraged such linguistic information and created behavioral models to specifically predict depression [Park et al. 2012, Nguyen et al. 2014]. On observing Twitter feeds to analyze depression, it was deduced that there is a basic difference in the reason for using social media amongst depressed and non-depressed cohorts. Non-depressed participants used Twitter as an information consuming and sharing platform, whereas depressed participants perceived it as a medium for emotional interaction and social awareness [Park et al. 2012]. A detailed comparison of the characteristics of online depression communities and other popular and trending online communities was conducted by Nguyen et al. [2014], who found a significant difference between the written content, writing styles, and affective information of the content of the two groups. Sentiment analysis showed that the textual content from depressed communities had lower valence (i.e., contained more negative words such as &#8220;hate&#8221;, &#8220;hard&#8221;, and &#8220;pain&#8221;) than that generated by people in the other online communities. Other studies have shown lowered online social activity, greater negative emotion, high self-attentional focus, increased relational and medicinal concerns, and heightened expression of religious thoughts [De Choudhury et al. 2013]. Linguistic approaches have also been combined very successfully with other modalities (see, e.g., Valstar et al. 2016 and citing papers), and there are interesting prospects for future research in this area.</p>
<p class="h1-1"><a id="ch12_8"/><b><span class="bg1">12.8</span>&#160;&#160;&#160;&#160;Multimodal Fusion</b></p>
<p class="noindent">There is an extensive literature in psychology that points to various affective impairments associated with depressive disorders, as discussed in previous sections in detail. However, clearly defined markers of behavioral changes due to depression are still to be identified, which can be attributed to the different means of <a id="page_396"/>eliciting and observing expressed behaviors as well as the heterogeneous nature of depression symptoms. Clinicians study various facial movements, gestures, body movements, and speech attributes to establish the differences in the expressivity in depressed patients. A multimodal automatic system incorporating inputs from various channels will likely prove to be better than unimodal techniques.</p>
<p class="indent">It has already been established that integration of different modalities yields a better performance for inferring affect, mainly in a classification context. One of the first works to integrate information from speech signals, facial dynamics, and body movement to detect depression was presented by Joshi et al. [2013a], also in a classification paradigm. Intra-facial muscle movements and the movements of the upper body were analyzed by computing spatio-temporal descriptors. In addition, various audio features (fundamental frequency, loudness, intensity, and mel-frequency cepstral coefficients) were computed. A BoW framework was then proposed to incorporate the audio and video signals separately. Two fusion paradigms were investigated: (i) Decision-level fusion (late fusion) classification was performed separately on each set of BoW features (audio and video), and then AND or OR decision-level fusion was performed; and (ii) Feature-level fusion (early fusion) classification was performed on the concatenated BoW features (audio and video). In their investigation, the feature-level (early) fusion proved more effective than decision-level (late) fusion in terms of two-class depression detection accuracy [Joshi et al. 2013a].</p>
<p class="indent">The BoW approach deserves some additional explanation. As originally proposed, BoW represents a document based on the unordered word frequency. In the context of the problem here, for example for vision based analysis [Joshi et al. 2013a], a video clip (set of video frames) is a document in the BoW sense. The computed spatio-temporal descriptors, which represent facial or body movement and provide discriminative information to detect depression, are &#8220;words&#8221;. During the training phase, a dictionary (video codebook) is learned from the computed words. One of the main advantages of the BoW approach is the resulting low-dimensional feature which is generated, in the form of a histogram. Another benefit of using a BoW framework is that it handles different feature lengths, since the recorded data to be processed will in general have different durations based on individuals&#8217; responses. The use of codebooks makes it simpler to deal with such samples of different length.</p>
<p class="indent">A further advantage is that if multiple modalities are represented using BoW, then (due to the histogram representation), they are well calibrated for fusion. Integration of audio and video codebooks in a multimodal framework has been shown to produce better performance than the corresponding unimodal audio or video <a id="page_397"/>systems, in a two-class depression classification context [Joshi et al. 2013a]. The high performance of the integrated system indicates the importance of different channels, which, when fused, were found to provide an absolute gain of about 10% over individual channels in two-class classification [Joshi et al. 2013a].</p>
<p class="indent">In an attempt to expedite the advancement of multimodal depression assessment technologies, a depression sub-challenge was organized as part of the Audio/Visual Emotion Challenge (AVEC) [Valstar et al. 2013, 2014b] with the aim of estimating the self-reported level of depression on a common database (see <a href="#ch12_9">Section 12.9</a>) and reference feature set. The AVEC challenge poses depression assessment as a prediction problem, and since its initiation, has seen multimodal depression assessment concentrate primarily on prediction problems and prediction fusion.</p>
<p class="indent">In one entry to the challenge, Cummins et al. [2013] proposed a multimodal framework. For the audio modality, acoustic supervectors were extracted using GMM of MFCC feature space. In the vision pipeline, clustering was performed on the video clips using spatio-temporal low-level features. For each cluster, Pyramid of Histogram of Gradient (PHOG) was computed on the central frame and further embedded in a BoW framework. The authors employed feature fusion and trialed various different SVR kernels for prediction. Results presented show that their multimodal approach was able to outperform both the audio and visual challenge baselines.</p>
<p class="indent">A multimodal framework based on audio, text, and video modalities was presented by Gupta et al. [2014]. In their system, the openSMILE toolkit based speech features shared by the AVEC 2014 organizers were augmented by video feature streams. Fiducial points and motion based features were computed and fused with the Local Binary Pattern features. For analyzing the text, crowd-sourcing was performed to generate the transcripts. Furthermore, a lexicon was generated using the transcripts. The authors trailed a range of systems consisting of feature-level fusion followed by supervised feature dimensionality reduction and a SVR back-end to infer the depression intensity label.</p>
<p class="indent">A multimodal approach based on speech spectral information and facial action units was also proposed for AVEC [Williamson et al. 2014]. Their speech system consisted of Vocal Tract Coordination features (outlined in <a href="#ch12_5">Section 12.5</a>), phoneme rate features, phoneme pitch dynamics, and an alternate vocal tract coordination feature; the correlation between formant frequencies and cepstral peak prominence. The authors complemented these features with facial features, derived from facial action units, designed to capture visual changes in muscle coordination. The training data were partitioned into eight depression score-dependent ranges and single <a id="page_398"/>Gaussian were estimated for each individual range, for each feature set. A Gaussian staircase regression (also outlined in <a href="#ch12_5">Section 12.5</a>) model was trained to map the input to a log-likelihood based score (between the low- and high-depression pseudo GMM&#8217;s created by the aforementioned partitioning) for each feature. Their system trialed three different predictors comprising linear combinations of GSR log-likelihood scores from the different features spaces. Then their predictions were combined according to (i) the accuracy of each predictor and (ii) whether the subjects in the AVEC database were &#8220;repeat&#8221; or &#8220;non-repeat&#8221; (appeared in database more than once or once, respectively).</p>
<p class="indent">Motivated by the highly promising GSR approach, Cummins [2016] proposed a generalized two-stage framework for prediction against an irregular variable. In the first stage, a series of models are used to segment the depression score axis and extract a rank score per segment, in the second stage the rank scores are used as the basis for performing the final depression score prediction. Provided the segmentation of the depression score axis is meaningfully chosen, the first stage of the two-stage regression thereby focuses on the relationships existing between speech features and depression only within localized regions along the depression axis. The second stage, which aggregates multiple rank scores, effectively allows for changes in these relationships along the axis to be accounted for in the prediction. Results gained from a comprehensive set of prediction tests presented in Cummins [2016] demonstrate the framework&#8217;s suitability for depression score prediction: all two-stage systems presented are shown to match or out-perform corresponding conventional regression systems. Work by Williamson et al. [2014] shows the benefits to using this kind of approach as part of a multimodal system. The continued exploration of two-stage prediction in all modalities is a promising area of future research.</p>
<p class="h1-1"><a id="ch12_9"/><b><span class="bg1">12.9</span>&#160;&#160;&#160;&#160;Implementation-Related Considerations and Elicitation Approaches</b></p>
<p class="noindent">Having reviewed recent developments in multimodal assessment of depression and related disorders based on behavioral signals, it is important to consider how data might be collected and what the deployment contexts might be for services or products based on this kind of system.</p>
<p class="indent">Example tasks that have been employed to elicit multimodal data from depressed individuals to date include speaking sustained vowels, diadochokinetic stimuli (e.g., &#8220;pa-ta-ka&#8221;), read speech, interview, virtual human interview (which can increase the comparability of behaviors between individuals) [DeVault et al. <a id="page_399"/>2014], and mood induction. The choice of <i>elicitation</i> method to maximize for depression assessment sensitivity is in general an open research problem. Although research has shown that spontaneous speech (such as that elicited in an interview) is more characteristic of depression level [Alghowinem et al. 2012], there may be advantages for example in constraining the spoken content (since phonetic diversity is a source of nuisance variability), or in specifically assessing psychomotor skills, or in provoking facial responses to mood induction stimuli that are valuable for discrimination. It is also important to consider the context in which the data are collected, as affective expressions may differ as a result. Intra-personal tasks (e.g., responding to stimuli on a computer screen) provide a different context than inter-personal tasks (e.g., interviews, couples, or family interactions). Work presented in Cummins et al. [2015b] suggests that the data-collection paradigm affects the strength of the manifestation of depression in speech samples; the authors speculate that tasks which invoke strong emotional or motivational effects in the reader will result in acoustically richer (in terms of depression information) samples.</p>
<p class="indent">Similarly, choosing or summarizing salient segments from a recording is in general an open research problem: some studies have suggested that as little as the first 10s of response is just as effective for analysis as many minutes of data. Alternatively, methods such BoW, clustering or statistical functionals can be employed to deal with different recording lengths.</p>
<p class="indent">Some examples of multimodal corpora that have appeared in multiple publications to date include (most other similar depression datasets are not publicly available) the following.</p>
<p class="bullt">&#8226;&#160;&#160;The Distress Assessment Interview Corpus (DAIC) comprises audio and video recordings of spontaneous conversations from over 500 U.S. English speakers during semi-structured interviews with a virtual human interviewer [Gratch et al. 2014] designed to simulate standard protocols for identifying people at risk for major depression or PTSD and to elicit nonverbal and verbal behavior indicative of psychological distress. Each conversation lasts for around 20 min on average. Binary and severity labels were recorded for depression and PTSD using (self-assessed) questionnaires PHQ-9 and PCL-C. A subset of this data has been made publicly available as part of the AVEC 2016 depression sub challenge [Valstar et al. 2016].<sup><a id="rfn1" href="#fn1">1</a></sup></p>
<p class="bullt">&#8226;&#160;&#160;<a id="page_400"/>The Audio-Visual Emotion Challenges (AVEC) of 2013 and 20142<sup><a id="rfn2" href="#fn2">2</a></sup> used subsets of the AViD corpus, which comprises 292 audio and video recordings of German speakers completing tasks including sustained vowels, counting, reading, and spontaneous story-telling. On average, each recording (of all tasks) lasts around 25 min. One (self-assessed) BDI-II scale depression rating was made for each speaker, and continuously rated arousal and valence annotation was also developed by a team of annotators.</p>
<p class="bullt">&#8226;&#160;&#160;The Black Dog database comprises recordings of audio and video from 40 speakers with a major depressive disorder, rated using the QIDS-SR scale by clinic psychiatrists at the Black Dog Institute, and 40 age-matched controls. Participants completed video viewing, image rating [McIntyre et al. 2009].</p>
<p class="bullt">&#8226;&#160;&#160;The Pittsburgh database contains audio and video (head and shoulders, and full body view) recordings of semi-structured clinical interviews of 57 depressed patients, who were each evaluated four times at 7-week intervals using the HAM-D scale.</p>
<p class="noindentt">Some of the likely use-case contexts for systems providing multimodal assessment of depression and related disorders include the following.</p>
<p class="bullt">&#8226;&#160;&#160;Clinic-based: This is a controlled environment in which more elaborate, sensitive, and/or expensive sensor configurations can be employed. This environment is also more conducive to a human or virtual human interviewer.</p>
<p class="bullt">&#8226;&#160;&#160;PC/laptop-based: This may be suitable for passive, ongoing monitoring, for example in a home or office environment.</p>
<p class="bullt">&#8226;&#160;&#160;Smartphone-based and wearable-based: This is the least controlled environment in general, in terms of background audio, lighting, movement and other types of noise. However, it is perhaps the richest in terms of assessing behavior, especially where the aim is to detect &#8220;triggers&#8221; for depressive episodes, or to perform quick checks at regular periods after taking medication, for example.</p>
<p class="indentt">The growth in the number of systems based on smartphones or wearable devices has been notable in recent years, and in turn this has spurred a good deal of commercial activity, as noted by Glenn and Monteith [2014]. Opportunities possibly afforded by these devices could include identifying situational, pharmacological, <a id="page_401"/>behavioral, or cyclical triggers that deepen depression in individuals, or interventions where depression is detected [Donker et al. 2013].</p>
<p class="indent">As implied in <a href="#ch12_8">Section 12.8</a>, the type of output a system produced by the system could be categorical (e.g., &#8220;non-depressed&#8221;, &#8220;depressed&#8221;), ordinal (e.g., &#8220;mild&#8221;, &#8220;moderate&#8221;, &#8220;severe&#8221;, as per many scales), or continuous (e.g., predicting a numerical score). At present, there is no consensus as to which may be preferred; it is likely that this will emerge as clinical trials of automatic systems of this kind are conducted. For systems designed to monitor patients who have already been diagnosed, the task design and system output type will need to be carefully selected according to the clinical objective, for example identifying responders to a particular treatment (e.g., 50% or more reduction in initial symptoms). It would also be of interest to find indicators of depression that are not sensitive to treatments, since these may act as trait-like indicators of susceptibility to depression. Similarly to how tests such as the Complete Blood Test, which is used by doctors to get an overview of a patient&#8217;s overall physical health, a multimodal system could be used to review a patient&#8217;s overall mental health. Instead of providing a single score or diagnosis the objective analysis of behavioral changes could provide clinicians and medical practitioners with a rich battery of information on a patient&#8217;s emotional, cognitive, or mental state. The output of such a test could be used to support diagnosis of a range mental and neurological disorders.</p>
<p class="h1-2"><a id="ch12_10"/><b><span class="bg1">12.10</span>&#160;&#160;Conclusion and Current Challenges</b></p>
<p class="noindent">This chapter has given an overview of the role multimodal systems can play in the assessment of depression, with a particular emphasis on behavioral signals, together with an introduction to some of the key methods proposed and studies conducted to date. It seems clear that depression is in general very challenging to assess by any means using any one scale, due to its broad and heterogeneous clinical profile. This challenge, together with the compelling societal need for accessible and accurate assessment, seems a perfect motivation for research into automatic and multimodal systems.</p>
<p class="indent">Readers may note some similarities between this chapter and others that touch on affective computing, for example the small databases (relative to the &#8220;big data&#8221; of multimedia research), the lack of definitive feature set (especially that is specific to the attribute of interest) and the significant signal variability posed by individual differences and environmental factors. However, it is also worth noting some of the differences between depression assessment and much other affective computing research: depression is a long-term rather than short-term state; databases may <a id="page_402"/>often only contain one depression level example per individual (emotion databases typically have several emotions per individual); a particular depression score could possibly be due to many different combinations of symptoms; and depression databases will usually have only one labeler (inter-labeler agreement is unknown).</p>
<p class="indent">The approaches reviewed here all take a categorical view of depression. That is, that depression is a qualitatively different disorder from GAD, PTSD, and other clinical disorders, and that the goal of multimodal assessment is to contribute to diagnosis of depression. As noted previously, the assumption that depression is qualitatively unique among psychopathologies has been challenged. Ample evidence suggests that depression often is comorbid with GAD and depression symptoms often are components of other disorders, especially PTSD. These and similar findings have motivated dimensional conceptualizations of psychopathology. Theory and some data suggest that GAD, depression (MDD), and PTSD, as well as Dysthymia and Borderline Personality Disorder, are better understood as &#8220;distress&#8221; disorders. Multimodal assessment can inform this debate by including a broader range of psychopathology in our research. Scherer et al. [2014] made initial advances in this direction. They include both depression and PTSD in their research. It is hoped that future work will continue to broaden patient characteristics in order to inform conceptual advances in psychopathology.</p>
<p class="indent">Future research challenges in this area abound, and some are highlighted as follows.</p>
<p class="bullt">&#8226;&#160;&#160;Since depression is measured on a numerical scale, prediction is an important problem. However, to date there are few methods for fusing predictors that have been shown in this context to provide an improvement over the individual predictors - these will be essential to multimodal prediction.</p>
<p class="bullt">&#8226;&#160;&#160;Relatively little is known about the generalizability of findings with respect to individual differences in participants or countries of origin. Initial work suggests that computer vision based methods are robust to skin color [Girard et al. 2014a], for instance, but replication is needed. Similarly, the extent to which findings generalize across different cultures [Alghowinem et al. 2016] is not yet well understood.</p>
<p class="bullt">&#8226;&#160;&#160;Most work has focused on single modalities, such as face or voice. Greater attention to multimodal approaches is needed, for example the contribution of unique variance to detection of depression [Dibeklioglu et al. 2017]by different modalities. Further studies investigating the relative contributions of different features across different modalities are needed: these may be either statistical in nature (e.g., percentage of variance accounted for by <a id="page_403"/>major predictors) or machine learning-based empirical comparisons (e.g., classification accuracy of individual features vs. fused features/systems).</p>
<p class="bullt">&#8226;&#160;&#160;Although features from various different modes have been shown to be <i>sensitive</i> to depression, there is virtually no research on whether they are <i>specific</i> to depression. This is an issue both because behavioral indicators tend to also be sensitive to other related disorders, and because other disorders (e.g., anxiety) often co-occur with depression. Alternatively, as argued above, lack of specificity may be inherent in any measure of depression to the extent that dimensional models of psychopathology prove more valid than traditional diagnostic categories.</p>
<p class="bullt">&#8226;&#160;&#160;Because of the wide clinical profile of depression and the co-occurrence of other disorders with depression, there is interest in assessing sub-symptoms of depression rather depression directly. Work of this kind is already underway, for example assessment of psychomotor coordination [Trevino et al. 2011], of cognitive function, and of responses to negative-valence emotional stimuli [McIntyre et al. 2011]. However, more work is needed and consensus is yet to be reached even on which sub-symptoms should be assessed.</p>
<p class="bullt">&#8226;&#160;&#160;Multimodal depression assessment to date has employed the most straightforward combination of modes (i.e., feature- or score-level fusion). There may be significant opportunities for features that exploit more complex multimodal information, for example temporal patterns across two or more modes, for example as captured by VTC features.</p>
<p class="bullt">&#8226;&#160;&#160;Mitigation of confounding factors, such as acoustic variability due to speaker characteristics or phonetic content or variability in pose or illumination, is a key issue remaining to be satisfactorily resolved, although some advances have been made in this area [Williamson et al. 2013, Cummins et al. 2014].</p>
<p class="bullt">&#8226;&#160;&#160;Approaches are needed for mitigating the irregular, non-linear relationships between changes in behavior features and changes in depression status or sub-symptom status.</p>
<p class="bullt">&#8226;&#160;&#160;More study of longitudinal multimodal data in depressed individual is needed to determine how effective within-user automatic analysis of depression may be as the extent of depression symptoms vary over time.</p>
<p class="bullt">&#8226;&#160;&#160;Building the evidence base for using automatically-extracted behavioral signals as the basis for new diagnostic criteria.</p>
<p class="bullt">&#8226;&#160;&#160;Bridging the ongoing need for close collaboration with clinicians.</p>
<p class="h1n"><a id="page_404"/><a id="ch12_11"/><b>Acknowledgments</b></p>
<p class="noindent">Preparation of this chapter was supported in part by the National Institute of Mental Health of the U.S. National Institutes of Health under Award Number MH096951.</p>
<p class="h1n"><a id="ch12_12"/><b>Focus Questions</b></p>
<p class="noindentt"><b>12.1.</b> Why might analysis of behavior be a promising approach for assessing depression? In particular, why can a multimodal approach be expected to be more successful than unimodal approaches?</p>
<p class="noindentt"><b>12.2.</b> Discuss the benefits and limitations of three different methods for eliciting behavior for automatic assessment of depression, and the possible implications for interface design.</p>
<p class="noindentt"><b>12.3.</b> Choose five different kinds of features from this chapter and explain for each what aspect of depression they characterize and the system requirements to implement them.</p>
<p class="noindentt"><b>12.4.</b> Choose one feature from this chapter, and based on further reading, present a detailed explanation of the mathematical/computational approach, explaining all symbols, why the feature takes the mathematical form it has, and what other features [if any] might capture similar information about depressed behavior.</p>
<p class="noindentt"><b>12.5.</b> Bag-of-words has proven useful in conjunction with a variety of different features in the depression assessment context. What are some reasons why?</p>
<p class="noindentt"><b>12.6.</b> For some types of features, toolboxes are available that allow extraction of a wide variety of features. What are some important considerations when choosing which toolboxes and/or features to employ? What is a limitation of large feature sets?</p>
<p class="noindentt"><b>12.7.</b> For a particular modality of your choice, what are (at least) two sources of variability that are undesirable from the perspective of depression assessment? Based on further reading, what kinds of approaches might help mitigate them?</p>
<p class="noindentt"><b>12.8.</b> Compare and contrast two different approaches for fusing automatic depression assessment systems. Based on further reading, has one been found more effective than the other?</p>
<p class="noindentt"><b>12.9.</b> What are the desirable attributes of research datasets comprising behavior of depressed individuals? List as many as possible.</p>
<p class="noindentt"><b>12.10.</b> Considering use-cases of a multimodal automatic depression assessment system, what are some of the benefits and limitations of the different possible <a id="page_405"/>contexts? Consider in particular (i) in a clinic, (ii) at home, and (iii) in a mobile, everyday context.</p>
<p class="h1-2"><a id="ch12_13"/><b>References</b></p>
<p class="ref">L. A. Abel, L. Friedman, J. Jesberger, A. Malki, and H. Y. Meltzer. 1991. Quantitative assessment of smooth pursuit gain and catch-up saccades in schizophrenia and affective disorders. <i>Biological Psychiatry</i> 29: 1063&#8211;1072. DOI: 10.1016/0006-3223(91)90248-K. 394</p>
<p class="ref">S. Alghowinem, R. Goecke, M. Wagner, J. Epps, M. Breakspear, and G. Parker. 2012. From joyous to clinically depressed: mood detection using spontaneous speech. In <i>Proceedings of the International FLAIRS Conference AAAI</i>, Marco Island, FL. pp. 141&#8211;146. 387, 390, 391, 399</p>
<p class="ref">S. Alghowinem, R. Goecke, M. Wagner, G. Parker, and M. Breakspear. 2013a. Eye movement analysis for depression detection. In <i>Proceedings of the IEEE International Conference on Image Processing</i>, pp. 4220&#8211;4224. DOI: 10.1109/ICIP.2013.6738869. 394</p>
<p class="ref">S. Alghowinem, R. Goecke, M. Wagner, G. Parker, and M. Breakspear. 2013b. Head pose and movement analysis as an indicator of depression. In <i>Proceedings of the International Conference on Affective Computing and Intelligent Interaction</i>. pp. 283&#8211;288. DOI: 10.1109/ACII.2013.53. 394</p>
<p class="ref">S. Alghowinem, R. Goecke, M. Wagner, and J. Epps. 2013c. A comparative of different classifiers for detecting depression from spontaneous speech. In <i>Proceedings of the IEEE International Confereence on Acoustics, Speech and Signal Processing</i>. pp. 8022&#8211;8026. 390</p>
<p class="ref">S. Alghowinem, R. Goecke, J. Epps, M. Wagner, and J. F. Cohn. 2016. Cross-cultural depression recognition from vocal biomarkers. in <i>Proceedings of INTERSPEECH</i>. pp. 1943&#8211;1947. DOI: 10.21437/INTERSPEECH.2016. 402</p>
<p class="ref">M. Alpert, E. R. Pouget, and R. R. Silva. 2001. Reflections of depression in acoustic measures of the patient&#8217;s speech. <i>Journal of Affective Disorders</i> 66: 59&#8211;69. DOI: 10.1016/S0165-0327(00)00335-9. 387</p>
<p class="ref">American Psychiatric Association. 2015. <i>Diagnostic and Statistical Manual of Mental Disorders?: DSM-V</i>. 5th ed. American Psychiatric Association, Washington, DC. 376, 379</p>
<p class="ref">A. Asthana, A. Khwaja, and R. Goecke. 2009. Automatic frontal face annotation and AAM building for arbitrary expressions from a single frontal image only. In <i>Proceedings of the IEEE International Conference on Image Processing</i>, Cairo, Egypt. pp. 2445&#8211;2448. DOI: 10.1109/ICIP.2009.5414123. 383</p>
<p class="ref">A. Asthana, S. Lucey, and R. Goecke. 2011. Regression based automatic face annotation for deformable model building. <i>Pattern Recognition</i> 44(10&#8211;11): 2598&#8211;2613. DOI: 10.1016/j.patcog.2011.03.014.</p>
<p class="ref"><a id="page_406"/>S. Baker, I. Matthews, and J. Schneider. 2004. Automatic construction of active appearance models as an image coding problems. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 26(10), 1380&#8211;1384. DOI: 10.1109/TPAMI.2004.77. 383</p>
<p class="ref">T. Baltrusaitis, P. Robinson, and L.-P. Morency. 2012. 3D constrained local model for rigid and non-rigid facial tracking. In <i>Proceedings of IEEE Computer Vision and Pattern Recognition</i>, Providence, RI. 383</p>
<p class="ref">A. T. Beck, R. A. Steer, R. Ball, and W. F. Ranieri. 1996. Comparison of Beck depression inventories-IA and-II in psychiatric outpatients. <i>Journal of Personality Assessment</i> 67: 588&#8211;597. DOI: 10.1207/s15327752jpa6703_13. 379</p>
<p class="ref">G. W. Blair-West, C. H. Cantor, G. W. Mellsop, and M. L. Eyeson-Annan. 1999. Lifetime suicide risk in major depression: sex and age determinants. <i>Journal of Affective Disorders</i> 55(2&#8211;3): 171&#8211;178. DOI: 10.1016/S0165-0327(99)00004-X. 376</p>
<p class="ref">M. Blais and L. Baer. 2010. Understanding rating scales and assessment instruments. In L. Baer and M.A. Blais, editors, <i>Handbook of Clinical Rating Scales and Assessment in Psychiatry and Mental Health, Current Clinical Psychiatry</i>. Humana Press, New York. DOI: 10.1007/978-1-59745-387-5_1. 379</p>
<p class="ref">K. L. Burkhouse, G. J. Siegle, M. L. Woody, A. Y. Kudinova, and B. E. Gibb. 2015. Pupillary reactivity to sad stimuli as a biomarker of depression risk: Evidence from a prospective study of children. <i>Journal of Abnormal Psychology</i> 124(3): 498. DOI: 10.1037/abn0000072. 394</p>
<p class="ref">L. M. Bylsma, B. H. Morris, and J. Rottenberg. 2008. A meta-analysis of emotional reactivity in major depressive disorder. <i>Clinical Psychology Review</i> 28(4): 676&#8211;691. DOI: 10.1016/j.cpr.2007.10.001. 392</p>
<p class="ref">M. E. Calkins, W. G. Iacono, and D. S. Ones. 2008. Eye movement dysfunction in first-degree relatives of patients with schizophrenia: A meta-analytic evaluation of candidate endophenotypes. <i>Brain and Cognition</i> 68: 436&#8211;461. DOI: 10.1016/j.bandc.2008.09.001. 394</p>
<p class="ref">S. B. Campbell and J. F. Cohn. 1991. Prevalence and correlates of postpartum depression in first-time mothers. <i>Journal of Abnormal Psychology</i> 100(4): 594&#8211;599.</p>
<p class="ref">B. J. Carroll. 1984. Dexamethasone suppression test. In R. C. W. Hall and T. P. Beresford, editors, <i>Handbook of Psychiatric Diagnosis</i>, vol. 1, pp. 3&#8211;28. Springer, Netherlands. 379</p>
<p class="ref">M. De Choudhury, M. Gamon, S. Counts, and E. Horvitz. 2013. Predicting depression via social media. In <i>Proceedings of the AAAI Conference on Web and Social Media</i>.</p>
<p class="ref">W.-S. Chu, F. De la Torre, and J. F. Cohn. 2016. Selective transfer machine for personalized facial action unit detection. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i> 3515&#8211;3522. DOI: 10.1109/TPAMI.2016.2547397. 384</p>
<p class="ref">J. F. Cohn, Z. Ambadar, and P. Ekman. 2007. Observer-based measurement of facial expression with the facial action coding system. In J. A. Coan and J. J. B. Allen, editors, <i>Handbook of Emotion Elicitation and Assessment</i>. Oxford University Press Series in Affective Science, pp. 203&#8211;221. Oxford University, New York. 382</p>
<p class="ref"><a id="page_407"/>J. F. Cohn, T. S. Kruez, I. Matthews, Y. Yang, M. H. Nguyen, M. T. Padilla, F. Zhou, and F. De la Torre. 2009. Detecting depression from facial actions and vocal prosody. In <i>Proceedings of the International Conference on Affective Computing and Intelligent Interaction</i>. pp. 1&#8211;7. DOI: 10.1109/ACII.2009.5349358. 382, 383, 390, 391</p>
<p class="ref">J. F. Cohn and F. De la Torre. 2015. Automated face analysis for affective computing. In R. A. Calvo, S. K. D&#8217;Mello, J. Gratch, and A. Kappas, editors, <i>Handbook of Affective Computing</i>, pp. 131&#8211;150. Oxford, New York. 384</p>
<p class="ref">T. F. Cootes, G. V. Wheeler, K. N. Walker, and C. J. Taylor. 2002. View-based active appearance models. <i>Image and Vision Computing</i> 20(9&#8211;10): 657&#8211;664. DOI: 10.1016/S0262-8856(02)00055-0. 383</p>
<p class="ref">T. F. Cootes, and C. J. Taylor. 2004. Statistical models of appearance for computer vision. Technical Report, University of Manchester.</p>
<p class="ref">C. Corneanu, M. Oliu, J. F. Cohn, and S. Escalera. 2015. Survey on RGB, thermal, and multimodal approaches for facial expression analysis: History, trends, and affect-related applications. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 36 (8), 1548&#8211;1568. 384</p>
<p class="ref">N. Cummins. 2016. Automatic assessment of depression from speech: paralinguistic analysis, modelling and machine learning. Ph.D. Thesis, UNSW Australia. 389, 391, 398</p>
<p class="ref">N. Cummins, J. Epps, M. Breakspear, and R. Goecke. 2011. An investigation of depressed speech detection: features and normalization. In <i>Proceedings of INTERSPEECH</i>, Florence, Italy. pp. 2997&#8211;3000. 390, 391</p>
<p class="ref">N. Cummins, J. Epps, V. Sethu, M. Breakspear, and R. Goecke. 2013a. Modeling spectral variability for the classification of depressed speech. In <i>Proceedings of INTERSPEECH</i>, Lyon, France. pp. 857&#8211;861. 390</p>
<p class="ref">N. Cummins, J. Epps, V. Sethu, and J. Krajewski. 2014. Variability compensation in small data: oversampled extraction of i-vectors for the classification of depressed speech. In <i>Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</i>, Florence, Italy. pp. 970&#8211;974. DOI: 10.1109/ICASSP.2014.6853741. 390, 391, 403</p>
<p class="ref">N. Cummins, J. Joshi, A. Dhall, V., Sethu, R. Goecke, and J. Epps. 2013. Diagnosis of depression by behavioral signals: a multimodal approach. In <i>Proceedings of the ACM International Workshop on Audio/Visual Emotion Challenge (AVEC&#8217;13)</i>, Barcelona, Spain. pp. 11&#8211;20. DOI: 10.1145/2512530.2512535. 381, 390, 391, 397</p>
<p class="ref">N. Cummins, S. Scherer, J. Krajewski, S. Schnieder, J. Epps, and T.F. Quatieri. 2015a. A review of depression and suicide risk assessment using speech analysis. <i>Speech Communication</i> 71: 10&#8211;49. DOI: 10.1016/j.specom.2015.03.004. 380, 385, 386, 387, 388</p>
<p class="ref">N. Cummins, V. Sethu, J. Epps, and J. Krajewski. 2015b. Relevance vector machine for depression prediction. In <i>Proceedings of INTERSPEECH</i>, Dresden, Germany. pp. 110&#8211;114. 391, 399</p>
<p class="ref"><a id="page_408"/>N. Cummins, V. Sethu, J. Epps, S. Schnieder, and J. Krajewski. 2015c. Analysis of acoustic space variability in speech affected by depression. <i>Speech Communication</i> 75: 27&#8211;49. DOI: 10.1016/j.specom.2015.09.003. 388, 390</p>
<p class="ref">B. N. Cuthbert and T. R. Insel. 2013. Toward the future of psychiatric diagnosis: the seven pillars of RDoC. <i>BMC Medicine</i>, 11: 126. DOI: 10.1186/1741-7015-11-126. 380</p>
<p class="ref">M. E. Dawson, A. M. Schell, J. R. Braaten, and J. J. Catania. 1985. Diagnostic utility of autonomic measures for major depressive disorders. <i>Psychiatry Research</i> 15.4: 261&#8211;270. DOI: 10.1016/0165-1781(85)90063-0. 394</p>
<p class="ref">M. De Choudhury, M. Gamon, S. Counts, and E. Horvitz. 2013. Predicting depression via social media. In <i>Proceedings of the AAAI Conference on Web and Social Media</i>. 395</p>
<p class="ref">D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast, A. Gainer, K. Georgilia, J. Gratch, A. Hartholt, M. Lhommet, G. Lucas, S. Marsella, F. Morbini, A. Nazarian, S. Scherer, G. Stratou, A. Suri, D. Traum, R. Wood, Y. Xu, A. Rizzo, and L.P. Morency. 2014. Simsensei kiosk: A virtual human interviewer for healthcare decision support. In <i>Proceedings of Autonomous Agents and Multiagent Systems</i>. pp. 1061&#8211;1068. 398, 399</p>
<p class="ref">H. Dibeklioglu, Z. Hammal, Y. Yang, and J.F. Cohn. 2015. Multimodal detection of depression in clinical interviews. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, Seattle, WA. DOI: 10.1145/2818346.2820776. 384, 385</p>
<p class="ref">H. Dibeklioglu, Z. Hammal, and J. F. Cohn. 2017. Multimodal measurement of depression severity in the context of clinical interviews. <i>IEEE Journal of Biomedical Health Informatics</i>. DOI: 10.1109/JBHI.2017.2676878. 402</p>
<p class="ref">T. Donker, K. Petrie, J. Proudfoot, J. Clarke, Birch, and H. Christensen. 2013. Smartphones for smarter delivery of mental health programs: a systematic review. <i>Journal of Medical Internet Research</i> 15(11). DOI: 10.2196/jmir.2791. 401</p>
<p class="ref">W. Eaton, C. Smith, M. Ybarra, C. Muntaner, and A. Tien. 2004. Center for epidemiologic studies depression scale: review and revision (CESD and CESD-R). In M. E. Maruish, editor, <i>The Use of Psychological Testing for Treatment Planning and Outcomes Assessment</i>, Lawrence Erlbaum, pp. 363&#8211;377. Lawrence Erlbaum, Mahawah, NJ. 379</p>
<p class="ref">G. J. Edwards, C. J. Taylor, and T. F. Cootes. 1998. Interpreting face images using active appearance models. <i>Proceedings of the Third IEEE International Conference on Automatic Face and Gesture Recognition</i>, Nara, Japan, 300&#8211;305. DOI: 10.1109/AFGR.1998.670965.</p>
<p class="ref">P. Ekman and W. V. Friesen. 1969. The repertoire of nonverbal behavior: Categories, origins, usage, and coding. <i>Semiotica</i> 1(1): 49&#8211;98. DOI: doi.org/10.1515/semi. 393</p>
<p class="ref">P. Ekman, and W. V. Friesen. 1978. Facial action coding system. Palo Alto, CA: Consulting Psychologists Press.</p>
<p class="ref">P. Ekman, W. V. Friesen, and J. C. Hager. 2002. <i>Facial Action Coding System: Research Nexus</i>. Network Research Information, Salt Lake City, UT 382</p>
<p class="ref"><a id="page_409"/>L. A. Fairbanks, M. T. McGuire, and C. J. Harris. 1982. Nonverbal interaction of patients and therapists during psychiatric interviews. <i>Journal of Abnormal Psychology</i> 91(2): 109. 392, 393</p>
<p class="ref">M. B. First, J. B. W. Williams, R. S. Karg, and R. L. Spitzer. 2015. <i>Structured Clinical Interview for DSM-5&#8212;Research Version</i>. American Psychiatric Association, Arlington, VA. 379</p>
<p class="ref">A. J. Flint, S. E. Black, I. Campbell-Taylor, G. F. Gailey, and C. Levinton. 1993. Abnormal speech articulation, psychomotor retardation, and subcortical dysfunction in major depression. <i>Journal of Psychiatric Research</i> 27: 309&#8211;319. DOI: 10.1016/0022-3956(93)90041-Y. 387</p>
<p class="ref">J. P. Forgas. 2014. <i>Four Ways Sadness May be Good for You</i>. The Greater Good Science Center. University of California Berkeley, Berkeley, California. 376</p>
<p class="ref">D. J. France, R. G. Shiavi, S. Silverman, M. Silverman, and M. Wilkes. 2000. Acoustical properties of speech as indicators of depression and suicidal risk. <i>IEEE Transactions on Biomedical Engineering</i> 47: 829&#8211;837. DOI: 10.1109/10.846676. 387, 388</p>
<p class="ref">S. Ghosh, E. Laksana, S. Scherer, and L.-P. Morency. 2015. A multi-label convolutional neural network approach to cross-domain action unit detection. In <i>Proceedings of the International Conference on Affective Computing and Intelligent Interaction</i>, pp. 609&#8211;615. DOI: 10.1109/ACII.2015.7344632. 384</p>
<p class="ref">R. D. Gibbons, D. J. Weiss, P. A. Pilkonis, E. Frank, T. Moore, J. B. Kim, and D. J. Kupfer. 2012. Development of a computerized adaptive test for depression. <i>Archives of General Psychiatry</i>. 69(11): 11041112. DOI: 10.1001/archgenpsychiatry.2012.14. 379</p>
<p class="ref">J. M. Girard, J. F. Cohn, M. H. Mahoor, S. Mavadati, and D. P. Rosenwald. 2013. Social risk and depression: Evidence from manual and automatic facial expression analysis. In <i>Proceedings of. IEEE International Conference on Automatic Face and Gesture Recognition</i>, pp. 1&#8211;8. DOI: 10.1109/FG.2013.6553748. 381, 382, 383</p>
<p class="ref">J. M. Girard, J. F. Cohn, L. A. Jeni, M. A. Sayette, and F. De la Torre. 2014a. Spontaneous facial expression can be measured automatically. <i>Behavior Research Methods</i> 47(4): 1136&#8211;1147. 384, 402</p>
<p class="ref">J. M. Girard, J. F. Cohn, M. H. Mahoor, S. M. Mavadati, Z. Hammal, and D. P. Rosenwald. 2014b. Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses. <i>Image and Vision Computing</i> 32(10): 641&#8211;647. DOI: 10.1016/j.imavis.2013.12.007. 382, 392</p>
<p class="ref">J. M. Girard and J. F. Cohn. 2015. Automated audiovisual depression analysis. <i>Current Opinion in Psychology</i> 4: 75&#8211;79. DOI: 10.1016/j.copsyc.2014.12.010. 380</p>
<p class="ref">T. Glenn and S. Monteith. 2014. New measures of mental state and behavior based on data collected from sensors, smartphones, and the Internet. <i>Current Psychiatry Reports</i> 16(12): 1&#8211;10. DOI: 10.1007/s11920-014-0523-3. 395, 400</p>
<p class="ref">I. H. Gotlib and C. L. Hammen. 2002. <i>Handbook of Depression</i>. Guilford, New York.</p>
<p class="ref">J. Gratch, R. Artstein, G. Lucas, S. Scherer, A. Nazarian, R. Wood, J. Boberg, D. DeVault, S. Marsella, D. Traum, A. Rizzo, and L.-P. Morency. 2014. The distress analysis interview <a id="page_410"/>corpus of human and computer interviews. In <i>Proceedings of Language Resources and Evaluation Conference</i>, pp. 3123&#8211;3128. 399</p>
<p class="ref">R. Gupta, N. Malandrakis, B. Xiao, T. Guha, M. Van Segbroeck, M. Black, A. Potamianos, and S. Narayanan. 2014. Multimodal prediction of affective dimensions and depression in human-computer interactions. In <i>Proceedings of the ACM International Workshop on Audio/Visual Emotion Challenge [AVEC&#8217;14]</i>, Orlando, FL. DOI: 10.1145/2661806.2661810. 397</p>
<p class="ref">J. A. Hall, J. A. Harrigan, and R. Rosenthal. 1995. Nonverbal behavior in clinician&#8212;patient interaction. <i>Applied and Preventative Psychology</i> 4: 21&#8211;37. DOI: 10.1016/S0962-1849(05)80049-6. 386, 393</p>
<p class="ref">H. Hamilton. 1960. HAMD: A rating scale for depression. <i>Neurosurgy and Psychiatry</i> 23: 56&#8211;62.</p>
<p class="ref">B. S. Helfer, T. F. Quatieri, J. R. Williamson, D. D. Mehta, R. Horwitz, and B. Yu. 2013. Classification of depression state based on articulatory precision. In <i>Proceedings in INTERSPEECH</i>, Lyon, France, pp. 2172&#8211;2176. 387, 390, 391</p>
<p class="ref">HiTOP. (Undated). The hierarchical taxonomy of psychopathology [HiTOP]. <a href="http://medicine.stonybrookmedicine.edu/HITOP">http://medicine.stonybrookmedicine.edu/HITOP</a>. 376, 379, 380</p>
<p class="ref">F. H&#246;nig, A. Batliner, E. N&#246;th, S. Schnieder, and J. Krajewski. 2014. Automatic modelling of depressed speech: relevant features and relevance of gender. In <i>Proceedings in INTERSPEECH</i>, Singapore, pp. 1248&#8211;1252. 387</p>
<p class="ref">L. Hong, K. Turano, H. O&#8217;Neil, L. Hao, I. Wonodi, and R. McMahon. 2008. Refining the predictive pursuit endophenotype in schizophrenia. <i>Biological Psychiatry</i> 63: 458&#8211;464. DOI: 10.1016/j.biopsych.2007.06.004. 394</p>
<p class="ref">R. Horwitz, T. F. Quatieri, B. S. Helfer, B. Yu, J. R. Williamson, and J. Mundt. 2013. On the relative importance of vocal source, system, and prosody in human depression. In <i>Proceedings of IEEE International Conference on Body Sensor Networks</i>, Cambridge, MA, pp. 1&#8211;6. DOI: 10.1109/BSN.2013.6575522. 387</p>
<p class="ref">L. A. Jeni, J. F. Cohn, and T. Kanade. 2015. Dense 3D face alignment from 2d videos in realtime. In <i>Proc. 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</i>. DOI: 10.1109/FG.2015.7163142. 384</p>
<p class="ref">J. Joshi, A. Dhall, R. Goecke, M. Breakspear, and G. Parker. 2012. Neural-net classification for spatio-temporal descriptor based depression analysis. In <i>Proceedings of the International Conference on Pattern Recognition</i>, Tsukuba, Japan, pp. 2634&#8211;2638. 386</p>
<p class="ref">J. Joshi, R. Goecke, S. Alghowinem, A. Dhall, M. Wagner, J. Epps, G. Parker, and M. Breakspear. 2013a. Multimodal Assistive Technologies for Depression Diagnosis and Monitoring. <i>Journal on Multimodal User Interfaces</i> (Special Issue on Multimodal Interfaces for Pervasive Assistance) 7(3): 217&#8211;228. DOI: 10.1007/s12193-013-0123-2. 381, 385, 396, 397</p>
<p class="ref">J. Joshi, R. J., Goecke, M. Breakspear, and G. Parker. 2013b. Can body expressions contribute to automatic depression analysis? In <i>Proceedings of the IEEE International Conference</i> <a id="page_411"/><i>on Automatic Face and Gesture Recognition</i>, Shanghai, China. DOI: 10.1109/FG.2013.6553796. 384</p>
<p class="ref">J. Joshi, A. Dhall, R. Goecke, and J.F. Cohn. 2013c. Relative body parts movement for automatic depression analysis. In <i>Proceedings of the Conference. on Affective Computing and Intelligent Interaction</i>, pp. 492&#8211;497. DOI: 10.1109/ACII.2013.87. 392, 393</p>
<p class="ref">H. Kaya, F. Cilli, and A. Salah. 2014a. Ensemble CCA for Continuous Emotion Prediction. In <i>Proceedings of the ACM International Workshop on Audio/Visual Emotion Challenge [AVEC&#8217;14]</i>, Orlando, FL, pp. 19&#8211;26. DOI: 10.1145/2661806.2661814. 391</p>
<p class="ref">H. Kaya, F. Eyben, and A.A. Salah. 2014b. CCA based Feature Selection with Application to Continuous Depression Recognition from Acoustic Speech Features. In <i>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</i>, Florence, Italy, pp. 3757&#8211;3761. DOI: 10.1109/ICASSP.2014.6854298. 391</p>
<p class="ref">R. D. Kent and Y.-J. Kim. 2003. Toward an acoustic typology of motor speech disorders. <i>Clinical Linguistics &#38; Phonetics</i> 17: 427&#8211;445. DOI: 10.1080/0269920031000086248. 388, 389</p>
<p class="ref">R. C. Kessler, W. T. Chiu, O. Demler, and E. E. Walters. 2005. Prevalence, severity, and comorbidity of 12-month DSM-IV disorders in the National Comorbidity Survey Replication. <i>Archives of General Psychiatry</i> 62: 617&#8211;627. DOI: 10.1001/archpsyc.62.6.617. 376</p>
<p class="ref">A. Kirsch and S. Brunnhuber. 2007. Facial expression and experience of emotions in psychodynamic interviews with patients with PTSD in comparison to healthy subjects. <i>Psychopathology</i> 40(5): 296&#8211;302. DOI: 10.1159/000104779. 393</p>
<p class="ref">R. Kotov, W. Gamez, F. Schmidt, and D. Watson. 2010. Linking big personality traits to anxiety, depressive, and substance use disorders: A meta-analysis. <i>Psychological Bulletin</i> 136(5): 768&#8211;821. DOI: 10.1037/a0020327.. 376</p>
<p class="ref">E. Kraepelin. 1921. Manic depressive insanity and paranoia. <i>The Journal of Nervous and Mental Disease</i> 53: 350. DOI: 10.1192/bjp.67.278.342. 385, 386</p>
<p class="ref">K. Kroenke, R. L. Spitzer, and J. B. W. Williams. 2001. The PHQ-9. <i>Journal of General Internal Medicine</i> 16: 606&#8211;613. DOI: 10.1046/j.1525-1497.2001.016009606.x. 379</p>
<p class="ref">M. E. Larsen, N. Cummins, T. W. Boonstra, B. O&#8217; Dea, J. Tighe, F. Shand, J. Epps, and H. Christensen. 2015. The use of technology in suicide prevention. In <i>Proceedings of the IEEE International Conference on Engineering in Medicine and Biology Society</i>, Milan, Italy, pp. 7316&#8211;7319. DOI: 10.1109/EMBC.2015.7320081. 395</p>
<p class="ref">D. L. Levy, P. S. Holzman, S. Matthysse, and N. R. Mendell. 1993. Eye tracking dysfunction and schizophrenia: A critical perspective. <i>Schizophrenia Bulletin</i> 19: 461&#8211;536. DOI: 10.1093/schbul/19.3.461. 394</p>
<p class="ref">L. S.A. Low, N. C. Maddage, M. Lech, L. Sheeber, and N. Allen. 2010. Influence of acoustic low-level descriptors in the detection of clinical depression in adolescents. In <i>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</i>, Dallas, Texas, pp. 5154&#8211;5157. DOI: 10.1109/ICASSP.2010.5495018. 390, 391</p>
<p class="ref"><a id="page_412"/>L. S.A. Low, N. C. Maddage, M. Lech, L. B. Sheeber, and N. B. Allen. 2011. Detection of clinical depression in adolescents; speech during family interactions. <i>IEEE Transactions on Biomedical Engineering</i> 58: 574&#8211;586. DOI: 10.1109/TBME.2010.2091640. 387, 390</p>
<p class="ref">C. D. Mathers and D. Loncar. 2011. Projections of global mortality and burden of disease from 2002 to 2030. <i>PLoS Medicine</i> 3(11). 376</p>
<p class="ref">I. Matthews, and S. Baker. 2004. Active appearance models revisited. <i>International Journal of Computer Vision</i>, 60(2), 135&#8211;164. DOI: 10.1023/B:VISI.0000029666.37597.d3.</p>
<p class="ref">G. McIntyre, R. G&#246;cke, M. Hyett, M. Green, and M. Breakspear. 2009. An approach for automatically measuring facial activity in depressed subjects. In <i>Proceedings of the International Conference on Affective Computing and Intelligent Interaction</i>, pp. 1&#8211;8. DOI: 10.1109/ACII.2009.5349593. 400</p>
<p class="ref">G. McIntyre, R. G&#246;cke, M. Breakspear, and G. Parker. 2011. Facial response to video content in depression. In <i>Proceedings of the ICMI Workshop on Inferring Cognitive and Emotional States from Multimodal Measures</i>. 403</p>
<p class="ref">P. A. McRae, K. Tjaden, and B. Schoonings. 2002. Acoustic and perceptual consequences of articulatory rate change in Parkinson disease. <i>Journal of Speech, Language, and Hearing Research</i> 45: 35&#8211;50. DOI: 1044/1092-4388(2002/003). 390</p>
<p class="ref">D. Milne, C. Paris, H. Christensen, P. Batterham, and B. O&#8217;Dea. 2015. We feel: taking the emotional pulse of the world. In <i>Proceedings of the Congress of the International Ergonomics Association</i>, Melbourne, Australia. 395</p>
<p class="ref">V. Mitra, E. Shriberg, M. McLaren, A. Kathol, C. Richey, D. Vergyri, and M. Graciarena. 2014. The SRI AVEC-2014 evaluation system. In <i>Proceedings of the ACM International Workshop on Audio/Visual Emotion Challenge (AVEC&#8217;14)</i>, Orlando, Florida, pp. 93&#8211;101. DOI: 10.1145/2661806.2661818. 391</p>
<p class="ref">R. Menke. 2011. Examining nonverbal shame markers among post-pregnancy women with maltreatment histories. Doctoral dissertation, Wayne State University, Detroit, Michigan. 393</p>
<p class="ref">E. Moore, M. A. Clements, J. W. Peifer, and L. Weisser. 2008. Critical analysis of the impact of glottal features in the classification of clinical depression in speech. <i>IEEE Transactions on Biomedical Engineering</i> 55: 96&#8211;107. DOI: 10.1109/TBME.2007.900562. 391</p>
<p class="ref">J. C. Mundt, P. J. Snyder, M. S. Cannizzaro, K. Chappie, and D. S. Geralts. 2007. Voice acoustic measures of depression severity and treatment response collected via interactive voice response (IVR) technology. <i>Journal of Neurolinguistics</i> 20: 50&#8211;64. DOI: 10.1016/j.jneuroling.2006.04.001. 387</p>
<p class="ref">J. C. Mundt, A. P. Vogel, D. E. Feltner, and W. R. Lenderking. 2012. Vocal acoustic biomarkers of depression severity and treatment response. <i>Biological Psychiatry</i>, 72: 580&#8211;587. DOI: 10.1016/j.biopsych.2012.03.015. 387</p>
<p class="ref">T. Nguyen, D. Phung, B. Dao, S. Venkatesh, and M. Berk. 2014. Affective and content analysis of online depression communities. <i>IEEE Transactions on Affective Computing</i> 5(3): 217&#8211;226. DOI: 10.1109/TAFFC.2014.2315623. 395</p>
<p class="ref"><a id="page_413"/>T. Nguyen, B. O&#8217;Dea, M. Larsen, D. Phung, S. Venkatesh, and H. Christensen. 2017. Using linguistic and topic analysis to classify sub-groups of online depression communities. <i>Multimedia Tools and Applications</i> 1&#8211;24. DOI: 10.1007/s11042-015-3128-x. 395</p>
<p class="ref">A. Nilsonne, J. Sundberg, S. Ternstrom, and A. Askenfelt. 1987. Measuring the rate of change of voice fundamental frequency in fluent speech during mental depression. <i>The Journal of the Acoustical Society of America</i> 83: 716&#8211;728. DOI: 10.1121/1.396114. 387</p>
<p class="ref">M. W. O&#8217;Hara, E. M. Zekoski, L. H. Philipps, and E. J. Wright. 1990. Controlled prospective study of postpartum mood disorders: comparison of childbearing and nonchild-bearing women. <i>Journal of Abnormal Psychology</i> 99(1): 3&#8211;15.</p>
<p class="ref">K. E. B. Ooi, M. Lech, and N. B. Allen. 2013. Multichannel weighted speech classification system for prediction of major depression in adolescents. <i>IEEE Transactions on Biomedical Engineering</i> 60: 497&#8211;506. DOI: 10.1109/TBME.2012.2228646. 390, 391</p>
<p class="ref">J. R. Orozco-Arroyave, E. A. Belalc&#225;zar-Bola&#241;os, J. D. Arias-Londo&#241;o, J. F. Vargas-Bonilla, T. Haderlein, and E. N&#246;th. 2014. Phonation and articulation analysis of spanish vowels for automatic detection of Parkinson&#8217;s Disease. In. P. Sojka, A. Hor&#225;k, I. Kope&#269;ek, and K. Pala, editors, <i>Text, Speech and Dialogue</i>, Lecture Notes in Computer Science, pp. 374&#8211;381. Springer International Publishing, Cham, Switzerland. DOI: 10.1007/978-3-319-10816-2_45. 390</p>
<p class="ref">S. D. &#216;stergaard, S. O. W. Jensen, and P. Bech. 2011. The heterogeneity of the depressive syndrome: when numbers get serious. <i>Acta Psychiatrica Scandinavica</i> 124: 495&#8211;496. DOI: 10.1111/j.1600-0447.2011.01744.x. 379</p>
<p class="ref">A. Ozdas, R. G. Shiavi, S. E. Silverman, M. K. Silverman, and D. M. Wilkes. 2004. Investigation of vocal jitter and glottal flow spectrum as possible cues for depression and near-term suicidal risk. <i>IEEE Transactions on Biomedical Engineering</i> 51: 1530&#8211;1540. DOI: 10.1109/TBME.2004.827544. 388</p>
<p class="ref">M. C. Pachucki, E. J. Ozer, A. Barrat, and C. Cattuto. 2015. Mental health and social networks in early adolescence: a dynamic study of objectively-measured social interaction behaviors. <i>Social Science &#38; Medicine</i> 125: 40&#8211;50. DOI: 10.1016/j.socscimed.2014.04.015. 395</p>
<p class="ref">M. Park, C. Cha, and M. Cha. 2012. Depressive moods of users portrayed in twitter. In <i>Proceedings of the ACM SIGKDD Workshop on Healthcare Informatics</i>, pp. 1&#8211;8. 395</p>
<p class="ref">J. E. Perez and R. E. Riggio. 2003. Nonverbal social skills and psychopathology. <i>Nonverbal Behavior in Clinical Settings</i>, 17&#8211;44. DOI: 10.1093/med:psych/9780195141092.003.0002. 392, 393, 394</p>
<p class="ref">H. P&#233;rez, H. J. Escalante, L. Villase&#241;or-Pineda, M. Montes-y-G&#243;mez, D. Pinto-Aveda&#241;o, and V. Reyes-Meza. 2014. Fusing affective dimensions and audio-visual features from segmented video for depression recognition. In <i>Proceedings of the ACM International Workshop on Audio/Visual Emotion Challenge (AVEC&#8217;14)</i>, Orlando, FL, pp. 49&#8211;55. DOI: 10.1145/2661806.2661815. 391</p>
<p class="ref"><a id="page_414"/>J. Proudfoot, J. Clarke, M.-R. Birch, A.E. Whitton, G. Parker, V. Manicavasagar, V. Harrison, H. Christensen, and D. Hadzi-Pavlovic. 2013. Impact of a mobile phone and web program on symptom and functional outcomes for people with mild-to-moderate depression, anxiety and stress: a randomised controlled trial. <i>BMC Psychiatry</i> 13.1: 1. DOI: 10.1186/1471-244X-13-312. 395</p>
<p class="ref">T. F. Quatieri and N. Malyska. 2012. Vocal-source biomarkers for depression: a link to psychomotor activity. In <i>Proceedings of INTERSPEECH</i>, Portland, OR, pp. 1059&#8211;1062. 387, 388</p>
<p class="ref">L. S. Radloff. 1977. The CES-D scale: A self-report depression scale for research in the general population. <i>Applied Psychological Measurement</i> 1(3):385-401. DOI: 10.1177/014662167700100306. 379</p>
<p class="ref">L. I. Reed, M. A. Sayette, and J. F. Cohn. 2007. Impact of depression on response to comedy: A dynamic facial coding analysis. <i>Journal of Abnormal Psychology</i> 116(4): 804&#8211;809. DOI: 10.1037/0021-843X.116.4.804. 382</p>
<p class="ref">J. N. Rosenquist, J. H. Fowler, and N. A. Christakis. 2011. Social network determinants of depression. <i>Molecular Psychiatry</i> 16: 273&#8211;281. DOI: 10.1038/mp.2010.13. 395</p>
<p class="ref">S. Sapir, L. O. Ramig, J. L. Spielman, and C. Fox. 2010. Formant centralization ratio: a proposal for a new acoustic measure of dysarthric speech. <i>Journal of Speech, Language, and Hearing Research</i> 53: 114&#8211;125. DOI: 10.1044/1092-4388(2009/08-0184). 390</p>
<p class="ref">J. M. Saragih, S. Lucey, and J. F. Cohn. 2009. Subspace constrained mean-shift. In <i>Proceedings of the IEEE International Conference on Computer Vision</i>, Kyoto, Japan. 383</p>
<p class="ref">J. T. M. Schelde. 1998. Major depression: Behavioral markers of depression and recovery. <i>The Journal of Nervous and Mental Disease</i> 186(3): 133&#8211;140. DOI: 10.1097/00005053-199803000-00001. 392, 394</p>
<p class="ref">S. Scherer, G. Stratou, J. Gratch, and L.-P. Morency. 2013a. Investigating voice quality as a speaker-independent indicator of depression and PTSD. In <i>Proceedings of INTERSPEECH</i>, Lyon, France, pp. 847&#8211;851. 387, 390, 391</p>
<p class="ref">S. Scherer, G. Stratou, M. Mahmoud, J. Boberg, and J. Gratch. 2013b. Automatic behavior descriptors for psychological disorder analysis. In <i>Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition</i>, Shanghai, China, pp. 1&#8211;8. DOI: 10.1109/FG.2013.6553789. 387, 390, 392, 393, 394</p>
<p class="ref">S. Scherer, G. Stratou, and L.P. Morency. 2013c. Audiovisual Behavior Descriptors for Depression Assessment. In <i>Proceedings of the ACM International Conference on Multimodal Interaction</i>, Sydney, Australia, pp. 135&#8211;140. DOI: 10.1145/2522848.2522886. 387, 391, 394</p>
<p class="ref">S. Scherer, G. Stratou, G. Lucus, M. Mahmoud, J. Boberg, J. Gratch, A. (&#8216;Skip&#8217;)_ Rizzo, L.-P. Morency, and G. Lucas. 2014. Automatic audiovisual behavior descriptors for psychological disorder analysis. <i>Image and Vision Computing</i> 32: 1&#8211;21. DOI: 10.1016/j.imavis.2014.06.001. 381, 382, 392, 393, 394, 402</p>
<p class="ref">S. Scherer, L. P. Morency, J. Gratch, and J. Pestian. 2015. Reduced vowel space is a robust indicator of psychological distress: A cross-corpus analysis. In <i>Proceedings of the</i> <a id="page_415"/><i>IEEE International Conference on Acoustics, Speech and Signal Processing</i>, Brisbane, Australia, pp. 4789&#8211;4793. DOI: 10.1109/ICASSP.2015.7178880. 388, 390</p>
<p class="ref">S. Scherer, G. M. Lucas, J. Gratch, A. Skip Rizzo and L. P. Morency. 2016. Self-reported symptoms of depression and PTSD are associated with reduced vowel space in screening interviews. In <i>IEEE Transactions on Affective Computing</i> 7(1): 59&#8211;73. DOI: 10.1109/TAFFC.2015.2440264. 387, 388, 390, 393</p>
<p class="ref">S. M. Schueller, M. Begale, F. J. Penedo, and D. C. Mohr. 2014. Purple: A modular system for developing and deploying behavioral intervention technologies. <i>Journal of Medical Internet Research</i> 16.7: e181. DOI: 10.2196/jmir.3376. 395</p>
<p class="ref">M. Senoussaoui, M. Sarria-Paja, J. F. Santos, and T. H. Falk. 2014. Model fusion for multimodal depression classification and level detection. In <i>Proceedings of the ACM International Workshop on Audio/Visual Emotion Challenge (AVEC&#8217;14)</i>, Orlando, FL, pp. 57&#8211;63. DOI: 10.1145/2661806.2661819. 391</p>
<p class="ref">D. V. Sheehan, Y. Lecrubier, K. Harnett-Sheehan, J. Janavs, E. Weiller, L. Bonora, A. Keskiner, J. Schinka, E. Knapp, M. F. Sheehan, and G. C. Dunbar. 1997. Reliability and validity of the M.I.N.I. International Neuropsychiatric Interview (M.I.N.I.). According to the SCID-P. <i>European Psychiatry</i> 12: 232&#8211;241. DOI: 10.1016/S0924-9338(97)83297-X. 379</p>
<p class="ref">M. Sidorov and W. Minker. 2014. Emotion recognition and depression diagnosis by acoustic and visual features: a multimodal approach. In <i>Proceedings of the ACM Internatinal Workshop on Audio/Visual Emotion Challenge (AVEC&#8217;14)</i>, Orlando, FL, pp. 81&#8211;86. DOI: 10.1145/2661806.2661816. 391</p>
<p class="ref">G. J. Siegle, E. Granholm, R. E. Ingram, and G. E. Matt. 2001. Pupillary and reaction time measures of sustained processing of negative information in depression. <i>Biological Psychiatry</i>, 49.7: 624&#8211;636. DOI: 10.1016/S0006-3223(00)01024-6. 394</p>
<p class="ref">G. J. Siegle, S. R. Steinhauer, C. S. Carter, W. Ramel, and M. E. Thase. 2003. Do the seconds turn into hours? Relationships between sustained pupil dilation in response to emotional information and self-reported rumination. <i>Cognitive Therapy and Research</i> 27(3): 365&#8211;382. 394</p>
<p class="ref">S. Skodda, W. Gr&#246;nheit, and U. Schlegel. 2012. Impairment of vowel articulation as a possible marker of disease progression in Parkinson&#8217;s disease. <i>PLoS One</i> 7: e32132. DOI: 10.1371/journal.pone.0032132. 390</p>
<p class="ref">C. Sobin and H. A. Sackeim. 1997. Psychomotor symptoms of depression. <i>American Journal of Psychiatry</i> 154: 4&#8211;17. 386</p>
<p class="ref">H. H. Stassen, M. Albers, J. P&#252;schel, C. Scharfetter, M. Tewesmeier, and B. Woggon. 1995. Speaking behavior and voice sound characteristics associated with negative schizophrenia. <i>Journal of Psychiatriatric Research</i> 29: 277&#8211;296. DOI: 10.1016/0022-3956(95)00004-O. 387</p>
<p class="ref">H. H. Stassen, G. Bomben, and E. Gunther. 1991. Speech characteristics in depression. <i>Psychopathology</i> 24: 88&#8211;105. DOI: 10.1159/000284700. 387</p>
<p class="ref"><a id="page_416"/>D. Sturim, P. A. Torres-Carrasquillo, T. F. Quatieri, N. Malyska, and A. McCree. 2011. Automatic detection of depression in speech using Gaussian mixture modeling with factor analysis. In <i>Proceedings of. INTERSPEECH</i>, Florence, Italy, pp. 2983&#8211;2986. 390</p>
<p class="ref">F. Tolkmitt, H. Helfrich, R. Standke, and K. R. Scherer. 1982. Vocal indicators of psychiatric treatment effects in depressives and schizophrenics. <i>Journal of Communication. Disorders</i> 15: 209&#8211;222. DOI: 10.1016/0021-9924(82)90034-X. 388</p>
<p class="ref">A. Trevino, T. Quatieri, and N. Malyska. 2011. Phonologically-based biomarkers for major depressive disorder. <i>EURASIP Journal of Advanced Signal Processing</i> 1&#8211;18. DOI: 10.1186/1687-6180-2011-42. 387, 390, 403</p>
<p class="ref">M. Valstar, B. Schuller, K. Smith, F. Eyben, B. Jiang, S. Bilakhia, S. Schnieder, R. Cowie, and M. Pantic. 2013. AVEC 2013: the continuous audio/visual emotion and depression recognition challenge. In <i>Proceedings of the ACM International Workshop on Audio/Visual Emotion Challenge (AVEC&#8217;13)</i>, pp. 3&#8211;10. DOI: 10.1145/2512530.2512533. 388, 390, 391, 397</p>
<p class="ref">M. Valstar. 2014a. Automatic behavior understanding in medicine. In <i>Proceedings of the ACM Workshop on Roadmapping the Future of Multimodal Interaction Research including Business Opportunities and Challenges</i>, pp. 57&#8211;60. 381</p>
<p class="ref">M. Valstar, B. Schuller, K. Smith, T. Almaev, F. Eyben, J. Krajewski, R. Cowie, and M. Pantic. 2014b. AVEC 2014: 3D dimensional affect and depression recognition challenge. <i>Proceedings of the ACM International Workshop on Audio/Visual Emotion Challenge (AVEC&#8217;14)</i>, pp. 3&#8211;10. DOI: 10.1145/2661806.2661807. 388, 391, 397</p>
<p class="ref">M. Valstar, J. Gratch, B. Schuller, F. Ringeval, D. Lalanne, M. Torres, S. Scherer, G. Stratou, R. Cowie, and M. Pantic., 2016. AVEC 2016-depression, mood, and emotion recognition workshop and Challenge. In <i>Proceedings of the ACM International Workshop on Audio/Visual Emotion Challenge [AVEC&#8217;16]</i>, pp.3&#8211;10. DOI: 10.1145/2988257.2988258. 399</p>
<p class="ref">D. Watson and L. A. Clark. 2006. Distress and fear disorders: An alternative empirically based taxonomy of the &#8216;mood&#8217; and &#8216;anxiety&#8217; disorders. <i>British Journal of Psychiatry</i> 189(6): 481&#8211;483. DOI: 10.1192/bjp.bp.106.03825. 379</p>
<p class="ref">D. Watson, M. W. O&#8217;Hara, L. J. Simms, R. Kotov, M. Chmielewski, E. A. McDade-Montez, W. Gamez and S. Stuart. 2007. Development and validation of the inventory of depression and anxiety symptoms (IDAS). <i>Psychological Assesment</i> 9(3): 253&#8211;268. DOI: 10.1037/1040-3590.19.3.253. 379</p>
<p class="ref">P. Waxer. 1974. Nonverbal cues for depression. <i>Journal of Abnormal Psychology</i> 83(3): 319. 394</p>
<p class="ref">J. R. Williamson, T. F. Quatieri, B. S. Helfer, R. Horwitz, B. Yu, and D. D. Mehta. 2013. Vocal biomarkers of depression based on motor incoordination. In <i>Proceedings of the ACM International Workshop on Audio/Visual Emotion Challenge (AVEC&#8217;13)</i>, Barcelona, Spain, pp. 41&#8211;48. DOI: 10.1145/2512530.2512531. 387, 388, 390, 391, 403</p>
<p class="ref">J. Williamson, T. Quatieri, B. Helfer, G. Ciccarelli, and D. D. Mehta. 2014. Vocal and facial biomarkers of depression based on motor incoordination and timing. In <i>Proceedings</i> <a id="page_417"/><i>of the ACM International Workshop on Audio/Visual Emotion Challenge (AVEC&#8217;14)</i>, Orlando, FL, pp. 65&#8211;72. DOI: 10.1145/2661806.2661809. 381, 390, 391, 397, 398</p>
<p class="ref">World Health Organization. 2014. <i>Global Estimates 2014 Summary Tables</i>. World Health Organization, Geneva, Switzerland. 376</p>
<p class="ref">X. Xiong and F. De la Torre. 2013. Supervised descent method and its applications to face alignment. In <i>Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</i>, Portland, OR. DOI: 10.1109/CVPR.2013.75. 384</p>
<p class="ref">Y. Yang and D. Ramana. 2011. Articulated pose estimation with flexible mixtures-of-parts. In <i>Proceedings of the International Conference on Computer Vision and Pattern Recognition</i>, pp. 1385&#8211;1392. DOI: 10.1109/CVPR.2011.5995741. 392</p>
<p class="ref">Y. Yang, C. Fairbairn, and J. Cohn. 2013. Detecting depression severity from vocal prosody. <i>IEEE Transactions on Affective Computing</i> 4: 142&#8211;150. DOI: 10.1109/T-AFFC.2012.38. 387</p>
<p class="ref">T. Yingthawornsuk, H. K. Keskinpala, D. France, D. M. Wilkes, R. G. Shiavi, and R. M. Salomon. 2006. Objective estimation of suicidal risk using vocal output characteristics. In <i>Proceedings of INTERSPEECH</i>, Pittsburgh, PA, pp. 649&#8211;652. 388</p>
<p class="ref">M. Zimmerman, I. Chelminski, and M. Posternak. 2004. A review of studies of the Hamilton depression rating scale in healthy controls: Implications for the definition of remission in treatment studies of depression. <i>Journal of Nervous &#38; Mental Disease</i> 192(9) 595&#8211;601. DOI: 10.1097/01.nmd.0000138226.22761.39.</p>
<p class="line"/>
<p class="note"><a id="fn1" href="#rfn1">1</a>.&#160;&#160;AVEC 2016 data available from <a href="http://sspnet.eu/avec2016/">http://sspnet.eu/avec2016/</a>.</p>
<p class="note"><a id="fn2" href="#rfn2">2</a>.&#160;&#160;AVEC 2013 and AVEC 2014 are available for research purposes <a href="http://sspnet.eu/avec2014/">http://sspnet.eu/avec2014/</a>.</p>
</body>
</html>