<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_227"/>8</p>
<p class="chtitle"><b>Real-Time Sensing of Affect and Social Signals in a Multimodal Framework: A Practical Approach</b></p>
<p class="chauthor"><b>Johannes Wagner, Elisabeth Andr&#233;</b></p>
<p class="h1"><a id="ch8_1"/><b><span class="bg1">8.1</span>&#160;&#160;&#160;&#160;Introduction</b></p>
<p class="noindent">During the last decade, automatic sensing of <i>affect and social signals</i> (for definitions see the Glossary) has emerged as a hot topic in the field of Human-Computer Interaction (HCI). In the long run, the ultimate goal of these efforts is to build machines that allow for natural and fluent interaction. This inevitably means that the time that may elapse between the occurrence of a social cue (e.g., a facial smile or a change in intonation) and the moment when it is perceived and interpreted by the machine is limited. Hence, the recognition of social behaviour is not only a question of accuracy, but also of efficiency. In fact, we may prefer a quick guess over a precise but time-consuming decision. At the same time, social behavior reveals itself as a complex interplay between various modalities (face, speech, body, etc.). Hence, to perceive the full picture of an interaction, we must not look at the modalities in isolation, but combine behavioral cues through multiple channels with respect to their temporal occurrence. Again, we may prefer several &#8220;weak&#8221; decisions over a single one, which is more accurate but possibly misleading.</p>
<p class="indent"><a id="page_228"/>Today, the core of work conducted within the field of <i><b>Affective computing and social signal processing</b></i> (SSP) still consists of offline studies<sup><a id="rfn1" href="#fn1">1</a></sup> or confines itself to a single modality (in most cases it is both). While it is easy to understand that researchers have a preference for looking at modalities in isolation and avoiding the additional effort to make an algorithm truly &#8220;online&#8221;, we are now at a point where many problems have been satisfactorily solved under static laboratory conditions, but still fail when applied in a &#8220;real world&#8221; setting. In this chapter we will identify what we believe are the main challenges that need to be tackled to push socially aware software (see the Glossary) into our every-day life. The most promising way to encourage developers to put more effort into building online systems is by providing adequate tools that take as much work off their hands as possible. We will complete the chapter by presenting our open-source framework Social Signal Interpretation (SSI), which has been called to life for the very purpose of tackling these challenges. A quick practical introduction to the framework will be given by means of an <i>online recognition</i> system for enjoyment.</p>
<p class="h1"><a id="ch8_2"/><b><span class="bg1">8.2</span>&#160;&#160;&#160;&#160;Database Collection</b></p>
<p class="noindent">Recognizing human behavior is a learning problem which requires the collection of representative training samples. In fact, it is the training data that defines the knowledge base any derived model can draw on. Hence, the collection of appropriate databases defines a crucial step towards building automatic detection systems. But what can be regarded as an &#8220;appropriate&#8221; database design and what are the difficulties in creating multimodal corpora (see the Glossary)?</p>
<p class="indent">Most studies in the past (and not only in the past) have analyzed social and affect behavior on data that had been recorded by professional actors (sometimes also non-actors) in controlled laboratory settings [Engberg et al. 1997, Battocchi et al. 2005, Burkhardt et al. 2005]. Although it is easier to instruct someone to perform a particular set of actions than trying to induce the same behavior spontaneously within a costly experiment, acted behavior generally does not cover the subtle and less prototypical manifestations we can observe in natural interaction. In addition, signals obtained within a perfectly controllable environment do not resemble reality in terms of noise, occlusions, movements, etc. Hence, we must not expect a system trained on acted data to show a satisfying performance when applied outside the lab. To this end, researchers have formulated a list of criteria that databases should fulfil to reflect non-verbal behavior as it occurs in daily life [Cowie and Cornelius 2003, Cowie et al. 2005, Douglas-Cowie et al. 2003, 2005, 2007].</p>
<div class="box">
<p class="bhead"><a id="page_229"/><b>Glossary</b></p>
<p class="hangbx"><b>Affect and social signals</b> can be described as temporal patterns of a multiplicity of nonverbal behavioral cues which last for a short time [Vinciarelli et al. 2009] and are expressed as changes in neuromuscular and physiological activity [Vinciarelli et al. 2008a]. Sometimes, we consciously draw on affect and social signals to alter the interpretation of a situation, e.g. by saying something in a sarcastic voice to signal that we actually mean the opposite. At another time, we use them without being aware of it, e.g., by showing sympathy towards our counterpart by mimicking his or her verbal and nonverbal expressions. See Chapter 7 of this volume for an overview of affective and social signals for which automated recognition approaches have been proposed.</p>
<p class="hangbx"><b>Affective computing and social signal processing</b> aims at perceiving and interpreting nonverbal behavior with a machine by detecting affect and social signals just as humans do among themselves. This will lead to a new generation of computers that is perceived as more natural, efficacious and trustworthy [Vinciarelli et al. 2008b, Vinciarelli et al. 2009], and it makes room for a more human-like and intuitive interaction [Pantic et al. 2007].</p>
<p class="hangbx"><b>A multimodal corpus</b> targets the recording and annotation of multiple communication modalities including speech, hand gesture, facial expression, body posture, etc. Today, most corpora that are multimodal consist of audio-visual data. Other modalities such as 3D body and gaze tracking, or physiological signals are hardly present, but are needed to provide a broader picture of human interaction. The collection of large databases rich of social behavior expressed through a variety of modalities is key to model the complexity of social interaction [Vinciarelli et al. 2012, Eerekoviae 2014].</p>
<p class="hangbx"><b>Multimodal fusion</b> defines the integration of multiple data sources, such as audio and video, into a homogenous and consistent representation. Combining affective and social cues across channels is important to resolve situations where social behavior is expressed in a complementary [Zeng et al. 2009] or even contradictory way [Douglas-Cowie et al. 2005]. This also involves a proper modeling of the complex temporal relationships that exist between the diverse channels.</p>
<p class="hangbx"><b>Online recognition</b> means that a system is able to detect and analyze affective and social cues on-the-fly from the raw sensor input. Decisions based on the perceived user state need to be made fast enough to allow for a fluent interaction and it is not possible to look ahead in time. Setting up an online system is more complex than processing data offline.</p>
</div>
<p class="numlistt"><a id="page_230"/>1. The <i>scope</i> of a database is determined by the number of subjects, their cultural background, age, gender, etc. as well as the kind of behavior, affective expressions, and actions that are represented. Existing corpora are dominated by subjects from Western Europe, mostly middle-aged with an academic background [Henrich et al. 2010] and social behavior is often understood as a narrow set of prototypical expressions (e.g., a set of &#8220;basic&#8221; states such as anger, joy, sadness, etc.). Obviously, this material does not cover the richness of blended and moderate expressions perceived in daily life [Cowie and Cornelius 2003]. As a matter of fact, we will need both: databases moving toward a full coverage of a large number of core states and databases focusing on a single or few expressions at varying intensity levels.</p>
<p class="numlistt">2.&#160;&#160;The <i>naturalness</i> of a database is defined by the origin of the material. Generally, we can distinguish between material that is acted, deliberately induced, or culled from existing sources [Douglas-Cowie et al. 2003]. To induce spontaneous reactions one may employ a Wizard-of-Oz scenario [Kelley 1984], but other methods have been reported, such as Velten mood induction [Kenealy 1986], emotive music [Scherer and Zentner 2001], affective pictures [Lang et al. 2008], or games [Johnstone 1996]. Broadcast material is regarded as a popular source to study truly natural behavior, e.g., TV talk shows. However, considerable effort has to be spent to establish a &#8220;gold standard&#8221; or an &#8220;affective ground truth&#8221; (see <a href="18_Chapter06.xhtml">Chapter 6</a>) against which to evaluate the performance of a recognition system. A possible middle way is the use of samples coming from a real-life situation to guide the artificial production by actors [Douglas-Cowie et al. 2003].</p>
<p class="numlist">3.&#160;&#160;Typically, there are two general types of <i>context</i> [Cowie and Cornelius 2003]: temporal and intermodal. On a short-term basis it is often the time course of a social cue that conveys the meaning rather than its absolute values. On a longer term, temporal context can be considered by interpreting behavioral cues on the basis of prior events rather than in isolation. A common shortcoming in emotion corpora is that affective content is represented in form of short and isolated episodes, which do not capture the ebbs and flows of emotions over time [Douglas-Cowie et al. 2003]. Regarding intermodal context, humans are used to express their intensions through several channels, namely, voice, face, gestures, etc. Since the interplay is highly complex, databases are required that include interactions through multiple modalities as <a id="page_231"/>well as descriptions of when and how these modalities come into play [Cowie et al. 2005]. A detailed analysis of the role of context within and between modalities is given in <a href="14_Chapter03.xhtml">Chapter 3</a>.</p>
<p class="numlist">4.&#160;&#160;The final feature of a database concerns the kind of <i>description</i> that is given [Douglas-Cowie et al. 2005]. In the case of acted material this is usually straightforward since a solid description can be directly derived from the experimental setup. When it comes to more naturalistic, possibly continuous material, raters have to be hired who will manually define the on- and offsets of social episodes and describe them according to a pre-defined coding scheme that specifies the information to be extracted from a corpus. Even though the development of commonly accepted measurement methods still remains a core challenge [Pentland 2007], two fundamental trends can be distinguished, namely categorical descriptors and continuous variables [Douglas-Cowie et al. 2005].</p>
<p class="indentt">Today, a number of <i><b>multimodal corpora</b></i> are available that to some respect meet above policies, e.g., the Belfast Naturalistic Database [Douglas-Cowie et al. 2000] containing 239 audio-visual recordings from a total of 100 subjects; the SmartKom corpus [Schiel et al. 2002] containing 172 recordings of subjects asked to test a system &#8220;prototype&#8221; for a market study that in fact was controlled by two human operators; the VAM corpus [Grimm et al. 2008] consisting of 12 hours of recordings of the German TV talk-show &#8220;Vera am Mittag&#8221; (Vera at noon); the SAL (Sensitive Artificial Listener) corpus [Douglas-Cowie et al. 2008] consisting of 11 hours of induced emotion using an interactive artificial personality; the IEMOCAP (Interactive Emotional Dyadic Motion Capture) database [Busso et al. 2008] gathering 12 hours of recordings of dyadic sessions where actors perform improvisations or scripted scenarios; the SEMAINE corpus [McKeown et al. 2010] composed of 100 sessions each about 5 minutes recorded during emotionally coloured conversations; and the Belfast Story Telling corpus [McKeown et al. 2015] featuring 25 hours of group recordings from 21 participants in spontaneous social interaction. Multimodal databases with focus on other social phenomena have been collected within meeting and game scenarios, offering insights into turn taking strategies, role allocation (e.g., dominance), and personality traits. To name a few, there is the AMI Meeting Corpus made of 100 hours of meeting recordings [Carletta et al. 2006], the Canal9 corpus providing roughly 42 hours political debate [Vinciarelli et al. 2009b], the Idiap Wolf data set containing around 81 hours of conversational data among groups of 8&#8211;12 people playing a role playing game [Hung and Chittaranjan 2010], and the MAHNOB Mimicry Database assembled of 13 hours interaction <a id="page_232"/>of in total 40 participants discussing on a political topic or attending a role-playing game [Sun et al. 2011, Lichtenauer et al. 2011].</p>
<p class="indent">Given the richness of observable social expressions there is still a lack of data. In particular, regarding the selection and placement of the sensors that are used to record the interaction as most databases are audio-visual only [Eerekoviae 2014]. Yet, this is not only due to a lack of alternatives. Microsoft&#8217;s Kinect and comparable depth sensors are good examples for a growing market of novel recording equipment that is easy to use and cheap. The reason that most current corpora are still restricted to audio-visual content comes from the difficulty to include those &#8220;exotic&#8221; sensors into the recordings scenario and synchronize them with each other. A proper synchronization is required if we aim to combine data from multiple devices. It is in particular crucial for long and continuous recordings to avoid time drifts of the individual signals. However, such recordings are required to train classifiers for online recognition tasks.</p>
<p class="indent">Also problematic is the fact that no coding scheme exists, as of yet, that would fully cover all the aspects in human interaction behavior. As a matter of fact, researchers either adapt existing schemes to their needs or build them from scratch. Unfortunately, a large part of the proposed multimodal schemes lacks proper validation and have proven to be unreliable [Cavicchio and Poesio 2009]. A few exceptions to this rule exist, though. The most prevalent standard is the well-known Facial Action Unit System (FACS) developed by Ekman and Friesen to describe the richness and complexity of facial expressions [Ekman and Friesen 1978]. Examples for truly multimodal coding scheme are MUMIN [Allwood et al. 2007], Augmented Multiparty Interaction with Distance Access (AMIDA) by Wilson [2008], Maptask [Carletta et al. 1997] and Dialog Act Markup in Several Layers (DAMSL) [Core and Allen 1997]. To overcome the limitation of most existing coding schemes to deal with only one or two modalities, Blache et al. [2009] propose to combine existing schemes and extend them to obtain a coding scheme that would be as complete as possible. One such approach is PAULA XML (Potsdam Exchange Format for Linguistic Annotations) [Chiarcos et al. 2008]. To cope with the different descriptive schemes of emotion a XML-based language named Emotion Annotation and Representation Language<sup><a id="rfn2" href="#fn2">2</a></sup> (EARL) has been developed. EARL supports discrete and time-varying encoding of emotion dimensions as well as independent annotation of multiple modalities modeling of specific relations such as blending or masking of emotions [Douglas-Cowie et al. 2005]. Likewise, EmotionML<sup><a id="rfn3" href="#fn3">3</a></sup> or short Emotion <a id="page_233"/>Markup Language (EML) [Schr&#246;der et al. 2011] aim to strike a balance between practical applicability and scientific well-foundedness defined by W3C.</p>
<p class="indent">Nowadays, at least, researchers can draw on a number of annotation tools that have been released to the community free of charge. Popular layer-based tools are EUDICO Linguistic Annotator (ELAN) [Wittenburg et al. 2006, Sloetjes et al. 2007], ANVIL [Kipp 2013], or Extensible Markup Language for Discourse Annotation (EXMARaLDA) [Schmidt 2004] which offer freely definable tracks to insert time-anchored labelled segments. These tools are well suited to describe social cues that feature precise starting and ending times, such as speech and gestures turns. They also allow capturing longer states as long as the behavioral content can be decomposed into more or less homogenous segments. In addition, there are tools for annotating continuous attributes such as the intensity of a smile or the level of a negative emotion. Examples are Feeltrace [Schr&#246;der et al. 2000] and its successor GTrace (General Trace program), which allow an observer to track the emotional content of an audio-visual stimulus over time based on activation-evaluation space, or Continuous Measurement System (CMS) [Messinger et al. 2008, Messinger et al. 2009], a tool for continuous expert coding of videotaped behavior. A drawback of existing tools, however, comes from the fact that they offer little automation. Hence, creating descriptions for several hours of interaction remains an extremely time-consuming task and it will be important to ease the creation of large multimodal databases wherever possible, both in terms of recording and annotation.</p>
<p class="h1"><a id="ch8_3"/><b><span class="bg1">8.3</span>&#160;&#160;&#160;&#160;Multimodal Fusion</b></p>
<p class="noindent"><i><b>Multimodal fusion</b></i> is the integration of multiple data sources into a homogenous and consistent representation. If the new representation gives a more complete picture of the world than the independent sources do, we can expect an improvement in recognition accuracy compared to a unimodal system. Finding an appropriate representation format that captures the complementarity and synchrony between modalities represents a fundamental challenge of any multimodal fusion approach, see <a href="12_Chapter01.xhtml">Chapter 1</a>.</p>
<p class="indent">Traditionally, the two most common approaches are feature level fusion and decision level fusion, which both showed promising results in early studies [Chen et al. 1998, De Silva and Ng 2000]. However, it turned out impossible to come up with general guidelines on what can be regarded as an ideal fusion method and what is the expected improvement in recognition accuracy. Some studies report results that clearly emphasize the usefulness of combined classification, often yielding improvements of 10% and more compared to the best unimodal prediction (e.g., Kanluan et al. [2008]). But other studies exist which report no or only marginal <a id="page_234"/>improvements (e.g., Jayagopi et al. [2009]). Likewise, some studies favour early fusion approaches (e.g., Petridis and Pantic [2008]), while others find late fusion to be superior (e.g., Aran and Gatica-Perez [2010]). See <a href="15_Chapter04.xhtml">Chapter 4</a> for the distinction between early, middle, and late fusion. A meta study on 30 published studies on multimodal affect detection by D&#8217;Mello and Kory [2012] reveals that in fact the performance correlates significantly with the naturalness of the underlying corpus. While an overall mean multimodal effect of 8.12% is reported, they also found that improvements are three times lower when classifiers are trained on natural or semi-natural data (4.39%) compared to acted data (12.1%). At a first glance, this suggests that under realistic conditions there is less room for improvements than in case of acted material. However, especially in natural interaction people are supposed to make use of a mix of behavioral cues through multiple channels, which actually makes multimodal fusion even more promising as it calls for smart ways to assemble the incoming cues to a full picture.</p>
<p class="indent">The crucial point here is that adding more information also increases the choices, especially since conventional fusion algorithms are designed to incorporate information from all modalities at all time. De facto this means that in case of contradictory signs they have to decide in favour of one, which can either turn out to be the winning ticket or a loss. As long as acted data is the benchmark, it is in fact relatively likely that expressions in the various channels are in unison, which explains the solid gain in performance for convenient fusion methods. In natural interaction, however, users draw on a mixture of strategies to express emotion [Batliner et al. 2000] leading to a complementary rather than consistent display of social behavior [Zeng et al. 2009]. In fact, large parts of naturalistic emotion databases have been proven to carry contradictory multimodal cues [Douglas-Cowie et al. 2005].</p>
<p class="indent">So what are the consequences for a fusion system? Let us assume we measure emotion from two channels and observe a positive state in the first channel and a neutral in the second (see <a href="#fig8_1">Figure 8.1</a>). Since conventional fusion approaches start from a single segmentation, which is applied to all modalities, a decision will be forced from all modalities. In case of audio-visual emotion recognition, for instance, it is a common strategy to trigger analysis by the audio channel with the consequence that facial expressions are only considered when voice is active [Lingenfelser et al. 2011]. This approach of triggering multimodal fusion from a single annotation or modality has at least one severe drawback: additional cues in further modalities can be expected but are not guaranteed. Important facial components, e.g., may be signaled before the start of an utterance and in the worst case may already be back to neutral at that point. It may even be the case that <a id="page_235"/>lip movements during speech production distort the facial expression. A neutral or misleading facial expressions during the utterance may then have a negative influence on the fused decision.</p>
<div class="cap" id="fig8_1">
<p class="image"><img src="../images/fig8_1.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 8.1</b>&#160;&#160;&#160;&#160;In conventional fusion approaches, information is combined over fixed time segments, e.g., between beginning and ending of an utterance. This has the drawback that cues from other modalities outside the segment will be missed. To overcome these limitations, the lower part of the figure sketches an alternate fusion approach which combines cues asynchronously: Instead of postponing decisions until activity is detected and then forcing all modalities to contribute, modalities can contribute individually.</p>
</div>
<p class="indent">A possible solution to avoid decisions from all channels in every time slot is dynamic classification. Since dynamic classifiers work on continuous streams of <a id="page_236"/>short-term features, it is not necessary to force a fusion decision &#8220;from above&#8221;. Instead, they (principally) have the ability to model temporal relations between the streams and learn when and how multimodal information should be combined. To this end, Song et al. [2004] proposed a tripled Hidden Markov Model (THMM) which is able to integrate three or more streams of data and allows the state asynchrony of the sequences while preserving their natural correlation over time. [Zeng et al. 2008] applied Multi-stream Fused Hidden Markov Model (MFHMM), where state transitions of different component Hidden Markov Models (HMMs) do not necessarily occur at the same time across different streams so that the synchrony constraint among different streams is also relaxed. Coupled Hidden Markov Models (CHMM), where the probability of the next state of a sequence depends on the current state of all HMMs and therefore enables an improved modeling of intrinsic temporal correlations between multiple modalities, have also been proposed (see Nicolaou et al. [2010]). See also <a href="14_Chapter03.xhtml">Chapter 3</a> for a description of fusion approaches that account for temporal relationships among modalities.</p>
<p class="indent">To overcome the computational complexity of asynchronous Hidden Markov model (AHMM), W&#246;llmer et al. [2009] suggested a multidimensional dynamic time warping (DTW) algorithm for hybrid fusion of asynchronous data, requiring significantly less decoding time while providing the same data fusion flexibility as the AHMM. Finally, Recurrent Neural Networks (RNN) offer a third alternative for asynchronous fusion; in particular, in the form of Long Short-Term Memory Neural Networks (LSTM-NNs), which replace the traditional neural network nodes with memory cells, essentially allowing the network to learn when to store or relate to bimodal information over long periods of time. In fact, LSTM-NNs have been successfully applied to combine acoustic and linguistic features to continuously predict the current quadrant in a two-dimensional emotional space spanned by the dimensions valence and activation [W&#246;llmer et al. 2010]. Likewise, in a similar emotion recognition task, this approach successfully fuses facial expressions, shoulder gestures and audio cues [Nicolaou et al. 2011]. We refer to <a href="15_Chapter04.xhtml">Chapter 4</a> and <a href="18_Chapter06.xhtml">6</a> for more detailed information on how to employ LSTM-NNs for fusing multisensorial and multimodal information.</p>
<p class="indent">Even if the temporal dependencies between modalities are relaxed, most algorithms still start from the assumption that data from all modalities is available at all time. Of course, in offline mode this condition is easy to meet by simply omitting parts where input from one or more modalities is corrupted or completely missing. Thinking of a real-time system, on the other side, obviously this assumption no longer holds. Either, as remarked by Chen and Huang [2000], because no useful information <i>can</i> be detected (e.g., the user is not looking into the camera), or <a id="page_237"/>because there <i>is</i> nothing useful to detect (e.g., the user is not talking), or last but not least, due to a failure in one of the sensors. In short, to be used in a realistic application a fusion system has to be able to handle <i>missing data</i> in any modality at any time.</p>
<p class="indent">One way to tackle the problem of missing data and making the fusion process more transparent is by shifting from segmentation-based processing to an <i>event-driven</i> approach. Introducing events as an abstract intermediate layer effectively decouples unimodal processing from the final decision making. In this view, each modality serves as a client which individually decides when to add information. Signal processing components can be added or replaced without having to touch the actual fusion system and missing input from one of the modalities does not cause the collapse of the whole fusion process. In some sense, this kind of event-driven fusion is similar to semantic fusion used to analyze the semantics of multimodal commands and typically investigates the combination of gestures and speech in new-generation multimodal user interfaces (see the following section). Lingenfelser et al. [2014] propose a vector-driven approach to combine audio-visual events to detect the enjoyment level in group discussions. Eyben et al. [2011] predict user affect in a continuous-dimensional space based on verbal and non-verbal behavioral events (e.g., smiles, head shakes, or laughter) by seeing system events as &#8220;words&#8221; which are joined for each time segment and converted to a feature vector representation through a binary bag-of-words (BOW) approach.</p>
<p class="h1"><a id="ch8_4"/><b><span class="bg1">8.4</span>&#160;&#160;&#160;&#160;Online Recognition</b></p>
<p class="noindent">In offline studies it is common practice to take a set of pre-recorded files and process them in independent steps. Only when all files have successfully passed a stage are results handed over to the next step. Typically, this starts by reviewing the data and excluding parts where things went wrong, e.g., due to sensor failure or a too noisy environment. More data might be removed to balance the number of samples per class or to remove parts with sparse interaction. In an iterative manner each step is fine-tuned until a good configuration is found and the best accuracy is reported as a sort of benchmark for what can be achieved with the system. While this defines the common way to evaluate the performance of a recognition approach and offers a convenient format to compare results, it says little about the applicability in real applications. This is because an online algorithm does not have the entire input available from the start, nor can it leave out certain parts of the input (see <a href="#fig8_2">Figure 8.2</a>). Instead, all input needs to be processed in small portions in <a id="page_238"/>serial fashion. The following section is dedicated to the manifold reasons which make it so challenging to build an online recognition system (see the Glossary).</p>
<div class="cap" id="fig8_2">
<p class="image"><img src="../images/fig8_2.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 8.2</b>&#160;&#160;&#160;&#160;In online processing we can only draw on what we have seen so far. In particular, we need strategies to deal with noise and react to missing data.</p>
</div>
<p class="indent">In a single channel classification problem, missing data means that no reliable output can be given. In a multimodal scenario with several input nodes, however, things look different. If a single channel fails, there is still a good chance of finding useful information in the other channels. This also applies if one channel is interrupted by noise and hence can be trusted less. However, the tricky part is to dynamically decide in which channels to exploit in the fusion process and to what extent the present signals can be trusted. Techniques such as Kalman filtering [Kalman 1960] or adaptive fuzzy systems [Cox 1993] are well known for their application in sensor fusion to reduce uncertainty and produce a more accurate prediction than any of the original signals considered separately [Sasiadek and Hartana 2000]. An advantage of the aforementioned Multi-stream Fused Hidden Markov Model (MFHMM) [Zeng et al. 2008] is its ability to exploit the complementarity and redundancy of multiple channels during the fusion process: if one component HMM fails due to some reason, the other HMM can still work. Wagner et al. [2011] showed that many of the standard fusion techniques can be adjusted to successfully cope with missing data. A simple yet efficient strategy, e.g., is to maintain single channel classifiers as a backup if the fusion model is not applicable.</p>
<p class="indent">Another property that distinguishes online from offline processing is the fact that a fully automated online system has to be prepared to handle incoming data without exception. Even the decision to skip samples has to be made in realtime based on the incoming streams. Handling of <i>non-prototypical</i> behavior has been claimed to be one of the most challenging barrier when moving to real-life <a id="page_239"/>technology [Schuller et al. 2011]. Fortunately, this does not mean a model is needed that precisely knows and classifies each and every possible state a user may adopt. Even a real-time application will still function within a specific context and must only detect behavior relevant to its task. Yet, it should properly ignore anything that is not relevant and map it into a <i>garbage class</i>. Hence, an according garbage model has to be trained by including non-relevant data in the training phase. This is something that is usually ignored in offline studies as the designer is given the possibility to choose data according to his needs.</p>
<p class="indent">Shifting from discrete categories toward a continuous prediction is an attempt to deal with different shades of expression. Some studies have used quantization to map a continuous range onto discrete levels [Kleinsmith and Bianchi-Berthouze 2007, W&#246;llmer et al. 2008]. An early step toward a true continuous dimensional affect prediction was made by Hanjalic [2006] who investigated the correlation of some basic video and audio features in affective movies with a valence-activation space. In the recent years, a couple of studies have been published proposing approaches to realize continuous classification: W&#246;llmer et al. [2010] and Nicolaou et al. [2011] performed regression using a combination of Long Short-Term Memory and Dynamic Bayesian Networks (BLSTM-NN) classifiers. Wu et al. [2010] tested three approaches (Robust Regression, Support Vector Regression (SVR), and Locally Linear Reconstruction) for emotion primitives estimation in a 3D space spanned by valence, activation, and dominance. Metallinou et al. [2013] applied a Gaussian Mixture Model-based approach to map a set of audio-visual cues to an underlying emotional state.</p>
<p class="indent">Deciding on an appropriate unit of analysis is another issue which requires special attention. In offline studies, the length of a processed segment can be optimally chosen with respect to the boundaries of the observed user behavior. This is not possible in an online system for two reasons: (1) a fluent interaction typically requires a fast prediction and there might be not enough time to wait until the optimal boundary is reached [Schuller et al. 2011]; and (2) we might not even know in advance where the optimal boundaries will be. In gesture analysis, for instance, the boundaries of a gesture are fluid making it difficult to define when a meaningful sequence starts. A common solution to this problem is to manually trigger segmentation, e.g., by introducing a special start gesture or use something like <i>push-to-talk</i>. This, however, is only acceptable as an intermediate solution as it interrupts the natural interaction flow.</p>
<p class="indent">Humans are extremely good at coming up with an interpretation before all information is processed. This helps accomplish a fluent interaction and requires a so-called <i>incremental processing</i>. For instance, when hearing a sentence, we usually <a id="page_240"/>get its meaning before it is completed, which gives us time to come up with a proper response. In the context of gesture recognition, Kristensson and Denby [2011] propose a system that estimates the posterior probabilities of the user&#8217;s currently incomplete stroke to predict a user&#8217;s intended template gesture. This enables them to provide continuous feedback to the user while producing a stroke. Similar approaches are needed in other areas, too, where a fluent interaction is wished that requires quick response to user behavior.</p>
<p class="h1"><a id="ch8_5"/><b><span class="bg1">8.5</span>&#160;&#160;&#160;&#160;Requirements for a Multimodal Framework</b></p>
<p class="noindent">Based on the thoughts in the last sections we can prepare a list of requirements for a multimodal framework. Afterward, we introduce our open-source Social Signal Interpretation (SSI) framework, which has been developed to tackle these requirements.</p>
<p class="hangt"><b>Multisensory</b>. The availability of new sensor devices like 3D cameras or wearables provides researchers with novel signals that are of great potential. Unfortunately, the use of such devices is often hampered as a higher level of expertise is required and no standardized ways exist to sync them with other equipment. Taking this into account, the framework has to provide a backbone for building up complex recording setups including multiple and exotic sensor devices. This requires uniform treatment of the captured sensor data, which has to be independent of the measured quantity, yet taking into account temporal characteristics.</p>
<p class="hangt"><b>Synchronization</b>. Despite different characteristics (e.g., varying sample rate), the captured signals have to share a common time-line. That is, at any point in time, we want to be able to match the diverse channels and decide which measurement in one stream corresponds to a measurement in another stream. To this end, strategies will be needed to keep captured signals in sync without relying on additional synchronization hardware. If necessary, a setup may include several machines which are connected in a network and have to be kept in sync as well. The stored data should be made available in formats that are easy to access and widely supported to ease further processing.</p>
<p class="hangt"><b>Automated Transcription</b>. In addition to the raw sensor streams, automated creation of supplementary descriptions should be considered to bolster the following annotation steps. This concerns the pre-segmentation of the recorded material into regions of interest as well as the extraction of meaningful <a id="page_241"/>features and transcriptions. So far, such treatments have been usually postponed to a postprocessing phase. However, extracting meta information on-the-fly opens up richer possibilities for steering experiments. Voice activity detection, for instance, can be used to control turn taking with a virtual agent in a Wizard-of-Oz like scenario without depending on human supervision.</p>
<p class="hangt"><b>Multimodal Fusion</b>. Although it is commonly agreed that integrating information from multiple channels bears great potential, most studies still focus on a single modality. Works concerned with the fusion of multimodal data often dismiss the complex temporal relationships that exist between the diverse channels. Given that we are able to capture and process synchronized multimodal data, possibilities are needed to combine information on different levels of processing. To support fusion at an early stage means including constructs that allow merging streams, either at a raw stream level or at feature level. To combine high-level information representing disjointed periods the architecture should be able to handle information on an asynchronous time base, e.g., in form of events.</p>
<p class="hangt"><b>Missing Data</b>. For different reasons, shifting from offline to online processing is not a trivial task. One particular challenge arises from the fact that there is no simple way to skip &#8220;inconvenient&#8221; data. In fact, a signal has to be processed as it comes, which requires special strategies to handle the case when information becomes corrupted or meaningless. To account for this, methods for handling missing data and possibilities to dynamically decide when to trust data are required. In particular, a garbage class is needed to intercept information that is not relevant to the current task.</p>
<p class="hangt"><b>Continuous Classification</b>. Instead of decomposing an input signal into discrete and isolated parts and assigning each segment to one out of a set of fixed categories, a continuous classification is required which updates the current state at a regular time basis on a dynamic scale. This calls for new classification schemes embedded in a flexible architecture which not only gives support for conventional statistic-based classification, but also dynamic classifiers, such as Hidden Markov Models (HMMs) or Recurrent Neural Networks (RNNs). Also, special attention has to be paid to the segmentation problem since detected boundaries have to suit recognition but also allow for a fluent interaction.</p>
<p class="hangt"><a id="page_242"/><b>Real-time</b>. To assure that efforts spent will translate into a real-time-capable application, the framework should support the development and testing of algorithms in an online manner. This, of course, should not demand a great deal of additional work. Generic tasks, such as threading and buffering of the processed data, should be handled without further developer involvement. To further cut down development time, the distribution of re-usable software should be fostered. To this end, facilities should be provided that encourage developers to reuse and modularly expand the pool of available solutions, whereas end users should be offered an easy-to-use interface for setting up systems without demanding certain programming skills and third-party development tools.</p>
<p class="h1"><a id="ch8_6"/><b><span class="bg1">8.6</span>&#160;&#160;&#160;&#160;The Social Signal Interpretation Framework</b></p>
<p class="noindent">The Social Signal Interpretation (SSI) framework [Wagner et al. 2013] is a framework for recording, analyzing, and fusing social signals in real-time. It has been designed to provide a general architecture to tackle the challenges we have discussed in the previous sections. The core idea of SSI is to accomplish complex signal processing pipelines for machine learning from simple reusable units. These units can, for example, be sensors, transformers computing features on the signal, or consumers for output or classification This means that SSI includes the possibility to collect multimodal data and to apply on-the-fly signal processing, but also gives support for the training and application of learning algorithms to extract higher-level information in real-time (see <a href="#fig8_3">Figure 8.3</a>). Important concepts such as introduction of a garbage class or replacing static categories with continuous dimensions are integral parts of the system architecture. A demonstration of the possibilities by means of a concrete examples is given in the next section.</p>
<p class="h2"><a id="ch8_6_1"/><b><span class="bg2">8.6.1</span>&#160;&#160;&#160;&#160;Basic Concepts</b></p>
<p class="noindent">To support the collection of large and rich multimodal corpora, SSI allows synchronized reading from a variety of different sensing devices, such as audio-visual sensors, eye trackers, physiological feedback systems, motion capture suits, etc. To account for future technology, SSI provides a flexible interface to integrate novel sensory elements. Dealing with such diverse modalities requires a generic data structure, which allows handling signal streams independently of origin and content. This is achieved by splitting signals into small segments and exchanging them in uniform packages of one or more values. A pool of circulate buffers is maintained to temporarily store raw and processed signal streams. This is realized in a fully automatic manner so that developers only need to connect sensing and processing <a id="page_243"/>units in desired order. By putting several of these units, also called components, in series, a processing pipeline is created. To process a data stream in different ways, it is possible to branch out into multiple forks. Likewise, it is possible to join (fuse!) forks, too. At every processing step, an individual window size can be chosen. To give an example, facial expression analysis may start from a frame-to-frame base, where each frame is analyzed independently. At later stages, processing may rely on bundles of frames to capture temporal dynamics (see <a href="14_Chapter03.xhtml">Chapter 3</a>).</p>
<div class="cap" id="fig8_3">
<p class="image"><img src="../images/fig8_3.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 8.3</b>&#160;&#160;&#160;&#160;SSI allows the synchronized reading from multiple sensor devices (s). Signal data can be processed continuously (p) or in form of high-level events (e). Information from multiple sources can be fused along the way (f). Raw and processed data (in form of streams or at event-level) can be recorded (r) and stored for later analysis and model learning (l).</p>
</div>
<p class="indent">When working with multimodal data we want to decide which parts of two signals temporally correspond with each other. Typically, this is solved using interleaved data chunks or by explicit relative time-stamping. However, in order to not lose synchronization, we either have to keep signals in interleaved order or keep and compare timestamps at each processing step. Both approaches are not practical solutions for a generic framework. To this end, SSI follows a different approach and implements strategies which ensure that signals stick to their sample rate. In short, the relative drift of a signal is regularly determined and synchronization is retained by removing redundant values or adding missing ones.</p>
<p class="indent">A strict synchronization and the possibility to combine signal streams after applying individual treatment create the necessary conditions to fuse multimodal data in various ways. In fact, SSI supports both early and late fusion approaches. Following a classical approach, we can combine features from multiple sources over common time frames (feature level fusion). By interposing classifiers to each <a id="page_244"/>branch and combining class probabilities instead of raw features we implement decision level fusion. However, SSI also offers the possibility to handle information in form of events. In contrast to continuous streams, events do not need to occur at a regular frequency, but represent high-level information, which occurs at irregular periods. Fusing at event level has the advantage that modalities can decide individually when to contribute to the fusion process (see <a href="#ch8_3">Section 8.3</a>). The different stages at which multimodal information can be combined are depicted in <a href="#fig8_4">Figure 8.4</a>.</p>
<div class="cap" id="fig8_4">
<p class="image"><img src="../images/fig8_4.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 8.4</b>&#160;&#160;&#160;&#160;Stages at which multimodal information can be combined. Feature fusion: feature sets (here <i>f<sub>A</sub></i> and <i>f<sub>B</sub></i>) are aggregated and analyzed with a single classifier. Decision-level fusion: results of individual classification steps are combined. Event-level fusion: moves away from fixed time segments and combines events as they occur in the individual channels.</p>
</div>
<p class="indent">Aiming to reach a large community, SSI follows a methodology that addresses both: developers interested in extending the framework with new functions and end-users whose primary objective is to create processing pipelines from what is there. The possibility to build and maintain complex recognition pipelines from single re-usable components using a simple XML-based language (see the next section) offers a simple-to-use, yet flexible interface for both experts and novices. The simulation of sensor input from pre-recorded files is an important feature, as it allows for a quick prototyping without setting up a complete recording setup, yet providing realistic conditions, e.g., by ruling out access to future data. Since all data communication is shifted to the framework, additional efforts are minimized.</p>
<p class="h2"><a id="ch8_6_2"/><b><span class="bg2">8.6.2</span>&#160;&#160;&#160;&#160;Multimodal Enjoyment Recognition</b></p>
<p class="noindent">As a practical demonstration we will describe a multimodal recognition system developed within the EU FET Project ILHAIRE<sup><a id="rfn4" href="#fn4">4</a></sup>. Although it is a significant feature <a id="page_245"/>of human communication, laughter is one of the least understood human behaviors [Ruch and Ekman 2001]. The objectives of ILHAIRE to bridge the gap between knowledge on human laughter and its use in automatic emotion recognition and synthesis, thus enabling sociable conversational agents to understand and express natural-sounding laughter. In the course of the project, SSI was employed to realize multiuser recording setups. Based on the collected data, a novel event-driven fusion algorithm was developed to recognize user enjoyment in real-time.</p>
<p class="h3"><a id="ch8_6_2_1"/><b>8.6.2.1&#160;&#160;&#160;&#160;Belfast Storytelling Corpus</b></p>
<p class="noindent">Laughter is a social phenomenon which occurs mainly in the presence of other individuals. And it is a behavioral-acoustic event which includes respiratory, vocal, facial, and skeletomuscular elements [Ruch and Ekman 2001]. This makes data capturing a challenging task as it requires technologies to gather multimodal data from several individuals concurrently. The Belfast Storytelling Corpus [McKeown et al. 2015], which was recorded with SSI, is comprised of six sessions of groups of three or four people telling stories to one another in either English or Spanish. The storytelling task is based on the 16 Enjoyable Emotions Induction Task [Hofmann et al. 2012].</p>
<p class="indent">During a recording session each participant wore a head-mounted microphone to capture high-quality audio recordings. Video signals were recorded using Logitech Pro HD webcams. Kinect motion capture technology was used to capture facial features, gaze direction and depth information (see <a href="#fig8_5">Figure 8.5</a>). Since such a complex recording setup exceeds the capabilities of a standard PC, it was distributed over several machines using a host-client architecture in which multiple clients wait for a host to send a start command. Each recording session lasted about 120 min, resulting in approximately 75 min recording time, and featured groups of 3 or 4 participants. The amount of laughter varied depending on which emotion was being recalled and the nature of the story that was being recounted.</p>
<p class="h3"><a id="ch8_6_2_2"/><b>8.6.2.2&#160;&#160;&#160;&#160;Event-driven Fusion</b></p>
<p class="noindent">In <a href="#ch8_3">Section 8.3</a> we have brought up the question whether for the task of affect recognition a segmentation-based combination of modalities can be regarded as an appropriate solution. Forcing a decision over all involved modalities may not satisfy the complex temporal relations of social cues. In the following, we present an event-driven fusion algorithm as an alternative to conventional segmentation-based approaches.</p>
<p class="indent">Laughter is a complex multimodal phenomenon. Its expression includes repetitive rhythmic shoulder and torso movements, visible inhalation, various facial actions and is often accompanied with some rhythmic as well as communicative <a id="page_246"/>gestures [Niewiadomski and Pelachaud 2012]. Through a complex temporal interplay, these elements allow us to vary and control the quality and intensity of laughter. Instead of pure joy, for instance, there can be voluntary laughter, a blend of enjoyment with other emotions or attempts to suppress laughter [Ruch and Ekman 2001]. In the following, we use the term <i>enjoyment</i>, which we define as an episode of positive emotion, indicated by visual and auditory cues of enjoyment, such as smiles and voiced laughter. This enables us to derive a global measurement of enjoyment from accumulated indication-events.</p>
<div class="cap" id="fig8_5">
<p class="image"><img src="../images/fig8_5.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 8.5</b>&#160;&#160;&#160;&#160;In the Belfast sessions four participants are recorded using Kinect, headset, and webcam. Left: Signals collected in one session (faces in video blurred). Right: Setup sketch involving several computers synchronized via network broadcast.</p>
</div>
<p class="indent">To account for the complex interplay between modalities, a finer annotation featuring detailed descriptions of enjoyment cues in all channels is required. <a href="#fig8_6">Figure 8.6</a> shows the tracks introduced to cover visual and audible cues, such as smiles and laughs. The bottom track finally defines where an enjoyment episode starts and ends. The task of a classifier is to correctly predict enjoyment on a frame-to-frame basis. The suggested event-driven approach uses the intermediate annotations to <a id="page_247"/>train classifiers that are specialized to certain cues. A final decision is derived based on the cues that are currently present.</p>
<div class="cap" id="fig8_6">
<p class="image"><img src="../images/fig8_6.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 8.6</b>&#160;&#160;&#160;&#160;Exemplary annotation of a full enjoyment episode aligned with various voiced and visual cues emitted by the user. For each frame (bordered by dotted lines) a decision has to be made by the fusion system. In a conventional segmentation-based approach each frame is seen in isolation, i.e., a decision is derived from the multimodal information within the frame. However, we can see that the single cues only partly overlap with the enjoyment episode: While other frames align with cues from a single modality (see, e.g., frame 2 and 4), some of the frames which are spanned by the enjoyment episode do actually not overlap with any observable cues (see, e.g., frame 9 and 10). Those frames are likely to be misclassified by a segmentation-based approach. The event-driven fusion approach we propose here takes in account the temporal asynchronicity of the events and is able to overcome frames with sparse cues of enjoyment based on information of preceding frames.</p>
</div>
<p class="indent">We only give a rough outline of the algorithm here, for details please see Lingenfelser et al. [2014]. The fusion scheme is based on social cues, which are detected by independent classifiers. For enjoyment recognition we picked visual smiles and audible laughs as meaningful cue events. Before these cues can enter the fusion process we have to map them to a common representation. Here, we use a one-dimensional space in which events are represented as vectors of a certain initial strength (score) expressing the confidence that the user is in an enjoyment state (high value) or not (low value). Accordingly, an event has either an increasing or decreasing effect on the current fusion output, which is also represented as a vector. This effect becomes less over time until at some point the influence of the vector becomes zero and is completely removed. If within short distance vectors of similar value pop up, this leads to a reinforcing impact that bolsters the confidence of the fusion result, whereas contradictory cues will neutralize each other. If for some time no new events are detected the likelihood for the observed behavior class automatically decreases and approaches a zero probability (see <a href="#fig8_7">Figure 8.7</a>). <a id="page_248"/>In the case that a modality is not available for some time, the algorithm can still make a prediction based on the remaining channels.</p>
<div class="cap" id="fig8_7">
<p class="image"><img src="../images/fig8_7.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 8.7</b>&#160;&#160;&#160;&#160;A fused score for enjoyment (dotted line) is derived from individual cues represented by vectors (vertical arrows) that express the confidence that the user is in an enjoyment state or not. Over time the influence of the vectors decreases and if no new cues are detected the likelihood for enjoyment approaches zero. By adjusting response time of the fusion vector to new events, we are able to tune the fusion outcome. The picture illustrates the effect when the decay time of the cue vectors is decreased and at the response time of the fusion vector is increased (grey line).</p>
</div>
<p class="h3"><a id="ch8_6_2_3"/><b>8.6.2.3&#160;&#160;&#160;&#160;Walk-through of a Recognition Pipeline</b></p>
<p class="noindent">The implementation of a real-time fusion system can be broken down in three core steps: (1.) capture the raw audio-visual data; (2.) detect and recognize the social cues; and (3.) combine the cues in the described manner. To cover the three tasks in a single pipeline, SSI offers a simple-to-use XML interface. In the following, we present an example of an audio-visual recognition pipeline.</p>
<p class="indent">We start by adding the sensor devices and tapping the desired channels. In particular, we include an audio source and a Microsoft Kinect to access the stream with the action units (AUs) extracted from the user&#8217;s face. An individual sample rate is chosen for each device:</p>
<div class="pre">
<p class="pindent"><code>&#60;sensor create=&#8220;MicrosoftKinect&#8221; sr=&#8220;25&#8221;&#62;</code></p>
<p class="pindent1"><code>&#60;input channel=&#8220;au&#8221; pin=&#8220;kinect_au&#8221;&#62;</code></p>
<p class="pindent"><code>&#60;/sensor&#62;</code></p>
<p class="pindent"><code>&#60;sensor create=&#8220;Audio&#8221; sr=&#8220;48000&#8221;&#62;</code></p>
<p class="pindent1"><code>&#60;input channel=&#8220;audio&#8221; pin=&#8220;audio_wav&#8221;/&#62;</code></p>
<p class="pindent"><code>&#60;/sensor&#62;</code></p>
</div>
<p class="indent">Next, we add a transformer to convert the AUs into a compact feature set. We choose a frame size of 10 samples and a delta size of 15 samples to output features <a id="page_249"/>every 400 ms for a window of 1 s. To the audio stream we add a voice activity component to detect speech segments and send an event named vad@audio. The decision is made on chunks of 100 ms. By setting the option incdur we configure the component to send an event each second until the end of an utterance is detected. Later on, this will allow for an incremental decision and a faster reaction.</p>
<div class="pre">
<p class="pindent"><code>&#60;transformer create=&#8220;MicrosoftKinectAUFeat&#8221;&#62;</code></p>
<p class="pindent1"><code>&#60;input pin=&#8220;kinect_au&#8221; frame=&#8220;10&#8221; delta=&#8220;15&#8221;/&#62;</code></p>
<p class="pindent1"><code>&#60;output pin=&#8220;kinect_au_feat&#8221;/&#62;</code></p>
<p class="pindent"><code>&#60;/transformer&#62;</code></p>
<p class="pindent"><code>&#60;transformer create=&#8220;VoiceActivitySender&#8221; incdur=&#8220;1.0&#8221;&#62;</code></p>
<p class="pindent1"><code>&#60;address=&#8220;vad@audio&#8221; input pin=&#8220;audio_wav&#8221; frame=&#8220;100ms&#8221;/&#62;</code></p>
<p class="pindent"><code>&#60;/consumer&#62;</code></p>
</div>
<p class="indent">For the recognition of smile and laugh events we now connect our input streams with pre-trained classifiers.<sup><a id="rfn5" href="#fn5">5</a></sup> Note that the audio classification is triggered by the voice activity event and features are extracted from the raw segments. Both components will fire an event if the according social cue is present.</p>
<div class="pre">
<p class="pindent"><code>&#60;consumer create=&#8220;Classifier&#8221; trainer=&#8220;smile&#8221; address=&#8220;smile@cue&#8221;&#62;</code></p>
<p class="pindent1"><code>&#60;input pin=&#8220;kinect_au_feat&#8221; frame=&#8220;1&#8221;/&#62;</code></p>
<p class="pindent"><code>&#60;/consumer&#62;</code></p>
<p class="pindent"><code>&#60;consumer create=&#8220;Classifier&#8221; trainer=&#8220;laugh&#8221; address=&#8220;laugh@cue&#8221;&#62;</code></p>
<p class="pindent1"><code>&#60;input pin=&#8220;audio_wav&#8221; listen=&#8220;vad@audio&#8221;&#62;</code></p>
<p class="pindent2"><code>&#60;transformer create=&#8220;EmoVoiceFeat&#8221;/&#62;</code></p>
<p class="pindent"><code>&#60;/input&#62;</code></p>
<p class="pindent1"><code>&#60;/consumer&#62;</code></p>
</div>
<p class="indent">Finally, we include the fusion component that collects the cues and outputs every 400 ms a prediction of the current enjoyment level:</p>
<div class="pre">
<p class="pindent"><code>&#60;object create=&#8220;VectorFusionModality&#8221; update_ms=&#8220;400&#8221;&#62;</code></p>
<p class="pindent1"><code>&#60;address=&#8220;joy@fusion&#8221; listen address=&#8220;laugh,smile@cue&#8221;/&#62;</code></p>
<p class="pindent"><code>&#60;/object&#62;</code></p>
</div>
<p class="indent">Individual response and decay time are given in a separate file:</p>
<table class="table1">
<tr>
<td><p class="tab1"><code>laugh@cue</code></p></td>
<td><p class="tab1"><code>0.06</code></p></td>
<td><p class="tab1"><code>0.7</code></p></td>
</tr>
<tr>
<td><p class="tab1"><code>smile@cue</code></p></td>
<td><p class="tab1"><code>0.1</code></p></td>
<td><p class="tab1"><code>0.9</code></p></td>
</tr>
</table>
<p class="indent"><a id="page_250"/>The first column contains the full event address, the second and third value define the response and decay time. We can, for instance, see that smile events are given a higher speed than laughs. This is because laughs are a strong indicator of enjoyment and should therefore have a long-term influence on fusion; smiles on the other hand need quick reaction time as they describe well the margins of enjoyment. The exact values have been empirically determined using grid search; see Lingenfelser et al. [2014] for a detailed description.</p>
<p class="indent">Finally, we want to share the detected enjoyment level with an external application. Therefore, we have the possibility for defining templates that at run-time are filled with the according values. For instance, we can define the following template which we connect to the output of our fusion component:</p>
<div class="pre">
<p class="pindent"><code>&#60;SSI time=&#8220;$(joy@fusion{field=time_system})&#8221;&#62;</code></p>
<p class="pindent1"><code>&#60;Enjoyment&#62;$(joy@fusion)&#60;/Enjoyment&#62;</code></p>
<p class="pindent"><code>&#60;/SSI&#62;</code></p>
</div>
<p class="indent">The result is then published through a socket connection.</p>
<div class="pre">
<p class="pindent"><code>&#60;object create=&#8220;XMLEventSender&#8221; address=&#8220;enjoyment@xml&#8221;&#62;</code></p>
<p class="pindent1"><code>&#60;listen address=&#8220;joy@fusion&#8221;/&#62;</code></p>
<p class="pindent"><code>&#60;/object&#62;</code></p>
<p class="pindent"><code>&#60;object create=&#8220;SocketEventWriter&#8221; host=&#8220;127.0.0.1&#8221; port=&#8220;9999&#8221;></code></p>
<p class="pindent1"><code>&#60;listen address=&#8220;joy@xml&#8221;/&#62;</code></p>
<p class="pindent"><code>&#60;/object&#62;</code></p>
</div>
<p class="indent">A listener would then receive a string of the following kind:</p>
<div class="pre">
<p class="pindent"><code>&#60;SSI time=&#8220;14600888&#8221;&#62;</code></p>
<p class="pindent1"><code>&#60;Enjoyment&#62;0.243&#60;/Enjoyment&#62;</code></p>
<p class="pindent"><code>&#60;/SSI&#62;</code></p>
</div>
<p class="h1"><a id="ch8_7"/><b><span class="bg1">8.7</span>&#160;&#160;&#160;&#160;Conclusion</b></p>
<p class="noindent">The experiences of the last decade have shown that building socially aware machines is an extremely challenging task. In order to fully exploit the potential of nonverbal communication, a system must be able to give prompt reactions to keep up the natural flow of interaction. This requires quick and efficient processing in (near) real-time. Moreover, the quality of the analysis should be independent of the environment in which the interaction takes place. In other words, a system should be able to detect social cues &#8220;in the wild&#8221;, for instance while driving in a car or walking down a street, just as reliably as in front of a computer. Finally, it has to take into <a id="page_251"/>account that every human is an individual who is shaped by a certain cultural background, different personality traits, and his own personal values. For a description of approaches that recognize such long-term traits, we refer to <a href="17_Chapter05.xhtml">Chapter 5</a>.</p>
<p class="indent">Hence, despite a good deal of work that has been carried out in the area of affective computing and social signal processing, these efforts have not translated into many applications. In this chapter, we have argued that this is not only due to the complexity of the problem itself, but also due to a proliferation of studies that investigate social interaction under laboratory conditions. The problem we are facing is that the plain offline studies which still dominate literature convey too optimistic a picture of what can actually be achieved with a proposed system. This is because they avoid problems which occur only when a system is tested in the &#8220;open world&#8221;. To this end, we have identified what we believe are the main challenges that need to be tackled in the future. In short, these are the following.</p>
<p class="hangt"><b>Constructing large multimodal corpora specifically aimed at the analysis of social phenomena</b>. Today, most SSP corpora are composed of discrete and exaggerated samples which do not reflect the variations of social behavior in everyday life [Douglas-Cowie et al. 2003]. In <a href="#ch8_2">Section 8.2</a> we discussed why a shift is needed toward material composed of natural expressions deliberately induced and presented in a continuous manner. Here, broadcast material, e.g., TV talk shows [Vinciarelli et al. 2009b], Wizard-of-Oz scenarios [Batliner et al. 2000], and game scenarios [Hung and Chittaranjan 2010] have been identified as suitable sources. Since the richness of observable social expressions makes it impossible to obtain a single corpus for all social phenomena, data should be collected in as many different scenarios as possible including as many sensor devices as possible.</p>
<p class="hangt"><b>Exploring novel ways to model multimodal fusion on multiple time scales and realize temporal correlations within and between different modalities</b>. In <a href="#ch8_3">Section 8.3</a> we argued that conventional fusion approaches, which have been developed on acted data, are not capable to properly model the complex temporal relationship between different modalities. In case of contradictory cues, for example, standard bimodal fusion approaches are as likely to decide in favour of a &#8220;correct&#8221; as an &#8220;incorrect&#8221; prediction [Lingenfelser et al. 2011]. The fact that large parts of naturalistic databases carry contradictory multimodal cues [Douglas-Cowie et al. 2005] explains that the gain in performance is generally lower for realistic scenarios [D&#8217;Mello and Kory 2012]. Instead of forcing decisions over fixed time segments, fusion approaches should be favored that loosen the temporal constraints between modalities and allow <a id="page_252"/>them to individually decide when to contribute to the final decision-making process. Several algorithms have been proposed to tackle this problem. Most of them are either variants of HMMs [Song et al. 2004, Zeng et al. 2008, Nicolaou et al. 2010] or are based on Recurrent Neural Networks (RNNs) featuring some kind of memory cells [W&#246;llmer et al. 2010, Nicolaou et al. 2011]. Also, see Chapters 3, 4, and 6 for a description of fusion mechanisms that take the temporal dynamics between and within modalities into account. Event-based fusion algorithms such as the vector fusion system proposed by Lingenfelser et al. [2014] offer an alternate approach.</p>
<p class="hangt"><b>Proposing A More Application-Related Methodology To Prepare The System For Use In Realistic Environments</b>. As discussed in <a href="#ch8_4">Section 8.4</a>, it may require a good deal of work to turn an offline approach into an online application, which may still not perform satisfactory. The reason is that during the training phase the underlying data base is usually manually tweaked to suit the classification process, for instance by removing parts with sparse interaction and rare behavior. An online system, however, has to deal with input as it comes. For instance, it must autonomously decide how to deal with non-prototypical behavior [Schuller et al. 2011]. Hence, ideally, all recorded data should be considered during training and processed in a continuous frame-by-frame manner. The decision to skip one or more frames has to be made by the system based only on information that will be available at run-time, too. Also, in offline studies the segmentation of the training data is often manually aligned to the observed behavior. This, however, can lead to suboptimal results if the learning process does not fit the application. Hence, in order to translate well into the final system, the same segmentation algorithm should be applied during training as well.</p>
<p class="indentt">To see more applications for use in our everyday life we need tools that support the development of socially aware software. To this end, we have introduced our SSI framework, which tries to tackle the mentioned problems. A simple-to-use XML interface allows developers to quickly put together complex processing pipelines from a pool of re-usable components. In particular, SSI supports a large variety of sensor devices such as eye trackers, motion capture suites or physiological sensors, and allows synchronized recordings of these sensors possibly distributed over several machines. Signals can be filtered on the fly and directly used to feed online classifiers to recognize high-level social behavior. In that sense, SSI covers <a id="page_253"/>the whole machine learning pipeline and provides the necessary tools to implement complex recognition tasks in real-time.</p>
<p class="indent">However, in the end the success of online systems depends on whether the idea is accepted by the user or not. A system that shows appropriate behavior for most of the time but then fails at some crucial point in the interaction may be regarded as disturbing rather than helpful. In fact, there is the danger that strategies actually meant to ease the interaction for the user may cause the exact opposite. In that sense, assessing the &#8220;real&#8221; quality of an online system is way more complicated than calculating the percentage of correct predictions for a set of preselected samples. In fact, even a correct prediction may be useless if it comes with delay and the right moment for a reaction has passed. On the other hand, a technically incorrect prediction might be tolerable if it triggers a behavior that is still reasonable. Or a false prediction may remain without consequences if the system is able to recognize and correct its error. In the long run, an online system should be equipped with strategies to incorporate the user&#8217;s feedback and should thus continuously asses the quality of the interaction. Otherwise, it may not achieve its objective. Although there is no definitive solution yet, it is obvious that answer is only found by putting a system &#8220;in the wild&#8221;. That is, building online systems and run them outside the lab in realistic environments for days or weeks. What we can learn from such an experiment may be worth a thousand offline studies.</p>
<p class="h1n"><a id="ch8_8"/><b>Focus Questions</b></p>
<p class="numlist">1.&#160;&#160;List six affective or social cues expressed through at least three different modalities.</p>
<p class="numlist">2.&#160;&#160;Think of two situations where a social cue in one modality changes the meaning of a social behavior expressed in another channel.</p>
<p class="numlist">3.&#160;&#160;What constitutes a proper database design? Name at least five properties.</p>
<p class="numlist">4.&#160;&#160;What ways to elicit user behavior must you know here?</p>
<p class="numlist">5.&#160;&#160;Name the three different levels of fusion we have mentioned throughout the chapter.</p>
<p class="numlist">6.&#160;&#160;What are the problems of traditional fusion strategies?</p>
<p class="numlist">7.&#160;&#160;What do we understand by missing data?</p>
<p class="numlist">8.&#160;&#160;Name common problems an online recognition system has to deal with compared to an offline system?</p>
<p class="h1"><a id="page_254"/><a id="ch8_9"/><b>References</b></p>
<p class="ref">J. Allwood, L. Cerrato, K. Jokinen, C. Navarretta, and P. Paggio. 2007. The MUMIN coding scheme for the annotation of feedback, turn management and sequencing phenomena. <i>Language Resources and Evaluation</i>, 41(3-4): 273&#8211;287. DOI: 10.1007/s10579-007-9061-5. 232</p>
<p class="ref">O. Aran and D. Gatica-Perez. 2010. Fusing audio-visual nonverbal cues to detect dominant people in conversations. In <i>International Conference on Pattern Recognition (ICPR)</i>. DOI: 10.1109/ICPR.2010.898. 234</p>
<p class="ref">T. Baltrusaitis, C. Ahuja, and L.-P. Morency. 2018. Multimodal machine learning: Challenges and applications. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, eds., <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Chapter 1. Morgan &#38; Claypool Publishers, San Rafael, CA.</p>
<p class="ref">A. Batliner, K. Fischer, R. Huber, J. Spilker, and N&#246;th. 2000. Desperately seeking emotions: Actors, wizards, and human beings. In <i>Workshop on Speech and Emotion: A Conceptual Framework for Research at International Symposium on Computer Architecture (ISCA)</i>. 234, 251</p>
<p class="ref">A. Battocchi, F. Pianesi, and D. Goren-Bar. 2005. DaFEx: Database of facial expressions. In M. T. Maybury, O. Stock, and W. Wahlster, eds., <i>INTETAIN</i>, vol. 3814 of <i>Lecture Notes in Computer Science</i>, pp. 303&#8211;306. Springer, Berlin Heidelberg. DOI: 10.1007/11590323_39. 228</p>
<p class="ref">P. Blache, R. Bertrand, and G. Ferr&#233;. 2009. Creating and exploiting multimodal annotated corpora: The ToMA project. In M. Kipp, J.-C. Martin, P. Paggio, and D. Heylen, eds., <i>Multimodal Corpora</i>, vol. 5509 of <i>Lecture Notes in Computer Science</i>, pp. 38&#8211;53. Springer, Berlin Heidelberg. DOI: 10.1007/978-3-642-04793-0_3. 232</p>
<p class="ref">F. Burkhardt, A. Paeschke, M. Rolfes, W. Sendlmeier, and B. Weiss. 2005. A database of german emotional speech. In <i>Conference of the International Speech Communication Association (INTERSPEECH)</i>, pp. 1517&#8211;1520. DOI: 10.1.1.130.8506&#38;rep=rep1 &#38;type=pdf. 228</p>
<p class="ref">C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. Narayanan. 2008. IEMOCAP: interactive emotional dyadic motion capture database. <i>Language Resources and Evaluation</i>, 42(4): 335&#8211;359. DOI: 10.1007/s10579-008-9076-6. 231</p>
<p class="ref">J. Carletta, S. Isard, G. Doherty-Sneddon, A. Isard, J. C. Kowtko, and A. H. Anderson. 1997. The reliability of a dialogue structure coding scheme. <i>Computational Linguistics</i>, 23(1): 13&#8211;31. 232</p>
<p class="ref">J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner. 2006. The AMI meeting corpus: A pre-announcement. In <i>Machine Learning for Multimodal Interaction (MLMI)</i>, pp. 28&#8211;39. Springer, Berlin Heidelberg. DOI: 10.1007/11677482_3. 231</p>
<p class="ref"><a id="page_255"/>F. Cavicchio and M. Poesio. 2009. Multimodal corpora annotation: Validation methods to assess coding scheme reliability. In M. Kipp, J.-C. Martin, P. Paggio, and D. Heylen, eds., <i>Multimodal Corpora</i>, vol. 5509 of <i>Lecture Notes in Computer Science</i>, pp. 109&#8211;121. Springer, Berlin Heidelberg. DOI: 10.1007/978-3-642-04793-0_7. 232</p>
<p class="ref">L. Chen and T. Huang. 2000. Emotional expressions in audiovisual human computer interaction. In <i>International Conference on Multimedia and Expo (ICME)</i>, vol. 1, pp. 423&#8211;426. DOI: 10.1109/ICME.2000.869630. 236</p>
<p class="ref">L. Chen, T. Huang, T. Miyasato, and R. Nakatsu. 1998. Multimodal human emotion/expression recognition. In <i>International Conference on Automatic Face and Gesture Recognition (FGR)</i>, pp. 366&#8211;371. DOI: 10.1109/AFGR.1998.670976. 233</p>
<p class="ref">C. Chiarcos, S. Dipper, M. G&#246;tze, U. Leser, A. L&#252;deling, J. Ritz, and M. Stede. 2008. A flexible framework for integrating annotations from different tools and tag sets. <i>Translation and Literature</i>, 49(2): 217&#8211;246. 232</p>
<p class="ref">M. G. Core and J. F. Allen. 1997. Coding dialogs with the DAMSL annotation scheme. In <i>Working Notes of the AAAI Fall Symposium on Communicative Action in Humans and Machines</i>, pp. 28&#8211;35. Cambridge, MA. DOI: 10.1.1.50.7024. 232</p>
<p class="ref">R. Cowie and R. R. Cornelius. 2003. Describing the emotional states that are expressed in speech. <i>Speech Communication</i>, 40(1-2): 5&#8211;32. DOI: 10.1016/S0167-6393(02)00071-7. 230</p>
<p class="ref">R. Cowie, E. Douglas-Cowie, and C. Cox. 2005. Beyond emotion archetypes: Databases for emotion modelling using neural networks. <i>Neural Networks</i>, 18(4): 371&#8211;88. DOI: 10.1016/j.neunet.2005.03.002. 230, 231</p>
<p class="ref">E. Cox. 1993. Adaptive fuzzy systems. <i>Spectrum</i>, 30(2): 27&#8211;31. 238</p>
<p class="ref">L. De Silva and P. C. Ng. 2000. Bimodal emotion recognition. In <i>International Conference on Automatic Face and Gesture Recognition (FGR)</i>, pp. 332&#8211;335. DOI: 10.1109/AFGR.2000.840655. 233</p>
<p class="ref">S. D&#8217;Mello and J. Kory. 2012. Consistent but modest: A meta-analysis on unimodal and multimodal affect detection accuracies from 30 studies. In <i>International Conference on Multimodal Interaction (ICMI &#8217;12)</i>, pp. 31&#8211;38. ACM, New York. DOI: 10.1145/2388676.2388686. 234, 251</p>
<p class="ref">S. D&#8217;Mello, N. Bosch, and H. Chen. 2018. Multimodal-multisensor affect detection. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, eds., <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Chapter 6. Morgan &#38; Claypool Publishers, San Rafael, CA.</p>
<p class="ref">E. Douglas-Cowie, R. Cowie, and M. Schr&#246;der. 2000. A new emotion database: Considerations, sources and scope. In <i>ISCA Workshop on Speech and Emotion: A Conceptual Framework for Research</i>, pp. 39&#8211;44. Textflow, Belfast. 231</p>
<p class="ref">E. Douglas-Cowie, N. Campbell, R. Cowie, and P. Roach. 2003. Emotional speech: Towards a new generation of databases. <i>Speech Communication</i>, 40(c): 33&#8211;60. DOI: 10.1016/S0167-6393(02)00070-5. 230, 251</p>
<p class="ref"><a id="page_256"/>E. Douglas-Cowie, L. Devillers, J.-C. Martin, R. Cowie, S. Savvidou, S. Abrilian, and C. Cox. 2005. Multimodal databases of everyday emotion: facing up to complexity. In <i>Conference of the International Speech Communication Association (INTERSPEECH)</i>, pp. 813&#8211;816. ISCA. 229, 230, 231, 232, 234, 251, 527</p>
<p class="ref">E. Douglas-Cowie, R. Cowie, I. Sneddon, C. Cox, O. Lowry, M. McRorie, J.-C. Martin, L. Devillers, S. Abrilian, A. Batliner, N. Amir, and K. Karpouzis. 2007. The HUMAINE database: Addressing the collection and annotation of naturalistic and induced emotional data. In A. Paiva, R. Prada, and R. W. Picard, eds., <i>International Conference on Affective Computing and Intelligent Interaction (ACII)</i>, vol. 4738 of <i>Lecture Notes in Computer Science</i>, pp. 488&#8211;500. Springer. DOI: 10.1007/978-3-540-74889-2_43. 230</p>
<p class="ref">E. Douglas-Cowie, R. Cowie, C. Cox, N. Amir, and D. Heylen. 2008. The sensitive artificial listener: an induction technique for generating emotionally coloured conversation. In L. Devillers, J.-C. Martin, R. Cowie, E. Douglas-Cowie, and A. Batliner, eds., <i>LREC Workshop on Corpora for Research on Emotion and Affect</i>, pp. 1&#8211;4. ELRA, Paris, France. 231</p>
<p class="ref">A. Eerekoviae. 2014. An insight into multimodal databases for social signal processing: acquisition, efforts, and directions. <i>Artificial Intelligence Review</i>, 42(4): 663&#8211;692. DOI: 10.1007/s10462-012-9334-2. 229, 232, 527</p>
<p class="ref">P. Ekman and W. Friesen. 1978. <i>Facial Action Coding System: A Technique for the Measurement of Facial Movement</i>. Consulting Psychologists Press, Palo Alto, CA. 232</p>
<p class="ref">I. S. Engberg, A. V. Hansen, O. Andersen, and P. Dalsgaard. 1997. Design, recording and verification of a danish emotional speech database. In G. Kokkinakis, N. Fakotakis, and E. Dermatas, eds., <i>European Conference on Speech Communication and Technology (EUROSPEECH)</i>. ISCA. 228</p>
<p class="ref">F. Eyben, M. W&#246;llmer, M. F. Valstar, H. Gunes, B. Schuller, and M. Pantic. 2011. String-based audiovisual fusion of behavioural events for the assessment of dimensional affect. In <i>International Conference on Automatic Face and Gesture Recognition (FGR)</i>, pp. 322&#8211;329. IEEE Computer Society. DOI: 10.1109/FG.2011.5771417. 237</p>
<p class="ref">M. Grimm, K. Kroschel, and S. Narayanan. 2008. The vera am mittag german audio-visual emotional speech database. In <i>International Conference on Multimedia and Expo (ICME)</i>, pp. 865&#8211;868. DOI: 10.1109/ICME.2008.4607572. 231</p>
<p class="ref">A. Hanjalic. 2006. Extracting moods from pictures and sounds: towards truly personalized TV. <i>IEEE Signal Processing Magazine</i>, 23(2): 90&#8211;100. DOI: 10.1109/MSP.2006.1621452. 239</p>
<p class="ref">J. Henrich, S. J. Heine, and A. Norenzayan. 2010. The weirdest people in the world? <i>Behavioral and Brain Sciences</i>, 33(2-3): 61&#8211;83. DOI: 10.1017/S0140525X0999152X. 230</p>
<p class="ref">J. Hofmann, F. Stoffel, A. Weber, and T. Platt, 2012. The 16 enjoyable emotions induction task (16s-EEIT). Unpublished. 245</p>
<p class="ref">H. Hung and G. Chittaranjan. 2010. The idiap wolf corpus: exploring group behaviour in a competitive role-playing game. In A. D. Bimbo, S.-F. Chang, and A. W. M. <a id="page_257"/>Smeulders, eds., <i>International Conference on Multimedia (MM)</i>, pp. 879&#8211;882. ACM. DOI: 10.1145/1873951.1874102. 231, 251</p>
<p class="ref">D. B. Jayagopi, H. Hung, C. Yeo, and D. Gatica-Perez. 2009. Modeling dominance in group conversations using nonverbal activity cues. <i>Audio, Speech and Language Processing</i>, 17(3): 501&#8211;513. DOI: 10.1109/TASL.2008.2008238. 234</p>
<p class="ref">T. Johnstone. 1996. Emotional speech elicited using computer games. In <i>International Conference on Spoken Language Processing (ICSLP)</i>. ISCA. DOI: 10.1109/ICSLP.1996.608026. 230</p>
<p class="ref">R. E. Kalman. 1960. A new approach to linear filtering and prediction problems. <i>Basic Engineering</i>, 82(Series D): 35&#8211;45. 238</p>
<p class="ref">I. Kanluan, M. Grimm, and K. Kroschel. 2008. Audio-visual emotion recognition using an emotion space concept. In <i>European Signal Processing Conference (EUSIPCO)</i>. 233</p>
<p class="ref">J. F. Kelley. 1984. An iterative design methodology for user-friendly natural language office information applications. <i>Information Systems</i>, 2(1): 26&#8211;41. DOI: 10.1145/357417.357420. 230</p>
<p class="ref">P. M. Kenealy. 1986. The velten mood induction procedure: A methodological review. <i>Motivation and Emotion</i>, 10(4): 315&#8211;335. DOI: 10.1007/BF00992107. 230</p>
<p class="ref">G. Keren, A. E.-D. Mousa, O. Pietquin, S. Zafeiriou, and B. Schuller. 2018. Deep learning for multisensorial and multimodal interaction. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, eds., <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Chapter 4. Morgan &#38; Claypool Publishers, San Rafael, CA.</p>
<p class="ref">M. Kipp. 2013. ANVIL: The video annotation research tool. In J. Durand, U. Gut, and G. Kristofferson, eds., <i>Handbook of Corpus Phonology</i>. University Press, Oxford, UK. 233</p>
<p class="ref">A. Kleinsmith and N. Bianchi-Berthouze. 2007. Recognizing affective dimensions from body posture. In A. Paiva, R. Prada, and R. Picard, eds., <i>Affective Computing and Intelligent Interaction</i>, vol. 4738 of <i>Lecture Notes in Computer Science</i>, pp. 48&#8211;58. Springer, Berlin Heidelberg. DOI: 10.1007/978-3-540-74889-2_5.pdf. 239</p>
<p class="ref">P. O. Kristensson and L. C. Denby. 2011. Continuous recognition and visualization of pen strokes and touch-screen gestures. In <i>Eurographics Symposium on Sketch-Based Interfaces and Modeling (SBIM)</i>, pp. 95&#8211;102. ACM, New York. DOI: 10.1145/2021164.2021181. 240</p>
<p class="ref">P. J. Lang, M. M. Bradley, and B. N. Cuthbert. 2008. International affective picture system (iaps): Affective ratings of pictures and instruction manual. Technical Report A-8, The Center for Research in Psychophysiology, University of Florida, Gainesville, FL. 230</p>
<p class="ref">J. Lichtenauer, J. Shen, M. F. Valstar, and M. Pantic. 2011. Cost-effective solution to synchronised audio-visual data capture using multiple sensors. <i>Image and Vision Computing</i>, 29: 666&#8211;680. DOI: 10.1016/j.imavis.2011.07.004. 232</p>
<p class="ref"><a id="page_258"/>F. Lingenfelser, J. Wagner, and E. Andr&#233;. 2011. A systematic discussion of fusion techniques for multi-modal affect recognition tasks. In <i>International Conference on Multimodal Interfaces (ICMI)</i>, pp. 19&#8211;26. ACM, NewYork, NY, USA. DOI: 10.1145/2070481.2070487. 234, 251</p>
<p class="ref">F. Lingenfelser, J. Wagner, E. Andr&#233;, G. McKeown, and W. Curran. 2014. An event driven fusion approach for enjoyment recognition in real-time. In <i>International Conference on Multimedia (MM)</i>, pp. 377&#8211;386. ACM, New York. DOI: 10.1145/2647868.2654924.237, 247, 250, 252</p>
<p class="ref">G. McKeown, M. Valstar, R. Cowie, and M. Pantic. 2010. The SEMAINE corpus of emotionally coloured character interactions. In <i>International Conference on Multimedia and Expo (ICME)</i>, pp. 1079&#8211;1084. DOI: 10.1109/ICME.2010.5583006. 231</p>
<p class="ref">G. McKeown, W. Curran, J. Wagner, F. Lingenfelser, and E. Andr&#233;. 2015. The belfast storytelling database&#8212;a spontaneous social interaction database with laughter focused annotation. In <i>International Conference on Affective Computing and Intelligent Interaction and Workshops (ACII)</i>. Xi&#8217;an, China. DOI: 10.1109/ACII.2015.7344567. 231, 245</p>
<p class="ref">D. S. Messinger, T. D. Cassel, S. I. Acosta, Z. Ambadar, and J. F. Cohn. 2008. Infant smiling dynamics and perceived positive emotion. <i>Nonverbal Behavior</i>, 32(3): 133&#8211;155. 233</p>
<p class="ref">D. S. Messinger, M. H. Mahoor, S.-M. Chow, and J. F. Cohn. 2009. Automated measurement of facial expression in infant-mother interaction: A pilot study. <i>Infancy</i>, 14(3): 285&#8211;305. 233</p>
<p class="ref">A. Metallinou, A. Katsamanis, and S. S. Narayanan. 2013. Tracking continuous emotional trends of participants during affective dyadic interactions using body language and speech information. <i>Image and Vision Computing</i>, 31(2): 137&#8211;152. DOI: 10.1016/j.imavis.2012.08.018. 239</p>
<p class="ref">M. A. Nicolaou, H. Gunes, and M. Pantic. 2010. Audio-visual classification and fusion of spontaneous affective data in likelihood space. In <i>International Conference on Pattern Recognition (ICPR)</i>, pp. 3695&#8211;3699. IEEE. DOI: 10.1109/ICPR.2010.900. 236, 252</p>
<p class="ref">M. A. Nicolaou, H. Gunes, and M. Pantic. 2011. Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space. <i>Affective Computing</i>, 2(2): 92&#8211;105. DOI: 10.1109/T-AFFC.2011.9. 236, 239, 252</p>
<p class="ref">R. Niewiadomski and C. Pelachaud. 2012. Towards multimodal expression of laughter. In Y. Nakano, M. Neff, A. Paiva, and M. A. Walker, eds., <i>International Conference on Intelligent Virtual Agents (IVA)</i>, vol. 7502 of <i>Lecture Notes in Computer Science</i>, pp. 231&#8211;244. Springer. DOI: 10.1007/978-3-642-33197-8_24. 246</p>
<p class="ref">Y. Panagakis, O. Rudovic, and M. Pantic. 2018. Learning for multi-modal and context-sensitive interfaces. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, eds., <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Chapter 3. Morgan &#38; Claypool Publisher, San Rafael, CA.</p>
<p class="ref"><a id="page_259"/>M. Pantic, A. Pentland, A. Nijholt, and T. S. Huang. 2007. Human computing and machine understanding of human behavior: A survey. In <i>Artifical Intelligence for Human Computing, ICMI 2006 and IJCAI 2007 International Workshops</i>, pp. 47&#8211;71. Banff, Canada, November 3, 2006, Hyderabad, India, January 6, 2007, Revised Seleced and Invited Papers. 229, 518</p>
<p class="ref">A. Pentland. 2007. Social signal processing. <i>IEEE Signal Processing Magazine</i>, 24(4): 108&#8211;111. 231</p>
<p class="ref">S. Petridis and M. Pantic. 2008. Audiovisual discrimination between laughter and speech. In <i>Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on</i>, pp. 5117&#8211;5120. DOI: 10.1109/ICASSP.2008.4518810. 234</p>
<p class="ref">W. Ruch and P. Ekman. 2001. The expressive pattern of laughter. In A. W. Kaszniak, ed., <i>Emotion Qualia, and Consciousness</i>, pp. 426&#8211;443. Word Scientific Publisher. 245, 246</p>
<p class="ref">J. Sasiadek and P. Hartana. 2000. Sensor data fusion using Kalman filter. In <i>International Conference on Information Fusion (FUSION)</i>, vol. 2, pp. WED5/19&#8211;WED5/25. DOI: 10.1109/IFIC.2000.859866. 238</p>
<p class="ref">K. R. Scherer and M. R. Zentner. 2001. Emotional effects of music: Production rules. In P. N. Juslin and J. A. Sloboda, eds., <i>Music and Emotion: Theory and Research</i>, pp. 361&#8211;392. University Press, Oxford. 230</p>
<p class="ref">F. Schiel, S. Steininger, and U. T&#252;rk. 2002. The smartkom multimodal corpus at BAS. In <i>International Conference on Language Resources and Evaluation (LREC)</i>. European Language Resources Association. DOI: 10.1.1.7.9049. 231</p>
<p class="ref">T. Schmidt. 2004. Transcribing and annotating spoken language with EXMARaLDA. In <i>International Conference on Language Resources and Evaluation (LREC) Workshop on XML based Richly Annotated Corpora</i>, pp. 879&#8211;896. ELRA, Paris. 233</p>
<p class="ref">M. Schr&#246;der, R. Cowie, E. Douglas-Cowie, S. Savvidou, E. McMahon, and M. Sawey. 2000. FEELTRACE: An instrument for recording perceived emotion in real time. In <i>ISCA Workshop on Speech and Emotion: A Conceptual Framework for Research</i>, pp. 19&#8211;24. Textflow, Belfast. 233</p>
<p class="ref">M. Schr&#246;der, P. Baggia, F. Burkhardt, C. Pelachaud, C. Peter, and E. Zovato. 2011. EmotionML&#8212;an upcoming standard for representing emotions and related states. In S. K. D&#8217;Mello, A. C. Graesser, B. Schuller, and J.-C. Martin, eds., <i>International Conference on Affective Computing and Intelligent Interaction (ACII)</i>, vol. 6974 of <i>Lecture Notes in Computer Science</i>, pp. 316&#8211;325. Springer. 233</p>
<p class="ref">B. Schuller. 2018. Multimodal user state and trait recognition: An overview. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, eds., <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Chapter 5. Morgan &#38; Claypool Publishers, San Rafael, CA.</p>
<p class="ref">B. Schuller, A. Batliner, S. Steidl, and D. Seppi. 2011. Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge. <i>Speech Communication</i>, 53(9-10): 1062&#8211;1087. 239, 252</p>
<p class="ref"><a id="page_260"/>H. Sloetjes, A. Russel, and A. Klassmann. 2007. ELAN: a free and open-source multimedia annotation tool. In <i>Conference of the International Speech Communication Association (INTERSPEECH)</i>, pp. 4015&#8211;4016. ISCA. 233</p>
<p class="ref">M. Song, J. Bu, C. Chen, and N. Li. 2004. Audio-visual based emotion recognition&#8212;a new approach. In <i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pp. 1020&#8211;1025. DOI: 10.1109/CVPR.2004.1315276. 236, 252</p>
<p class="ref">X. Sun, J. Lichtenauer, M. F. Valstar, A. Nijholt, and M. Pantic. 2011. A multimodal database for mimicry analysis. In <i>International Conference on Affective Computing and Intelligent Interaction (ACII)</i>. Memphis, TN. 232</p>
<p class="ref">A. Vinciarelli and A. Esposito. 2018. Multimodal analysis of social signals. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, eds., <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Chapter 7. Morgan &#38; Claypool Publishers, San Rafael, CA.</p>
<p class="ref">A. Vinciarelli, M. Pantic, H. Bourlard, and A. Pentland. 2008a. Social signals, their function, and automatic analysis: A survey. In <i>International Conference on Multimodal Interfaces (ICMI)</i>, pp. 61&#8211;68. ACM, New York. DOI: 10.1145/1452392.1452405. 229, 517</p>
<p class="ref">A. Vinciarelli, M. Pantic, H. Bourlard, and A. Pentland. 2008b. Social signal processing: State of the art and future perspectives of an emerging domain. In <i>International Conference on Multimedia (MM)</i>, pp. 1061&#8211;1070. Vancouver, Canada. 229, 518</p>
<p class="ref">A. Vinciarelli, A. Dielmann, S. Favre, and H. Salamin. 2009a. Canal9: A database of political debates for analysis of social interactions. In <i>International Conference on Affective Computing and Intelligent Interaction (ACII)</i>, pp. 1&#8211;4.</p>
<p class="ref">A. Vinciarelli, M. Pantic, and H. Bourlard. 2009b. Social signal processing: Survey of an emerging domain. <i>Image Vision Computing</i>, 27(12): 1743&#8211;1759. DOI: 10.1016/j.imavis.2008.11.007. 231, 251</p>
<p class="ref">A. Vinciarelli, M. Pantic, D. Heylen, C. Pelachaud, I. Poggi, F. D&#8217;Ericco, and M. Schr&#246;der. 2012. Bridging the gap between social animal and unsocial machine: A survey of social signal processing. <i>Affective Computing</i>, 3(1): 69&#8211;87. Issue 1. DOI: 10.1109/TAFFC.2011.27. 229, 527</p>
<p class="ref">J. Wagner, F. Lingenfelser, E. Andr&#233;, J. Kim, and T. Vogt. 2011. Exploring fusion methods for multimodal emotion recognition with missing data. <i>Affective Computing</i>, 2(4): 206&#8211;218. DOI: 10.1109/T-AFFC.2011.12. 238</p>
<p class="ref">J. Wagner, F. Lingenfelser, T. Baur, I. Damian, F. Kistler, and E. Andr&#233;. 2013. The social signal interpretation (SSI) framework: multimodal signal processing and recognition in real-time. In <i>International Conference on Multimedia (MM)</i>, pp. 831&#8211;834. ACM, New York. DOI: 10.1145/2502081.2502223. 242</p>
<p class="ref">T. Wilson. 2008. Annotating subjective content in meetings. In <i>International Conference on Language Resources and Evaluation (LREC)</i>. European Language Resources Association. 232</p>
<p class="ref"><a id="page_261"/>P. Wittenburg, H. Brugman, A. Russel, A. Klassmann, and H. Sloetjes. 2006. ELAN: a professional framework for multimodality research. In <i>International Conference on Language Resources and Evaluation (LREC)</i>. 233</p>
<p class="ref">M. W&#246;llmer, F. Eyben, S. Reiter, B. Schuller, C. Cox, E. Douglas-Cowie, and R. Cowie. 2008. Abandoning emotion classes&#8212;towards continuous emotion recognition with modelling of long-range dependencies. In <i>Conference of the International Speech Communication Association (INTERSPEECH)</i>, pp. 597&#8211;600. ISCA. DOI: 10.1016/j.chb.2010.10.027. 239</p>
<p class="ref">M. W&#246;llmer, M. Al-Hames, F. Eyben, B. Schuller, and G. Rigoll. 2009. A multidimensional dynamic time warping algorithm for efficient multimodal fusion of asynchronous data streams. <i>Neurocomputing</i>, 73(1-3): 366&#8211;380. DOI: 10.1016/j.neucom.2009.08.005. 236</p>
<p class="ref">M. W&#246;llmer, B. Schuller, F. Eyben, and G. Rigoll. 2010. Combining long short-term memory and dynamic Bayesian networks for incremental emotion-sensitive artificial listening. <i>Selected Topics Signal Processing</i>, 4(5): 867&#8211;881. DOI: 10.1109/JSTSP.2010.2057200. 236, 239, 252</p>
<p class="ref">D. Wu, T. D. Parsons, E. Mower, and S. Narayanan. 2010. Speech emotion estimation in 3D space. In <i>Proceedings of IEEE</i>. Singapore. DOI: 10.1109/ICME.2010.5583101. 239</p>
<p class="ref">Z. Zeng, J. Tu, B. M. Pianfetti, and T. S. Huang. 2008. Audio-visual affective expression recognition through multistream fused HMM. <i>Multimedia</i>, 10(4): 570&#8211;577. DOI: 10.1109/TMM.2008.921737. 236, 238, 252</p>
<p class="ref">Z. Zeng, M. Pantic, G. Roisman, and T. Huang. 2009. A survey of affect recognition methods: Audio, visual, and spontaneous expressions. <i>Pattern Analysis and Machine Intelligence</i>, 31(1): 39&#8211;58. DOI: 10.1109/TPAMI.2008.52. 229, 234</p>
<p class="line"/>
<p class="note"><a id="fn1" href="#rfn1">1</a>.&#160;&#160;With offline study we refer to experiments that are conducted and evaluated on pre-recorded files only. In particular, they are not transferred into a system which allows it to interact in realtime.</p>
<p class="note"><a id="fn2" href="#rfn2">2</a>.&#160;&#160;<a href="http://emotion-research.net/projects/humaine/earl">http://emotion-research.net/projects/humaine/earl</a></p>
<p class="note"><a id="fn3" href="#rfn3">3</a>.&#160;&#160;<a href="http://www.w3.org/TR/emotionml/">http://www.w3.org/TR/emotionml/</a></p>
<p class="note"><a id="fn4" href="#rfn4">4</a>.&#160;&#160;Incorporating Laughter into Human Avatar Interactions: Research and Experiments. <a href="http://www.ilhaire.eu/">http://www.ilhaire.eu/</a></p>
<p class="note"><a id="fn5" href="#rfn5">5</a>.&#160;&#160;The classification models have been trained with SSI, too. However, due to the lack of space this is not covered here. You may refer to a more detailed tutorial published elsewhere: <a href="http://records.mlab.no/2014/10/29/ssi-an-open-source-platform-for-social-signal-interpretation/">http://records.mlab.no/2014/10/29/ssi-an-open-source-platform-for-social-signal-interpretation/</a></p>
</body>
</html>