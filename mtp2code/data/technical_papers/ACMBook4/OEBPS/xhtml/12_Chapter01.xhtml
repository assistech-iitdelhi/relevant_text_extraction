<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_19"/>1</p>
<p class="chtitle"><b>Challenges and Applications in Multimodal Machine Learning</b></p>
<p class="chauthor"><b>Tadas Baltru&#353;aitis, Chaitanya Ahuja, Louis-Philippe Morency</b></p>
<p class="h1"><a id="ch1_1"/><b><span class="bg1">1.1</span>&#160;&#160;&#160;&#160;Introduction</b></p>
<p class="noindent">The world surrounding us involves multiple modalities. We see objects, hear sounds, feel texture, smell odors, and so on. In general terms, a <i>modality</i> refers to the way in which something happens or is experienced. Most people associate the word modality with the <i>sensory modalities</i> which represent our primary channels of communication and sensation, such as vision or touch. In this chapter we focus primarily, but not exclusively, on three such modalities: <i>linguistic</i> modality which can be both written or spoken; <i>visual</i> modality which is often represented with images or videos; and <i>vocal</i> modality which encodes sounds and para-verbal information such as prosody and vocal expressions.</p>
<p class="indent">In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret and reason about multimodal messages. <i>Multimodal machine learning</i> aims to build models that can process and relate information from multiple modalities. From early research on audio-visual speech recognition to the recent explosion of interest in language and vision models, multimodal machine learning is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential.</p>
<div class="box">
<p class="bhead"><a id="page_20"/><b>Glossary</b></p>
<p class="hangbx"><b>Representation</b> learns how to represent and summarize multimodal data in a way that exploits the complementarity and redundancy of multiple modalities. The heterogeneity of multimodal data makes it challenging to construct such representations. For example, language is often symbolic while audio and visual modalities will be represented as signals.</p>
<p class="hangbx"><b>Translation</b> addresses how to translate (map) data from one modality to another. Not only is the data heterogeneous, but the relationship between modalities is often open-ended or subjective. For example, there exist a number of <i>correct</i> ways to describe an image and one perfect translation may not exist.</p>
<p class="hangbx"><b>Alignment</b> identifies the direct relations between (sub)elements from two or more different modalities. For example, we may want to align the steps in a recipe to a video showing the dish being made. To tackle this challenge we need to measure similarity between different modalities and deal with possible long-range dependencies and ambiguities.</p>
<p class="hangbx"><b>Fusion</b> joins information from two or more modalities to perform a prediction. For example, for audio-visual speech recognition, the visual description of the lip motion is fused with the speech signal to predict spoken words. The information coming from different modalities may have varying predictive power and noise topology, with possibly missing data in at least one of the modalities.</p>
<p class="hangbx"><b>Co-learning</b> transfers knowledge between modalities, their representation, and their predictive models. This is exemplified by algorithms of co-training, conceptual grounding, and zero shot learning. Co-learning explores how knowledge learning from one modality can help a computational model trained on a different modality. This challenge is particularly relevant when one of the modalities has limited resources (e.g., few annotated data).</p>
</div>
<p class="indent">The research field of multimodal machine learning brings some unique challenges for computational researchers given the heterogeneity of the data. Learning from multimodal sources offers the possibility of capturing correspondences between modalities and gaining an in-depth understanding of natural phenomena. In a recent survey paper, Baltru&#353;aitis et al. [2017] identify five core technical challenges (and related sub-challenges) surrounding multimodal machine learning. They are (a) <i><b>representation,</b></i> (b) <i><b>translation,</b></i> (c) <i><b>alignment,</b></i> (d) <i><b>fusion,</b></i> and (e) <i><b>co-learning</b></i> (for definitions see the Glossary). They are central to the multimodal setting and need to be tackled in order to progress the field.</p>
<p class="indent"><a id="page_21"/>We start with a discussion of main applications of multimodal machine learning (<a href="#ch1_2">Section 1.2</a>). In this chapter we focus on two out of the five core technical challenges facing multimodal machine learning: representation (<a href="#ch1_3">Section 1.3</a>) and co-learning (<a href="#ch1_4">Section 1.4</a>). The fusion challenge is addressed in <a href="13_Chapter02.xhtml">Chapters 2</a>, 4 and Jameson and Kristensson [2017], and part of translation challenge is discussed in <a href="15_Chapter04.xhtml">Chapter 4</a>. More details about all five challenges are also available in the survey paper [Baltru&#353;aitis et al. 2017].</p>
<p class="h1"><a id="ch1_2"/><b><span class="bg1">1.2</span>&#160;&#160;&#160;&#160;Multimodal Applications</b></p>
<p class="noindent">Multimodal machine learning enables a wide range of applications: from audiovisual speech recognition to image captioning. In this section we present a brief history of multimodal applications, from its beginnings in audio-visual speech recognition to a recently renewed interest in language and vision applications.</p>
<p class="indent">One of the earliest examples of multimodal research is audio-visual speech recognition (AVSR) [Yuhas et al. 1989]. It was motivated by the McGurk effect [McGurk and MacDonald 1976], an interaction between hearing and vision during speech perception. When human subjects heard the syllable /ba-ba/ while watching the lips of a person saying /ga-ga/, they perceived a third sound: /da-da/. These results motivated many researchers from the speech community to extend their approaches with visual information. Given the prominence of hidden Markov models (HMMs) in the speech community at the time [Juang and Rabiner 1991], it is without surprise that many of the early models for AVSR were based on various HMM extensions [Bourlard and Dupont 1996, Brand et al. 1997]. While research into AVSR is not as common these days, it has seen renewed interest from the deep learning community [Ngiam et al. 2011].</p>
<p class="indent">While the original vision of AVSR was to improve speech recognition performance (e.g., word error rate) in all contexts, the experimental results showed that the main advantage of visual information was when the speech signal was noisy (i.e., low signal-to-noise ratio) [Yuhas et al. 1989, Gurban et al. 2008, Ngiam et al. 2011]. In other words, the captured interactions between modalities were supplementary rather than complementary. The same information was captured in both, improving the robustness of the multimodal models but not improving the speech recognition performance in noiseless scenarios.</p>
<p class="indent">A second important category of multimodal applications comes from the field of multimedia content indexing and retrieval [Snoek and Worring 2005, Atrey et al. 2010]. With the advance of personal computers and the internet, the quantity <a id="page_22"/>of digitized multimedia content has increased dramatically.<sup><a id="rfn1" href="#fn1">1</a></sup> While earlier approaches for indexing and searching these multimedia videos were keyword-based [Snoek and Worring 2005], new research problems emerged when trying to search the visual and multimodal content directly. This led to new research topics in multimedia content analysis such as automatic shot-boundary detection [Lienhart 1999] and video summarization [Evangelopoulos et al. 2013]. These research projects were supported by the TrecVid initiative from the National Institute of Standards and Technologies which introduced many high-quality datasets, including the multimedia event detection (MED) tasks started in 2011.<sup><a id="rfn2" href="#fn2">2</a></sup></p>
<p class="indent">A third category of applications was established in the early 2000s around the emerging field of multimodal interaction with the goal of understanding human multimodal behaviors during social interactions. One of the first landmark datasets collected in this field is the AMI Meeting Corpus which contains more than 100 hours of video recordings of meetings, all fully transcribed and annotated [Carletta et al. 2005]. Another important dataset is the SEMAINE corpus which allowed to study interpersonal dynamics between speakers and listeners [McKeown et al. 2010]. This dataset formed the basis of the first audio-visual emotion challenge (AVEC) organized in 2011 [Schuller et al. 2011]. The fields of emotion recognition and affective computing bloomed in the early 2010s thanks to strong technical advances in automatic face detection, facial landmark detection, and facial expression recognition [De la Torre and Cohn 2011]. The AVEC challenge continued annually afterward with the later instantiation including healthcare applications such as automatic assessment of depression and anxiety [Valstar et al. 2013]. A great summary of recent progress in multimodal affect recognition was published by D&#8217;Mello and Kory [2015]. Their meta-analysis revealed that a majority of recent work on multimodal affect recognition show improvement when using more than one modality, but this improvement is reduced when recognizing naturally occurring emotions.</p>
<p class="indent">Most recently, a new category of multimodal applications emerged with an emphasis on language and vision: media description. One of the most representative applications is image captioning where the task is to generate a text description of the input image [Hodosh et al. 2013]. This is motivated by the ability of such systems to help the visually impaired in their daily tasks [Bigham et al. 2010]. The main challenges media description is evaluation: how to evaluate the quality of the <a id="page_23"/>predicted descriptions. The task of visual question-answering (VQA) was recently proposed to address some of the evaluation challenges [Antol et al. 2015], where the goal is to answer a specific question about the image.</p>
<p class="indent">In order to bring some of the mentioned applications to the real world we need to address a number of technical challenges facing multimodal machine learning. We summarize the relevant technical challenges for the above-mentioned application areas in <a href="#tab1_1">Table 1.1</a>. One of the most important challenges is multimodal representation, the focus of our next section.</p>
<p class="h1"><a id="ch1_3"/><b><span class="bg1">1.3</span>&#160;&#160;&#160;&#160;Multimodal Representations</b></p>
<p class="noindent">Representing raw data in a format that a computational model can work with has always been a big challenge in machine learning. Following the work of Bengio et al. [2013], we use the term feature and representation interchangeably, with each referring to a vector or tensor representation of an entity, be it an image, audio sample, individual word, or a sentence. A multimodal representation is a representation of data using information from multiple such entities. Representing multiple modalities poses many difficulties: how to combine the data from heterogeneous sources; how to deal with different levels of noise; and how to deal with missing data. The ability to represent data in a meaningful way is crucial to multimodal problems, and forms the backbone of any model.</p>
<p class="indent">Good representations are important for the performance of machine learning models, as evidenced behind the recent leaps in performance of speech recognition [Amodei et al. 2016, Hinton et al. 2012] and visual object classification [Krizhevsky et al. 2012] systems. Bengio et al. [2013] identify a number of properties for good representations: smoothness, temporal and spatial coherence, sparsity, and natural clustering, among others. Srivastava and Salakhutdinov [2012b] identify additional desirable properties for multimodal representations: similarity in the representation space should reflect the similarity of the corresponding concepts, the representation should be easy to obtain even in the absence of some modalities, and, finally, it should be possible to fill-in missing modalities given the observed ones.</p>
<p class="indent">The development of unimodal representations has been extensively studied [Anagnostopoulos et al. 2015, Bengio et al. 2013, Li et al. 2015]. In the past decade there has been a shift from hand-designed representations to data-driven ones. For example, one of the most famous image descriptors in the early 2000s, the scale invariant feature transform (SIFT) was hand designed [Lowe 2004], but currently most visual descriptions are learned from data using neural architectures such as convolutional neural networks (CNN) [Krizhevsky et al. 2012]. Similarly, in the audio domain, acoustic features such as Mel-frequency cepstral coefficients (MFCC) have been superseded by data-driven deep neural networks in speech recognition [Amodei et al. 2016, Hinton et al. 2012] and recurrent neural networks for para-linguistic analysis [Trigeorgis et al. 2016]. In natural language processing, the textual features initially relied on counting word occurrences in documents, but have been replaced by data-driven word embeddings that exploit the word context [Mikolov et al. 2013]. While there has been a huge amount of work on unimodal representation, up until recently most multimodal representations involved simple concatenation of unimodal ones [D&#8217;Mello and Kory 2015], but this has been rapidly changing.</p>
<p class="tcaption"><a id="page_24"/><a id="tab1_1"/><b>Table 1.1</b>&#160;&#160;&#160;&#160;A summary of applications enabled by multimodal machine learning. For each application area we identify the core technical challenges that need to be addressed in order to tackle it.</p>
<p class="image"><img src="../images/tab1_1.png" alt="Image"/></p>
<p class="indentt"><a id="page_25"/>To help understand the breadth of work, we propose two categories of multimodal representation: <i>joint</i> and <i>coordinated</i>. Joint representations combine the unimodal signals into the same representation space, while coordinated representations process unimodal signals separately, but enforce certain similarity constraints on them to bring them to what we term a coordinated space. An illustration of different multimodal representation types can be seen in <a href="#fig1_1">Figure 1.1</a>.</p>
<p class="indent">Mathematically, the joint representation is expressed as:</p>
<p class="eqn"><a id="eq1_1"/><img src="../images/eq1_1.png" alt="Image"/></p>
<p class="noindent">where the multimodal representation <b>x<sub><i>m</i></sub></b> is computed using function <i>f</i> (e.g., a deep neural network, restricted Boltzmann machine, or a recurrent neural network) that relies on unimodal representations <b>x<sub>1</sub>,</b> &#8230; <i><b>x<sub>n</sub>.</b></i> On the other hand, the coordinated representation is as follows:</p>
<p class="eqn"><a id="eq1_2"/><img src="../images/eq1_2.png" alt="Image"/></p>
<p class="noindent">where each modality has a corresponding projection function (<i>f</i> and <i>g</i> above) that maps it into a coordinated multimodal space. While the projection into the multimodal space is independent for each modality, but the resulting space is coordinated between them (indicated as ~). Examples of such coordination include minimizing cosine distance [Frome et al. 2013], maximizing correlation [Andrew et al. 2013], and enforcing a partial order [Vendrov et al. 2016] between the resulting spaces.</p>
<p class="h2"><a id="ch1_3_1"/><b><span class="bg2">1.3.1</span>&#160;&#160;&#160;&#160;Joint Representations</b></p>
<p class="noindent">We start our discussion with joint representations that project unimodal representations together into a multimodal space (<a href="#eq1_1">Equation 1.1</a>). Joint representations are mostly (but not exclusively) used in tasks where multimodal data is present both during training and inference steps. The simplest example of a joint representation is a concatenation of individual modality features (also referred to as early fusion [D&#8217;Mello and Kory 2015]). In this section we discuss more advanced methods for creating joint representations starting with neural networks, followed by graphical models and recurrent neural networks (representative works can be seen in <a href="#tab1_2">Table 1.2</a>).</p>
<div class="cap" id="fig1_1">
<p class="image"><a id="page_26"/><img src="../images/fig1_1.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 1.1</b>&#160;&#160;&#160;&#160;Structure of <i>joint</i> and <i>coordinated</i> representations. Joint representations are projected to the same space using all of the modalities as input. Coordinated representations, on the other hand, exist in their own space, but are coordinated through a similarity (e.g., euclidean distance) or structure constraint (e.g., partial order).</p>
</div>
<p class="indent"><b>Neural networks</b> have become a very popular method for unimodal data representation [Bengio et al. 2013]. They are used to represent visual, acoustic, and textual data, and are increasingly used in the multimodal domain [Ngiam et al. 2011, Wang et al. 2015a, Ouyang et al. 2014]. In this section we describe how neural <a id="page_27"/>networks can be used to construct a joint multimodal representation, and what advantages they offer.</p>
<p class="tcaption" id="tab1_2"><b>Table 1.2</b>&#160;&#160;&#160;&#160;A summary of multimodal representation techniques. We identify three subtypes of joint representations (<a href="#ch1_3_1">Section 1.3.1</a>) and two subtypes of coordinated ones (<a href="#ch1_3_2">Section 1.3.2</a>). For modalities + indicates the modalities combined.</p>
<table class="table1">
<tr>
<td><p class="tab1">Representation</p></td>
<td><p class="tab1">Modalities</p></td>
<td><p class="tab1">Reference</p></td>
</tr>
<tr>
<td><p class="tab1"><b>Joint</b></p></td>
<td></td>
<td></td>
</tr>
<tr>
<td><p class="tab1">Neural networks</p></td>
<td><p class="tab1">Images + Audio</p></td>
<td><p class="tab1">[Ngiam et al. 2011, Mroueh et al. 2015]</p></td>
</tr>
<tr>
<td></td>
<td><p class="tab1">Images + Text</p></td>
<td><p class="tab1">[Silberer and Lapata 2014]</p></td>
</tr>
<tr>
<td><p class="tab1">Graphical models</p></td>
<td><p class="tab1">Images + Text</p></td>
<td><p class="tab1">[Srivastava and Salakhutdinov 2012b]</p></td>
</tr>
<tr>
<td></td>
<td><p class="tab1">Images + Audio</p></td>
<td><p class="tab1">[Kim et al. 2013]</p></td>
</tr>
<tr>
<td><p class="tab1">Sequential</p></td>
<td><p class="tab1">Audio + Video</p></td>
<td><p class="tab1">[Kahou et al. 2016, Nicolaou et al. 2011]</p></td>
</tr>
<tr>
<td></td>
<td><p class="tab1">Images + Text</p></td>
<td><p class="tab1">[Rajagopalan et al. 2016]</p></td>
</tr>
<tr>
<td class="t1"><p class="tab1"><b>Coordinated</b></p></td>
<td class="t1"></td>
<td class="t1"></td>
</tr>
<tr>
<td class="t1"><p class="tab1">Similarity</p></td>
<td class="t1"><p class="tab1">Images + Text</p></td>
<td class="t1"><p class="tab1">[Frome et al. 2013, Kiros et al. 2015]</p></td>
</tr>
<tr>
<td class="t1"></td>
<td class="t1"><p class="tab1">Video + Text</p></td>
<td class="t1"><p class="tab1">[Xu et al. 2015, Pan et al. 2016]</p></td>
</tr>
<tr>
<td class="t1"><p class="tab1">Structured</p></td>
<td class="t1"><p class="tab1">Images + Text</p></td>
<td class="t1"><p class="tab1">[Cao et al. 2016, Vendrov et al. 2016]</p></td>
</tr>
<tr>
<td class="t1"></td>
<td class="t1"><p class="tab1">Audio + Articulatory</p></td>
<td class="t1"><p class="tab1">[Wang et al. 2015b]</p></td>
</tr>
</table>
<p class="indent">In general, neural networks are made up of successive layers of inner products followed by nonlinear activation functions. In order to use a neural network as a way to represent data, it is first trained to perform a specific task (e.g., recognizing objects in images). Due to the multilayer nature of deep neural networks, each successive layer is hypothesized to represent the data in a more abstract way [Bengio et al. 2013], hence it is common to use the final or penultimate neural layers as a form of data representation. To construct a multimodal representation using neural networks each modality starts with several individual neural layers followed by a hidden layer that projects the modalities into a joint space [Wu et al. 2014, Mroueh et al. 2015, Antol et al. 2015, Ouyang et al. 2014]. The joint multimodal representation is then passed through multiple hidden layers or used directly for prediction. Such models can be trained end-to-end, learning both to represent the data and to perform a particular task. This results in a close relationship between multimodal representation learning and multimodal fusion when using neural networks.</p>
<p class="indent"><a id="page_28"/>As neural networks require a lot of labeled training data, one approach is to pretrain such representations using an autoencoder on unsupervised data [Hinton and Zemel 1994]. The model proposed by Ngiam et al. [2011] extended the idea of using autoencoders to the multimodal domain. They used stacked denoising autoencoders to represent each modality individually and then fused them into a multimodal representation using another autoencoder layer. Similarly, Silberer and Lapata [2014] proposed using a multimodal autoencoder for the task of semantic concept grounding (see <a href="#ch1_4_2">Section 1.4.2</a>). In addition to using a reconstruction loss to train the representation they introduce a term into the loss function that uses the representation to predict object labels. It is also common to fine-tune the resulting representation on a particular task at hand as the representation constructed using an autoencoder is generic and not necessarily optimal for a specific task [Wang et al. 2015a].</p>
<p class="indent">The major advantage of neural network-based joint representations comes from their often superior performance and the ability to pre-train the representations in an unsupervised manner. The performance gain is, however, dependent on the amount of data available for training. One of the disadvantages comes from the model not being able to handle missing data naturally, although there are ways to alleviate this issue [Ngiam et al. 2011, Wang et al. 2015a]. Finally, deep networks are often difficult to train [Glorot and Bengio 2010], but the field is making progress in better training techniques [Srivastava et al. 2014].</p>
<p class="indent"><b>Probabilistic graphical models</b> are another popular way to construct representations through the use of latent random variables [Bengio et al. 2013]. In this section we describe how probabilistic graphical models are used to represent unimodal and multimodal data.</p>
<p class="indent">One approach for graphical model-based representation is deep Boltzmann machines (DBM) [Salakhutdinov and Hinton 2009], which stack restricted Boltzmann machines (RBM) [Hinton et al. 2006] as building blocks. Similar to neural networks, each successive layer of a DBM is expected to represent the data at a higher level of abstraction. The appeal of DBMs comes from the fact that they do not need supervised data for training [Salakhutdinov and Hinton 2009]. As they are graphical models, the representation of data is probabilistic. However it is possible to convert them to a deterministic neural network, but this loses the generative aspect of the model [Salakhutdinov and Hinton 2009].</p>
<p class="indent">Work by Srivastava and Salakhutdinov [2012a] introduced multimodal deep belief networks as a multimodal representation. Kim et al. [2013] used a deep belief network for each modality and then combined them into joint representation for audiovisual emotion recognition. Huang and Kingsbury [2013] used a similar <a id="page_29"/>model for AVSR, and Wu and Shao [2014] for audio and skeleton joint-based gesture recognition.</p>
<p class="indent">Multimodal deep belief networks have been extended to multimodal DBMs by Srivastava and Salakhutdinov [2012b]. Multimodal DBMs are capable of learning joint representations from multiple modalities by merging two or more undirected graphs using a binary layer of hidden units on top of them. They allow for the low-level representations of each modality to influence each other after the joint training due to the undirected nature of the model.</p>
<p class="indent">Ouyang et al. [2014] explore the use of multimodal DBMs for the task of human pose estimation from multi-view data. They demonstrate that integrating the data at a later stage, after unimodal data underwent nonlinear transformations, was beneficial for the model. Similarly, Suk et al. [2014] use multimodal DBM representation to perform Alzheimer&#8217;s disease classification from positron emission tomography and magnetic resonance imaging data.</p>
<p class="indent">One of the big advantages of using multimodal DBMs for learning multimodal representations is their generative nature, which allows for an easy way to deal with missing data even if a whole modality is missing, the model has a natural way to cope. It can also be used to generate samples of one modality in the presence of the other one, or both modalities from the representation. Similar to autoencoders, the representation can be trained in an unsupervised manner enabling the use of unlabeled data. The major disadvantage of DBMs is the difficulty of training them, the high computational cost, and the need to use approximate variational training methods [Srivastava and Salakhutdinov 2012b].</p>
<p class="indent"><b>Sequential Representation.</b> Sequential Representations are designed to be able to represent sequences of varying lengths. This is in contrast with the approaches previously described which are for static data or datasets with fixed length. In this section we describe models that can be used to represent such sequences.</p>
<p class="indent">Recurrent neural networks (RNNs), and their variants such as long-short term memory (LSTMs) networks [Hochreiter and Schmidhuber 1997], have recently gained popularity due to their success in sequence modeling across various tasks [Bahdanau et al. 2014, Venugopalan et al. 2015]. So far, RNNs have mostly been used to represent unimodal sequences of words, audio, or images, with most success in the language domain. Similar to traditional neural networks, the hidden state of an RNN can be seen as a representation of the data, i.e., the hidden state of RNN at timestep <i>t</i> can be seen as the summarization of the sequence up to that timestep. This is especially apparent in RNN encoder-decoder frameworks where the task of an encoder is to represent a sequence in the hidden state of an RNN in such a way that a decoder could reconstruct it [Bahdanau et al. 2014].</p>
<p class="indent"><a id="page_30"/>The use of RNN representations has not been limited to the unimodal domain. An early use of constructing a multimodal representation using RNNs comes from work by Cosi et al. [1994] on AVSR. They have also been used for representing audiovisual data for affect recognition [Nicolaou et al. 2011, Chen et al. 2015] and to represent multi-view data such as different visual cues for human behavior analysis [Rajagopalan et al. 2016].</p>
<p class="h2"><a id="ch1_3_2"/><b><span class="bg2">1.3.2</span>&#160;&#160;&#160;&#160;Coordinated Representations</b></p>
<p class="noindent">An alternative to a joint multimodal representation is a coordinated representation. Instead of projecting the modalities together into a joint space, we learn separate representations for each modality but coordinate them through a constraint. We start our discussion with coordinated representations that enforce similarity between representations, moving on to coordinated representations that enforce more structure on the resulting space (representative works of different coordinated representations can be seen in <a href="#tab1_2">Table 1.2</a>).</p>
<p class="indent"><b>Similarity models</b> minimize the distance between modalities in the coordinated space. For example, such models encourage the representation of the word <i>dog</i> and an image of a dog to have a smaller distance between them than distance between the word <i>dog</i> and an image of a car [Frome et al. 2013]. One of the earliest examples of such a representation comes from the work by Weston et al. [2011] on the WSABIE (web scale annotation by image embedding) model, where a coordinated space was constructed for images and their annotations. WSABIE constructs a simple linear map from image and textual features such that corresponding annotation and image representation would have a higher inner product (smaller cosine distance) between them than non-corresponding ones.</p>
<p class="indent">More recently, neural networks have become a popular way to construct coordinated representations, due to their ability to learn representations. Their advantage lies in the fact that they can jointly learn coordinated representations in an end-to-end manner. An example of such coordinated representation is DeViSE, a deep visual-semantic embedding [Frome et al. 2013]. DeViSE uses a similar inner product and ranking loss function to WSABIE but uses more complex image and word embeddings. Kiros et al. [2015] extended this to sentence and image coordinated representation by using an LSTM model and a pairwise ranking loss to coordinate the feature space. Socher et al. [2014] tackle the same task, but extend the language model to a dependency tree RNN to incorporate compositional semantics. A similar model was also proposed by Pan et al. [2016], but using videos instead of images. Xu et al. [2015] also constructed a coordinated space between videos <a id="page_31"/>and sentences using a &#9001;subject, verb, object&#9002; compositional language model and a deep video model. This representation was then used for the task of cross-modal retrieval and video description.</p>
<p class="indent">While the above models enforced similarity between representations, <b>structured coordinated space</b> models go beyond that and enforce additional constraints between the modality representations. The type of structure enforced is often based on the application, with different constraints for hashing, cross-modal retrieval, and image captioning.</p>
<p class="indent">Structured coordinated spaces are commonly used in cross-modal hashing which is compression of high-dimensional data into compact binary codes with similar binary codes for similar objects [Wang et al. 2014]. The idea of cross-modal hashing is to create such codes for cross-modal retrieval [Kumar and Udupa 2011, Bronstein et al. 2010, Jiang et al. 2015]. Hashing enforces certain constraints on the resulting multimodal space: (1) it has to be an <i>N</i>-dimensional Hamming space, a binary representation with controllable number of bits; (2) the same object from different modalities has to have a similar hash code; and (3) the space has to be similarity-preserving. Learning how to represent the data as a hash function attempts to enforce all of these three requirements [Kumar and Udupa 2011, Bronstein et al. 2010]. For example, Jiang and Li [2017] introduced a method to learn such common binary space between sentence descriptions and corresponding images using end-to-end trainable deep learning techniques. While Cao et al. [2016] extended the approach with a more complex LSTM sentence representation and introduced an outlier insensitive bit-wise margin loss and a relevance feedback based semantic similarity constraint. Similarly, Wang et al. [2016] constructed a coordinated space in which images (and sentences) with similar meanings are closer to each other.</p>
<p class="indent">Another example of a structured coordinated representation comes from order-embeddings of images and language [Vendrov et al. 2016, Zhang et al. 2016]. The model proposed by Vendrov et al. [2016] enforces a dissimilarity metric that is asymmetric and implements the notion of partial order in the multimodal space. The idea is to capture a partial order of the language and image representations, enforcing a hierarchy on the space; for example, image of &#8220;a woman walking her dog&#8221; &#8594; text &#8220;woman walking her dog&#8221; &#8594; text &#8220;woman walking.&#8221; A similar model using denotation graphs was also proposed by Young et al. [2014] where denotation graphs are used to induce a partial ordering. Lastly, Zhang et al. [2016] present how exploiting structured representations of text and images can create concept taxonomies in an unsupervised manner.</p>
<p class="indent"><a id="page_32"/>A special case of a structured coordinated space is one based on canonical correlation analysis (CCA) [Hotelling 1936]. CCA computes a linear projection which maximizes the correlation between two random variables (in our case modalities) and enforces orthogonality of the new space. CCA models have been used extensively for cross-modal retrieval [Hardoon et al. 2004, Rasiwasia et al. 2010, Klein et al. 2015] and audiovisual signal analysis [Sargin et al. 2007, Slaney and Covell 2001]. Extensions to CCA attempt to construct a correlation maximizing nonlinear projection [Lai and Fyfe 2000, Andrew et al. 2013]. Kernel canonical correlation analysis (KCCA) [Lai and Fyfe 2000] uses reproducing kernel Hilbert spaces for projection. However, as the approach is nonparametric it scales poorly with the size of the training set and has issues with very large real-world datasets. Deep canonical correlation analysis (DCCA) [Andrew et al. 2013] was introduced as an alternative to KCCA and addresses the scalability issue, it was also shown to lead to better correlated representation space. Similar correspondence autoencoder [Feng et al. 2014] and deep correspondence RBMs [Feng et al. 2015] have also been proposed for cross-modal retrieval.</p>
<p class="indent">CCA, KCCA, and DCCA are unsupervised techniques and only optimize the correlation over the representations, thus mostly capturing what is shared across the modalities. Deep canonically correlated autoencoders [Wang et al. 2015b] also include an autoencoder based data reconstruction term. This encourages the representation to also capture modality specific information. Semantic correlation maximization method [Zhang and Li 2014] also encourages semantic relevance, while retaining correlation maximization and orthogonality of the resulting space, leading to a combination of CCA and cross-modal hashing techniques.</p>
<p class="h2"><a id="ch1_3_3"/><b><span class="bg2">1.3.3</span>&#160;&#160;&#160;&#160;Discussion</b></p>
<p class="noindent">In this section we identified two major types of multimodal representations: joint and coordinated. Joint representations project multimodal data into a common space and are best suited for situations when all of the modalities are present during inference. They have been extensively used for AVSR, affect, and multimodal gesture recognition. Coordinated representations, on the other hand, project each modality into a separate but coordinated space, making them suitable for applications where only one modality is present at test time, such as: multimodal retrieval and translation, conceptual grounding (<a href="#ch1_4_2">Section 1.4.2</a>), and zero shot learning (<a href="#ch1_4_2">Section 1.4.2</a>). Finally, while joint representations have been used in situations to construct representations of more than two modalities, coordinated spaces have, so far, been mostly limited to two modalities.</p>
<p class="h1"><a id="page_33"/><a id="ch1_4"/><b><span class="bg1">1.4</span>&#160;&#160;&#160;&#160;Co-learning</b></p>
<p class="noindent">The final multimodal challenge in our taxonomy is co-learning that aids the modeling of a (resource poor) modality by exploiting knowledge from another (resource rich) modality. It is particularly relevant when one of the modalities has limited resources, lack of annotated data, noisy input, and unreliable labels. We call this challenge co-learning as most often the helper modality is used only during model training and is not used during test time. We identify three types of co-learning approaches based on their training resources: parallel, non-parallel, and hybrid. <i>Parallel-data</i> approaches require training datasets where the observations from one modality are directly linked to the observations from other modalities. In other words, when the multimodal observations are from the same instances, such as in an audio-visual speech dataset where the video and speech samples are from the same speaker. In contrast, <i>non-parallel data</i> approaches do not require direct links between observations from different modalities. These approaches usually achieve co-learning by using overlap in terms of categories, e.g., in zero shot learning when the conventional visual object recognition dataset is expanded with a second text-only dataset from Wikipedia to improve the generalization of visual object recognition. In the <i>hybrid</i> data setting the modalities are <i>bridged</i> through a shared modality or a dataset. An overview of the taxonomy in co-learning can be seen in <a href="#tab1_3">Table 1.3</a> and summary of data parallelism in <a href="#fig1_2">Figure 1.2</a>.</p>
<p class="h2"><a id="ch1_4_1"/><b><span class="bg2">1.4.1</span>&#160;&#160;&#160;&#160;Parallel Data</b></p>
<p class="noindent">In parallel data co-learning both modalities share a set of instances, audio recordings with the corresponding videos, images, and their sentence descriptions. This allows for two types of algorithms to exploit that data to better model the modalities: co-training and representation learning.</p>
<p class="indent"><b>Co-training</b> is the process of creating more labeled training samples when we have few labeled samples in a multimodal problem [Blum and Mitchell 1998]. The basic algorithm builds weak classifiers in each modality to bootstrap each other with labels for the unlabeled data. It has been shown in the seminal work of Blum and Mitchell [1998] that more training samples for web page classification can be discovered on the web page itself and hyper-links leading to it. By definition this task requires parallel data as it relies on the overlap of multimodal samples.</p>
<p class="indent">Co-training has been used for statistical parsing [Sarkar 2001] to build better visual detectors [Levin et al. 2003] and for audio-visual speech recognition [Christoudias et al. 2006]. It has also been extended to deal with disagreement between modalities by filtering out unreliable samples [Christoudias et al. 2008]. While co-training is a powerful method for generating more labeled data, it can also lead to biased training samples resulting in overfitting.</p>
<p class="tcaption"><a id="page_34"/><a id="tab1_3"/><b>Table 1.3</b>&#160;&#160;&#160;&#160;A summary of co-learning taxonomy, based on data parallelism. Parallel data&#8212;multiple modalities can see the same instance. Non-parallel data&#8212;unimodal instances are independent of each other. Hybrid data&#8212;the modalities are <i><b>pivoted</b></i> through a shared modality or dataset.</p>
<table class="table1">
<tr>
<td><p class="tab1">Data parallelism</p></td>
<td><p class="tab1">Task</p></td>
<td><p class="tab1">Reference</p></td>
</tr>
<tr>
<td><p class="tab1"><b>Parallel</b></p></td>
<td></td>
<td></td>
</tr>
<tr>
<td><p class="tab1">Co-training</p></td>
<td><p class="tab1">Mixture</p></td>
<td><p class="tab1">[Blum and Mitchell 1998]</p></td>
</tr>
<tr>
<td><p class="tab1">Transfer learning</p></td>
<td><p class="tab1">AVSR</p></td>
<td><p class="tab1">[Ngiam et al. 2011]</p></td>
</tr>
<tr>
<td></td>
<td><p class="tab1">Lip reading</p></td>
<td><p class="tab1">[Moon et al. 2015]</p></td>
</tr>
<tr>
<td class="t1"><p class="tab1"><b>Non-parallel</b></p></td>
<td class="t1"></td>
<td class="t1"></td>
</tr>
<tr>
<td class="t1"><p class="tab1">Transfer learning</p></td>
<td class="t1"><p class="tab1">Visual classification</p></td>
<td class="t1"><p class="tab1">[Frome et al. 2013]</p></td>
</tr>
<tr>
<td class="t1"></td>
<td class="t1"><p class="tab1">Action recognition</p></td>
<td class="t1"><p class="tab1">[Mahasseni and Todorovic 2016]</p></td>
</tr>
<tr>
<td class="t1"><p class="tab1">Concept grounding</p></td>
<td class="t1"><p class="tab1">Metaphor class.</p></td>
<td class="t1"><p class="tab1">[Shutova et al. 2016]</p></td>
</tr>
<tr>
<td class="t1"></td>
<td class="t1"><p class="tab1">Word similarity</p></td>
<td class="t1"><p class="tab1">[Kiela and Clark 2015]</p></td>
</tr>
<tr>
<td class="t1"><p class="tab1">Zero shot learning</p></td>
<td class="t1"><p class="tab1">Image class.</p></td>
<td class="t1"><p class="tab1">[Socher et al. 2013]</p></td>
</tr>
<tr>
<td class="t1"></td>
<td class="t1"><p class="tab1">Thought class.</p></td>
<td class="t1"><p class="tab1">[Palatucci et al. 2009]</p></td>
</tr>
<tr>
<td><p class="tab1"><b>Hybrid Data</b></p></td>
<td></td>
<td></td>
</tr>
<tr>
<td><p class="tab1">Bridging</p></td>
<td><p class="tab1">MT and image ret.</p></td>
<td><p class="tab1">[Rajendran et al. 2015]</p></td>
</tr>
<tr>
<td></td>
<td><p class="tab1">Transliteration</p></td>
<td><p class="tab1">[Nakov and Ng 2012]</p></td>
</tr>
</table>
<p class="indentt"><b>Transfer learning</b> is another way to exploit co-learning with parallel data. Multimodal representation learning (<a href="#ch1_3_1">Section 1.3.1</a>) approaches such as multimodal deep Boltzmann machines [Srivastava and Salakhutdinov 2012b] and multimodal autoencoders [Ngiam et al. 2011] transfer information from representation of one modality to that of another. This not only leads to multimodal representations, but also to better unimodal ones, with only one modality being used during test time.</p>
<p class="indent">Moon et al. [2015] show how to transfer information from a speech recognition neural network (based on audio) to a lip-reading one (based on images), leading to a better visual representation, and a model that can be used for lip-reading without need for audio information during test time. Similarly, Arora and Livescu [2013] build better acoustic features using CCA on acoustic and articulatory (location of lips, tongue, and jaw) data. They use articulatory data only during CCA construction and use only the resulting acoustic (unimodal) representation during test time.</p>
<div class="cap" id="fig1_2">
<p class="image"><a id="page_35"/><img src="../images/fig1_2.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 1.2</b>&#160;&#160;&#160;&#160;Types of data parallelism used in co-learning: <i>parallel</i>, modalities are from the same dataset and there is a direct correspondence between instances; <i>non-parallel</i>, modalities are from different datasets and do not have overlapping instances, but overlap in general categories or concepts; and <i>hybrid</i>, the instances or concepts are bridged by a third modality or a dataset.</p>
</div>
<p class="h2"><a id="ch1_4_2"/><b><span class="bg2">1.4.2</span>&#160;&#160;&#160;&#160;Non-parallel Data</b></p>
<p class="noindent">Methods that rely on non-parallel data do not require the modalities to have shared instances, but only shared categories or concepts. Non-parallel co-learning approaches can help when learning representations as they allow for better semantic concept understanding and even perform unseen object recognition.</p>
<p class="indent"><b>Transfer learning</b> is also possible on non-parallel data and allows for the learning of better representations through transferring information from a representation built using a data rich or clean modality to a data scarce or noisy modality. This type of transfer learning is often achieved by using coordinated multimodal representations (see <a href="#ch1_3_2">Section 1.3.2</a>). For example, Frome et al. [2013] used text to improve visual representations for image classification by coordinating CNN visual features with word2vec textual ones [Mikolov et al. 2013] trained on separate large datasets. Visual representations trained in such a way result in more meaningful errors, mistaking objects for ones of similar category [Frome et al. 2013]. Mahasseni and Todorovic [2016] demonstrated how to regularize a color video-based LSTM using an autoencoder LSTM trained on 3D skeleton data by enforcing similarities between their hidden states. Such an approach is able to improve the original LSTM and lead to state-of-the-art performance in action recognition.</p>
<p class="indent"><b>Conceptual grounding</b> refers to learning semantic meanings or concepts not purely based on language but also on additional modalities such as vision, sound, or even smell. While the majority of concept learning approaches are purely language-based, representations of meaning in humans are not merely a product <a id="page_36"/>of our linguistic exposure but are also <i>grounded</i> through our sensorimotor experience and perceptual system [Barsalou 2008, Louwerse 2011]. Human semantic knowledge relies heavily on perceptual information [Louwerse 2011] and many concepts are grounded in the perceptual system and are not purely symbolic [Barsalou 2008]. This implies that learning semantic meaning purely from textual information might not be optimal, and motivates the use of visual or acoustic cues to ground our linguistic representations.</p>
<p class="indent">Starting from work by Feng and Lapata [2010], grounding is usually performed by finding a common latent space between the representations [Feng and Lapata 2010, Silberer and Lapata 2012] (in case of parallel datasets) or by learning unimodal representations separately and then concatenating them to lead to a multimodal one [Regneri et al. 2013, Shutova et al. 2016, Kiela and Bottou 2014, Bruni et al. 2012] (in case of non-parallel data). Once a multimodal representation is constructed it can be used on purely linguistic tasks. Shutova et al. [2016] and Bruni et al. [2012] used grounded representations for better classification of metaphors and literal language. Such representations have also been useful for measuring conceptual similarity and relatedness, identifying how semantically or conceptually related two words are [Kiela and Bottou 2014, Bruni et al. 2014, Silberer and Lapata 2012] or actions [Regneri et al. 2013]. Furthermore, concepts can be grounded not only using visual signals, but also acoustic ones, leading to better performance especially on words with auditory associations [Kiela and Clark 2015], or even olfactory signals [Kiela et al. 2015] for words with smell associations. Finally, there is a lot of overlap between multimodal alignment and conceptual grounding, as aligning visual scenes to their descriptions leads to better textual or visual representations [Regneri et al. 2013, Plummer et al. 2015, Kong et al. 2014, Yu and Siskind 2013].</p>
<p class="indent">Conceptual grounding has been found to be an effective way to improve performance on a number of tasks. It also shows that language and vision (or audio) are complementary sources of information and combining them in multimodal models often improves performance. However, one has to be careful as grounding does not always lead to better performance [Kiela and Clark 2015, Kiela et al. 2015], and only makes sense when grounding has relevance for the task such as grounding using images for visually related concepts.</p>
<p class="indent"><b>Zero shot learning (ZSL)</b> refers to recognizing a concept without having explicitly seen any examples of it. For example, classifying a cat in an image without ever having seen (labeled) images of cats. This is an important problem to address as in a number of tasks such as visual object classification: it is prohibitively expensive to provide training examples for every imaginable object of interest.</p>
<p class="indent"><a id="page_37"/>There are two main types of ZSL: unimodal and multimodal. The unimodal ZSL looks at component parts or attributes of the object, such as phonemes to recognize an unheard word or visual attributes such as color, size, and shape to predict an unseen visual class [Farhadi et al. 2009]. The multimodal ZSL recognizes the objects in the primary modality through the help of the secondary one&#8212;in which the object has been seen. The multimodal version of ZSL is a problem facing non-parallel data by definition as the overlap of seen classes is different between the modalities.</p>
<p class="indent">Socher et al. [2013] map image features to a conceptual word space and are able to classify between seen and unseen concepts. The unseen concepts can be then assigned to a word that is close to the visual representation; this is enabled by the semantic space being trained on a separate dataset that has seen more concepts. Instead of learning a mapping from visual to concept space Frome et al. [2013] learn a coordinated multimodal representation between concepts and images that allows for ZSL. Palatucci et al. [2009] perform prediction of words people are thinking of based on functional magnetic resonance images; they show how it is possible to predict unseen words through the use of an intermediate semantic space. Lazaridou et al. [2014] present a fast mapping method for ZSL by mapping extracted visual feature vectors to text-based vectors through a neural network.</p>
<p class="h2"><a id="ch1_4_3"/><b><span class="bg2">1.4.3</span>&#160;&#160;&#160;&#160;Hybrid Data</b></p>
<p class="noindent">In the hybrid data setting two non-parallel modalities are bridged by a shared modality or a dataset (see <a href="#fig1_2">Figure 1.2c</a>). The most notable example is the Bridge Correlational Neural Network [Rajendran et al. 2015], which uses a pivot modality to learn coordinated multimodal representations in presence of non-parallel data. For example, in the case of multilingual image captioning, the image modality would always be paired with at least one caption in any language. Such methods have also been used to bridge languages that might not have parallel corpora but have access to a shared pivot language, such as for machine translation [Rajendran et al. 2015, Nakov and Ng 2012] and document transliteration [Khapra et al. 2010].</p>
<p class="indent">Instead of using a separate modality for bridging, some methods rely on existence of large datasets from a similar or related task to lead to better performance in a task that only contains limited annotated data. Socher and Fei-Fei [2010] use the existence of large text corpora in order to guide image segmentation. While Anne Hendricks et al. [2016] use separately trained visual model and a language model to lead to a better image and video description system, for which only limited data is available.</p>
<p class="h2"><a id="page_38"/><a id="ch1_4_4"/><b><span class="bg2">1.4.4</span>&#160;&#160;&#160;&#160;Discussion</b></p>
<p class="noindent">Multimodal co-learning allows for one modality to influence the training of another, exploiting the complementary information across modalities. It is important to note that co-learning is task independent and could be used to create better fusion, translation, and alignment models. This challenge is exemplified by algorithms such as co-training, multimodal representation learning, conceptual grounding, and zero shot learning (ZSL) and has found many applications in visual classification, action recognition, audio-visual speech recognition, and semantic similarity estimation.</p>
<p class="h1"><a id="ch1_5"/><b><span class="bg1">1.5</span>&#160;&#160;&#160;&#160;Conclusion</b></p>
<p class="noindent">Multimodal machine learning is a vibrant multi-disciplinary field which aims to build models that can process and relate information from multiple modalities. As part of this chapter, presented the taxonomy of two challenges in multimodal machine learning: representation and co-learning [Baltru&#353;aitis et al. 2017]. Some of them such as fusion have been studied for a long time, but more recent interest in alignment and translation have led to a large number of new multimodal algorithms and exciting multimodal applications.</p>
<p class="indent">Although the focus of this chapter was primarily on the last decade of multimodal research, it is important to address future challenges with a knowledge of past achievements. Moving forward, the proposed taxonomy gives researchers a framework to understand current research and identify understudied challenges for future research. We believe that all these aspects of multimodal research are needed if we want to build computers able to perceive, model and generate multimodal signals. One specific area of multimodal machine learning which seems to be under-studied is co-learning, where knowledge from one modality helps with modeling in another modality. This challenge is related to the concept of coordinated representations where each modality keeps its own representation but find a way to exchange and coordinate knowledge. We see these lines of research as promising directions for future research.</p>
<p class="h1n"><a id="ch1_6"/><b>Focus Questions</b></p>
<p class="noindent"><b>1.1.</b> Describe the different applications that have emerged in the multimodal domain. Also think of and list a few applications not listed in the chapter. Do these applications suggest that models using multiple modalities are essential when contrasted with unimodal models?</p>
<p class="noindentt"><b>1.2.</b> <a id="page_39"/>What are the 2 categories of Representation Learning that have been discussed in this chapter? Which of the 2 categories seems to be more explicit in estimating similarities between two modalities and how did you come to that conclusion?</p>
<p class="noindentt"><b>1.3.</b> Coordinated Representation methods have only been used for two modalities at a time. Can you come up with ideas to extend the current methods to multiple modalities?</p>
<p class="noindentt"><b>1.4.</b> Compare and contrast probabilistic graphical models and neural networks for representation learning. Can you think of a way to combine the best of both worlds for multimodal representations?</p>
<p class="noindentt"><b>1.5.</b> Limited and noisy data from a modality is a common problem in a hoards of real world tasks. Co-learning exploits knowledge from a resource rich modality to aid the resource poor modality. Can you think of conditions where the described methods could do more harm than good?</p>
<p class="noindentt"><b>1.6.</b> Transfer Learning could be done on both Parallel and Non-parallel data. What are the key differences between the approaches followed on both these kinds of data?</p>
<p class="noindentt"><b>1.7.</b> Although the chapter focuses more on visual, acoustic, and textual modalities there are other modalities (like olfactory signals) that can act as a bridge to ground associations made by humans. What are some ways that can help one decide which modalities are complimentary (help each other and boost performance on the task)?</p>
<p class="noindentt"><b>1.8.</b> Zero shot learning recognizes a concept without ever having explicitly seen it before. Which kind of representation is a good choice for this task and why?</p>
<p class="noindentt"><b>1.9.</b> Hybrid data is similar to Non-Parallel data apart form one key difference. What is that, and how are the models modified to take advantage of this kind of data?</p>
<p class="h1n"><a id="ch1_7"/><b>References</b></p>
<p class="ref">D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al. 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. In <i>International Conference on Machine Learning</i>, pp. 173&#8211;182. 23, 25</p>
<p class="ref">C.-N. Anagnostopoulos, T. Iliou, and I. Giannoukos. 2015. Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011. <i>Artificial Intelligence Review</i>, 43(2):155&#8211;177. DOI: 10.1007/s10462-012-9368-5. 23</p>
<p class="ref">G. <a id="page_40"/>Andrew, R. Arora, J. Bilmes, and K. Livescu. 2013. Deep canonical correlation analysis. In <i>International Conference on Machine Learning</i>, pp. 1247&#8211;1255. 25, 32</p>
<p class="ref">L. Anne Hendricks, S. Venugopalan, M. Rohrbach, R. Mooney, K. Saenko, and T. Darrell. 2016. Deep compositional captioning: Describing novel object categories without paired training data. In <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, pp. 1&#8211;10. 37</p>
<p class="ref">S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and D. Parikh. 2015. Vqa: Visual question answering. In <i>Proceedings of the IEEE International Conference on Computer Vision</i>, pp. 2425&#8211;2433, 2015. 23, 27</p>
<p class="ref">R. Arora and K. Livescu. 2013. Multi-view cca-based acoustic features for phonetic recognition across speakers and domains. In <i>Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</i>, pp. 7135&#8211;7139. IEEE. 10.1109/ICASSP.2013.6639047. 34</p>
<p class="ref">P. K. Atrey, M. A. Hossain, A. El Saddik, and M. S. Kankanhalli. 2010. Multimodal fusion for multimedia analysis: a survey. <i>Multimedia systems</i>, 16(6):345&#8211;379, 2010. DOI: 10.1007/s00530-010-0182-0. 21</p>
<p class="ref">D. Bahdanau, K. Cho, and Y. Bengio. 2014. Neural Machine Translation By Jointly Learning To Align and Translate. <i>ICLR</i>. 29</p>
<p class="ref">T. Baltru&#353;aitis, C. Ahuja, and L.-P. Morency. 2017. Multimodal machine learning: A survey and taxonomy. <i>arXiv preprint arXiv:1705.09406</i>. 20, 21, 38</p>
<p class="ref">L. W. Barsalou. 2008. Grounded cognition. <i>Annu. Rev. Psychol</i>., 59:617&#8211;645. DOI: 10.1146/annurev.psych.59.103006.093639. 36</p>
<p class="ref">Y. Bengio, A. Courville, and P. Vincent. 2013. Representation learning: A review and new perspectives. <i>IEEE transactions on pattern analysis and machine intelligence</i>, 35(8):1798&#8211;1828. DOI: 10.1109/TPAMI.2013.50. 23, 26, 27, 28</p>
<p class="ref">J. P. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, R. C. Miller, R. Miller, A. Tatarowicz, B. White, S. White, et al. 2010. Vizwiz: nearly real-time answers to visual questions. In <i>Proceedings of the 23nd annual ACM symposium on User interface software and technology</i>, pp. 333&#8211;342. ACM. DOI: 10.1145/1866029.1866080. 22</p>
<p class="ref">A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In <i>Proceedings of the eleventh annual conference on Computational learning theory</i>, pp. 92&#8211;100. ACM. DOI: 10.1145/279943.279962. 33, 34</p>
<p class="ref">H. Bourlard and S. Dupont. 1996. A mew asr approach based on independent processing and recombination of partial frequency bands. In <i>Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on</i>, volume 1, pp. 426&#8211;429. IEEE, 1996. DOI: 10.1109/ICSLP.1996.607145. 21</p>
<p class="ref">M. Brand, N. Oliver, and A. Pentland. 1997. Coupled hidden markov models for complex action recognition. In <i>Computer vision and pattern recognition, 1997. proceedings., 1997 ieee computer society conference on</i>, pp. 994&#8211;999. IEEE. DOI: 10.1109/CVPR.1997.609450. 21</p>
<p class="ref"><a id="page_41"/>M. M. Bronstein, A. M. Bronstein, F. Michel, and N. Paragios. 2010. Data fusion through cross-modality metric learning using similarity-sensitive hashing. In <i>Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</i>, pp. 3594&#8211;3601. IEEE. DOI: 10.1109/CVPR.2010.5539928. 31</p>
<p class="ref">E. Bruni, G. Boleda, M. Baroni, and N.-K. Tran. Distributional semantics in technicolor. 2012. In <i>Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1</i>, pp. 136&#8211;145. Association for Computational Linguistics. 36</p>
<p class="ref">E. Bruni, N.-K. Tran, and M. Baroni. 2014. Multimodal distributional semantics. <i>J. Artif. Intell. Res.(JAIR)</i>, 49(2014): 1&#8211;47. 36</p>
<p class="ref">Y. Cao, M. Long, J. Wang, Q. Yang, and S. Y. Philip. 2016. Deep visual-semantic hashing for cross-modal retrieval. In <i>KDD</i>, pp. 1445&#8211;1454. DOI: 10.1145/2939672.2939812. 27, 31</p>
<p class="ref">J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, et al. 2005. The ami meeting corpus: A pre-announcement. In <i>International Workshop on Machine Learning for Multimodal Interaction</i>, pp. 28&#8211;39. Springer. DOI: 10.1007/11677482_3. 22</p>
<p class="ref">X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Doll&#225;r, and C. L. Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. <i>arXiv preprint arXiv:1504.00325</i>. 30</p>
<p class="ref">C. M. Christoudias, K. Saenko, L.-P. Morency, and T. Darrell. 2006. Co-adaptation of audiovisual speech and gesture classifiers. In <i>Proceedings of the 8th international conference on Multimodal interfaces</i>, pp. 84&#8211;91. ACM. DOI: 10.1145/1180995.1181013. 33</p>
<p class="ref">C. M. Christoudias, R. Urtasun, and T. Darrell. 2008. Multi-view learning in the presence of view disagreement. In <i>UAI</i>. 33</p>
<p class="ref">P. Cosi, E. M. Caldognetto, K. Vagges, G. A. Mian, and M. Contolini. 1994. Bimodal recognition experiments with recurrent neural networks. In <i>Acoustics, Speech, and Signal Processing, 1994. ICASSP-94., 1994 IEEE International Conference on</i>, volume 2, pp. II&#8211;553. IEEE, 1994. DOI: 10.1109/ICASSP.1994.389596. 30</p>
<p class="ref">F. De la Torre and J. F. Cohn. 2011. Facial expression analysis. In <i>Visual analysis of humans</i>, pp. 377&#8211;409. Springer. DOI: 10.1007/978-0-85729-997-0_19. 22</p>
<p class="ref">S. K. D&#8217;Mello and J. Kory. 2015. A review and meta-analysis of multimodal affect detection systems. <i>ACM Computing Surveys (CSUR)</i>, 47(3): 43. DOI: 10.1145/2682899. 22, 25, 26</p>
<p class="ref">G. Evangelopoulos, A. Zlatintsi, A. Potamianos, P. Maragos, K. Rapantzikos, G. Skoumas, and Y. Avrithis. 2013. Multimodal saliency and fusion for movie summarization based on aural, visual, and textual attention. <i>IEEE Transactions on Multimedia</i>, 15(7): 1553&#8211;1568. DOI: 10.1109/TMM.2013.2267205. 22</p>
<p class="ref">A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. 2009. Describing objects by their attributes. In <i>IEEE Conference on Computer Vision and Pattern Recognition, 2009</i>., pp. 1778&#8211;1785. IEEE. DOI: 10.1109/CVPR.2009.5206772. 37</p>
<p class="ref"><a id="page_42"/>F. Feng, X. Wang, and R. Li. 2014. Cross-modal retrieval with correspondence autoencoder. In <i>Proceedings of the 22nd ACM international conference on Multimedia</i>, pp. 7&#8211;16. ACM. DOI: 10.1145/2647868.2654902. 32</p>
<p class="ref">F. Feng, R. Li, and X. Wang. 2015. Deep correspondence restricted boltzmann machine for cross-modal retrieval. <i>Neurocomputing</i>, 154: 50&#8211;60. DOI: 10.1145/2808205. 32</p>
<p class="ref">Y. Feng and M. Lapata. 2010. Visual information in semantic representation. In <i>Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</i>, pp. 91&#8211;99. Association for Computational Linguistics. 36</p>
<p class="ref">A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al. 2013. Devise: A deep visual-semantic embedding model. In <i>Advances in neural information processing systems</i>, pp. 2121&#8211;2129. 25, 27, 30, 34, 35, 37</p>
<p class="ref">X. Glorot and Y. Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In <i>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</i>, pp. 249&#8211;256. 28</p>
<p class="ref">M. Gurban, J.-P. Thiran, T. Drugman, and T. Dutoit. 2008. Dynamic modality weighting for multi-stream hmms in audio-visual speech recognition. In <i>Proceedings of the 10th international conference on Multimodal interfaces</i>, pp. 237&#8211;240. ACM. DOI: 10.1145/1452392.1452442. 21</p>
<p class="ref">D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. 2004. Canonical correlation analysis: An overview with application to learning methods. <i>Neural computation</i>, 16(12): 2639&#8211;2664. DOI: 10.1162/0899766042321814. 32</p>
<p class="ref">G. Hinton, L. Deng, D. Yu, G. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. 2012. Deep Neural Networks for Acoustic Modeling in Speech Recognition. <i>IEEE Signal Processing Magazine</i>. DOI: 10.1109/MSP.2012.2205597. 23, 25</p>
<p class="ref">G. E. Hinton and R. S. Zemel. 1994. Autoencoders, minimum description length and helmholtz free energy. In <i>Advances in neural information processing systems</i>, pp. 3&#8211;10. 28</p>
<p class="ref">G. E. Hinton, S. Osindero, and Y.-W. Teh. 2006. A fast learning algorithm for deep belief nets. <i>Neural computation</i>, 18(7): 1527&#8211;1554. DOI: 10.1162/neco.2006.18.7.1527. 28</p>
<p class="ref">S. Hochreiter and J. Schmidhuber. 1997. Long short-term memory. <i>Neural computation</i>, 9(8): 1735&#8211;1780. 29</p>
<p class="ref">M. Hodosh, P. Young, and J. Hockenmaier. 2013. Framing image description as a ranking task: Data, models and evaluation metrics. <i>Journal of Artificial Intelligence Research</i>, 47: 853&#8211;899, 2013. 22</p>
<p class="ref">H. Hotelling. 1936. Relations between two sets of variates. <i>Biometrika</i>, 28(3/4):321&#8211;377. 32</p>
<p class="ref">J. Huang and B. Kingsbury. 2013. Audio-visual deep learning for noise robust speech recognition. In <i>Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</i>, pp. 7596&#8211;7599. IEEE. DOI: 10.1109/ICASSP.2013.6639140. 28</p>
<p class="ref"><a id="page_43"/>A. Jameson and P. O. Kristensson. 2017. Understanding and supporting modality choices. In <i>The Handbook of Multimodal-Multisensor Interfaces</i>, pp. 201&#8211;238. Association for Computing Machinery and Morgan &#38; Claypool. DOI 10.1145/3015783.3015790. 21</p>
<p class="ref">Q.-y. Jiang and W.-j. Li. 2017. Deep Cross-Modal Hashing. In <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>. DOI: 10.1109/CVPR.2017.348. 31</p>
<p class="ref">X. Jiang, F. Wu, Y. Zhang, S. Tang, W. Lu, and Y. Zhuang. 2015. The classification of multimodal data with hidden conditional random field. <i>Pattern Recognition Letters</i>, 51: 63&#8211;69. DOI: 10.1016/j.patrec.2014.08.005. 31</p>
<p class="ref">B. H. Juang and L. R. Rabiner. 1991. Hidden markov models for speech recognition. <i>Technometrics</i>, 33(3): 251&#8211;272. 21</p>
<p class="ref">S. E. Kahou, X. Bouthillier, P. Lamblin, C. Gulcehre, V. Michalski, K. Konda, S. Jean, P. Froumenty, Y. Dauphin, N. Boulanger-Lewandowski, et al. 2016. Emonets: Multimodal deep learning approaches for emotion recognition in video. <i>Journal on Multimodal User Interfaces</i>, 10(2): 99&#8211;111. DOI: 10.1007/s12193-015-0195-2. 27</p>
<p class="ref">M. M. Khapra, A. Kumaran, and P. Bhattacharyya. 2010. Everybody loves a rich cousin: An empirical study of transliteration through bridge languages. In <i>Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</i>, pp. 420&#8211;428. Association for Computational Linguistics. 37</p>
<p class="ref">D. Kiela and L. Bottou. 2014. Learning image embeddings using convolutional neural networks for improved multi-modal semantics. In <i>EMNLP</i>, pp. 36&#8211;45. DOI: 10.3115/v1/D14-1005. 36</p>
<p class="ref">D. Kiela and S. Clark. 2015. Multi-and cross-modal semantics beyond vision: Grounding in auditory perception. <i>Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, pp. 2461&#8211;2470. DOI: 10.18653/v1/D15-1293. 34, 36</p>
<p class="ref">D. Kiela, L. Bulat, and S. Clark. 2015. Grounding semantics in olfactory perception. In <i>ACL (2)</i>, pp. 231&#8211;236. DOI: 10.3115/v1/P15-2038. 36</p>
<p class="ref">Y. Kim, H. Lee, and E. M. Provost. 2013. Deep learning for robust feature generation in audiovisual emotion recognition. In <i>Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</i>, pp. 3687&#8211;3691. IEEE. DOI: 10.1109/ICASSP.2013.6638346. 27, 28</p>
<p class="ref">R. Kiros, R. Salakhutdinov, and R. S. Zemel. 2015. Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models. <i>TACL</i>. 27, 30</p>
<p class="ref">B. Klein, G. Lev, G. Sadeh, and L. Wolf. 2015. Fisher Vectors Derived from Hybrid Gaussian-Laplacian Mixture Models for Image Annotation. In <i>CVPR</i>. DOI: 10.1109/CVPR.2015.7299073. 32</p>
<p class="ref">C. Kong, D. Lin, M. Bansal, R. Urtasun, and S. Fidler. 2014. What are you talking about? text-to-image coreference. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i>, pp. 3558&#8211;3565. DOI: 10.1.1.889.207&#38;rep=rep1&#38;type. 36</p>
<p class="ref"><a id="page_44"/>A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012. Imagenet classification with deep convolutional neural networks. In <i>Advances in neural information processing systems</i>, pp. 1097&#8211;1105. 23, 25</p>
<p class="ref">S. Kumar and R. Udupa. 2011. Learning hash functions for cross-view similarity search. In <i>IJCAI proceedings-international joint conference on artificial intelligence</i>, volume 22, p. 1360. DOI: 10.5591/978-1-57735-516-8/IJCAI11-230. 31</p>
<p class="ref">P. L. Lai and C. Fyfe. 2000. Kernel and nonlinear canonical correlation analysis. <i>International Journal of Neural Systems</i>, 10(05): 365&#8211;377. DOI: 10.1142/S012906570000034X. 32</p>
<p class="ref">A. Lazaridou, E. Bruni, and M. Baroni. 2014. Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world. In <i>ACL (1)</i>, pp. 1403&#8211;1414. DOI: 10.3115/v1/P14-1132. 37</p>
<p class="ref">A. Levin, P. Viola, and Y. Freund. 2003. Unsupervised improvement of visual detectors using cotraining. In <i>ICCV</i>. 33</p>
<p class="ref">Y. Li, S. Wang, Q. Tian, and X. Ding. 2015. A survey of recent advances in visual feature detection. <i>Neurocomputing</i>, 149: 736&#8211;751. DOI: 10.1016/j.neucom.2014.08.003. 23</p>
<p class="ref">R. Lienhart. 1999. Comparison of automatic shot boundary detection algorithms. In <i>Storage and Retrieval for Image and Video Databases (SPIE)</i>, pp. 290&#8211;301. 22</p>
<p class="ref">M. M. Louwerse. 2011. Symbol interdependency in symbolic and embodied cognition. <i>Topics in Cognitive Science</i>, 3(2): 273&#8211;302. DOI: 10.1111/j.1756-8765.2010.01106.x. 36</p>
<p class="ref">D. G. Lowe. 2004. Distinctive image features from scale-invariant keypoints. <i>International journal of computer vision</i>, 60(2): 91&#8211;110. DOI: 10.1023/B:VISI.0000029664.99615.94. 23</p>
<p class="ref">B. Mahasseni and S. Todorovic. 2016. Regularizing long short term memory with 3d human-skeleton sequences for action recognition. In <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, pp. 3054&#8211;3062. DOI: 10.1109/CVPR.2016.333. 34, 35</p>
<p class="ref">H. McGurk and J. MacDonald. 1976. Hearing lips and seeing voices. <i>Nature</i>, 264(5588): 746&#8211;748. 21</p>
<p class="ref">G. McKeown, M. F. Valstar, R. Cowie, and M. Pantic. 2010. The semaine corpus of emotionally coloured character interactions. In <i>Multimedia and Expo (ICME), 2010 IEEE International Conference on</i>, pp. 1079&#8211;1084. IEEE. DOI: 10.1109/ICME.2010.5583006. 22</p>
<p class="ref">T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In <i>Advances in neural information processing systems</i>, pp. 3111&#8211;3119. 25, 35</p>
<p class="ref">S. Moon, S. Kim, and H. Wang. 2015. Multimodal Transfer Deep Learning for Audio-Visual Recognition. <i>NIPS Workshops</i>. 34</p>
<p class="ref">Y. Mroueh, E. Marcheret, and V. Goel. 2015. Deep multimodal learning for audio-visual speech recognition. In <i>Acoustics, Speech and Signal Processing (ICASSP), 2015</i> <a id="page_45"/><i>IEEE International Conference on</i>, pp. 2130&#8211;2134. IEEE. DOI: 10.1109/ICASSP.2015.7178347. 27</p>
<p class="ref">P. Nakov and H. T. Ng. 2012. Improving statistical machine translation for a resource-poor language using related resource-rich languages. <i>Journal of Artificial Intelligence Research</i>, 44: 179&#8211;222. 34, 37</p>
<p class="ref">J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. 2011. Multimodal deep learning. In <i>Proceedings of the 28th international conference on machine learning (ICML-11)</i>, pp. 689&#8211;696. 21, 26, 27, 28, 34</p>
<p class="ref">M. A. Nicolaou, H. Gunes, and M. Pantic. 2011. Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space. <i>IEEE Transactions on Affective Computing</i>, 2(2): 92&#8211;105. DOI: 10.1109/T-AFFC.2011.9. 27, 30</p>
<p class="ref">W. Ouyang, X. Chu, and X. Wang. 2014. Multi-source deep learning for human pose estimation. In <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, pp. 2329&#8211;2336. DOI: 10.1109/CVPR.2014.299. 26, 27, 29</p>
<p class="ref">M. Palatucci, D. Pomerleau, G. E. Hinton, and T. M. Mitchell. 2009. Zero-shot learning with semantic output codes. In <i>Advances in neural information processing systems</i>, pp. 1410&#8211;1418. 34, 37</p>
<p class="ref">Y. Pan, T. Mei, T. Yao, H. Li, and Y. Rui. 2016. Jointly modeling embedding and translation to bridge video and language. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i>, pp. 4594&#8211;4602. DOI: 10.1109/CVPR.2016.497. 27, 30</p>
<p class="ref">B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In <i>Proceedings of the IEEE international conference on computer vision</i>, pp. 2641&#8211;2649. DOI: 10.1109/ICCV.2015.303. 36</p>
<p class="ref">S. S. Rajagopalan, L.-P. Morency, T. Baltru&#353;aitis, and R. Goecke. 2016. Extending long short-term memory for multi-view structured learning. In <i>European Conference on Computer Vision</i>, pp. 338&#8211;353. Springer. DOI: 10.1007/978-3-319-46478-7_21. 27, 30</p>
<p class="ref">J. Rajendran, M. M. Khapra, S. Chandar, and B. Ravindran. 2015. Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning. In <i>NAACL</i>. 34, 37</p>
<p class="ref">N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, G. R. Lanckriet, R. Levy, and N. Vasconcelos. 2010. A new approach to cross-modal multimedia retrieval. In <i>Proceedings of the 18th ACM international conference on Multimedia</i>, pp. 251&#8211;260. ACM. DOI: 10.1145/1873951.1873987. 32</p>
<p class="ref">M. Regneri, M. Rohrbach, D. Wetzel, S. Thater, B. Schiele, and M. Pinkal. 2013. Grounding Action Descriptions in Videos. <i>TACL</i>. ISSN 2307-387X. 36</p>
<p class="ref">R. Salakhutdinov and G. Hinton. 2009. Deep boltzmann machines. In <i>Artificial Intelligence and Statistics</i>, pp. 448&#8211;455. 28</p>
<p class="ref">M. E. Sargin, Y. Yemez, E. Erzin, and A. M. Tekalp. 2007. Audiovisual synchronization and fusion using canonical correlation analysis. <i>IEEE Transactions on Multimedia</i>, 9(7): 1396&#8211;1403. DOI: 10.1109/TMM.2007.906583. 32</p>
<p class="ref"><a id="page_46"/>A. Sarkar. 2001. Applying co-training methods to statistical parsing. In <i>Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies</i>, pp. 1&#8211;8. Association for Computational Linguistics. DOI: 10.3115/1073336.1073359. 33</p>
<p class="ref">B. Schuller, M. Valstar, F. Eyben, G. McKeown, R. Cowie, and M. Pantic. 2011. Avec 2011&#8211;the first international audio/visual emotion challenge. <i>Affective Computing and Intelligent Interaction</i>, pp. 415&#8211;424. 22</p>
<p class="ref">E. Shutova, D. Kiela, and J. Maillard. 2016. Black holes and white rabbits: Metaphor identification with visual features. In <i>HLT-NAACL</i>, pp. 160&#8211;170. DOI: 10.18653/v1/N16-1020. 34, 36</p>
<p class="ref">C. Silberer and M. Lapata. 2012. Grounded models of semantic representation. In <i>Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</i>, pp. 1423&#8211;1433. Association for Computational Linguistics. 36</p>
<p class="ref">C. Silberer and M. Lapata. 2014. Learning grounded meaning representations with autoencoders. In <i>ACL (1)</i>, pp. 721&#8211;732. DOI: 10.3115/v1/P14-1068. 27, 28</p>
<p class="ref">M. Slaney and M. Covell. 2001. Facesync: A linear operator for measuring synchronization of video facial images and audio tracks. In <i>Advances in Neural Information Processing Systems</i>, pp. 814&#8211;820. 32</p>
<p class="ref">C. G. Snoek and M. Worring. 2005. Multimodal video indexing: A review of the state-of-the-art. volume 25, pp. 5&#8211;35. Springer. DOI: 10.1023/B:MTAP.0000046380.27575.a5. 21, 22</p>
<p class="ref">R. Socher and L. Fei-Fei. 2010. Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora. In <i>Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</i>, pp. 966&#8211;973. IEEE. DOI: 10.1109/CVPR.2010.5540112. 37</p>
<p class="ref">R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. 2013. Zero-shot learning through cross-modal transfer. In <i>Advances in neural information processing systems</i>, pp. 935&#8211;943. 34, 37</p>
<p class="ref">R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. <i>Transactions of the Association for Computational Linguistics</i>, 2: 207&#8211;218. 30</p>
<p class="ref">N. Srivastava and R. Salakhutdinov. 2012a. Learning representations for multimodal data with deep belief nets. In <i>International conference on machine learning workshop</i>. 28</p>
<p class="ref">N. Srivastava and R. R. Salakhutdinov. 2012b. Multimodal learning with deep boltzmann machines. In <i>Advances in neural information processing systems</i>, pp. 2222&#8211;2230. 23, 27, 29, 34</p>
<p class="ref">N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. <i>Journal of machine learning research</i>, 15(1): 1929&#8211;1958. 28</p>
<p class="ref"><a id="page_47"/>H.-I. Suk, S.-W. Lee, D. Shen, A. D. N. Initiative, et al. 2014. Hierarchical feature representation and multimodal fusion with deep learning for ad/mci diagnosis. <i>NeuroImage</i>, 101: 569&#8211;582. DOI: 10.1016/j.neuroimage.2014.06.077. 29</p>
<p class="ref">G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nicolaou, B. Schuller, and S. Zafeiriou. 2016. Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network. In <i>Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</i>, pp. 5200&#8211;5204. IEEE. DOI: 10.1109/ICASSP.2016.7472669. 25</p>
<p class="ref">M. Valstar, B. Schuller, K. Smith, F. Eyben, B. Jiang, S. Bilakhia, S. Schnieder, R. Cowie, and M. Pantic. 2013. Avec 2013: the continuous audio/visual emotion and depression recognition challenge. In <i>Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge</i>, pp. 3&#8211;10. ACM. DOI: 10.1145/2512530.2512533. 22</p>
<p class="ref">I. Vendrov, R. Kiros, S. Fidler, and R. Urtasun. 2016. Order-Embeddings of Images and Language. In <i>ICLR</i>. 25, 27, 31</p>
<p class="ref">S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and K. Saenko. 2015. Translating Videos to Natural Language Using Deep Recurrent Neural Networks. <i>NAACL</i>. 29</p>
<p class="ref">D. Wang, P. Cui, M. Ou, and W. Zhu. 2015a. Deep multimodal hashing with orthogonal regularization. In <i>IJCAI</i>, pp. 2291&#8211;2297. 26, 28</p>
<p class="ref">J. Wang, H. T. Shen, J. Song, and J. Ji. 2014. Hashing for similarity search: A survey. <i>arXiv preprint arXiv:1408.2927</i>. 31</p>
<p class="ref">L. Wang, Y. Li, and S. Lazebnik. 2016. Learning deep structure-preserving image-text embeddings. In <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, pp. 5005&#8211;5013. 31</p>
<p class="ref">W. Wang, R. Arora, K. Livescu, and J. Bilmes. 2015b. On deep multi-view representation learning. In <i>Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</i>, pp. 1083&#8211;1092. 27, 32</p>
<p class="ref">J. Weston, S. Bengio, and N. Usunier. 2011. Wsabie: Scaling up to large vocabulary image annotation. In <i>IJCAI</i>, volume 11, pp. 2764&#8211;2770. DOI: 10.5591/978-1-57735-516-8/IJCAI11-460. 30</p>
<p class="ref">D. Wu and L. Shao. 2014. Multimodal dynamic networks for gesture recognition. In <i>Proceedings of the 22nd ACM international conference on Multimedia</i>, pp. 945&#8211;948. ACM. DOI: 10.1145/2647868.2654969. 29</p>
<p class="ref">Z. Wu, Y.-G. Jiang, J. Wang, J. Pu, and X. Xue. 2014. Exploring inter-feature and interclass relationships with deep neural networks for video classification. In <i>Proceedings of the 22nd ACM international conference on Multimedia</i>, pp. 167&#8211;176. ACM. DOI: 10.1145/2647868.2654931. 27</p>
<p class="ref">R. Xu, C. Xiong, W. Chen, and J. J. Corso. 2015. Jointly modeling deep video and compositional text to bridge vision and language in a unified framework. In <i>AAAI</i>, volume 5, p. 6. 27, 30</p>
<p class="ref"><a id="page_48"/>P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. <i>Transactions of the Association for Computational Linguistics</i>, 2: 67&#8211;78. 31</p>
<p class="ref">H. Yu and J. M. Siskind. 2013. Grounded language learning from video described with sentences. In <i>ACL (1)</i>, pp. 53&#8211;63. 36</p>
<p class="ref">B. P. Yuhas, M. H. Goldstein, and T. J. Sejnowski. 1989. Integration of acoustic and visual speech signals using neural networks. <i>IEEE Communications Magazine</i>, 27(11): 65&#8211;71. DOI: 10.1109/35.41402. 21</p>
<p class="ref">D. Zhang and W.-J. Li. 2014. Large-scale supervised multimodal hashing with semantic correlation maximization. In <i>AAAI</i>, volume 1, p. 7. 32</p>
<p class="ref">H. Zhang, Z. Hu, Y. Deng, M. Sachan, Z. Yan, and E. P. Xing. 2016. Learning concept taxonomies from multi-modal data. <i>arXiv preprint arXiv:1606.09239</i>. 31</p>
<p class="line"/>
<p class="note"><a id="fn1" href="#rfn1">1</a>. <a href="http://www.youtube.com/intl/en-US/yt/about/press/">http://www.youtube.com/intl/en-US/yt/about/press/</a> (accessed May 2018)</p>
<p class="note"><a id="fn2" href="#rfn2">2</a>. <a href="http://www.nist.gov/multimodal-information-group/trecvid-multimedia-event-detection-2011-evaluation">http://www.nist.gov/multimodal-information-group/trecvid-multimedia-event-detection-2011-evaluation</a> (accessed May 2018)</p>
</body>
</html>