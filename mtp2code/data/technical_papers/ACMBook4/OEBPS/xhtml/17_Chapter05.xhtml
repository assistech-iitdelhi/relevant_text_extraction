<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_131"/>5</p>
<p class="chtitle"><b>Multimodal User State and Trait Recognition: An Overview</b></p>
<p class="chauthor"><b>Bj&#246;rn Schuller</b></p>
<p class="h1"><a id="ch5_1"/><b><span class="bg1">5.1</span>&#160;&#160;&#160;&#160;Introduction</b></p>
<p class="noindent">It seems intuitive, if not obvious, that for intelligent interaction and communication between technical systems and human users the knowledge of the user states and traits (for definitions see the Glossary) is beneficial, if not required, on the system&#8217;s end. Economist Peter Drucker&#8217;s words seem quite inspiring in this context:</p>
<p class="center">The most important thing in communication is hearing what isn&#8217;t said.</p>
<p class="indent">Thus, acquiring information as to the emotional, cognitive, or physical load level (see also <a href="23_Chapter10.xhtml">Chapters 10</a> and <a href="24_Chapter11.xhtml">11</a>), degree of sleepiness or intoxication, or health state&#8212;alongside the age, gender, personality, or ethnicity, etc. from the sound of a user&#8217;s speech and other information streams available may help to increase the flow of the interaction and allow an interface to adapt on all sorts of levels. While one might ask directly for such information from the user&#8212;e.g., age or gender&#8212;this may often be inefficient, cumbersome, or inappropriate as time is precious, and modern intelligent systems are increasingly expected to show similar emotional and social intelligence (see <a href="19_Chapter07.xhtml">Chapter 7</a>) capabilities as one would expect from a human. Indeed, in comparison to a human they often have access to a richer amount of information in these days of increasingly &#8220;big&#8221; data that may be collected comparably effortlessly from an ever-growing amount of ambient, body-worn, or contact-less sensors. Traditionally, these include the more &#8220;natural&#8221; sensors in the sense that the human has these available as well as audio, video, and tactile interaction. In addition, bio-parameters such as the heart rate, skin conductance, <a id="page_132"/>thermal images, accurate movement data, or fingerprint fall less into what a human would usually have available, but systems such as &#8220;simple&#8221; smartphones do have astonishing amounts of such information available. Interestingly, however, on the other hand the human relies on some information such as the olfactory channel that technical systems do not (yet?) employ on a broader basis. Untouched by that, technical systems still mostly fall behind human abilities when it comes to assessing user&#8217;s states and traits. The general opinion and experience is that, given a multimodal approach, such recognition can be rendered more robust.</p>
<p class="indent">In this chapter, an overview on multimodal user state and trait recognition is given starting off with modeling and the definition of states and traits. Next, states and traits already considered for multimodal automatic assessment are shown. Then, architectures for a synergistic technical processing and fusion of these are discussed including a modern view on how such systems could be realized. In the sections that follow, an overview on modalities and their peculiarities and requirements as well as strengths is provided, namely: spoken and written language; video including facial expression, body posture, and movement; fingerprints; physiological signals such as brain waves; and tactile interaction data. Recent trends and potential for future improvement are provided at the end of this chapter.</p>
<p class="h1"><a id="ch5_2"/><b><span class="bg1">5.2</span>&#160;&#160;&#160;&#160;Modeling</b></p>
<p class="noindent">Here, we first consider how states and traits or more generally speaking person or user attributes can be classified. In Schuller and Batliner [2013], a dozen of exemplary &#8220;taxonomies&#8221; are given including the time aspect used here to group the tasks of interest into <i><b>long-term traits, longer-term states,</b></i> and <i><b>short-term states</b></i> (see Glossary). The tasks further need to be represented in a way that can best be automatically assessed. Usually, one decides between discrete classes suited for classification and a <i><b>(pseudo-) continuous representation</b></i> suited for regression (see Glossary).</p>
<p class="h1"><a id="ch5_3"/><b><span class="bg1">5.3</span>&#160;&#160;&#160;&#160;An Overview on Attempted Multimodal Stait and Trait Recognition</b></p>
<p class="noindent">A broader variety of states and traits of users or human individuals in general already has been considered for automatic assessment. <a href="#tab5_1">Table 5.1</a> provides a selection of typical ones including exemplary literature references and modality combinations considered in those publications. The table is grouped into states and traits.</p>
<p class="indent">A well-defined benchmark for obtainable results is given by the challenge events organized in this field, mostly focused on affect. The first audiovisual challenge was <a id="page_133"/>the 2011 Audio/Visual Emotion Challenge and Workshop (AVEC) [Schuller et al. 2011b]. Binary above/below average decisions for the activity (arousal), expectation, power, and valence dimensions were made on the frame- or word-level. In 2012, the same data was re-used as a fully continuous task. The same data was labeled in time and value continuous perceived personality dimensions, attractiveness, likability, and engagement for the first audiovisual personality challenge (MAPTRAITS) [Gunes et al. 2014]. The AVEC 2013 and 2014 follow-ups introduced audiovisual depression recognition. The 2015 edition was the first to introduce physiology (AV+EC 2015) alongside audio and video for affect acquisition in a challenge event since further tasks have been run in the series including depression and sentiment from audiovisual data. Similar challenges exist, such as EmotiW or MediaEval, which are, however, based on multimedia such as TV material rather than user interaction data.</p>
<div class="box">
<p class="bhead"><b>Glossary</b></p>
<p class="hangbx"><b>Continuous or discrete (i.e., categorical) representation</b> refer to the modeling of a user state or trait. As an example, the age of a user can be modeled as continuum such as the age in years. As opposed to this, a discretized representation would be broader age classes such as &#8220;young,&#8221; &#8220;adult&#8217;s&#8221;, and &#8220;elderly.&#8221; In addition, the time can be discretized or continuous (in fact, it is always discretized in some respect&#8212;at least by the sample rate of the digitized sampling of the sensor signals). However, one would speak of continuous measurement if processing is delivering a continuous output stream on a (short) frame-by-frame basis rather than an asynchronous processing of (larger) segments or chunks of the signal such as per spoken word or per body gesture.</p>
<p class="hangbx">The user <b>(long-term) traits</b> include biological trait primitives (e.g., age, gender, height, weight), cultural trait primitives in the sense of group/ethnicity membership (e.g,. culture, race, social class, or linguistic concepts such as dialect or first language), personality traits (e.g., the &#8220;OCEAN big five&#8221; dimensions openness, conscientiousness, extraversion, agreeableness, and neuroticism or likability), and traits that constitute subject idiosyncrasy, i.e., ID.</p>
<p class="hangbx">A <b>longer-term state</b> can subsume (partly self-induced) non-permanent, yet longer-term states (e.g., sleepiness, intoxication, mood such as depression (see also Chapter 12) the health state such as having a flu), structural (behavioral, interactional, social) signals (e.g., role in dyads and groups, friendship and identity, positive/negative attitude, intimacy, interest, politeness), and (non-verbal) social signals (see Chapters 7 and 8) and discrepant signals (e.g., deception (see also Chapter 13) irony, sarcasm, sincerity).</p>
<p class="hangbx">A <b>pseudo-multimodal</b> approach exploits a modality not only by itself, but in addition to estimate another modality&#8217;s behavior to replace it. An example is estimating the heart rate from speech parameters and using it alongside (other) speech parameters.</p>
<p class="hangbx">A <b>short-term state</b> includes the mode (e.g., speaking style and voice quality), emotions, and affects (e.g., confidence, stress, frustration, pain, uncertainty, see also Chapters 6 and 8).</p>
</div>
<p class="tcaption" id="tab5_1"><a id="page_134"/><b>Table 5.1</b>&#160;&#160;&#160;&#160;Examples of user or subject states and traits attempted for automatic recognition in a multimodal way in the literature. Several combinations of modalities are contained in these examples such as face and fingerprint, face and gait, speech and visual cues, visual cues and driving data in the car, or physiology in combination with some of the above. The engines partially assess single users, sometimes also groups.</p>
<table class="table1">
<tr>
<td><p class="tab1">Trait</p></td>
<td><p class="tab1">Reference</p></td>
</tr>
<tr>
<td><p class="tab1">Age</p></td>
<td><p class="tab1">[Hofmann et al. 2013]</p></td>
</tr>
<tr>
<td><p class="tab1">Attractiveness</p></td>
<td><p class="tab1">[Gunes et al. 2014]</p></td>
</tr>
<tr>
<td><p class="tab1">Ethnicity</p></td>
<td><p class="tab1">[Lu et al. 2005]</p></td>
</tr>
<tr>
<td><p class="tab1">Gender</p></td>
<td><p class="tab1">[Li et al. 2010b], [Shan et al. 2007, Shan et al. 2008], [Huang and Wang 2007], [Matta et al. 2008], [Hofmann</p>
<p class="tab1">et al. 2013]</p></td>
</tr>
<tr>
<td><p class="tab1">Height</p></td>
<td><p class="tab1">[Hofmann et al. 2013]</p></td>
</tr>
<tr>
<td><p class="tab1">ID</p></td>
<td><p class="tab1">[Ko 2005, Lu et al. 2005, Ceting&#252;l et al. 2006, Sargin</p>
<p class="tab1">et al. 2006, Farr&#250;s et al. 2007]</p></td>
</tr>
<tr>
<td><p class="tab1">Leader</p></td>
<td><p class="tab1">[Sanchez-Cortes et al. 2013]</p></td>
</tr>
<tr>
<td><p class="tab1">Likability</p></td>
<td><p class="tab1">[Gunes et al. 2014]</p></td>
</tr>
<tr>
<td><p class="tab1">Nativeness</p></td>
<td><p class="tab1">[Georgakis et al. 2014]</p></td>
</tr>
<tr>
<td><p class="tab1">Personality</p></td>
<td><p class="tab1">[Pianesi et al. 2008, Batrinca et al. 2011, Batrinca et al.</p>
<p class="tab1">2012]</p></td>
</tr>
<tr>
<td><p class="tab1">State</p></td>
<td><p class="tab1">Reference</p></td>
</tr>
<tr>
<td><p class="tab1">Alertness</p></td>
<td><p class="tab1">[Abouelenien et al. 2015]</p></td>
</tr>
<tr>
<td><p class="tab1">Cognitive Load</p></td>
<td><p class="tab1">[Putze et al. 2010]</p></td>
</tr>
<tr>
<td><p class="tab1">Deception</p></td>
<td><p class="tab1">[Qin et al. 2005]</p></td>
</tr>
<tr>
<td><p class="tab1">Depression</p></td>
<td><p class="tab1">[Cohn et al. 2009]</p></td>
</tr>
<tr>
<td><p class="tab1">Distraction</p></td>
<td><p class="tab1">[W&#246;llmer et al. 2011]</p></td>
</tr>
<tr>
<td><p class="tab1">Drowsiness</p></td>
<td><p class="tab1">[Andreeva et al. 2004]</p></td>
</tr>
<tr>
<td><p class="tab1">Emotion</p></td>
<td><p class="tab1">[Schuller et al. 2011b]</p></td>
</tr>
<tr>
<td><p class="tab1">Engagement</p></td>
<td><p class="tab1">[Gunes et al. 2014]</p></td>
</tr>
<tr>
<td><p class="tab1">Interest</p></td>
<td><p class="tab1">[Schuller et al. 2009]</p></td>
</tr>
<tr>
<td><p class="tab1">Laughter</p></td>
<td><p class="tab1">[Melder et al. 2007]</p></td>
</tr>
<tr>
<td><p class="tab1">Physical Activity</p></td>
<td><p class="tab1">[Maurer et al. 2006, Li et al. 2010a, McCowan et al.</p>
<p class="tab1">2005]</p></td>
</tr>
<tr>
<td><p class="tab1">Sentiment</p></td>
<td><p class="tab1">[W&#246;llmer et al. 2013]</p></td>
</tr>
<tr>
<td><p class="tab1">Stress</p></td>
<td><p class="tab1">[Bo&#345;il et al. 2012, Sharma and Gedeon 2012]</p></td>
</tr>
<tr>
<td><p class="tab1">Swallowing</p></td>
<td><p class="tab1">[Amft and Tr&#246;ster 2006]</p></td>
</tr>
<tr>
<td><p class="tab1">Violence</p></td>
<td><p class="tab1">(MediaEval)</p></td>
</tr>
</table>
<p class="h1"><a id="page_135"/><a id="ch5_4"/><b><span class="bg1">5.4</span>&#160;&#160;&#160;&#160;Architectures</b></p>
<p class="noindent">The typical flow of processing in user classification with respect to her or his states and traits is shown in <a href="#fig5_1">Figure 5.1</a>. In the sections that follow, a short step-by-step description following the typical sequence of processing is given in accordance with this figure (for a more detailed description, see Schuller [2013]). However, several blocks are optional and the order of steps may (slightly) vary in some parts of this chain such as whether features of several modalities are fused, first, and commonly enhanced or the other way around.</p>
<div class="cap" id="fig5_1">
<p class="image"><img src="../images/fig5_1.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 5.1</b>&#160;&#160;&#160;&#160;Workflow of a state-of-the art user state and trait analyser for arbitrary modalities. Dark blue boxes indicate mandatory units, light blue ones typical further units, and lighter blue optional ones. Red indicates external units and light red units interface to other modalities and contextual information and knowledge bases. External connections are indicated by arrows from/to the outside.</p>
</div>
<p class="h2"><a id="page_136"/><a id="ch5_4_1"/><b><span class="bg2">5.4.1</span>&#160;&#160;&#160;&#160;Capture</b></p>
<p class="noindent">The first block &#8220;capture&#8221; in <a href="#fig5_1">Figure 5.1</a> is connected to a sensor such as a microphone, camera, or bio-sensor. The data is made available as digital value and time-quantized &#8220;raw&#8221; signal at this stage. The major parameters of interest are usually the sample rate and the word length for quantization as well as potential encoding schemes.</p>
<p class="h2"><a id="ch5_4_2"/><b><span class="bg2">5.4.2</span>&#160;&#160;&#160;&#160;Signal-level Fusion</b></p>
<p class="noindent">A rather unusual option for fusion of multiple modalities is already given at this early stage. This could, for example, be the case for several bio-sensors. In practice, one would also speak of signal-level fusion if the merging of signals took place after the next step, namely the pre-processing.</p>
<p class="h2"><a id="ch5_4_3"/><b><span class="bg2">5.4.3</span>&#160;&#160;&#160;&#160;Pre-processing</b></p>
<p class="noindent">During pre-processing, the signal of interest is enhanced, e.g., if in the presence of noise. A range of methods can be employed depending on the modality such as independent component analysis if several independent sensors are available for the same modality (e.g., a microphone array for audio sensing) or blind source separation, e.g., by non-negative matrix factorization or usage of deep learning such as by autoencoders (see also <a href="15_Chapter04.xhtml">Chapter 4</a>). Simpler efforts include filtering.</p>
<p class="h2"><a id="ch5_4_4"/><b><span class="bg2">5.4.4</span>&#160;&#160;&#160;&#160;Frame-level Features</b></p>
<p class="noindent">The pre-processing is usually followed by feature extraction. In user state and trait recognition often one finds three sampling rates: the first is the one of the sampling of the analogue sensor signal, e.g., at 8 kHz or 16 kHz for speech. The second sampling with a larger window of analysis is found at this level, e.g., at 100 Hz for speech or 20&#8211;30 Hz (or &#8220;frames per second&#8221;) in the case of video. The third one follows in the next subsection on the segment-level. Based on this windowing, features are extracted per frame (thus, the name &#8220;frame-level&#8221; features) such as energy, fundamental frequency, zero-crossing rates, spectral characteristics, and many such tailored to the characteristics of the signal. In fact, the frame length may vary with the frame-level feature of interest. This process is often carried out hierarchically such as also calculating first-order derivatives (the &#8220;delta&#8221; (regression) coefficients) or correlations across frame-level feature contours. Also, further (potentially data-driven) quantization could take place on this level.</p>
<p class="h2"><a id="page_137"/><a id="ch5_4_5"/><b><span class="bg2">5.4.5</span>&#160;&#160;&#160;&#160;Segmentation</b></p>
<p class="noindent">In off-line, non-continuous signal analysis, the data is (pre-)segmented, e.g., by an event such as a spoken word or a facial or body action unit (see also <a href="14_Chapter03.xhtml">Chapter 3</a>). In real-life use-cases, however, the data stream usually has to be segmented automatically [Gunes and Pantic 2010]. Such &#8220;chunking&#8221; clusters a mostly varying number of frames to segments (or chunks) of analysis such as the named words or action units. In the case of states or perceived traits (i.e., the impression of the trait varies over time), one would desire for the chunk to start and end with the state or perception. Yet, this is usually not feasible, as one would not know the beginning and end at this stage. Thus, it is often related to the named words (e.g., based on voice activity detection) or action units (e.g., based on Bayesian information criterion), etc., depending on the modality. In addition, the desire for on-line quick reactions asks for short segment lengths (e.g., one second or slightly below, [Chanel et al. 2009]) in contrast to the desire for longer such to ensure higher robustness [Berntson et al. 1997, Salahuddin et al. 2007]. That is, just as in Heisenberg&#8217;s uncertainty relation, one cannot ensure both at a time: maximum resolution in time and maximum accuracy for the estimation of the target.</p>
<p class="h2"><a id="ch5_4_6"/><b><span class="bg2">5.4.6</span>&#160;&#160;&#160;&#160;Segment-level Features</b></p>
<p class="noindent">On the segment-level, higher-level features are calculated by the application of functionals to the frame-level contour [Schuller 2013]. Likewise, the time-series of (usually) varying length is projected onto a scalar per segment-level feature and a single-feature vector independent of the length of the segment, overall. Often, these are also called &#8220;supra-segmental&#8221; features (accordingly naming the framelevel features segment-level features). Such functionals usually include frequencies of occurence (e.g., of words), and histograms, moments, extremes, peaks, segments, and many others. This is often done in &#8220;brute force&#8221; manner producing up to several thousands of features to next reduce this feature variety to those being most salient. This often includes &#8220;hierarchical functionals&#8221; such as the mean of extremes of peak points, etc. Again, also on this level (potentially data driven) quantization could take place in the sense of a hierarchical functional: after calculation of the functionals, a vector quantization functional could be applied to parts of or the entire so far calculated feature vector [Pokorny et al. 2015]. Note that different segment lengths could be used for different segment-level features. Finally, consider that even feature-free approaches have recently been proven successful in this field by &#8220;end-to-end&#8221; learning (see also <a href="15_Chapter04.xhtml">Chapter 4</a>) from the signal directly through to the user state or trait [Trigeorgis et al. 2016].</p>
<p class="h2"><b>5.4.7</b> <a id="page_138"/><b>Reduction</b></p>
<p class="noindent">The reduction of features helps reduce complexity for the following machine learning algorithm. A (usually lower-dimensional) new feature space is computed based on some suited transformation such as principle component analysis or linear discriminant analysis and variants. Note that, even if one projects into a lower dimensional space or does not keep all components of the new space (see <a href="#ch5_4_8">Section 5.4.8</a>), the individual components&#8217; calculation usually still requires all original features, i.e., the extraction effort is increased (and not reduced), as one needs to calculate the original space <i>and</i> apply the transformation. Thus, the aim in fact is to reduce complexity (by reducing the number of free parameters to be trained) of the learning algorithm (cf. below). Strictly speaking, this step could thus be called transformation. As such, it relates to the hierarchical functional principle, as for example principle components can be considered as a &#8220;weighted sum of functionals&#8221; functional. Yet, like the quantization functional named previously, it would be data-based, as the weights need to be learned at first.</p>
<p class="h2"><a id="ch5_4_8"/><b><span class="bg2">5.4.8</span>&#160;&#160;&#160;&#160;Selection and Generation</b></p>
<p class="noindent">The selection of features can take place in the original space, a transformed space, or across combinations of these. In addition, a random-injected generation of new features can take place during an &#8220;exploratory feature selection&#8221; such as by genetic algorithms with a (randomized) feature generation step. Usually, one cannot explore all possible feature combinations when searching for the optimal feature space. Thus, one requires (a) a search function and (b) some quality measure of the value of a certain feature sub-ensemble. Popular search functions comprise greedy and floating searches, e.g., in forward (i.e., starting with no feature and enlarging the set gradually), and backward direction (i.e., starting with all features and reducing the size gradually) or bidirectionally. Other search functions include random and genetic searches. Popular measures of the value of feature combinations comprise the target learning algorithm&#8217;s accuracy (so-called &#8220;wrapper search&#8221;) or other measures such as correlation between the feature and the target, information-theoretic measures, and alike. It is important to also consider inter-correlation of features within a selected feature set aiming at keeping it low in dimension (i.e., one should not select slight variations of a highly predictive feature only, but aim at a synergistic feature compound&#8212;or, as a metaphor, you cannot play soccer with top defenders only&#8212;someone also has to be able to shoot a goal).</p>
<p class="h2"><a id="page_139"/><a id="ch5_4_9"/><b><span class="bg2">5.4.9</span>&#160;&#160;&#160;&#160;Adaptation and Enhancement</b></p>
<p class="noindent">Just as on the signal level, one can enhance also on (any of) the feature levels. Popular methods include training a classifier to predict &#8220;clean&#8221; features from noisy features such that features extracted from a corrupted signal serve as training input and such extracted from the non-corrupted signal as learning target. In <a href="#fig5_1">Figure 5.1</a>, the according box is shown only once, but each extractor can write to the data storage from which this module can read. In addition, also any form of adaptation such as to the subject or the current noise-level can take place, e.g., by subtracting a mean typical for the current subject or noise condition.</p>
<p class="h2"><a id="ch5_4_10"/><b><span class="bg2">5.4.10</span>&#160;&#160;&#160;&#160;Feature-level Fusion</b></p>
<p class="noindent">The fusion of multiple modalities can also take place on the feature level. This can be any of the feature types, i.e., on the frame or segment-level. While unusual, it could also be on both levels. The fusion aspect is also described in more detail, e.g., in <a href="12_Chapter01.xhtml">Chapters 1</a> and <a href="13_Chapter02.xhtml">2</a>.</p>
<p class="h2"><a id="ch5_4_11"/><b><span class="bg2">5.4.11</span>&#160;&#160;&#160;&#160;Crowd Sourcing</b></p>
<p class="noindent">An ever-ongoing bottleneck is the amount of <i>labeled</i> data for training of an automatic user state and trait acquisition system. This is even more true for multimodal data, in particular if &#8220;less usual&#8221; modalities are involved. Crowd sourcing is a popular approach these days to quickly reach a reasonable amount of such data. Crowd sourcing thereby can be used both to collect the actual multimodal data and to collect the labels such as the emotion or personality perceived by humans in data. A popular first &#8220;pay per click&#8221; crowd sourcing platform was Amazon&#8217;s Mechanical Turk; however, recently a rich variety of alternatives including gamified versions and such without payment of annotators exist. An elegant way is to source information directly from the users. The reason for the box being integrated in <a href="#fig5_1">Figure 5.1</a> is that, ideally, the crowd sourcing should be embedded in an ever-ongoing efficient learning of a system keeping humans in the loop.</p>
<p class="h2"><a id="ch5_4_12"/><b><span class="bg2">5.4.12</span>&#160;&#160;&#160;&#160;Decision</b></p>
<p class="noindent">At this point, the actual decision on a user state or trait is made. Given the previously sketched data scarcity for some (multimodal) user state and trait acquisition tasks (e.g., for less usual states and traits or atypical populations) make &#8220;zero&#8221;-resource approaches interesting. In such a model, rules are employed such as female users should have a higher pitch on average. However, data-trained models are usually exceeding the performance reached by such approaches&#8212;they base their decision <a id="page_140"/>on a comparison with training instances or on a model trained based on the training instances.</p>
<p class="h2"><a id="ch5_4_13"/><b><span class="bg2">5.4.13</span>&#160;&#160;&#160;&#160;Confidence Estimation</b></p>
<p class="noindent">In addition to the result of the acquisition such as &#8220;the user is interested,&#8221; an interface can benefit from the additional provision of a degree of confidence such as &#8220;I am <i>certain</i> that the user is interested.&#8221; Ideally, such confidence measures are determined independently of the actual decision, such as by training an independent system to recognize misclassifications of the system that classifies the user&#8212;potentially also in a semi-supervized way (see below) [Deng and Schuller 2012]. In the best case, the system is provided with a confidence per class such that based on context it may prefer the second best result as the delta in confidence might be small, but the second best result simply better fits the current situation.</p>
<p class="h2"><a id="ch5_4_14"/><b><span class="bg2">5.4.14</span>&#160;&#160;&#160;&#160;Decision-level Fusion</b></p>
<p class="noindent">After the decision (per modality) one can also fuse the different modalities based on their individual results. This referred to as <i>late fusion</i>. Early fusion options along the chain shown in <a href="#fig5_1">Figure 5.1</a> is best carried out in a &#8220;soft&#8221; fashion including the confidence levels per class and modality. For a final decision, a voting scheme&#8212;potentially weighted by the confidences&#8212;may be employed. Alternatively, a learning algorithm may be trained with data to allow for more complex and potentially non-linear decision strategies in case of &#8220;disagreement&#8221; between modalities. As pointed out above, more information on fusion is found, e.g., in <a href="12_Chapter01.xhtml">Chapter 1</a>.</p>
<p class="h2"><a id="ch5_4_15"/><b><span class="bg2">5.4.15</span>&#160;&#160;&#160;&#160;Encoding</b></p>
<p class="noindent">The result has to be made readable to the interface with the application in some way. Ideally, standards are followed to allow easy re-usage of a user state and trait recognition engine with various human-machine interfaces. Existing standards comprise such for annotation of emotion, e.g., EARL [Schr&#246;der et al. 2006]or the follow-up W3C EmotionML [Burkhardt et al. 2016] recommendation. Owing to the option that individual categories and dimensions can be specified, the standard can be used to encode a broad range of user states and traits. A more broadly defined standard is the W3C Extensible MultiModal Annotation (EMMA) markup language [Baggia et al. 2007]. However, manifold alternatives exist.</p>
<p class="h2"><a id="ch5_4_16"/><b><span class="bg2">5.4.16</span>&#160;&#160;&#160;&#160;Interface</b></p>
<p class="noindent">The interface is not an actual part of the engine for the recognition of user states and traits, but rather shown in <a href="#fig5_1">Figure 5.1</a> to indicate where the communication with <a id="page_141"/>it takes part. However, given that it should ideally feed back knowledge such as its current state and the reaction of the user for integration of contextual knowledge, as well as receive demands from the engine such as when to ask for a certain label or information concerning a data of interest to the engine (see &#8220;Off- and On-line Learning&#8221; below) enabling of reinforcement or (active and) supervized learning from user feedback, it can be seen as part of the chain of processing.</p>
<p class="h2"><a id="ch5_4_17"/><b><span class="bg2">5.4.17</span>&#160;&#160;&#160;&#160;Off- and Online Learning</b></p>
<p class="noindent">If one does not rely on a mere rule set for decisions as described previously, learning of a suited machine learning algorithm will be needed. There is a broad selection of popular such algorithms including (deep) neural networks, support vector machines, or hidden Markov models, and more general graphical models&#8212;a broad body of literature exists on these&#8212;for an entry within this domain see [Schuller and Batliner 2013]. Today, machine learning in the field is almost exclusively carried out in a supervized manner. This means that human labeled data are presented offline only once to the recognition engine which learns in one pass from it. However, William Osler&#8217;s advice to humans can also be given to current and future multimodal person state and trait recognition systems:</p>
<p class="quote">&#8220;Observe, record, tabulate, communicate. Use your five senses. Learn to see, learn to hear, learn to feel, learn to smell, and know that by practice alone you can become expert.&#8221;</p>
<p class="indent">This alludes to the idea to keep the recognition engine learning throughout its usage time&#8212;potentially only by &#8220;practice.&#8221; This will make sense in many ways, as usually training data exactly from the usage domain is particularly scarce. Also, many user interfaces are coined by longitudinal interaction, i.e., the interface would miss quite a chance to &#8220;get to know the user&#8221; if it would not exploit the opportunity to keep learning about her or him during interactions. To design this learning process most efficient, the engine has to either partially learn by itself or with help from the user or the crowd in a targeted way.</p>
<p class="indent">Learning by the engine itself includes semi-supervized learning that uses machine labels [Davidov et al. 2010]&#8212;ideally only of data instances with a high confidence and potentially considering multiple modalities [Zhang et al. 2016b]. Semi-supervized learning can also be used to cross-label data if several states and traits are targeted, and for some data instances some target labels are missing [Zhang et al. 2016a]. Alternatively, reinforcement learning can be used [Sutton and Barto 1998], where in contrast to supervized learning no correct data instance/label pairs are given, sub-optimal actions are not explicitly corrected. Instead, some indirect <a id="page_142"/>form of feedback is exploited such as the reaction of the user. Imagine the interface is assuming the user to be of an elderly user group. Accordingly, it addresses the user rather formal. If the user in her reaction gives the impression that this was expected, the user state and trait recognition engine would learn reinforced from this observation that the decision of the user&#8217;s age group was correct for future reference. While this type of learning has not yet been exploited in user state and trait recognition, it seems an obvious efficient avenue in the context of interaction. A key challenge in reinforcement learning is usually designing the reward function [Berridge 2000], which is decisive for the success given the high level of involved uncertainty&#8212;luckily, it can be learned itself from data [El Asri et al. 2012]. Already used for state and trait classification [Zhang et al. 2015a], <i>active learning</i> asks directly for human help, but only in pre-selected cases. Likewise, rather than asking for every user response about, e.g., the user emotion, the engine only asks when it feels the data is of particular interest, e.g., if the expected change in its model parameters is significant [Settles et al. 2008], or if it assumes that the current data instance could be new or rare in its type (e.g., not &#8220;again&#8221; (just) of neutral emotion, but of some other rarely seen emotion). Thereby, it can additionally learn how much to trust a user to design the process even more efficient [Zhang et al. 2015a]. Active and semi-supervized learning united increase the efficiency, as the part of the data the computer can label by itself based on high confidence does not require aid by the user, but the less certain, yet interesting, cases can be solved with aid from the user. This is known as <i>cooperative learning</i> [Zhang et al. 2015b]. Without human labeling or reinforcment, <i>unsupervized learning</i> allows to &#8220;blindly&#8221; cluster the data instances by distances in the feature space and whatnots. To avoid clustering mainly by the largest variation in the data, hierarchical clustering approaches can cluster without supervision into different states and traits such as identity and emotion [Trigeorgis et al. 2014]. Further, the currently popular deep learning often uses unsupervized initialization of hidden layers of neurons [Stuhlsatz et al. 2011]. In fact, features also can be based upon unsupervized clustering such as by vector quantization (see <a href="#fig5_1">Figure 5.1</a>) to produce &#8220;bag-of-word&#8221; frequency of occurance-type features for arbitrary modalities [Pokorny et al. 2015]. Finally, learning can also be used to best synchronize modalities, e.g., by some advanced form of time warping such as the recent deep canonical time warping approach [Trigeorgis et al. 2015].</p>
<p class="h2"><a id="ch5_4_18"/><b><span class="bg2">5.4.18</span>&#160;&#160;&#160;&#160;Optimization</b></p>
<p class="noindent">To improve the performance, optimization of the learning algorithm&#8217;s parameters is needed. This could be, e.g., by some search function and measure of value just as in the previously sketched feature selection process with the main difference that <a id="page_143"/>at this stage free parameters such as the number of hidden layers or the learning rate in a neural network are optimized. A typical search function is, for example, grid search.</p>
<p class="h2"><a id="ch5_4_19"/><b><span class="bg2">5.4.19</span>&#160;&#160;&#160;&#160;Transfer Learning</b></p>
<p class="noindent">Transfer learning can be a useful approach if one has little labeled data available for a specific user state or trait of interest. Consider, e.g., the desire to make an interface for children that recognizes children&#8217;s emotion (the &#8220;target domain&#8221;), but there is only emotionally labeled data from adults (the &#8220;source domain&#8221;) available. Transfer learning now allows to exploit such data from a similar domain for training by &#8220;transferring&#8221; between the source and the target domain. Transfer learning in general is a rather loosely formulated paradigm and can take place on different levels. For this reason, the according box for transfer learning is found between the features and the model in <a href="#fig5_1">Figure 5.1</a>&#8212;in principle, one can also transfer on other levels such as regarding the optimization or type of enhancement, etc. Putting links to any possible box, however, would have rendered the figure hard to interpret. In user state and trait recognition, so far mostly feature transfer learning has been applied [Deng et al. 2014] where unsupervized auto-encoder neural networks learn to map input features of one domain onto themselves with usually a sparse (neurons) hidden layer to reach a compact representation of the data (a so-called &#8220;compression auto encoder&#8221;). Then, data from the other domain is run through this network to reach the domain adaptation or transfer, respectively. This can also be carried out bidirectionally between the domains to make them more similar. On the other hand, model transfer learning aims at re-usage of learned models such as using data from a similar task to pre-train a neural network. Besides, one can use transfer learning not only across different user populations or tasks such as using data from sentiment recognition to train a valence recognizer, but even across modalities [Socher et al. 2013].</p>
<p class="indent">In addition to these building blocks of an automatic user state and trait acquisition system, a number of databases are needed, as also shown in <a href="#fig5_1">Figure 5.1</a> where these are summarized as three in total.</p>
<p class="h2"><a id="ch5_4_20"/><b><span class="bg2">5.4.20</span>&#160;&#160;&#160;&#160;Data</b></p>
<p class="noindent">Data summarizes the original user data such as audio, video, or sensor data recordings alongside labels (including, e.g., also individual ratings of different annotators and their personal confidences, meta-data, and other additional information of use) and potentially also other representations such as enhanced or noisy versions of the data, or features extracted on different levels.</p>
<p class="h2"><b>5.4.21</b> <a id="page_144"/><b>Model</b></p>
<p class="noindent">The model is the actual model used by the machine-learning algorithm(s) inside the engine. It might be a set of rules, probabilities, weights, etc.</p>
<p class="h2"><a id="ch5_4_22"/><b><span class="bg2">5.4.22</span>&#160;&#160;&#160;&#160;Knowledge</b></p>
<p class="noindent">Knowledge potentially includes information on the annotators, the state of the application or interface, the general situational context, or diverse knowledge databases used during decision making and confidence estimation including, e.g., dictionaries.</p>
<p class="h1"><a id="ch5_5"/><b><span class="bg1">5.5</span>&#160;&#160;&#160;&#160;A Modern Architecture Perspective</b></p>
<p class="noindent">In <a href="#fig5_2">Figure 5.2</a> a modern view is given on the architecture for multimodal subject state and trait acquisition. There, the &#8220;learning analyzer&#8221; includes the various aspects along the chain of signal processing up to the decision-making process and confidence calculation. It communicates via potentially more elaborate protocols with the sensors and also actuators, the interface, as well as the crowd, and has access to the same types of databases. As such, this architecture is very generic, as it may be based on feature extraction or simply end-to-end learning. At the same time, the link to the sensors and actuators is shown as bidirectional indicating that the learning analyzer can control parameters of these in a smart way, e.g., to move sensors. The &#8220;crowd&#8221; interacts via these sensors and actuators, and can be &#8220;sourced&#8221; via the interface. Ideally, this analyzer recognizes several states and traits in parallel to better see the &#8220;whole picture.&#8221; For example, knowing that a user is male or female, the personality of the user, and the age group will make it easier to estimate&#8212;say&#8212;the emotion correctly. Accordingly, such states and traits should not be targeted in isolation, but in common.</p>
<p class="h1"><a id="ch5_6"/><b><span class="bg1">5.6</span>&#160;&#160;&#160;&#160;Modalities</b></p>
<p class="noindent">In the following, let us have a closer look at the different modalities. There exist a plethora of overviews on these mostly for multimodal affect recognition (see, e.g., Gunes and Schuller [2013]). There is no definitive gold standard on which modality is best suited for which state or trait&#8212;in fact, the use-case is often the decisive factor, as it may not always be feasible to use invasive body-contact sensors and cameras may not always be able to be mounted in a position where they &#8220;see&#8221; the user, e.g., when considering mobile user interfaces. In addition, privacy issues may come into play when, e.g., considering &#8220;open&#8221; sensor usage that record environmental data continuously such as audio or video also when other individuals may be present. Each modality thus has strengths and weaknesses, and multiple modalities can be congruent or incongruent for the same state or trait targeted&#8212;e.g., when a user is hiding an emotion. While this is known from human perception [Meeren et al. 2005, Van den Stock et al. 2007], automatic recognition yet has to exploit such effects.</p>
<div class="cap" id="fig5_2">
<p class="image"><a id="page_145"/><img src="../images/fig5_2.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 5.2</b>&#160;&#160;&#160;&#160;A modern view on the workflow of an automatic user state and trait analyzer. The arrows indicate transfer of information. The learning analyzer as core-piece unites abilities to learn from data, transfer learn, self-learn, actively learn dynamically sourcing labels from the crowd via an interface, and learn re-inforced based on feedback from the application via the interface. Further, it includes abilities to hierarchically explicitly or implicitly extract/model features, self-select these, including unsupervized feature learning such as bag-of-words. Features are understood as attributes just as target (affect) attributes are in a multi-target learning paradigm. The analyzer may incorporate self-adaptation and self-optimization abilities. Actuators are needed to close the analysis-synthesis loop and allow the analyzer to carry out &#8220;imitation learning&#8221; by re-producing state and trait data to better understand the underlying concepts.</p>
</div>
<p class="h2"><a id="ch5_6_1"/><b><span class="bg2">5.6.1</span>&#160;&#160;&#160;&#160;Audio and Spoken and Written Language</b></p>
<p class="noindent">As to audio analysis, the information is usually coming from the acoustic speech signal, spoken words by linguistic analysis, and &#8220;non-verbal&#8221; sounds such as laughter or hesitations. Beyond, other forms of audio generated by a subject may be of interest, such as step sounds of walking [Geiger et al. 2014].</p>
<p class="indent"><a id="page_146"/>The acoustic feature spaces are often large in recent works reaching up to some thousands of supra-segmental features stemming from some hundred frame-level features. A typical example could be the average speaking rate or the pitch range.</p>
<p class="indent">As to acquisition of states and traits from words&#8212;be they spoken or written&#8212;one generally distinguishes between domain-trained and knowledge-based approaches [Cambria et al. 2013]. Frequently seen domain-trained approaches consider posterior probabilities of words or word (or character) sequences (&#8220;N-Grams&#8221;, where <i>N</i> is the sequence length, i.e., the number of words per sequence) representing the state or trait of interest, or the modeling in a vector space, where each word or word/character sequence represents a feature by its frequency of occurrence in the material to be assessed. Different normalization and representation forms of the frequencies are known such as log-frequency; however, with sufficiently large corpora the choice may play a minor role [Schuller et al. 2015a]. Often, the words are stemmed, i.e., clustered by their morphological stem, or re-tagged, e.g., by part-of-speech classes such as adjective, noun, or verb [Matsumoto and Ren 2011], or combinations of these such as adjective-adverb [Benamara et al. 2007] or adjective-verb-adverb [Subrahmanian and Reforgiato 2008], as well as by semantic classes [Batliner et al. 2011] such as &#8220;politics,&#8221; &#8220;sports,&#8221; etc. In addition, manifold knowledge databases exist that can be exploited to map word sequences onto states or traits of individuals [Strapparava and Mihalcea 2010] such as Concept Net, General Inquirer, or WordNet(-Affect) [Cambria et al. 2013]. Prior to this, different handling respects the origin of the data: For spoken language, one can integrate acoustic confidence measures or several best hypothesis coming from the automatic speech recognizer, albeit &#8220;perfect recognition&#8221; seems less crucial if one only aims at assessing the states and traits of a speaker [Metze et al. 2010]. For written text, one may want to remove extra characters, punctuation, de-capitalize all characters, or handle certain sequences such as &#8220;smileys&#8221; separately. Non-verbal sound-events can be included into the word string after their recognition (e.g., inline with speech recognition) such as in &#8220;oh no &#60;sigh&#62;&#8221; [Schuller et al. 2009, Eyben et al. 2011]. Accordingly, &#8220;&#60;sigh&#62;&#8221; would be handled as a word/character or feature in the probabilistic or vector space modeling, respectively.</p>
<p class="h2"><a id="ch5_6_2"/><b><span class="bg2">5.6.2</span>&#160;&#160;&#160;&#160;Images and Video</b></p>
<p class="noindent">Using vision-based approaches allows one to capture the fingerprint, face, facial actions, such as raising the cheeks, and facial expressions, such as producing a smile [Pantic and Bartlett 2007], and the less pursued body postures and gestures such as head tilts and raising arms [Dael et al. 2012]. Also, the gait pattern may be of interest [Hofmann et al. 2013].</p>
<p class="indent"><a id="page_147"/>Sensors to capture the information are recently often including depth information such as the broadly used consumer-level Microsoft Kinect device or simply standard cameras included in many consumer devices such as smart phones or tablets. For high robustness of tracked feature points albeit often not practical in daily user-interfaces, motion capture systems are an option for body posture [Kleinsmith et al. 2005, Kleinsmith and Bianchi-Berthouze 2007], body language [Metallinou et al. 2011], and even facial expression [W&#246;llmer et al. 2010] analysis.</p>
<p class="indent">In video analysis such as of the face, one often categorizes the approaches into appearance-based vs. feature-based approaches and combinations there of [Pantic and Bartlett 2007]. While appearance-based modeling bases on texture and motion of regions, feature-based modeling is based on tracking, e.g., facial features such as the corners of the eyes or mouth exploiting knowledge on the anatomy. Derived features from their coordinates can be, for example, distances between these points to indicate, e.g., the degree of widening of the eyes. In a similar fashion, hand or body gesture recognition and human motion analysis can be categorized into appearance-based consideration of color or grey-scale images or edges and silhouettes, motion-based information without modeling of the structure of the body, and model-based modeling or recovering 3D configurations of the body parts [Poppe 2007, Poppe 2010].</p>
<p class="indent">As for motion capture, geometrical features prevail. These require registration, the definition of a coordinate system followed by calculation of relative positions, Euclidean or other suited distances, and velocities of and between captured points. Further, orientation such as of the shoulder axes can be of interest depending on the trait or state of interest such as affect [Kleinsmith and Bianchi-Berthouze 2007, Kleinsmith et al. 2005].</p>
<p class="indent">When employing thermal infrared imagery, blobs or shape contours [Tsiamyrtzis et al. 2007] are frequently applied among several alternatives. The images can also be divided into grids of squares. Next, the highest temperature per square is identified as a reference [Khan et al. 2006]. Alternatively, differential images with a frequency transformation between the body or face per class are an option [Yoshitomi et al. 2000]. Finally, templates of thermal variation are often used per class as reference. Tracking is fulfilled in the same manner as in the visible spectrum by condensation algorithm, particle filtering, and further methods. The problems thereby often remain the same [Tsiamyrtzis et al. 2007].</p>
<p class="h2"><a id="ch5_6_3"/><b><span class="bg2">5.6.3</span>&#160;&#160;&#160;&#160;Physiology</b></p>
<p class="noindent">Physiological signal analysis often consists of multichannel biosignals as recorded from the central and autonomic nervous systems. Examples include the heart <a id="page_148"/>rate, galvanic skin response (GSR) [Chanel et al. 2007], and electromyography for measurement of the electrical potential that correlates well with muscular cell activities [Haag et al. 2004]. Further, for example, specific breast belts allow for the measurement of the respiration rate leading to features such as depth, speed, and regularity of breathing [Chanel et al. 2007, Haag et al. 2004]. Brain waves measured over the amygdala have also been successfully used for user state modeling [Pun et al. 2006, Chanel et al. 2006, Jenke et al. 2014]. Features include the activation of the left or right frontal cortex indicating, e.g., asymmetrical brain activity [Davidson and Fox 1982]. The deep location of the amygdala in the brain, however, complicates such EEG sensing. Finally, the degree of blood perfusion in the orbital muscles can be seen by thermal imagery [Tsiamyrtzis et al. 2007]. Aiming to avoid the usual requirement of &#8220;invasive&#8221; body contact of the sensors drives the recent efforts put into the development of wearable devices. In fact, a rich choice of such already has reached the mass consumer market. These can continuously sense the heart rate at the wrist and even brain waves, e.g., the &#8220;muse&#8221; device, the BodyANT sensor [Kusserow et al. 2009], or Emotiv&#8217;s Epoc neuroheadset.</p>
<p class="indent">As physical activity and electrical fields often interfere with the measurement, noise removal plays a key role. Simple mean filters are often employed directly on signals such as GSR, blood volume pressure, or respiration signals. In the spectral domain, bandpass filters are often applied. As in the above-described audio-analysis, delta-coefficients and supra-segmental features are often derived, e.g., by applying thresholds or detecting peaks [Liu et al. 2005] and projecting these onto scalar features by functionals&#8212;e.g., moments such as mean or standard deviation [Chanel et al. 2009, Picard et al. 2001].</p>
<p class="h2"><a id="ch5_6_4"/><b><span class="bg2">5.6.4</span>&#160;&#160;&#160;&#160;Tactile Signals</b></p>
<p class="noindent">Furthermore, manual interaction can be of interest for the recognition of human states and traits. For example, humans can recognize human emotion expressed via two degrees-of-freedom force-feedback joysticks [Bailenson et al. 2007]. Automatic approaches have successfully used mouse-movements and touch-screen interaction, e.g., [Schuller et al. 2002] and [Gao et al. 2012]. Pressure sensors (e.g., arranged in sensor arrays or fields) and accelerometers are an alternative option [Altun 2014, van Wingerden et al. 2014]. Special drawboards can also come into play when using handwriting or drawing as source of information to, e.g., identify the person behind or the emotion conveyed in the writing or painting.</p>
<p class="h2"><b>5.6.5</b> <a id="page_149"/><b>Pseudo-Multimdoality</b></p>
<p class="noindent">Besides further potential modalities covering other human senses such as the olfactory or gustatory ones, it seems noteworthy to introduce the ability to replace modalities to a certain extent by others. While this does not make up for an actual added modality, one may reach a more targeted representation of information in the sense of a &#8220;<i><b>pseudo-modality</b></i>&#8221; (see the Glossary). Note, however, that the term pseudo-modality has already been used in other ways in the literature such as to allude to the combination of modalities [Kreilinger et al. 2015]. The principle considered here is best illustrated by examples. Recently, it has been shown that human heart- ate can be determined from a facial video [Pursche et al. 2012]or speech [Schuller et al. 2013] analysis. Adding heart rate derived from the speech signal to other speech features would make up for a &#8220;pseudo-multimodality&#8221; by use of physiology and speech information, albeit coming from one modality only. Yet, this can be a more useful representation form for a machine learning algorithm rather than letting it learn itself that heart rate is reflected in the speech signal and may be of use, e.g., to determine stress level which would usually require massively more data to learn. Another example is the recognition of eye contact [Eyben et al. 2013] or selected facial action units [Ringeval et al. 2015] from the voice. By that, one could avoid the need for cameras in front of the face to enrich the speech features with facial action cues. Yet, again, the information would be only of pseudo-multimodal nature.</p>
<p class="h2"><a id="ch5_6_6"/><b><span class="bg2">5.6.6</span>&#160;&#160;&#160;&#160;Tools</b></p>
<p class="noindent">A larger selection of toolkits free for research purposes and often open source can be used to build up a user state and trait recognition system. Here, only a small selection of frequently applied and recent toolkits is given to enable quick building up of a system including many of the above-named aspects.</p>
<p class="indent">For annotation of data, such tools include ELAN for categorical and continuous labels in separated annotation layers [Brugman and Russel 2004]. Further, ANVIL [Kipp 2001] provides customized coding schemes and data storage in XML format for diverse modalities. Value and time-continuous annotation can be executed via the FEELtrace toolkit [Cowie et al. 2000] and its successor Gtrace [Cowie et al. 2012]. A gamified crowd-sourcing platform tailored for user state and trait data annotation and collection is the iHEARu-PLAY platform [Hantke et al. 2015].</p>
<p class="indent">For the fusion of sensor signals, the SSI toolkit [Wagner et al. 2013] provides an open-source environment well suited for multimodal user state and trait analysis in real time. While the enhancement of signals can be executed in manifold <a id="page_150"/>ways, a somewhat generic and modality independent tool is the blind signal source separation &#8220;openBlissART&#8221; toolkit [Weninger and Schuller 2012]. It is based on non-negative matrix factorization and additionally provides independent component analysis and whatnots.</p>
<p class="indent">For audiovisual and other-type feature extraction, the openSMILE C++ open-source library enables one to extract large feature spaces in real time [Eyben et al. 2010]. Many pre-defined feature sets make it easy to use and to compare with others&#8217; work. A closed-source alternative specialized more on speech is given by the EmoVoice toolkit [Vogt et al. 2008]. The Computer Expression Recognition Toolbox (CERT) [Littlewort et al. 2011] is a broadly used real-time-enabled facial expression recognition engine. For body movement and gestures, the EyesWeb XMI Expressive Gesture Processing Library [Glowinski et al. 2011] is frequently employed.</p>
<p class="indent">This leaves machine learning to add to the list of tools presented here to put up a running multimodal user state and trait recognizer. The amount of according toolboxes is overwhelming. Therefore, for this reason only a reference to the frequently used WEKA 3 datamining toolkit [Hall et al. 2009] is given here as well as to the state-of-the-art CURRENNT tool&#8212;a GPU-enabled fast deep-learning library with long-short-term memory ability that led to manifold benchmark results in the field [Weninger et al. 2015].</p>
<p class="indent">If one has no data at hand, but wants to set up a first system, luckily manifold standardized sets are available. Again, only three examples free to use are given here that have recently been featured in research competitions, namely the RECOLA database [Ringeval et al. 2013] featured in the AV+EC 2015 challenge providing audiovisual and physiological data for different affective dimensions alongside other subject information, the SEMAINE database [McKeown et al. 2012] that provides audiovisual user data labeled in five affect dimensions used in earlier AVEC challenges alongside several trait dimensions added during the MAPTRAITS [Gunes et al. 2014] challenge, and finally the audiovisual iHEARu-EAT [Schuller et al. 2015b] corpus providing audiovisual data on eating condition and several other subject meta-data.</p>
<p class="h1"><a id="ch5_7"/><b><span class="bg1">5.7</span>&#160;&#160;&#160;&#160;Walk-through of an Example State</b></p>
<p class="noindent">Let us now consider that we need a new user state in an interface. Let&#8217;s say for some reason your interface should be able to recognize whether a user appears to be arrogant. First, we would thus need to collect data from voices, faces, body postures, gestures, etc. and ensure according arrogance labels are provided, e.g., by crowd-sourcing. Before crowd-sourcing, however, we would need to decide on the <a id="page_151"/>representation such as two classes (&#8220;not arrogant&#8221; and &#8220;arrogant&#8221;) or a scale, say from one to ten in terms of the &#8220;degree of arrogance.&#8221; Next, we set up a system, as shown in <a href="#fig5_1">Figure 5.1</a> with at least the ability to capture data and extract meaningful features (unless we have so much data collected that these can be learned from the data). We could then select the optimal feature ensemble, prior to learning and optimizing a classifier whose output should be encoded in a way readable by the interface that uses the information.</p>
<p class="indent">To enrich our initial database, we could use transfer learning from a similar task or similar population or situation of which we have observations of arrogance and its absence. Further, to keep our system learning, we would employ active and semi-supervized learning &#8220;24-7&#8221; by sourcing the crowd whenever our engine would decide it needs help that is of importance. To this end, however, we would also need to integrate a module that estimates confidence levels related to the assumptions on arrogance or not arrogance made by the system. To fuse different modalities, we could choose one of the three entry points, namely on signal level, feature-level, or as late as after individual decisions.</p>
<p class="indent">In our mark four edition, we would add a knowledge database to integrate context and make sure we enhance the information that could be corrupted as our system has to work &#8220;in the wild&#8221; (see also <a href="#ch5_8">Section 5.8</a> on the different levels such as signal or feature. Obviously, we would then want to test it extensively.</p>
<p class="h1"><a id="ch5_8"/><b><span class="bg1">5.8</span>&#160;&#160;&#160;&#160;Emerging Trends and Future Directions</b></p>
<p class="noindent">To conclude this chapter, let us have a look at selected emerging trends and future directions. Even if these topics emerge now, they will likely remain a direction for quite some more time owing to their challenging nature.</p>
<p class="h2"><a id="ch5_8_1"/><b><span class="bg2">5.8.1</span>&#160;&#160;&#160;&#160;In the Wild Processing</b></p>
<p class="noindent">To <i>apply</i> user state and trait recognition in real-world tasks, acquisition &#8220;in the wild&#8221; is needed. This requires systems to be able to deal with non-prototypical instances [Schuller et al. 2011a] including ambiguous cases selected without &#8220;cherry picking&#8221; under often severe conditions such as noisy, reverberated, or partially missing data. This demands for particularly robust approaches for the assessment and according data collected in such &#8220;wild&#8221; conditions.</p>
<p class="h2"><a id="ch5_8_2"/><b><span class="bg2">5.8.2</span>&#160;&#160;&#160;&#160;Diversity of Culture and Language</b></p>
<p class="noindent">While humans are able to recognize states such as emotion across cultures [Scherer et al. 2001, Sauter et al. 2010] despite differences in expressivity [Scherer and Brosch <a id="page_152"/>2009], effort yet has to go into making machines more independent in this sense. In addition, the language of a user not only impacts on linguistic analysis [Banea et al. 2011], but potentially also on acoustic cues [Polzehl et al. 2010, Feraru et al. 2015, Sagha et al. 2016] or even facial cues, as the lip-movement will be different. Accordingly, novel multi-lingually enabled approaches and tests are needed.</p>
<p class="h2"><a id="ch5_8_3"/><b><span class="bg2">5.8.3</span>&#160;&#160;&#160;&#160;Multi-Subject Processing</b></p>
<p class="noindent">If applications have to cope with several users, they need to be able to infer the states and traits of these simultaneously. In image processing, this poses less problems as long as there are no occlusions by overlapping individuals. In audio processing, the signal requires separation prior to processing. Speaker diarization can help to identify the number of speakers at a time and their potential overlap [Nwe et al. 2010].</p>
<p class="h3"><a id="ch5_8_3_1"/><b>5.8.3.1&#160;&#160;&#160;&#160;Linking Analysis with Synthesis</b></p>
<p class="noindent">The methods of analysis and synthesis often differ significantly. In addition, many states and traits can be recognized from a user but not synthesized. For a balanced and symmetrical user-machine communication, however, it seems desirable that the machine can feedback the same range of states and traits to the user. Overall, it just seems promising to couple analysis and synthesis more tightly if both exist in one interface (e.g., Schr&#246;der et al. [2012]). This also allows a system to learn both reinforced or cooperatively with the user at a time in a synergistic fashion just like a young child would learn language not only by analysis but also by synthesis and getting feedback from the world around him/her. The major challenge lies, however, in embedding user state and trait analysis in more real-life products that operate &#8220;in the wild&#8221; and test them in longitudinal studies.</p>
<p class="h1n"><a id="ch5_9"/><b>Focus Questions</b></p>
<p class="noindent"><b>5.1.</b> Name different attributes that may be used to automatically classify a user and group these by taxonomies. Also think of, and list, a few attributes that have not been listed in the chapter, but could be meaningful in a user interface context.</p>
<p class="noindentt"><b>5.2.</b> Which modalities are the &#8220;usual suspects&#8221; in the classification of user states and traits? Briefly describe in which ways they are exploited and discuss the strengths and weaknesses, individually, and potential synergies of selected combinations of modalities.</p>
<p class="noindentt"><b>5.3.</b> What is understood by &#8220;pseudo-multimodal&#8221; as was discussed here?</p>
<p class="noindentt"><b>5.4.</b> <a id="page_153"/>Describe the flow of processing in an automatic user classification system starting from the input sensors and name mandatory and optional building blocks along the chain of processing. Briefly describe the principle of each such building block.</p>
<p class="noindentt"><b>5.5.</b> In question 5.4, at which points can multimodal fusion take place along this chain of processing?</p>
<p class="noindentt"><b>5.6.</b> In which way does a modern architecture of processing as described here differ from the &#8220;traditional&#8221; one?</p>
<p class="noindentt"><b>5.7.</b> Differentiate between <i>supervized, active, reinforced, semi-supervized, unsupervized</i>, and <i>transfer learning</i>.</p>
<p class="noindentt"><b>5.8.</b> What information needs to be provided to and read from the interface to classify the user states and traits?</p>
<p class="noindentt"><b>5.9.</b> Select an exemplary state or trait of your choice and sketch how you would realize a multimodal automatic system that recognizes it and interfaces with an application.</p>
<p class="noindentt"><b>5.10.</b> Name open issues in today&#8217;s automatic multimodal user classification. Share details about how different modalities are concerned.</p>
<p class="h1n"><a id="ch5_10"/><b>References</b></p>
<p class="ref">M. Abouelenien, M. Burzo, and R. Mihalcea. 2015. Cascaded multimodal analysis of alertness related features for drivers safety applications. In <i>Proceedings of the 8th ACM International Conference on PErvasive Technologies Related to Assistive Environments</i>, p. 59. ACM. DOI: 10.1145/2769493.2769505. 134</p>
<p class="ref">E. Alpaydin. 2018. Classifying multimodal data. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Ch. 2. Morgan &#38; Claypool Publishers, San Rafael, CA.</p>
<p class="ref">K. Altun and K. E. MacLean. 2015. Recognizing affect in human touch of a robot. <i>Pattern Recognition Letters</i>, vol. 66, pp. 31&#8211;40. DOI: 10.1016/j.patrec.2014.10.016. 148</p>
<p class="ref">O. Amft and G. Tr&#246;ster. 2006. Methods for detection and classification of normal swallowing from muscle activation and sound. In <i>Pervasive Health Conference and Workshops, 2006</i>, pp. 1&#8211;10. IEEE. DOI: 10.1109/PCTHEALTH.2006.361624. 134</p>
<p class="ref">E. O. Andreeva, P. Aarabi, M. G. Philiastides, K. Mohajer, and M. Emami. 2004. Driver drowsiness detection using multimodal sensor fusion. In <i>Defense and Security</i>, pp. 380&#8211;390. International Society for Optics and Photonics. DOI: 10.1117/12.541296. 134</p>
<p class="ref"><a id="page_154"/>P. Baggia, D. C. Burnett, J. Carter, D. A. Dahl, G. McCobb, and D. Raggett. 2009. <i>EMMA: Extensible MultiModal Annotation Markup Language</i>. W3C Recommendation. 140</p>
<p class="ref">J. N. Bailenson, N. Yee, S. Brave, D. Merget, and D. Koslow. 2007. Virtual interpersonal touch: expressing and recognizing emotions through haptic devices. <i>Human-Computer Interaction</i>, 22(3): 325&#8211;353. 148</p>
<p class="ref">T. Baltrusaitis, C. Ahuja, and L.-Ph. Morency. 2018. Multimodal machine learning. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Ch 1. Morgan &#38; Claypool Publishers San Rafael, CA.</p>
<p class="ref">C. Banea, R. Mihalcea, and J. Wiebe. 2011. Multilingual sentiment and subjectivity. In I. Zitouni and D. Bikel, editors, <i>Multilingual Natural Language Processing</i>. Prentice Hall. DOI: 10.1.1.221.4090. 152</p>
<p class="ref">A. Batliner, S. Steidl, B. Schuller, D. Seppi, T. Vogt, J. Wagner, L. Devillers, L. Vidrascu, V. Aharonson, L. Kessous, and N. Amir. 2011. Whodunnit&#8212;Searching for the most important feature types signalling emotion-related user states in speech. <i>Computer Speech &#38; Language</i>, 25(1): 4&#8211;28.e DOI: 10.1016/j.csl.2009.12.003. 146</p>
<p class="ref">L. Batrinca, B. Lepri, and F. Pianesi. 2011. Multimodal recognition of personality during short self-presentations. In <i>Proceedings of the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding</i>, pp. 27&#8211;28. ACM. 134</p>
<p class="ref">L. Batrinca, B. Lepri, N. Mana, and F. Pianesi. 2012. Multimodal recognition of personality traits in human-computer collaborative tasks. In <i>Proceedings of the 14th ACM International Conference on Multimodal Interaction</i>, pp. 39&#8211;46. ACM. DOI: 10.1145/2388676.2388687. 134</p>
<p class="ref">F. Benamara, C. Cesarano, A. Picariello, D. Reforgiato, and V.S. Subrahmanian. 2007. Sentiment analysis: Adjectives and adverbs are better than adjectives alone. In <i>Proceedings International Conference on Weblogs and Social Media</i>, pp. 1&#8211;7, Boulder, CO. 146</p>
<p class="ref">G. G. Berntson, J. T. Bigger, D. L. Eckberg, P. Grossman, P. G. Kaufmann, M. Malik, H. N. Nagaraja, S. W. Porges, J. P. Saul, P. H. Stone, and M. W. VanderMolen. 1997. Heart rate variability: origins, methods, and interpretive caveats. <i>Psychophysiology</i>, 34(6): 623&#8211;648. DOI: 10.1111/j.1469-8986.1997.tb02140.x/abstract. 137</p>
<p class="ref">K. C. Berridge. 2000. Reward learning: Reinforcement, incentives, and expectations. <i>Psycholology of Learning Motiva</i>, 40: 223&#8211;278. DOI: 10.1016/S0079-7421(00)80022-5. 142</p>
<p class="ref">H. Bo&#345;il, P. Boyraz, and J. H. L. Hansen. 2012. Towards multimodal driver&#8217;s stress detection. In <i>Digital Signal Processing for In-vehicle Systems and Safety</i>, pp. 3&#8211;19. Springer. 134</p>
<p class="ref">H. Brugman and A. Russel. 2004. Annotating Multi-media / Multi-modal resources with ELAN. In <i>Proceedings of LREC</i>, pp. 2065&#8211;2068, Lisbon, Portugal. 149</p>
<p class="ref"><a id="page_155"/>F. Burkhardt, C. Pelachaud, B. Schuller, and E. Zovato. 2017. Emotion ML. In D. Dahl, editor, <i>Multimodal Interaction with W3C Standards: Towards Natural User Interfaces to Everything</i>, pp. 65&#8211;80. Springer, Berlin/Heidelberg. 140</p>
<p class="ref">M. Burzo, M. Abouelenien, V. Perez-Rosas, and R. Mihalcea. 2018. Multimodal deception detection. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Ch 13. Morgan &#38; Claypool Publishers San Rafael, CA.</p>
<p class="ref">E. Cambria, B. Schuller, Y. Xia, and C. Havasi. 2013. New avenues in opinion mining and sentiment analysis. <i>IEEE Intelligent Systems Magazine</i>, 28(2): 15&#8211;21. DOI: 10.1109/MIS.2013.30. 146</p>
<p class="ref">H. E. Ceting&#252;l, E. Erzin, Y. Yemez, and A. M. Tekalp. 2006. Multimodal speaker/speech recognition using lip motion, lip texture and audio. <i>Signal Processing</i>, 86(12): 3549&#8211;3558. DOI: 10.1016/j.sigpro.2006.02.045. 134</p>
<p class="ref">G. Chanel, J. Kronegg, D. Grandjean, and T. Pun. 2006. Emotion assessment: Arousal evaluation using eeg&#8217;s and peripheral physiological signals. In <i>LNCS</i> vol. 4105, pp. 530&#8211;537. 148</p>
<p class="ref">G. Chanel, K. Ansari-Asl, and T. Pun. 2007. Valence-arousal evaluation using physiological signals in an emotion recall paradigm. In <i>Proceedings of SMC</i>, pp. 2662&#8211;2667, Montreal, QC. IEEE. DOI: 10.1109/ICSMC.2007.4413638. 148</p>
<p class="ref">G. Chanel, J. J. M. Kierkels, M. Soleymani, and T. Pun. 2009. Short-term emotion assessment in a recall paradigm. <i>International Journal of Human-Computer Studies</i>, 67(8): 607&#8211;627. DOI: 10.1016/j.ijhcs.2009.03.005. 137, 148</p>
<p class="ref">J. F. Cohn, T. S. Kruez, I. Matthews, Y. Yang, M. H. Nguyen, M. T. Padilla, F. Zhou, and F. De La Torre. 2009. Detecting depression from facial actions and vocal prosody. In <i>Affective Computing and Intelligent Interaction and Workshops, 2009. ACII 2009. 3rd International Conference on</i>, pp. 1&#8211;7. IEEE. DOI: 10.1109/ACII.2009.5349358. 134</p>
<p class="ref">J. F. Cohn, N. Cummins, J. Epps, R. Goecke, J. Joshi, and S. Scherer. 2018. Multimodal assessment of depression and related disorders based on behavioural signals. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Ch 12. Morgan &#38; Claypool Publishers San Rafael, CA.</p>
<p class="ref">R. Cowie, E. Douglas-Cowie, S. Savvidou, E. McMahon, M. Sawey, and M. Schr&#246;der. 2000. Feeltrace: An instrument for recording perceived emotion in real time. In <i>Proceedings of ISCA Workshop on Speech and Emotion</i>, pp. 19&#8211;24, Newcastle, UK. DOI: 10.1.1.384.9385. 149</p>
<p class="ref">R. Cowie, G. McKeown, and E. Douglas-Cowie. 2012. Tracing emotion: an overview. <i>International Journal of Synthetic Emotions</i>, 3(1): 1&#8211;17. DOI: 10.4018/jse.2012010101. 149</p>
<p class="ref"><a id="page_156"/>N. Dael, M. Mortillaro, and K. R. Scherer. 2012. The body action and posture coding system (bap): Development and reliability. <i>Journal of Nonverbal Behavior</i>, 36(2): 97&#8211;121. DOI: 10.1007/s10919-012-0130-0. 146</p>
<p class="ref">D. Davidov, O. Tsur, and A. Rappoport. 2010. Semi-supervised recognition of sarcastic sentences in Twitter and Amazon. In <i>Proceedings of CoNNL</i>, pp. 107&#8211;116, Uppsala, Sweden. DOI: 10.1.1.182.4112. 141</p>
<p class="ref">R. J. Davidson and N. A. Fox. 1982. Asymmetrical brain activity discriminates between positive and negative affective stimuli in human infants. <i>Science</i>, 218: 1235&#8211;1237. DOI: 10.1126/science.7146906. 148</p>
<p class="ref">J. Deng and B. Schuller. 2012. Confidence measures in speech emotion recognition based on semi-supervised learning. In <i>Proceedings of INTERSPEECH</i>, Portland, OR. ISCA. 140</p>
<p class="ref">J. Deng, Z. Zhang, F. Eyben, and B. Schuller. 2014. Autoencoder-based unsupervised domain adaptation for speech emotion recognition. <i>IEEE Signal Processing Letters</i>, 21(9): 1068&#8211;1072. DOI: 10.1109/LSP.2014.2324759. 143</p>
<p class="ref">S. K. D&#8217;Mello, N. Bosch, and H. Chen. 2018. Multimodal-multisensor affect detection. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Ch 6. Morgan &#38; Claypool Publishers San Rafael, CA.</p>
<p class="ref">L. El Asri, R. Laroche, and O. Pietquin. 2012. Reward function learning for dialogue management. In <i>Proceedings Sixth Starting AI Researchers&#8217; Symposium &#8211; STAIRS</i>, pp. 95&#8211;106. 142</p>
<p class="ref">F. Eyben, M. W&#246;llmer, and B. Schuller. 2010. openSMILE &#8211; The Munich versatile and fast open-source audio feature extractor. In <i>Proceedings of MM</i>, pp. 1459&#8211;1462, Florence, Italy. ACM. DOI: 10.1145/1873951.1874246. 150</p>
<p class="ref">F. Eyben, M. W&#246;llmer, M. Valstar, H. Gunes, B. Schuller, and M. Pantic. 2011. String-based audiovisual fusion of behavioural events for the assessment of dimensional affect. In <i>Proceedings of FG</i>, pp. 322&#8211;329, Santa Barbara, CA. IEEE. DOI: 10.1109/FG.2011.5771417. 146</p>
<p class="ref">F. Eyben, F. Weninger, L. Paletta, and B. Schuller. 2013. The acoustics of eye contact&#8212;Detecting visual attention from conversational audio cues. In <i>Proceedings 6th Workshop on Eye Gaze in Intelligent Human Machine Interaction: Gaze in Multimodal Interaction (GAZEIN 2013), held in conjunction with ICMI 2013</i>, pp. 7&#8211;12, Sydney, Australia. ACM. DOI: 10.1145/2535948.2535949. 149</p>
<p class="ref">M. Farr&#250;s, P. Ejarque, A. Temko, and J. Hernando. 2007. Histogram equalization in svm multimodal person verification. In <i>Advances in Biometrics</i>, pp. 819&#8211;827. Springer. DOI: 10.1007/978-3-540-74549-5_86. 134</p>
<p class="ref">S. M. Feraru, D. Schuller, and B. Schuller. 2015. Cross-language acoustic emotion recognition: an overview and some tendencies. In <i>Proceedings of ACII</i>, pp. 125&#8211;131, Xi&#8217;an, P.R. China. IEEE. DOI: 10.1109/ACII.2015.7344561. 152</p>
<p class="ref"><a id="page_157"/>Y. Gao, N. Bianchi-Berthouze, and H. Meng. 2012. What does touch tell us about emotions in touchscreen-based gameplay? <i>ACM Transactions on Computer-Human Interaction</i>, 19(4/31). DOI: 10.1145/2395131.2395138. 148</p>
<p class="ref">J. T. Geiger, M. Kneissl, B. Schuller, and G. Rigoll. 2014. Acoustic Gait-based Person Identification using Hidden Markov Models. In <i>Proceedings of the Personality Mapping Challenge &#38; Workshop (MAPTRAITS 2014), Satellite of ICMI)</i>, pp. 25&#8211;30, Istanbul, Turkey. ACM. DOI: 10.1145/2668024.2668027. 145</p>
<p class="ref">C. Georgakis, S. Petridis, and M. Pantic. 2014. Discriminating native from non-native speech using fusion of visual cues. In <i>Proceedings of the ACM International Conference on Multimedia</i>, pp. 1177&#8211;1180. ACM. DOI: 10.1145/2647868.2655026. 134</p>
<p class="ref">D. Glowinski, N. Dael, A. Camurri, G. Volpe, M. Mortillaro, and K. Scherer. 2011. Towards a minimal representation of affective gestures. <i>IEEE Transactions on Affective Computing</i>, 2(2): 106&#8211;118. DOI: 10.1109/T-AFFC.2011.7. 150</p>
<p class="ref">H. Gunes and M. Pantic. 2010. Automatic, dimensional and continuous emotion recognition. <i>International Journal of Synthetic Emototions</i>, 1(1): 68&#8211;99. DOI: 10.4018/jse.2010101605. 137</p>
<p class="ref">H. Gunes and B. Schuller. 2013. Categorical and dimensional affect analysis in continuous input: current trends and future directions. <i>Image and Vision Compututing Journal Special Issue</i>, 31(2): 120&#8211;136. DOI: 10.1016/j.imavis.2012.06.016. 144</p>
<p class="ref">H. Gunes, B. Schuller, O. Celiktutan, E. Sariyanidi, and F. Eyben, editors. 2014. <i>Proceedings of the Personality Mapping Challenge &#38; Workshop (MAPTRAITS 2014)</i>, Istanbul, Turkey. ACM. Satellite of the 16th ACM International Conference on Multimodal Interaction (ICMI). 133, 134, 150</p>
<p class="ref">A. Haag, S. Goronzy, P. Schaich, and J. Williams. 2004. Emotion recognition using biosensors: First steps towards an automatic system. In <i>LNCS 3068</i>, pp. 36&#8211;48. DOI: 10.1007/978-3-540-24842-2_4. 148</p>
<p class="ref">M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. 2009. The weka data mining software: an update. <i>ACM SIGKDD Explorations Newsletters</i>, 11(1): 10&#8211;18. DOI: 10.1145/1656274.1656278. 150</p>
<p class="ref">S. Hantke, T. Appel, F. Eyben, and B. Schuller. 2015. iHEARu-PLAY: Introducing a game for crowdsourced data collection for affective computing. In <i>Proceedings of the 1st International Workshop on Automatic Sentiment Analysis in the Wild (WASA 2015) held in conjunction with ACII</i>, pp. 891&#8211;897, Xi&#8217;an, P.R. China. IEEE. DOI: 10.1109/ACII.2015.7344680. 149</p>
<p class="ref">M. Hofmann, J. Geiger, S. Bachmann, B. Schuller, and G. Rigoll. 2013. The TUM Gait from Audio, Image and Depth (GAID) Database: Multimodal Recognition of Subjects and Traits. <i>Journal of Visual Communication and Image Representation Special Issue on Visual Understanding Application with RGB-D Cameras</i>, 25(1): 195&#8211;206. 134, 146</p>
<p class="ref">G. Huang and Y. Wang. 2007. Gender classification based on fusion of multi-view gait sequences. In <i>Computer Vision&#8211;ACCV 2007</i>, pp. 462&#8211;471. Springer. DOI: 10.1007/9783-540-76386-4_43. 134</p>
<p class="ref"><a id="page_158"/>R. Jenke, A. Peer, and M. Buss. 2014. Feature extraction and selection for emotion recognition from eeg. <i>IEEE Transactions on Affective Computing</i>, 5(3): 327&#8211;339. DOI: 10.1109/TAFFC.2014.2339834. 148</p>
<p class="ref">G. Keren, A. E.-D. Mousa, O. Pietquin, S. Zafeiriou, and B. Schuller. 2018. Deep learning for multisensorial and multimodal interaction. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Ch 4. Morgan &#38; Claypool Publishers San Rafael, CA.</p>
<p class="ref">M. M. Khan, R. D. Ward, and M. Ingleby. 2006. Infrared thermal sensing of positive and negative affective states. In <i>Proceedings of the International Conference on Robotics, Automation and Mechatronics</i>, pp. 1&#8211;6. IEEE. DOI: 10.1109/RAMECH.2006.252608.147</p>
<p class="ref">M. Kipp. 2001. Anvil - a generic annotation tool for multimodal dialogue. In <i>Proceedings of the 7th European Conference on Speech Communication and Technology</i>, pp. 1367&#8211;1370. 149</p>
<p class="ref">A. Kleinsmith and N. Bianchi-Berthouze. 2007. Recognizing affective dimensions from body posture. In <i>Proceedings of ACII</i>, pp. 48&#8211;58, Lisbon, Portugal. DOI: 10.1007/978-3-54074889-2_5. 147</p>
<p class="ref">A. Kleinsmith, P. R. De Silva, and N. Bianchi-Berthouze. 2005. Recognizing emotion from postures: Cross&#8211;cultural differences in user modeling. In <i>Proceedings of the Conference on User Modeling</i>, pp. 50&#8211;59, Edinburgh, UK. 147</p>
<p class="ref">T. Ko. 2005. Multimodal biometric identification for large user population using fingerprint, face and iris recognition. In <i>Applied Imagery and Pattern Recognition Workshop, 2005. Proceedings 34th</i>, p. 6. IEEE. DOI: 10.1109/AIPR.2005.35. 134</p>
<p class="ref">A. Kreilinger, H. Hiebel, and G. Muller-Putz. 2015. Single versus multiple events error potential detection in a BCI-controlled car game with continuous and discrete feedback. <i>IEEE Transactions on Biomedical Engineering</i>, (3): 519&#8211;29. DOI: 10.1109/TBME.2015.2465866. 149</p>
<p class="ref">M. Kusserow, O. Amft, and G. Troster. 2009. Bodyant: Miniature wireless sensors for naturalistic monitoring of daily activity. In <i>Proceedings of the International Conference on Body Area Networks</i>, pp. 1&#8211;8, Sydney, Australia. DOI: 10.4108/ICST.BODYNETS2009.5899. 148</p>
<p class="ref">M. Li, V. Rozgi&#263;, G. Thatte, S. Lee, A. Emken, M. Annavaram, U. Mitra, D. Spruijt-Metz, and S. Narayanan. 2010a. Multimodal physical activity recognition by fusing temporal and cepstral information. <i>IEEE Transactions on Neural Systems Rehabilitation Engineering</i>, 18(4): 369&#8211;380. DOI: 10.1109/TNSRE.2010.2053217. 134</p>
<p class="ref">X. Li, X. Zhao, Y. Fu, and Y. Liu. 2010b. Bimodal gender recognition from face and fingerprint. In <i>Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</i>, pp. 2590&#8211;2597. IEEE. DOI: 10.1109/CVPR.2010.5539969. 134</p>
<p class="ref"><a id="page_159"/>G. Littlewort, J. Whitehill, T. Wu, I. R. Fasel, M. G. Frank, J. R. Movellan, and M. S. Bartlett. 2011. The computer expression recognition toolbox (cert). In <i>Proceedings of FG</i>, pp. 298&#8211;305, Santa Barbara, CA. IEEE. DOI: 10.1109/FG.2011.5771414. 150</p>
<p class="ref">C. Liu, P. Rani, and N. Sarkar. 2005. An empirical study of machine learning techniques for affect recognition in human-robot interaction. In <i>Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems</i>, pp. 2662&#8211;2667. DOI: 10.1109/IROS.2005.1545344. 148</p>
<p class="ref">X. Lu, H. Chen, and A. K. Jain. 2005. Multimodal facial gender and ethnicity identification. In <i>Advances in Biometrics</i>, pp. 554&#8211;561. Springer. DOI: 10.1007/11608288_74. 134</p>
<p class="ref">K. Matsumoto and F. Ren. 2011. Estimation of word emotions based on part of speech and positional information. <i>Computters in Human Behavior</i>, 27(5): 1553&#8211;1564. DOI: 10.1016/j.chb.2010.10.028. 146</p>
<p class="ref">F. Matta, U. Saeed, C. Mallauran, and J.-L. Dugelay. 2008. Facial gender recognition using multiple sources of visual information. In <i>Multimedia Signal Processing, 2008 IEEE 10th Workshop on</i>, pp. 785&#8211;790. IEEE. DOI: 10.1109/MMSP.2008.4665181. 134</p>
<p class="ref">U. Maurer, A. Smailagic, D. P. Siewiorek, and M. Deisher. 2006. Activity recognition and monitoring using multiple sensors on different body positions. In <i>Wearable and Implantable Body Sensor Networks, 2006. BSN 2006. International Workshop on</i>, pp. 4&#8211;pp. IEEE. DOI: 10.1109/BSN.2006.6. 134</p>
<p class="ref">I. McCowan, D. Gatica-Perez, S. Bengio, G. Lathoud, M. Barnard, and D. Zhang. 2005. Automatic analysis of multimodal group actions in meetings. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 27(3): 305&#8211;317. DOI: 10.1109/TPAMI.2005.49. 134</p>
<p class="ref">G. McKeown, M. Valstar, R. Cowie, M. Pantic, and M. Schr&#246;der. 2012. The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent. <i>IEEE Transactions on Affective Computing</i>, 3(1): 5&#8211;17. DOI: 10.1109/T-AFFC.2011.20. 150</p>
<p class="ref">H. K. Meeren, C. C. Van Heijnsbergen, and B. De Gelder. 2005. Rapid perceptual integration of facial expression and emotional body language. <i>Proceedings of the National Academy of Sciences of the USA</i>, 102: 16518&#8211;16523. DOI: 10.1073/pnas.0507650102. 145</p>
<p class="ref">W. A. Melder, K. P. Truong, M. D. Uyl, D. A. Van Leeuwen, M. A. Neerincx, L. R. Loos, and B. Plum. 2007. Affective multimodal mirror: sensing and eliciting laughter. In <i>Proceed. International Workshop on Human-centered Multimedia</i>, pp. 31&#8211;40. ACM. DOI: 10.1145/1290128.1290134. 134</p>
<p class="ref">A. Metallinou, A. Katsamanis, Y. Wang, and S. Narayanan. 2011. Tracking changes in continuous emotion states using body language and prosodic cues. In <i>Proceedings of ICASSP</i>, pp. 2288&#8211;2291, Prague, Czech Republic. IEEE. DOI: 10.1109/ICASSP.2011.5946939. 147</p>
<p class="ref">F. Metze, A. Batliner, F. Eyben, T. Polzehl, B. Schuller, and S. Steidl. 2010. Emotion recognition using imperfect speech recognition. In <i>Proceedings INTERSPEECH</i>, pp. 478&#8211;481, Makuhari, Japan. ISCA. 146</p>
<p class="ref"><a id="page_160"/>T. L. Nwe, H. Sun, N. Ma, and H. Li. 2010. Speaker diarization in meeting audio for single distant microphone. In <i>Proceedings of INTERSPEECH</i>, pp. 1505&#8211;1508, Makuhari, Japan. ISCA. 152</p>
<p class="ref">S. Oviatt, J. F. Grafsgaard, L. Chen, and X. Ochoa. 2018. Multimodal learning analytics: Assessing learners&#8217; mental state during the process of learning. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Ch 11. Morgan &#38; Claypool Publishers San Rafael, CA.</p>
<p class="ref">Y. Panagakis, O. Rudovic, and M. Pantic. 2018. Learning for multi-modal and context-sensitive interfaces. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Ch 3. Morgan &#38; Claypool Publishers San Rafael, CA.</p>
<p class="ref">M. Pantic and M.S. Bartlett. 2007. Machine analysis of facial expressions. In K. Delac and M. Grgic, editors, <i>Face Recognition</i>, pp. 377&#8211;416. I-Tech Education and Publishing, Vienna, Austria. 146, 147</p>
<p class="ref">F. Pianesi, N. Mana, A. Cappelletti, B. Lepri, and M. Zancanaro. 2008. Multimodal recognition of personality traits in social interactions. In <i>Proceedings of the 10th International Conference on Multimodal Interfaces</i>, pp. 53&#8211;60. ACM. DOI: 10.1145/1452392.1452404.134</p>
<p class="ref">R. W. Picard, E. Vyzas, and J. Healey. 2001. Toward machine emotional intelligence: analysis of affective physiological state. <i>IEEE Transactions on Pattern Analysis Machine Intelligence</i>, 23(10): 1175&#8211;1191. DOI: 10.1109/34.954607. 148</p>
<p class="ref">F. Pokorny, F. Graf, F. Pernkopf, and B. Schuller. 2015. Detection of negative emotions in speech signals using bags-of-audio-words. In <i>Proceedings of the 1st International Workshop on Automatic Sentiment Analysis in the Wild (WASA 2015) held in conjunction with ACII</i>, pp. 879&#8211;884, Xi&#8217;an, P.R. China. IEEE. DOI: 10.1109/ACII.2015.7344678.137, 142</p>
<p class="ref">T. Polzehl, A. Schmitt, and F. Metze. 2010. Approaching multi-lingual emotion recognition from speech&#8212;on language dependency of acoustic/prosodic features for anger detection. In <i>Proceedings of Speech Prosody</i>. ISCA. 152</p>
<p class="ref">R. Poppe. 2007. Vision-based human motion analysis: An overview. <i>Computer Vision and Image Understanding</i>, 108(1&#8211;2): 4&#8211;18. DOI: 10.1016/j.cviu.2006.10.016. 147</p>
<p class="ref">R. Poppe. 20101. A survey on vision-based human action recognition. <i>Image and Vision Computing</i>, 28(6): 976&#8211;990. DOI: 10.1016/j.imavis.2009.11.014. 147</p>
<p class="ref">T. Pun, T. I. Alecu, G. Chanel, J. Kronegg, and S. Voloshynovskiy. 2006. Brain&#8211;computer interaction research at the computer vision and multimedia laboratory, University of Geneva. <i>IEEE Transactions on Neural Systems Rehabilitation Engineering</i>, 14(2): 210&#8211;213. DOI: 10.1109/TNSRE.2006.875544. 148</p>
<p class="ref"><a id="page_161"/>T. Pursche, J. Krajewski, and R. Moeller. 2012. Video-based heart rate measurement from human faces. In <i>Consumer Electronics (ICCE), 2012 IEEE International Conference on</i>, pp. 544&#8211;545. IEEE. DOI: 10.1109/ICCE.2012.6161965. 149</p>
<p class="ref">F. Putze, J.-P. Jarvis, and T. Schultz. 2010. Multimodal recognition of cognitive workload for multitasking in the car. In <i>Pattern Recognition (ICPR), 2010 20th International Conference on</i>, pp. 3748&#8211;3751. IEEE. DOI: 10.1109/ICPR.2010.913. 134</p>
<p class="ref">T. Qin, J. K. Burgoon, J. P. Blair, and J. F. Nunamaker Jr. 2005. Modality effects in deception detection and applications in automatic-deception-detection. In <i>System Sciences, 2005. HICSS&#8217;05. Proceedings of the 38th Annual Hawaii International Conference on</i>, pp. 23b&#8211;23b. IEEE. DOI: 10.1109/HICSS.2005.436. 134</p>
<p class="ref">F. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. 2013. Introducing the recola multimodal corpus of remote collaborative and affective interactions. In <i>Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on</i>, pp. 1&#8211;8. IEEE. DOI: 10.1109/FG.2013.6553805. 150</p>
<p class="ref">F. Ringeval, E. Marchi, M. M&#233;hu, K. Scherer, and B. Schuller. 2015. Face reading from speech&#8212;predicting facial action units from audio cues. In <i>Proceedings INTERSPEECH 2015, 16th Annual Conference of the International Speech Communication Association</i>, pp. 1977&#8211;1981, Dresden, Germany. ISCA. 149</p>
<p class="ref">H. Sagha, J. Deng, M. Gavryukova, J. Han, and B. Schuller. 2016. Cross lingual speech emotion recognition using canonical correlation analysis on principal component subspace. In <i>Proceedings 41st IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2016</i>, Shanghai, P.R. China. IEEE. DOI: 10.1109/ICASSP.2016.7472789. 152</p>
<p class="ref">L. Salahuddin, J. Cho, M. G. Jeong, and D. Kim. 2007. Ultra short term analysis of heart rate variability for monitoring mental stress in mobile settings. In <i>Proceedings of the IEEE International Conference of Engineering in Medicine and Biology Society</i>, pp. 39&#8211;48. DOI: 10.1109/IEMBS.2007.4353378. 137</p>
<p class="ref">D. Sanchez-Cortes, O. Aran, D. B. Jayagopi, M. Mast, and D. Gatica-Perez. 2013. Emergent leaders through looking and speaking: from audio-visual data to multimodal recognition. <i>Journal of Multimodal User Interface</i>, 7(1&#8211;2): 39&#8211;53. DOI: 10.1007/s12193012-0101-0. 134</p>
<p class="ref">M. E. Sargin, E. Erzin, Y. Yemez, and A. M. Tekalp. 2006. Multimodal speaker identification using canonical correlation analysis. In <i>Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on</i>, vol. 1, pp. I&#8211;I. IEEE. DOI: 10.1109/ICASSP.2006.1660095. 134</p>
<p class="ref">D. A. Sauter, F. Eisner, P. Ekman, and S. K. Scott. 2010. Cross-cultural recognition of basic emotions through nonverbal emotional vocalizations. <i>Proceedings of the National Academy of Sciences of the U.S.A</i>., 107(6): 2408&#8211;2412. DOI: 10.1073/pnas.0908239106.151</p>
<p class="ref"><a id="page_162"/>K. R. Scherer and T. Brosch. 2009. Culture-specific appraisal biases contribute to emotion dispositions. <i>European Journal of Personality</i>, 23: 265&#8211;288. DOI: 10.1002/per.714/abstract. 151, 152</p>
<p class="ref">K. R. Scherer, R. Banse, and H. G. Wallbott. 2001. Emotion inferences from vocal expression correlate across languages and cultures. <i>Journal of Cross-Cultural Psychology</i>, 32(1): 76&#8211;92. DOI: 10.1177/0022022101032001009. 151</p>
<p class="ref">M. Schr&#246;der, H. Pirker, and M. Lamolle. 2006. First suggestions for an emotion annotation and representation language. In <i>Proceedings LREC</i>, vol. 6, pp. 88&#8211;92, Genoa, Italy. ELRA. 140</p>
<p class="ref">M. Schr&#246;der, E. Bevacqua, R. Cowie, F. Eyben, H. Gunes, D. Heylen, M. ter Maat, G. McKeown, S. Pammi, M. Pantic, C. Pelachaud, B. Schuller, E. de Sevin, M. Valstar, and M. W&#246;llmer. 2012. Building autonomous sensitive artificial listeners. <i>IEEE Transactions on Affectective Computing</i>, pp. 1&#8211;20. DOI: 10.1109/T-AFFC.2011.34. 152</p>
<p class="ref">B. Schuller. 2013. <i>Intelligent Audio Analysis</i>. Signals and Communication Technology. Springer. 135, 137</p>
<p class="ref">B. Schuller and A. Batliner. 2013. <i>Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing</i>. Wiley. 132, 141</p>
<p class="ref">B. Schuller, Manfred Lang, and G. Rigoll. 2002. Multimodal emotion recognition in audiovisual communication. In <i>Proceedings of ICME</i>, vol. 1, pp. 745&#8211;748, Lausanne, Switzerland. IEEE. DOI: 0.1109/ICME.2002.1035889. 148</p>
<p class="ref">B. Schuller, R. M&#252;ller, F. Eyben, J. Gast, B. H&#246;rnler, M. W&#246;llmer, G. Rigoll, A. H&#246;thker, and H. Konosu. 2009. Being Bored? Recognising natural interest by extensive audiovisual integration for real-life application. <i>Image and Vision Computing Journal</i>, 27(12): 1760&#8211;1774. DOI: 10.1016/j.imavis.2009.02.013. 134, 146</p>
<p class="ref">B. Schuller, A. Batliner, S. Steidl, and D. Seppi. 2011a. Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge. <i>Speech Communications</i>, 53(9/10): 1062&#8211;1087. DOI: 10.1016/j.specom.2011.01.011. 151</p>
<p class="ref">B. Schuller, M. Valstar, R. Cowie, and M. Pantic. 2011b. Avec 2011&#8212;the first audio/visual emotion challenge and workshop - an introduction. In <i>Proceedings of the 1st International Audio/Visual Emotion Challenge and Workshop</i>, pp. 415&#8211;424, Memphis, TN. 133, 134</p>
<p class="ref">B. Schuller, F. Friedmann, and F. Eyben. 2013. Automatic recognition of physiological parameters in the human voice: heart rate and skin conductance. In <i>Proceedings 38th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2013</i>, pp. 7219&#8211;7223, Vancouver, Canada. IEEE. DOI: 10.1109/ICASSP.2013.6639064. 149</p>
<p class="ref">B. Schuller, A. El-Desoky Mousa, and V. Vasileios. 2015a. Sentiment analysis and opinion mining: on optimal parameters and performances. <i>WIREs Data Mining and Knowledge Discovery</i>, 5: 255&#8211;263. DOI: 10.1002/widm.1159/abstract. 146</p>
<p class="ref">B. Schuller, S. Steidl, A. Batliner, S. Hantke, F. H&#246;nig, J.<span class="f2">R&#771;</span>. Orozco-Arroyave, E. N&#246;th, Y. Zhang, and F. Weninger. 2015b. The INTERSPEECH 2015 computational paralinguistics challenge: degree of nativeness, Parkinson&#8217;s &#38; eating condition. <a id="page_163"/>In <i>Proceedings INTERSPEECH 2015, 16th Annual Conference of the International Speech Communication Association</i>, pp. 478&#8211;482, Dresden, Germany. ISCA. 150</p>
<p class="ref">B. Settles, M. Craven, and S. Ray. 2008. Multiple-instance active learning. In <i>Proceedings of NIPS</i>, pp. 1289&#8211;1296, Vancouver, BC, Canada. 142</p>
<p class="ref">C. Shan, S. Gong, and P. W. McOwan. 2007. Learning gender from human gaits and faces. In <i>Advanced Video and Signal Based Surveillance, 2007. AVSS 2007. IEEE Conference on</i>, pp. 505&#8211;510. IEEE. DOI: 10.1109/AVSS.2007.4425362. 134</p>
<p class="ref">C. Shan, S. Gong, and P. W. McOwan. 2008. Fusing gait and face cues for human gender recognition. <i>Neurocomputing</i>, 71(10):1931&#8211;1938. DOI: 10.1016/j.neucom.2007.09.023. 134</p>
<p class="ref">N. Sharma and T. Gedeon. 2012. Objective measures, sensors and computational techniques for stress recognition and classification: A survey. <i>Computer Methods and Programs in Biomedicine</i>, 108(3): 1287&#8211;1301. DOI: 10.1016/j.cmpb.2012.07.003. 134</p>
<p class="ref">R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. 2013. Zero-shot learning through cross-modal transfer. In <i>NIPS&#8217;13 Proceedings of the 26th International Conference on Neural Information Processing Systems</i>, vol. 1, pp. 935&#8211;943. 143</p>
<p class="ref">C. Strapparava and R. Mihalcea. 2010. Annotating and identifying emotions in text. In G. Armano, M. de Gemmis, G. Semeraro, and E. Vargiu, editors, <i>Intelligent Information Access, Studies in Computational Intelligence</i>, vol. 301, pp. 21&#8211;38. Springer Berlin/Heidelberg. ISBN 978-3-642-13999-4. DOI: 10.1007/978-3-642-14000-6_2. 146</p>
<p class="ref">A. Stuhlsatz, C. Meyer, F. Eyben, T. Zielke, G. Meier, and B. Schuller. 2011. Deep neural networks for acoustic emotion recognition: Raising the benchmarks. In <i>Proceedings of ICASSP</i>, pp. 5688&#8211;5691, Prague, Czech Republic. IEEE. DOI: 10.1109/ICASSP.2011.5947651. 142</p>
<p class="ref">V. S. Subrahmanian and D. Reforgiato. 2008. AVA: adjective-verb-adverb combinations for sentiment analysis. <i>Intelligent Systems</i>, 23(4):43&#8211;50. DOI: 10.1109/MIS.2008.57. 146</p>
<p class="ref">R. S. Sutton and A. G. Barto. 1998. <i>Reinforcement Learning: An Introduction</i>, volume 1. MIT Press Cambridge. 141</p>
<p class="ref">G. Trigeorgis, K. Bousmalis, S. Zafeiriou, and B. Schuller. 2014. A deep semi-NMF model for learning hidden representations. In <i>Proceedings of ICML</i>, vol. 32, pp. 1692&#8211;1700, Beijing, China. IMLS. 142</p>
<p class="ref">G. Trigeorgis, M.A. Nicolaou, S. Zafeiriou, and B. Schuller. 2015. Towards deep alignment of multimodal data. In <i>Proceedings 2015 Multimodal Machine Learning Workshop held in conjunction with NIPS 2015 (MMML@NIPS)</i>, Montr&#233;al, QC. NIPS. 142</p>
<p class="ref">G. Trigeorgis, F. Ringeval, R. Br&#252;ckner, E. Marchi, M. Nicolaou, B. Schuller, and S. Zafeiriou. 2016. Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network. In <i>Proceedings 41st IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2016</i>, Shanghai, P.R. China. IEEE. DOI: 10.1109/ICASSP.2016.7472669. 137</p>
<p class="ref"><a id="page_164"/>P. Tsiamyrtzis, J. Dowdall, D. Shastri, I. T. Pavlidis, M. G. Frank, and P. Ekman. 2007. Imaging facial physiology for the detection of deceit. <i>Intelligent Journal of Computer Vision</i>, 71(2): 197&#8211;214. DOI: 10.1007/s11263-006-6106-y. 147, 148</p>
<p class="ref">J. Van den Stock, R. Righart, and B. De Gelder. 2007. Body expressions influence recognition of emotions in the face and voice. <i>Emotion</i>, 7(3):487&#8211;494. DOI: 10.1037/1528-3542.7.3.487. 145</p>
<p class="ref">S. van Wingerden, T. J. Uebbing, M. M. Jung, and M. Poel. 2014. A neural network based approach to social touch classification. In <i>Proceedings of the 2nd International Workshop on Emotion representations and modelling in Human-Computer Interaction systems, ERM4HCI</i>, pp. 7&#8211;12, Istanbul, Turkey. ACM. DOI: 10.1145/2668056.2668060.148</p>
<p class="ref">A. Vinciarelli and A. Esposito. 2018. Multimodal analysis of social signals. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Ch 7. Morgan &#38; Claypool Publishers San Rafael, CA.</p>
<p class="ref">T. Vogt, E. Andr&#233;, and N. Bee. 2008. Emovoice &#8211; a framework for online recognition of emotions from voice. In <i>Proceedings of IEEE PIT</i>, volume 5078 of <i>LNCS</i>, pp. 188&#8211;199. Springer, Kloster Irsee. DOI: 10.1007/978-3-540-69369-7_21. 150</p>
<p class="ref">J. Wagner and E. Andr&#233;. 2018. Real-time sensing of affect and social signals in a multimodal framework: a practical approach. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Ch 8. Morgan &#38; Claypool Publishers San Rafael, CA.</p>
<p class="ref">J. Wagner, F. Lingenfelser, T. Baur, I. Damian, F. Kistler, and E. Andre. 2013. The social signal interpretation (ssi) framework: multimodal signal processing and recognition in real-time. In <i>Proceedings of the 21st ACM International Conference on Multimedia</i>, pp. 831&#8211;834. ACM. DOI: 10.1145/2502081.2502223. 149</p>
<p class="ref">F. Weninger and B. Schuller. 2012. Optimization and parallelization of monaural source separation algorithms in the openBliSSART toolkit. <i>Journal of Signal Processing Systems</i>, 69(3): 267&#8211;277. DOI: 10.1007/s11265-012-0673-7. 150</p>
<p class="ref">F. Weninger, J. Bergmann, and B. Schuller. 2015. Introducing CURRENNT: the Munich open-source CUDA RecurREnt neural network toolkit. <i>Journal of Machine Learning Research</i>, 16: 547&#8211;551. 150</p>
<p class="ref">M. W&#246;llmer, A. Metallinou, F. Eyben, B. Schuller, and S. Narayanan. 2010. Context-sensitive multimodal emotion recognition from speech and facial expression using bidirectional lstm modeling. In <i>Proceedings of INTERSPEECH</i>, pp. 2362&#8211;2365, Makuhari, Japan. ISCA. 147</p>
<p class="ref">M. W&#246;llmer, C. Blaschke, T. Schindl, B. Schuller, B. F&#228;rber, S. Mayer, and B. Trefflich. 2011. On-line driver distraction detection using long short-term memory. <i>IEEE</i> <a id="page_165"/><i>Transactions on Intelligent Transportation Systems</i>, 12(2): 574&#8211;582. DOI: 10.1109/TITS.2011.2119483. 134</p>
<p class="ref">M. W&#246;llmer, F. Weninger, T. Knaup, B. Schuller, C. Sun, K. Sagae, and L. P. Morency. 2013. YouTube movie reviews: Sentiment analysis in an audiovisual context. <i>IEEE Intelligent Systems</i>, 28(2): 2&#8211;8. DOI: 10.1109/MIS.2013.34. 134</p>
<p class="ref">Y. Yoshitomi, S. I. Kim, T. Kawano, and T. Kitazoe. 2000. Effect of sensor fusion for recognition of emotional states using voice, face image and thermal image of face. In <i>IEEE International Workshop on Robot and Human Interactive Communication</i>, pp. 178&#8211;183. DOI: 10.1109/ROMAN.2000.892491. 147</p>
<p class="ref">Y. Zhang, E. Coutinho, Z. Zhang, M. Adam, and B. Schuller. 2015a. Introducing rater reliability and correlation based dynamic active learning. In <i>Proceedings of ACII</i>, pp. 70&#8211;76, Xi&#8217;an, P.R. China. IEEE. DOI: 10.1109/ACII.2015.7344553. 142</p>
<p class="ref">Z. Zhang, E. Coutinho, J. Deng, and B. Schuller. 2015b. Cooperative Learning and its Application to Emotion Recognition from Speech. <i>IEEE ACM Transactions on Audio, Speech and Language Processing</i>, 23(1): 115&#8211;126. DOI: 10.1109/TASLP.2014.2375558. 142</p>
<p class="ref">Y. Zhang, Y. Zhou, J. Shen, and B. Schuller. 2016a. Semi-autonomous data enrichment based on cross-task labelling of missing targets for holistic speech analysis. In <i>Proceedings 41st IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2016</i>, Shanghai, P.R. China. IEEE. 141</p>
<p class="ref">Z. Zhang, F. Ringeval, B. Dong, E. Coutinho, E. Marchi, and B. Schuller. 2016b. Enhanced semi-supervised learning for multimodal emotion recognition. In <i>Proceedings 41st IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2016</i>, Shanghai, P.R. China. IEEE. DOI: 10.1109/ICASSP.2016.7472666. 141</p>
<p class="ref">J. Zhou, K. Yu, F. Chen, Y. Wang, and S. Z. Arshad. 2018. Multimodal behavioural and physiological signals as indicators of cognitive load. In S. Oviatt, B. Schuller, P. Cohen, D. Sonntag, G. Potamianos, and A. Krueger, editors, <i>The Handbook of Multimodal-Multisensor Interfaces, Volume 2: Signal Processing, Architectures, and Detection of Emotion and Cognition</i>, Ch 10. Morgan &#38; Claypool Publishers San Rafael, CA.</p>
</body>
</html>