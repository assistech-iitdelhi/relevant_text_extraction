<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Sparse Fourier Transform: Theory and Practice</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_129"/><b>9</b></p>
<p class="chtitle"><b>Light Field Reconstruction Using Continuous Fourier Sparsity</b></p>
<p class="h1"><b><a id="ch9_1"/><span class="big">9.1</span>&#160;&#160;&#160;&#160;Introduction</b></p>
<p class="noindent">Fourier analysis is a critical tool in rendering and computational photography. It tells us how to choose sampling rates (e.g., Durand et al. [2005], Egan et al. [2009], Heckbert [1989], Mitchell [1991]), predict upper bounds on sharpness (e.g., Levin et al. [2009], Ng [2005], Levin et al. [2008b]), do fast calculations (e.g., Soler and Sillion [1998]), model wave optics (e.g., Goodman [1996], Zhang and Levoy [2009]), perform light field multiplexing (e.g., Veeraraghavan et al. [2007]), and do compressive sensing (e.g., Candes et al. [2006]). In particular, the sparsity of natural spectra, such as those of light fields, makes it possible to reconstruct them from smaller sets of samples (e.g., Levin and Durand [2010], Veeraraghavan et al. [2007]). This sparsity derives naturally from the continuous Fourier transform, where continuous-valued depth in a scene translates to 2D subspaces in the Fourier domain. However, practical algorithms for reconstruction, including the Sparse Fourier Transform algorithms from <a href="10_Part01.xhtml">Part I</a> of this book, usually operate on the Discrete Fourier Transform (DFT). Unfortunately, little attention is usually paid to the impact of going from the continuous Fourier domain to the discrete one, and it is often assumed that the sparsity derived from continuous principles holds for discrete sampling and computation. In this chapter, we make the critical observation that much of the sparsity in continuous spectra is lost in the discrete domain, and that this loss of sparsity can severely limit reconstruction quality. We propose a new approach to reconstruction that recovers a discrete signal by optimizing for sparsity in the continuous domain. We first describe our approach in general terms, then demonstrate its application in the context of 4D light field acquisition and reconstruction, where <a id="page_130"/>we show that it enables high-quality reconstruction of dense light fields from fewer samples without requiring extra priors or assumptions such as Lambertian scenes.</p>
<div class="cap">
<p class="image"><a id="fig9_1"/><img src="../images/fig9_1.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.1</b> Sparsity in the discrete vs. continuous Fourier domain, and our reconstruction results. (a) The discrete Fourier transform (top) of a particular 2D angular slice <i>&#969;<sub>u</sub>, &#969;<sub>v</sub></i> of the crystal ball&#8217;s light field, and its reconstructed continuous version (bottom). (b) A grid showing the original images from the Stanford light field archive. The images used by our algorithm are highlighted (courtesy of Stanford [2008]); (c) and (d) Two examples of reconstructed viewpoints showing successful reconstruction of this highly non-Lambertian scene which exhibits caustics, specularities, and nonlinear parallax. The uv locations of (c) and (d) are shown as blue and green boxes in (b).</p>
</div>
<p class="indent">The difference between continuous and discrete sparsity is due to the windowing effect. Sampling a signal, such as a light field, inside some finite window is analogous to multiplying that signal by a box function. In the frequency domain, this multiplication becomes convolution by an infinite sinc. If the nonzero frequencies of the spectrum are not perfectly aligned with the resulting discretization of the frequency domain (and therefore the zero crossings of the sinc), this convolution destroys much of the sparsity that existed in the continuous domain. This effect is shown in <a href="#fig9_1">Figure 9.1(a)</a> which plots a 2D angular slice of the 4D light field spectrum of the Stanford crystal ball. In practice, natural spectra, including those of light fields, are never so conveniently aligned, and this loss of sparsity is always observed.</p>
<p class="indent">We introduce an approach to recover the sparsity of the original continuous spectrum based on nonlinear gradient descent. Starting with some initial approximation of the spectrum, we optimize for sparsity in the continuous frequency domain through careful modeling of the projection of continuous sparse spectra into the discrete domain. The output of this process is an approximation of the <a id="page_131"/>continuous spectrum. In the case of a light field, this approximation can be used to reconstruct high quality views that were never captured and even extrapolate to new images outside the aperture of recorded samples.</p>
<p class="indent">Our approach effectively reduces the sampling requirements of 4D light fields by recovering the sparsity of the original continuous spectrum. We show that it enables the reconstruction of full 4D light fields from only a 1D trajectory of viewpoints, which could greatly simplify light field capture. We demonstrate a prototype of our algorithm on multiple datasets to show that it is able to accurately reconstruct even highly non-Lambertian scenes. <a href="#fig9_1">Figures 9.1(b)</a>, <a href="#fig9_1">9.1(c)</a>, and <a href="#fig9_1">9.1(d)</a> show our reconstruction of a highly non-Lambertian scene and the 1D trajectory of viewpoints used by our implementation.</p>
<p class="indent">We believe that our observations on continuous versus discrete sparsity and careful handling of sampling effects when going from a sparse continuous Fourier transform into the discrete Fourier domain can also have important applications for computational photography beyond light field reconstruction as well as other areas like medical imaging as we will see in <a href="20_Chapter10.xhtml">Chapter 10</a>.</p>
<p class="h1"><b><a id="ch9_2"/><span class="big">9.2</span>&#160;&#160;&#160;&#160;Related Work</b></p>
<p class="noindent">Light field capture is challenging because of the 4D nature of light fields and the high sampling rate they require. Capture can be done with a micro lens array at the cost of spatial resolution (e.g., Adelson and Wang [1992], Georgeiv et al. [2006], Ng et al. [2005]), using robotic gantries [Levoy and Hanrahan 1996], using camera arrays [Wilburn et al. 2005], or with a handheld camera moved over time around the scene [Buehler et al. 2001, Davis et al. 2012, Gortler et al. 1996]. All these solutions require extra hardware or time, which has motivated the development of techniques that can reconstruct dense light fields from fewer samples.</p>
<p class="indent">Levin and Durand [2010] and Levin et al. [2008a] argue that the fundamental differences between reconstruction strategies can be seen as a difference in prior assumptions made about the light field. Such priors usually assume a particular structure of sparsity in the frequency domain.</p>
<p class="indent">Perhaps the most common prior on light fields assumes that a captured scene is made up of Lambertian objects at known depths. Conditioning on depth, the energy corresponding to a Lambertian surface is restricted to a plane in the frequency domain. Intuitively, this means that given a single image and its corresponding depth map we could reconstruct all four dimensions of the light field (as is done in many image based rendering techniques). The problems with this approach are that the Lambertian assumption does not always hold and that depth estimation <a id="page_132"/>usually involves fragile nonlinear inference that depends on angular information, meaning that sampling requirements are not reduced to 2D in practice. However, paired with a coded aperture [Levin et al. 2007, Veeraraghavan et al. 2007] or plenoptic camera [Bishop et al. 2009], this prior can be used to recover superresolution for Lambertian scenes in the spatial or angular domain.</p>
<p class="indent">Levin and Durand [2010] use a Lambertian prior, but do not assume that depth is known. This corresponds to a prior that puts energy in a 3D subspace of the light field spectrum, and reduces reconstruction to a linear inference problem. As a result they require only three dimensions of sampling, typically in the form of a focal stack. Like our example application, their technique can also reconstruct a light field from a 1D set of viewpoints. However, they still rely on the Lambertian assumption and the views they reconstruct are limited to the aperture of input views. In contrast, we show how our approach can be used to synthesize higher quality views both inside and outside the convex hull of input images without making the Lambertian assumption. For a comparison, see <a href="#ch9_7">Section 9.7</a>.</p>
<p class="indent">The work of Marwah et al. [2013] assumes a different kind of structure to the sparsity of light fields. This structure is learned from training data. Specifically, they use sparse coding techniques to learn a dictionary of basis vectors for representing light fields. The dictionary is chosen so that training light fields may be represented as sparse vectors. Their underlying assumption is that new light fields will have similar structure to those in their training data.</p>
<p class="h1"><b><a id="ch9_3"/><span class="big">9.3</span>&#160;&#160;&#160;&#160;Sparsity in the Discrete vs. Continuous Fourier Domain</b></p>
<p class="noindent">In this section, we show how the discretization of a signal that is sparse in the continuous Fourier domain results in a loss of sparsity. We then give an overview of our approach for recovering sparse continuous spectra. In subsequent sections, we will describe in detail one application of this theory, namely, reconstructing full 4D light fields from a few 1D viewpoint segments. We will also show results of this application on real light field data.</p>
<p class="indent">A signal <i>x</i>(<i>t</i>) of length <i>N</i> is <i>k</i>-sparse in the continuous Fourier domain if it can be represented as a combination of <i>k</i> non-zero continuous frequency coefficients:</p>
<p class="eqn"><a id="eq9_1"/><img src="../images/eq9_1.png" alt="Image"/></p>
<p class="noindent">where <img src="../images/in132_1.png" alt="Image"/> are the continuous positions of frequencies (i.e., each <i>&#969;<sub>i</sub></i> is not necessarily an integer), and <img src="../images/in132_2.png" alt="Image"/> are the coefficients or values corresponding to these <a id="page_133"/>frequencies. The same signal is sparse in the discrete Fourier domain only if all of the <i>&#969;<sub>i</sub></i>&#8217;s happen to be integers. In this case, the output of its <i>N</i>-point DFT has only <i>k</i> non-zero coefficients. Consequently, any signal that is <i>k</i>-sparse in the discrete Fourier domain is also <i>k</i>-sparse in the continuous Fourier domain; however, as we will show next, a signal that is sparse in the continuous Fourier domain is not necessarily sparse in the discrete Fourier domain.</p>
<p class="h2"><a id="ch9_3_1"/><b><span class="big1">9.3.1</span>&#160;&#160;The Windowing Effect</b></p>
<p class="noindent">The windowing effect is a general phenomenon that occurs when one computes the discrete Fourier transform (DFT) of a signal using a finite window of samples. Since it is not limited to the light field, we will explain the concept using one-dimensional signals. It naturally extends to higher dimensions.</p>
<p class="indent">Consider computing the discrete Fourier transform of a time signal <b>y</b>(<i>t</i>). To do so, we would sample the signal over a time window <img src="../images/in133_1.png" alt="Image"/>, then compute the DFT of the samples. Since the samples come from a limited window, it is as if we multiplied the original signal <b>y</b>(<i>t</i>) by a box function that is zero everywhere outside of this acquisition window. Multiplication in the time domain translates into convolution in the Fourier domain. Since acquisition multiplies the signal by a box, the resulting DFT returns the spectrum of the original signal <b>y</b>(<i>t</i>) convolved with a sinc function.</p>
<p class="indent">Convolution with a sinc, in most cases, significantly reduces the sparsity of the original signal. To see how, consider a simple example where the signal <b>y</b>(<i>t</i>) is one sinusoid, i.e., <b>y</b>(<i>t</i>) = exp (&#8722;2<i>j&#960;&#8182;t</i>). The frequency domain of this signal has a single impulse at <i>&#8182;</i>. Say we sample the signal over a window <img src="../images/in133_1.png" alt="Image"/>, and take its DFT. The spectrum will be convolved with a sinc, as explained above. The DFT will discretize this spectrum to the DFT grid points located at integer multiples of <img src="../images/in133_2.png" alt="Image"/>. Because a sinc function of width <img src="../images/in133_2.png" alt="Image"/> has zero crossings at multiples of <img src="../images/in133_2.png" alt="Image"/>, (as can be seen in <a href="#fig9_2">Figure 9.2(a)</a>), if <i>&#8182;</i> is an integer multiple of <img src="../images/in133_2.png" alt="Image"/> then the grid points of the DFT will lie on the zeros of the sinc(&#183;) function and we will get a single spike in the output of the DFT. However, if <i>&#8182;</i> is a not an integer multiple of <img src="../images/in133_2.png" alt="Image"/>, then the output of the DFT will have a sinc tail, as shown in <a href="#fig9_2">Figure 9.2(b)</a>.</p>
<p class="indent">Like most natural signals, the sparsity of natural light fields is not generally aligned with any sampling grid. Thus, the windowing effect is almost always observed in the DFT of light fields along spatial and angular dimensions. Consider the effect of windowing in the angular domain (which tends to be more limited in the number of samples, and consequently exhibits a stronger windowing effect). Light fields are sampled within a limited 2D window of <b>uv</b> coordinates. As a result, <a id="page_134"/>the DFT of each 2D angular slice, <img src="../images/in134_1.png" alt="Image"/>, is convolved with a 2D sinc function, reducing sparsity. <a href="#fig9_3">Figure 9.3(a)</a> shows the DFT of an angular slice from the crystal ball light field. As can be seen in the figure, the slice shows a sparse number of peaks but these peaks exhibit tails that decay very slowly. These tails ruin sparsity and make reconstruction more difficult. We propose an approach to light field reconstruction that removes the windowing effect by optimizing for sparsity in the continuous spectrum. <a href="#fig9_3">Figure 9.3(b)</a> shows a continuous Fourier transform of the same crystal ball slice, recovered using our approach. Note that the peak tails caused by windowing have been removed and the underlying sparsity of light fields has been recovered.</p>
<div class="cap">
<p class="image"><a id="fig9_2"/><img src="../images/fig9_2.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.2</b> The windowing effect. Limiting samples to an aperture <i>A</i> is equivalent to convolving the spectrum with a sinc function. (a) If a frequency spike lies on a DFT grid point then the sinc disappears when it is discretized, and the original sparsity of the spectrum is preserved. (b) If the frequency spike is not on the DFT grid, once we discretize we get a sinc tail and the spectrum is no longer as sparse as in the continuous domain.</p>
</div>
<div class="cap">
<p class="image"><a id="page_135"/><a id="fig9_3"/><img src="../images/fig9_3.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.3</b> Light field spectrum in the discrete and continuous Fourier domains. A 2D angular slice of the 4D light field spectrum of the Stanford crystal ball for (<i>&#969;<sub>x</sub>, &#969;<sub>y</sub></i>) = (50, 50). (a) In the discrete Fourier domain, we have sinc tails and the spectrum is not very sparse. (b) In the continuous Fourier domain, as reconstructed by our algorithm, the spectrum is much sparser. It is formed of four peaks which do not fall on the grid points of the DFT.</p>
</div>
<p class="h2"><a id="ch9_3_2"/><b><span class="big1">9.3.2</span>&#160;&#160;Recovering the Sparse Continuous Fourier Spectrum</b></p>
<p class="noindent">From sparse recovery theory we know that signals with sparse Fourier spectra can be reconstructed from a number of time samples proportional to sparsity in the Fourier domain. Most practical sparse recovery algorithms work in the discrete Fourier domain. However, as described above, the non-zero frequency coefficients of most light fields are not integers. As a result, the windowing effect ruins sparsity in the discrete Fourier domain and can cause existing sparse recovery algorithms to fail. Our approach is based on the same principle of sparse recovery, but operates in the continuous Fourier domain where the sparsity of light fields is preserved.</p>
<p class="indent">Recall our model in <a href="#eq9_1">Equation 9.1</a> of a signal that is sparse in the continuous Fourier domain. Given a set of discrete time samples of <i>x</i>(<i>t</i>), our goal is to recover the unknown positions <img src="../images/in135_1.png" alt="Image"/> and values <img src="../images/in135_2.png" alt="Image"/> of the non-zero frequency coefficients. From <a href="#eq9_1">Equation 9.1</a>, we see that this problem is linear in the values <img src="../images/in135_2.png" alt="Image"/> <a id="page_136"/>and nonlinear in the positions <img src="../images/in135_1.png" alt="Image"/> of the non-zero coefficients. Thus, to recover the values and positions, we use a combination of a linear and nonlinear solver.</p>
<p class="indent"><i>Recovering coefficient values</i> <img src="../images/in135_2.png" alt="Image"/>: If we know the positions of non-zero coefficients (i.e., each <i>&#969;<sub>i</sub></i>) then <a href="#eq9_1">Equation 9.1</a> becomes a system of linear equations with unknowns <img src="../images/in135_2.png" alt="Image"/>, and given &#62; <i>k</i> discrete samples of <i>x</i>(<i>t</i>), we can form an over-determined system allowing us to solve for each <i>a<sub>i</sub></i>.</p>
<p class="indent"><i>Recovering continuous positions</i> <img src="../images/in135_1.png" alt="Image"/>: We use nonlinear gradient descent to find the continuous positions <img src="../images/in135_1.png" alt="Image"/> that minimize the square error between observed discrete samples of <i>x</i>(<i>t</i>) and the reconstruction of these samples given by our current coefficient positions and values. Thus, the error function we wish to minimize can be written as</p>
<p class="eqn"><a id="eq9_2"/><img src="../images/eq9_2.png" alt="Image"/></p>
<p class="noindent">where <i>&#227;<sub>i</sub></i> and <i>&#8182;<sub>i</sub></i> are our estimates of <i>a<sub>i</sub></i> and <i>&#969;<sub>i</sub></i> and the above summation is taken over all the observed discrete samples.</p>
<p class="indent">As with any gradient descent algorithm, in practice, we begin with some initial guess of discrete integer positions <img src="../images/in136_1.png" alt="Image"/>. We use the discrete Sparse Fourier Transform algorithm described in <a href="#ch9_5_2">Section 9.5.2</a> for our initialization. From this initial guess, we use gradient descent on <img src="../images/in135_1.png" alt="Image"/> to minimize our error function. In practice, the gradient is approximated using finite differences. In other words, we calculate error for perturbed peak locations <img src="../images/in136_2.png" alt="Image"/> and update our <img src="../images/in136_3.png" alt="Image"/> with the &#8714;<sub><i>i</i></sub> that result in the smallest error. We keep updating until the error converges.</p>
<p class="indent">Once we have recovered both <i>a<sub>i</sub></i> and <i>&#969;<sub>i</sub></i>, we can reconstruct the signal <i>x</i>(<i>t</i>) for any sample <i>t</i> using <a href="#eq9_1">Equation 9.1</a>.</p>
<p class="h1"><b><a id="ch9_4"/><span class="big">9.4</span>&#160;&#160;&#160;&#160;Light Field Notation</b></p>
<p class="noindent">A 4D light field <b>L</b>(<i>x, y, u, v</i>) characterizes the light rays between two parallel planes: the <b>uv</b> camera plane and the <b>xy</b> image plane, which we refer to as angular and spatial dimensions, respectively. Each (<i>u, v</i>) coordinate corresponds to the location of the viewpoint of a camera and each (<i>x, y</i>) coordinate corresponds to a pixel location. <img src="../images/in136_3a.png" alt="Image"/> characterizes the 4D spectrum of this light field. We will use <img src="../images/in136_4.png" alt="Image"/> to denote a 2D angular slice of this 4D spectrum for fixed spatial frequencies (<i>&#969;<sub>x</sub>, &#969;<sub>y</sub></i>). Similarly, <b>L</b><sub><i>u,v</i></sub>(<i>x, y</i>) denotes the 2D image captured by a camera with its center of projection at location (<i>u, v</i>). <a href="#tab9_1">Table 9.1</a> presents a list of terms used throughout this chapter.</p>
<p class="tcaption"><a id="page_137"/><a id="tab9_1"/><b>Table 9.1 Light field notation</b></p>
<p class="image"><img src="../images/tab9_1.png" alt="Image"/></p>
<p class="h1"><b><a id="ch9_5"/><span class="big">9.5</span>&#160;&#160;&#160;&#160;Light Field Reconstruction Algorithm</b></p>
<p class="noindent">To demonstrate the power of sparse recovery in the continuous Fourier domain, we show how it can be used to reconstruct light fields from 1D viewpoint trajectories. We choose to work with 1D trajectories because it simplifies the initialization of our gradient descent. However, the continuous Fourier recovery described in the previous section is general and does not require this assumption.</p>
<p class="indent">In this section, we describe the Sparse Fourier Transform used for our initialization as well as our sparse continuous Fourier recovery. The reconstruction algorithm described in this section is shown in Algorithm 9.1. The initialization is given by the S<small>PARSE</small>D<small>ISCRETERE</small>C<small>OVERY</small> procedure shown in Algorithm 9.2 and the continuous recovery is given by the S<small>PARSE</small>C<small>ONTINUOUS</small>R<small>ECOVERY</small> procedure shown in Algorithm 9.3.</p>
<p class="noindentt"><a id="page_138"/><b>Algorithm 9.1 Light field reconstruction algorithm</b></p>
<p class="image"><img src="../images/alg9_1.png" alt="Image"/></p>
<p class="h2"><a id="ch9_5_1"/><b><span class="big1">9.5.1</span>&#160;&#160;Input</b></p>
<p class="noindent">Our input is restricted to 1D viewpoint trajectories that consist of discrete lines. A discrete line in the angular domain is defined by the set of (<i>u, v</i>) points such that:</p>
<p class="eqn"><a id="eq9_3"/><img src="../images/eq9_3.png" alt="Image"/></p>
<p class="noindent">where 0 &#8804; <i>&#945;<sub>u</sub>, &#945;<sub>v</sub>, &#964;<sub>u</sub>, &#964;<sub>v</sub> &#60; N</i> and GCD(<i>&#945;<sub>u</sub>, &#945;<sub>v</sub></i>) = 1.</p>
<p class="indent">This lets us use the Fourier projection-slice theorem to recover a sparse discrete spectrum, which we use to initialize our gradient descent. <a href="#fig9_4">Figure 9.4</a> shows the specific sampling patterns used in our experiments.</p>
<p class="indent">For a light field <b>L</b>(<i>x, y, u, v</i>), our algorithm operates in the intermediate domain, <img src="../images/in138_1.png" alt="Image"/>, which describes spatial frequencies as a function of viewpoint. We start by taking the 2D DFT of each input image, which gives us the spatial frequencies (<i>&#969;<sub>x</sub>, &#969;<sub>y</sub></i>) at a set of viewpoints <i>S</i> consisting of our 1D input lines. We call this set of known samples <img src="../images/in138_2.png" alt="Image"/>. Our task is to recover the 2D angular spectrum <img src="../images/in138_3.png" alt="Image"/> for each spatial frequency (<i>&#969;<sub>x</sub>, &#969;<sub>y</sub></i>) from the known samples <img src="../images/in138_2.png" alt="Image"/>. For generality and parallelism we do this at each spatial frequency independently, but one could possibly use a prior on the relationship between different (<i>&#969;<sub>x</sub>, &#969;<sub>y</sub></i>) to improve our current implementation.</p>
<div class="cap">
<p class="image"><a id="page_139"/><a id="fig9_4"/><img src="../images/fig9_4.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.4</b> Light field sampling patterns. Our algorithm samples the (<i>u, v</i>) angular domain along discrete lines. (a) Box and two diagonals and (b) box and two lines with slopes = &#177;2. Note that in this case the discrete line wraps around.</p>
</div>
<p class="h2"><a id="ch9_5_2"/><b><span class="big1">9.5.2</span>&#160;&#160;Initialization</b></p>
<p class="noindent">The goal of our initialization is to calculate some initial guess for the positions <img src="../images/in136_1.png" alt="Image"/> of our non-zero frequency coefficients. We do this using a Sparse Fourier Transform algorithm that leverages the Fourier projection-slice theorem in a voting scheme similar to a Hough transform. By the projection-slice theorem, taking the DFT of an input discrete line gives us the projection of our light field spectrum onto the line. Each projection gives the sum of several coefficients in our spectrum. Different projections provide us with different sums, and each sum above a given threshold votes for the discrete positions of coefficients that it sums similar to the voting scheme described in <a href="12_Chapter03.xhtml">Chapter 3</a>. The Sparse Fourier Transform algorithm then selects the discrete positions that receive a vote from every input projection and returns these as its initial guess. We refer to this algorithm as D<small>ISCRETE</small>S<small>PARSE</small>R<small>ECOVERY</small> and it is shown in Algorithm 9.2.</p>
<p class="h3"><a id="ch9_5_2_1"/><b><span class="big2">9.5.2.1</span>&#160;&#160;&#160;&#160;Computing Projections</b></p>
<p class="noindent">To simplify our discussion of slices, we will use <b>X</b> to denote the 2D slice <img src="../images/in138_1.png" alt="Image"/> in our intermediate domain and <img src="../images/xcap.png" alt="image"/> to denote its DFT, <img src="../images/in138_3.png" alt="Image"/>. Thus, our input is given by a subset of sample slices <b>X</b><sub>|<i>S</i></sub>, where the set <i>S</i> gives the coordinates of our input samples ((<i>u, v</i>) viewpoint positions).</p>
<p class="indent">For each slice <b>X</b> in our input <b>X</b><sub>|<i>S</i></sub>, the views in <b>X</b> lie on a discrete line. We perform a 1D DFT for each of these discrete lines, which yields the projection of our 2D spectrum onto a corresponding line in the Fourier domain. Specifically, let <i>y</i> be the 1D discrete line corresponding to a 2D slice <b>X</b>, (parameterized by <i>t</i> &#8712; [<i>N</i>])</p>
<p class="noindentt"><a id="page_140"/><b>Algorithm 9.2 Sparse discrete Fourier recovery algorithm</b></p>
<p class="image"><img src="../images/alg9_2.png" alt="Image"/></p>
<div class="cap">
<p class="image"><a id="page_141"/><a id="fig9_5"/><img src="../images/fig9_5.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.5</b> Discrete Fourier projections. Computing the DFT of a discrete line of a 2D signal is equivalent to projecting the 2D spectrum onto the line. The top row of figures shows the sampled lines and the bottom row of figures shows how the spectrum is projected. Frequencies of the same color are projected onto the same point. (a) Row projection, (b) column projection, (c) diagonal projection, and (d) line with slope = 2.</p>
</div>
<p class="eqn"><a id="eq9_4"/><img src="../images/eq9_4.png" alt="Image"/></p>
<p class="noindent">where 0 &#8804; <i>&#945;<sub>u</sub>, &#945;<sub>v</sub>, &#964;<sub>u</sub>, &#964;<sub>v</sub> &#60; N</i> and GCD(<i>&#945;<sub>u</sub>, &#945;<sub>v</sub></i>) = 1.</p>
<p class="indent">Then, <img src="../images/ycap.png" alt="image"/>, the DFT of <b>y</b>, is a projection of <img src="../images/xcap.png" alt="image"/> onto this line. That is, each point in <img src="../images/ycap.png" alt="image"/> is a summation of the <i>N</i> frequencies that lie on a discrete line orthogonal to <b>y</b>, as shown in <a href="#fig9_5">Figure 9.5</a>. Specifically, the frequencies (<i>&#969;<sub>u</sub>, &#969;<sub>v</sub></i>) that satisfy <i>&#945;<sub>u</sub>&#969;<sub>u</sub></i> + <i>&#945;<sub>v</sub>&#969;<sub>v</sub></i> = <i>&#969;</i> mod <i>N</i> project together onto <b>&#375;</b>(<i>&#969;</i>) (recall that discrete lines may &#8216;wrap around&#8217; the input window).</p>
<p class="h3"><a id="ch9_5_2_2"/><b><span class="big2">9.5.2.2</span>&#160;&#160;&#160;&#160;Voting</b></p>
<p class="noindent">To recover the discrete positions of the nonzero frequency coefficients, we use a voting approach. For each line projection, the projected sums which are above some threshold vote for the frequencies that map to them (similar to a Hough transform). Since the spectrum is sparse, most projected values are very small and only the coefficients of large frequencies receive votes from every line projection. Thus, by selecting frequencies that receive votes from every projection, we get an estimate of the discrete positions of nonzero coefficients.</p>
<p class="indent">To better illustrate how voting works, consider the simple example shown in <a href="#fig9_6">Figure 9.6(a)</a>. The 2D spectrum has only three large frequencies at (5, 5), (5, 9), <a id="page_142"/>and (9, 5). When we project along the rows of our grid the 5<i>th</i> and 9<i>th</i> entry of the projection will be large and this projection will vote for all frequencies in the 5<i>th</i> and 9<i>th</i> columns. Similarly, when we project along columns the projection will vote for all frequencies in the 5<i>th</i> and 9<i>th</i> rows. At this point, frequencies (5, 5), (5, 9), (9, 5), (9, 9) have two votes. However, when we project on the diagonal, frequency (9, 9) will not get a vote. After three projections only the three correct frequencies get three votes. Another example is shown in <a href="#fig9_6">Figure 9.6(b)</a>.</p>
<p class="h2"><a id="ch9_5_3"/><b><span class="big1">9.5.3</span>&#160;&#160;Optimization in the Continuous Fourier Domain</b></p>
<p class="noindent">Recall from <a href="#ch9_3_2">Section 9.3.2</a> that our optimization takes the initial positions <img src="../images/in136_1.png" alt="Image"/> and a subset of discrete samples as input. With both provided by the input and initialization, we now minimize the error function of our reconstruction using the gradient descent approach outlined in <a href="#ch9_3_2">Section 9.3.2</a>. This optimization is shown in Algorithm 9.3.</p>
<p class="h3"><a id="ch9_5_3_1"/><b><span class="big2">9.5.3.1</span>&#160;&#160;&#160;&#160;Recovering Frequency Coefficients</b></p>
<p class="noindent">As we discussed in <a href="#ch9_3_2">Section 9.3.2</a>, when we fix the coefficient positions <img src="../images/in136_1.png" alt="Image"/>, <a href="#eq9_1">Equation 9.1</a> becomes linear in the coefficient values <img src="../images/in135_2.png" alt="Image"/>. To solve for the full light field spectrum at each iteration of our gradient descent we express each of our known discrete input samples as a linear combination of the complex exponentials given by our current choice of <img src="../images/in135_1.png" alt="Image"/>.</p>
<p class="indent">With the appropriate system of linear equations we can solve for the coefficient values that minimize the error function described in <a href="#eq9_2">Equation 9.2</a>. To construct our system of linear equations, we concatenate the discrete input (<i>u, v</i>) samples from <b>X</b><sub>|<i>S</i></sub> into an |<i>S</i>| &#215; 1 vector which we denote as <b>x</b><sub><i>S</i></sub>. Given the set <i>P</i> of frequency positions we let <img src="../images/xpcap.png" alt="image"/> be the |<i>P</i>| &#215; 1 vector of the frequency coefficients we want to recover (with each coefficient in <img src="../images/xpcap.png" alt="image"/> corresponding to a frequency position in <i>P</i>). Finally, let <b>A</b><sub><i>P</i></sub> be a matrix of |<i>S</i>| &#215; |<i>P</i> | entries. Each row of <b>A</b><sub><i>P</i></sub> corresponds to a (<i>u, v</i>) sample, each column corresponds to an (<i>&#969;<sub>u</sub>, &#969;<sub>v</sub></i>) frequency, and the value of each entry is given by a complex exponential:</p>
<p class="eqn"><a id="eq9_5"/><img src="../images/eq9_5.png" alt="Image"/></p>
<p class="indent">Now our system of linear equations becomes:</p>
<p class="eqn"><a id="eq9_6"/><img src="../images/eq9_6.png" alt="Image"/></p>
<div class="cap">
<p class="image"><a id="page_143"/><a id="fig9_6"/><img src="../images/fig9_6.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.6</b> Voting based frequency estimation. Two examples of the voting approach used to recover the discrete positions of the large frequencies from projections on discrete lines. The 2D spectrum is projected on a row, column, and diagonal. Each large projection votes for the frequencies that map to it. Using only projections on a row and column, many frequencies get two votes. By adding a third projection on the diagonal, only the large frequencies get three votes. (a) Frequencies (5,5), (5,9), and (9,5) are large and only they get three votes. (b) Some frequencies on the diagonal are large and only these frequencies get three votes.</p>
</div>
<p class="noindent"><a id="page_144"/><b>Algorithm 9.3 Sparse continuous Fourier recovery algorithm</b></p>
<p class="image"><img src="../images/alg9_3.png" alt="Image"/></p>
<p class="indent"><a id="page_145"/>We then use the pseudo-inverse of <b>A</b><sub><i>P</i></sub> to calculate the vector <img src="../images/in145_1.png" alt="Image"/> of coefficient values that minimize our error function:</p>
<p class="eqn"><a id="eq9_7"/><img src="../images/eq9_7.png" alt="Image"/></p>
<p class="noindent">The above is given by the procedure R<small>ECOVER</small>C<small>OEFFICIENTS</small> shown in Algorithm 9.3.<sup><a id="fn1" href="#rfn1">1</a></sup></p>
<p class="h3"><a id="ch9_5_3_2"/><b><span class="big2">9.5.3.2</span>&#160;&#160;&#160;&#160;Gradient Descent</b></p>
<p class="noindent">Recall from <a href="#ch9_3_2">Section 9.3.2</a> that our gradient descent algorithm minimizes the error function, which we can now rewrite as:</p>
<p class="eqn"><a id="eq9_8"/><img src="../images/eq9_8.png" alt="Image"/></p>
<p class="noindent">In the above equation, the frequency positions in the list <i>P</i> are continuous, but the input samples <b>x</b><sub><i>S</i></sub> that we use to compute our error are discrete. Thus, our optimization minimizes error in the <i>discrete</i> reconstruction of our light field by finding optimal <i>continuous</i> frequency positions.</p>
<p class="indent">In our gradient descent, each iteration of the algorithm updates the list of frequency positions <i>P</i>. For each recovered frequency position in <i>P</i>, we fix all other frequencies and shift the position of this frequency by a small fractional step <i>&#948;</i> &#8810; 1. We shift it in all eight directions, as shown in <a href="#fig9_7">Figure 9.7</a>, and compute the new error <i>e</i>(<i>P</i>) given the new position. We then pick the direction that best minimizes the error <i>e</i>(<i>P</i>) and change the position of the frequency in that direction. If none of the directions minimize the error, we do not change the position of this frequency. We repeat this for every frequency position in <i>P</i>.</p>
<p class="indent">Our gradient descent ensures that from iteration <i>i</i> to iteration (<i>i</i> + 1), we always reduce error, i.e., <i>e</i>(<i>P</i><sup>(<i>i</i>+1)</sup>) &#60; <i>e</i>(<i>P</i><sup>(<i>i</i>)</sup>). The algorithm keeps iterating over the frequencies until the error <i>e</i>(<i>P</i>) falls below a minimum acceptable error <i>&#8714;</i>. Once we have a final list of continuous frequency positions, we can recover their coefficients using <a href="#eq9_7">Equation 9.7</a>. The above gradient descent is given by the procedure G<small>RADIENT</small>S<small>EARCH</small> shown in Algorithm 9.3.</p>
<p class="h2"><a id="ch9_5_4"/><b><span class="big1">9.5.4</span>&#160;&#160;Reconstructing the Viewpoints</b></p>
<p class="noindent">As explained in <a href="#ch9_3_2">Section 9.3.2</a>, once we have the continuous positions and values of our non-zero frequency coefficients, we can reconstruct the missing viewpoints by <a id="page_146"/>expressing <a href="#eq9_1">Equation 9.1</a> in terms of our data:</p>
<div class="cap">
<p class="image"><a id="fig9_7"/><img src="../images/fig9_7.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.7</b> Optimizing the continuous frequency positions. The gradient descent shifts the frequency by a small step in every iteration. The frequency is shifted in the direction that minimizes the error. (a) Frequency (4,5) is shifted to a non-integer position that best reduces the error. (b) The frequency is shifted again to minimize the error. (c) The frequency position converges since shifting in any direction will increase the error.</p>
</div>
<p class="eqn"><a id="eq9_9"/><img src="../images/eq9_9.png" alt="Image"/></p>
<p class="noindent">By setting (<i>u, v</i>) to the missing viewpoints, we are able to reconstruct the full light fields. <a href="#fig9_8">Figure 9.8</a> shows a flowchart of the entire reconstruction.</p>
<p class="indent">Note that the above equation lets us reconstruct any (<i>u, v</i>) position. We can interpolate between input views and even extend our reconstruction to images that are outside the convex hull of our input. This would not be possible if our sparse coefficients were limited to the discrete Fourier domain, since the above equation would be periodic modulo <i>N</i>. This would create a wrapping effect, and attempting to reconstruct views outside the span of our input would simply replicate views inside the span of our input. In other words, the discrete spectrum assumes that our signal repeats itself outside of the sampling window, but by recovering the continuous spectrum we can relax this assumption and extend our reconstruction to new views.</p>
<p class="h1"><b><a id="ch9_6"/><span class="big">9.6</span>&#160;&#160;&#160;&#160;Experiments</b></p>
<p class="noindent">We experimented with several datasets where full 4D coverage of the light field was available for comparison. For each of these datasets we extracted a small number <a id="page_147"/>of 1D segments, which we then used to reconstruct the full 2D set of viewpoints. We compare our reconstructed light fields against the complete original datasets in our accompanying videos.</p>
<div class="cap">
<p class="image"><a id="fig9_8"/><img src="../images/fig9_8.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.8</b> Flowchart of the 2D sparse light field reconstruction algorithm. The algorithm takes a set of sampled discrete lines. The initialization, Discrete Recovery, has two steps: computing the projections and recovering the discrete positions of the large frequency coefficients. In the sparse continuous recovery, the gradient search tries to shift the positions of the frequencies to non-integer locations and recover their coefficients. We keep repeating this gradient search until we get a small enough error. This stage will output a list of continuous frequency positions and their coefficients, which can then be used to reconstruct the full 2D slice.</p>
</div>
<p class="indent">Three of our datasets, the Stanford Bunny, the Amethyst, and the Crystal Ball, were taken from the Stanford light field archive [Stanford 2008]. Each of the Stanford datasets consists of a 17 &#215; 17 grid of viewpoints and was reconstructed using the box-and-X pattern shown in <a href="#fig9_4">Figure 9.4(a)</a>. On these datasets, we performed our reconstruction in the YUV color space. The U and V channels were reconstructed at half the resolution of the Y channel.</p>
<p class="indent">To show how our method scales with the number of input images we are given, we captured a larger dataset (the Gnome) consisting of 51 &#215; 51 viewpoints. This dataset was captured using a robotic gantry similar to the one from [Stanford 2008], and the double X pattern in <a href="#fig9_4">Figure 9.4(b)</a> was used to select our input. The total number of input images is the same for both the single X pattern and the double X pattern, as the effective spacing between input views along diagonals is changed. For this dataset our input consists of less than 12% of the original images.</p>
<p class="indent">Our code was designed for flexible experimentation and is currently slow. However, the algorithm is highly parallelizable and we run it on a PC cluster. The code <a id="page_148"/>is written in C++ using the Eigen library. The <i>&#969;<sub>u</sub>, &#969;<sub>v</sub></i> slices of a light field are divided among different machines, and the results are collected once all of the machines have finished.</p>
<p class="indent">Load balancing, variability in the number of machines used, and different convergence characteristics for different inputs make it difficult to estimate exact run times. Using a cluster of up to 14 machines at a time (averaging 5&#8211;6 cores each), typical runtimes ranged from 2&#8211;3 hours for a colored dataset (3 channels). There are several ways to accelerate the method; for example, one could leverage the coherence across slices or replace finite differences with a method that converges faster, but we leave this for future work.</p>
<p class="h1"><b><a id="ch9_7"/><span class="big">9.7</span>&#160;&#160;&#160;&#160;Results</b></p>
<p class="h2"><a id="ch9_7_1"/><b><span class="big1">9.7.1</span>&#160;&#160;Viewing our results</b></p>
<p class="noindent">Our results are best experienced by watching the accompanying videos which are available online through this link: <a href="http://netmit.csail.mit.edu/LFSparseRecon/">http://netmit.csail.mit.edu/LFSparseRecon/</a>.</p>
<p class="h2"><a id="ch9_7_2"/><b><span class="big1">9.7.2</span>&#160;&#160;The Stanford Bunny</b></p>
<p class="noindent">The Stanford Bunny dataset is our simplest test case. The scene is Lambertian and therefore especially sparse in the frequency domain. The spacing between input views is also very narrow, so there is little aliasing. Each image is 512 &#215; 512 pixels. Our reconstruction of the Bunny is difficult to distinguish from the full light field captured by Stanford [2008], as shown in <a href="#fig9_9">Figure 9.9</a>. <a href="#fig9_10">Figure 9.10</a> shows that the reconstruction error is small.</p>
<p class="h2"><a id="ch9_7_3"/><b><span class="big1">9.7.3</span>&#160;&#160;Amethyst</b></p>
<p class="noindent">The amethyst dataset is highly non-Lambertian. It exhibits both specular reflections and refraction. Again it is difficult to distinguish our reconstruction from the full captured light field, as shown in <a href="#fig9_11">Figure 9.11</a>. We reconstruct most of the reflected details, with the exception of some undersampled features that move so fast they do not appear at all in the input. <a href="#fig9_12">Figure 9.12</a> gives an example of this.</p>
<p class="h2"><a id="ch9_7_4"/><b><span class="big1">9.7.4</span>&#160;&#160;Crystal Ball</b></p>
<p class="noindent">The Crystal Ball scene is extremely non-Lambertian, exhibiting caustics, reflections, specularities, and nonlinear parallax. We are able to reproduce most of the complex properties that make this scene shown in <a href="#fig9_13">Figure 9.13</a> so challenging, as can be seen in our accompanying video online. If one looks closely, our reconstruction of this light field contains a small amount of structured noise which we discuss in <a href="#ch9_8">Section 9.8</a>.</p>
<div class="cap">
<p class="image"><a id="page_149"/><a id="fig9_9"/><img src="../images/fig9_9.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.9</b> Reconstruction of Stanford Bunny dataset. On the left and in the middle are the reference figure and the reconstruction from Levin and Durand [2010] (courtesy of Levin). The sampling pattern we used is the box-and-X pattern shown in <a href="#fig9_4">Figure 9.4(a)</a>. Although we used more samples than Levin &#38; Durand used, we did a much better job in terms of less blurring, preserving the textures, and having less noise.</p>
</div>
<div class="cap">
<p class="image"><a id="fig9_10"/><img src="../images/fig9_10.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.10</b> Reconstruction error. A color map of the difference in the Y channel between a reference view and a reconstructed view from the Stanford Bunny dataset. In about half of the pixels the difference is zero. There is some unstructured noise, but it is hard to tell whether this comes from the reference figure or our reconstruction. There is also some structured noise on the edge of the bunny, but again it is difficult to tell whether this comes from reconstruction error or an error in the pose estimate of the reference image.</p>
</div>
<div class="cap">
<p class="image"><a id="page_150"/><a id="fig9_11"/><img src="../images/fig9_11.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.11</b> Reconstruction of the Amethyst dataset. Our reconstruction of Amethyst dataset (right), the reference figure (left) and the reconstruction from Levin and Durand [2010] (middle, courtesy of Levin). We are using the box-and-X sampling pattern shown in <a href="#fig9_4">Figure 9.4(a)</a>, which is more than the number of samples used in Levin and Durand [2010]. However, we are able to reconstruct this highly non-Lambertian view and it is hard to distinguish our reconstruction from the full captured light field.</p>
</div>
<div class="cap">
<p class="image"><a id="fig9_12"/><img src="../images/fig9_12.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.12</b> Reconstruction of specularities. One example of a reconstructed view from the Amethyst dataset where we lose some reflection details. The missing specular reflection does not appear in any of our input views, so it cannot be recovered.</p>
</div>
<div class="cap">
<p class="image"><a id="page_151"/><a id="fig9_13"/><img src="../images/fig9_13.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.13</b> Reconstruction of the Crystal Ball dataset. We show one (<i>u, v</i>) view from the reconstruction of the Crystal Ball dataset. We are using the box plus diagonals sampling pattern (as shown in the blue box in the center). The red dot shows the position of reconstructed view in the angular domain. Despite the fact that the scene is extremely non-Lambertian and has complex structures, we are still able to reconstruct most details of the light field.</p>
</div>
<p class="h2"><a id="ch9_7_5"/><b><span class="big1">9.7.5</span>&#160;&#160;Gnome</b></p>
<p class="noindent">We acquired a new dataset consisting of 52 &#215; 52 viewpoints. The resolution of each image is 640 &#215; 480, and we reconstructed all channels at full resolution.</p>
<p class="indent">The Gnome scene is mostly Lambertian with a few specular highlights. In terms of the subject being captured, the difficulty of this scene sits somewhere between the Stanford Bunny and the Amethyst datasets. However, what makes this data more challenging is the level of noise in our input. The captured images of the Gnome have noticeable shot noise, flickering artifacts, and registration errors (&#8220;camera jitter&#8221;). Since these artifacts are not sparse in the frequency domain, our algorithm does not reproduce them in the output shown in <a href="#fig9_14">Figure 9.14</a>. For most of these artifacts, the result is a kind of denoising, making our output arguably better than the reference images available for comparison. This is especially clear in the case of camera jitter, where the effect of denoising can be seen clearly in an epipolar image shown in <a href="#fig9_15">Figure 9.15</a>. However, some of the shot noise in our input is reconstructed with greater structure. We have a more general discussion of noise in <a href="#ch9_8">Section 9.8</a>.</p>
<div class="cap">
<p class="image"><a id="page_152"/><a id="fig9_14"/><img src="../images/fig9_14.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.14</b> Reconstruction of the Gnome dataset. We show one (<i>u, v</i>) view from our reconstruction of the Gnome dataset. We use the sample pattern from <a href="#fig9_4">Figure 9.4(b)</a>, as shown by the blue box in the bottom right. The red marker shows where the view is in the angular domain. Although the captured dataset is noisy, we are still able to reconstruct it in good detail.</p>
</div>
<p class="h2"><a id="ch9_7_6"/><b><span class="big1">9.7.6</span>&#160;&#160;Extending views</b></p>
<p class="noindent">Reversing the windowing effect in the second step of our algorithm makes it possible to reconstruct views outside the original window of our input. To demonstrate this we extend each of the <i>u</i> and <i>v</i> dimensions in our Bunny dataset by an additional 4 views, increasing the size of our reconstructed aperture by ~ 53% (see <a href="#fig9_16">Figure 9.16</a>). These results are best appreciated in our accompanying video.</p>
<p class="h2"><a id="ch9_7_7"/><b><span class="big1">9.7.7</span>&#160;&#160;Informal comparison with Levin and Durand [2010]</b></p>
<p class="noindent">Like us, Levin and Durand [2010] reconstruct light fields from a 1D set of input images. Their technique is based on a Lambertian prior with unknown depth. We provide an informal comparison with their approach, but the different sampling patterns of the two techniques make it difficult to hold constant the number of input <a id="page_153"/>views used by each technique. Levin and Durand&#8217;s reconstruction uses fewer images but is restricted to synthesizing views within the convex hull of input viewpoints. Our sampling patterns use slightly more images, but let us synthesize views outside the convex hull of our input. Small differences in input aside, the comparison in <a href="#fig9_9">Figure 9.9</a> and <a href="#fig9_11">Figure 9.11</a> shows that our reconstruction is less blurry and does not have some of the ringing artifacts that appear in their results.</p>
<div class="cap">
<p class="image"><a id="fig9_15"/><img src="../images/fig9_15.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.15</b> Viewpoint denoising. Top: We see noise in the <i>u, v</i> dimensions of our reference data caused by registration errors. This error shows up as camera shake in the reference images. Bottom: Our algorithm effectively removes this noise in the reconstruction, essentially performing camera stabilization.</p>
</div>
<div class="cap">
<p class="image"><a id="fig9_16"/><img src="../images/fig9_16.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.16</b> Extending views. We extend our reconstruction of the Stanford Bunny dataset (<a href="#fig9_9">Figure 9.9</a>) and extend the camera views. The original view is 0 &#8804; <i>u</i> &#8804; 16 and 0 &#8804; <i>u</i> &#8804; 16, and here we show our extension to (&#8211;2, &#8211;2) and (18, 18).</p>
</div>
<p class="h1"><a id="ch9_8"/><a id="page_154"/><b><span class="big">9.8</span>&#160;&#160;&#160;&#160;Discussion</b></p>
<p class="h2"><a id="ch9_8_1"/><b><span class="big1">9.8.1</span>&#160;&#160;Viewpoint Denoising</b></p>
<p class="noindent">One advantage of reconstruction based on a sparse prior is the potential for denoising. Noisy input tends to create low power high frequencies that are not part of our scene. These frequencies make the spectrum less sparse and are usually zeroed out by our algorithm.</p>
<p class="indent">Since our reconstruction is based on sparsity in the <i>&#969;<sub>u</sub>, &#969;<sub>v</sub></i> domain, we remove noise in <i>u, v</i>. This noise corresponds to &#8220;jitter,&#8221; usually caused by registration errors or camera shake. We can see the effect of this denoising by examining a <i>v, y</i> slice of our light field, like the one in <a href="#fig9_15">Figure 9.15</a>. These slices are often referred to as epipolar plane images (EPI) in computer vision. To observe the visual effect of this denoising, the reader should watch our accompanying Gnome video. The reconstructed camera motion in this video is much smoother than the reference camera motion. One way to think of this effect is as a kind of video stabilization.</p>
<p class="indent">Our ability to denoise in <i>u, v</i> is limited by the number of input slices we have and the sparsity of the spectrum we are reconstructing. If the noise affects the sparsity of our scene too much, some of its power might be projected onto existing spikes from our signal, changing their estimated power. We can see some of this in the Gnome dataset, where some of the shot noise in our input is reconstructed with slight structure along the dominant orientation of our scene.</p>
<p class="h2"><a id="ch9_8_2"/><b><span class="big1">9.8.2</span>&#160;&#160;Importance of</b> <i>Continuous</i> <b>Fourier Recovery</b></p>
<p class="noindent">To better understand how operating in the continuous Fourier domain affects our reconstruction, we examine the impact of our continuous recovery on the reconstructed Bunny light field. We choose this light field because our results are almost indistinguishable from the reference data, so we can reasonably assume that the sparsity estimated by our full algorithm reflects the true sparsity of the captured scene.</p>
<p class="indent">We first compare our sparse continuous recovery in <a href="#fig9_17">Figure 9.17(c)</a> with the sparse discrete recovery used to initialize our gradient descent (shown in <a href="#fig9_17">Figure 9.17(a)</a>). The error in <a href="#fig9_17">Figure 9.17(a)</a> shows how existing sparse recovery theory is limited by the lack of sparsity in the discrete light field spectrum. However, this result does not necessarily isolate the effects of working in the discrete domain. To better isolate these limits, we generate a third reconstruction in <a href="#fig9_17">Figure 9.17(b)</a> by rounding the coefficients of our final reconstruction <a href="#fig9_17">Figure 9.17(c)</a> to the nearest discrete frequency positions and removing the sinc tails that result from this <a id="page_155"/>rounding. This reconstruction approximates the discrete spectrum that is closest to our continuous spectrum while exhibiting the same sparsity.</p>
<div class="cap">
<p class="image"><a id="fig9_17"/><img src="../images/fig9_17.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.17</b> Comparison of our final reconstruction in the continuous domain to two alternative reconstructions in the discrete domain. We compare our result (right) with the output of only our initialization (left), as well as the discrete approximation of our result with sinc tails removed (middle).</p>
</div>
<p class="indent">As we see in <a href="#fig9_17">Figure 9.17</a>, the effect of discretization predicted by our experiment is a kind of ghosting. To understand why, recall that the discrete Fourier transform assumes that signals are periodic in the primal domain, and that given a finite number of frequencies our reconstruction will be band limited. As a result, the IDFT will attempt to smooth between images at opposite ends of the primal domain. If we look at <a href="#fig9_18">Figure 9.18</a> we can see this effect across the set of viewpoints in our light field. When viewpoints near the center are averaged (smoothed) with their neighbors the artifact is less noticeable because their neighbors are very similar. However, when this smoothing wraps around the edges of our aperture, we average between more dissimilar images and the ghosting becomes more severe.</p>
<p class="h2"><a id="ch9_8_3"/><b><span class="big1">9.8.3</span>&#160;&#160;Potential Applications</b></p>
<p class="noindent">Our reconstruction from 1D viewpoint trajectories is directly applicable to capture techniques that seek to acquire a dense 2D sampling of viewpoints on a grid. One could, for instance, use it to significantly reduce the number of cameras needed by a camera array. Alternatively, for applications where light fields are captured by a single moving camera (e.g., Gortler et al. [1996], Buehler et al. [2001], Davis et al. [2012]), the algorithm could be used to greatly increase the speed of capture. In both of these cases, the sparse continuous spectrum we recover could also be used as a highly compressed representation of the light field.</p>
<div class="cap">
<p class="image"><a id="page_156"/><a id="fig9_18"/><img src="../images/fig9_18.png" alt="Image"/></p>
<p class="figcaption"><b>Figure 9.18</b> The ghosting effect. A demonstration of the ghosting that happens when we simply remove sinc tails in the frequency domain. We removed the sinc tails from the spectrum of the Stanford Bunny dataset and selected the same inset from each <i>u, v</i> image (we chose the same inset as in <a href="#fig9_17">Figure 9.17</a>). This figure shows how the inset changes across the (<i>u, v</i>) aperture (note that we subsampled the 17 &#215; 17 aperture by 2). Ghosting gets worse closer to the edge of the input views.</p>
</div>
<p class="indent">The theory of continuous recovery has many potential applications beyond our reconstruction from 1D viewpoint segments. Sparsity in the continuous Fourier domain is a powerful prior that is more general than Lambertianality, making it an exciting new direction for research. While our choice of initialization uses viewpoint sampling patterns that consist of discrete lines, one can imagine different initialization strategies that work with different input. That input could come from plenoptic or mask-based light field cameras, or even some combination of multi-view stereo and image-based rendering algorithms. However, continuous recovery is not necessarily convex, so proper initialization strategies will be an important and possibly non-trivial part of applying our continuous recovery approach to different types of data.</p>
<p class="h1"><a id="ch9_9"/><a id="page_157"/><b><span class="big">9.9</span>&#160;&#160;&#160;&#160;Conclusion</b></p>
<p class="noindent">In this chapter, we made the important observation that natural signals like light fields are much sparser in the continuous Fourier domain than in the discrete Fourier domain, and we showed how this difference in sparsity is the result of a windowing effect. Based on our observations, we presented an approach to light field reconstruction that optimizes for sparsity in the continuous Fourier domain. We then showed how to use this approach to reduce sampling requirements and improve reconstruction quality by applying it to the task of recovering high-quality non-Lambertian light fields from a small number of 1D viewpoint trajectories. We believe that our strategy of optimizing for sparsity in the discrete spectrum will lead to exciting new research in light field capture and reconstruction. Furthermore, we hope that our observations on sparsity in the discrete vs. continuous domain will have an impact on areas of computational photography beyond light field reconstruction.</p>
<p class="line"/>
<p class="foot"><a id="rfn1" href="#fn1">1</a>. Note that we did not specify the threshold used to determine a &#8220;vote&#8221; in our initialization. Rather than using a fixed threshold, we choose the smallest threshold such that the system of equations from <a href="#eq9_6">Equation 9.6</a> becomes well determined.</p>
</body>
</html>