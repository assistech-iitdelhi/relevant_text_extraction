<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Sparse Fourier Transform: Theory and Practice</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_59"/><b>5</b></p>
<p class="chtitle"><b>Optimizing Sample Complexity</b></p>
<p class="h1"><b><a id="ch5_1"/><span class="big">5.1</span>&#160;&#160;&#160;&#160;Introduction</b></p>
<p class="noindent">The algorithms presented in <a href="13_Chapter04.xhtml">Chapter 4</a> achieve very efficient running times. However, they still suffer from important limitations. The main limitation is that their sample complexity bounds are too high. In particular, the sample complexity of the exactly <i>k</i>-sparse algorithm is &#920;(<i>k</i> log <i>n</i>). This bound is suboptimal by a logarithmic factor, as it is known that one can recover any signal with <i>k</i> nonzero Fourier coefficients from <i>O</i>(<i>k</i>) samples [Akcakaya and Tarokh 2008], albeit in super-linear time. The sample complexity of the approximately-sparse algorithm is &#920;(<i>k</i> log(<i>n</i>) log(<i>n</i>/<i>k</i>)). This bound is also a logarithmic factor away from the lower bound of &#937;(<i>k</i> log(<i>n</i>/<i>k</i>)) [Price and Woodruff 2011].</p>
<p class="indent">Reducing the sample complexity is highly desirable as it typically implies a reduction in signal acquisition time, measurement overhead and communication cost. For example, in medical imaging the main goal is to reduce the sample complexity in order to reduce the time the patient spends in the MRI machine [Lustig et al. 2008], or the radiation dose she receives [Sidky 2011]. Similarly in spectrum sensing, a lower average sampling rate enables the fabrication of efficient analog to digital converters (ADCs) that can acquire very wideband multi-GHz signals [Yoo et al. 2012a]. In fact, the central goal of the area of compressed sensing is to reduce the sample complexity.</p>
<p class="indent">A second limitation of the previous algorithms is that most of them are designed for 1D signals. We showed in <a href="13_Chapter04.xhtml#ch4_4">Section 4.4</a> that a 2D adaptation of the SFT 4.0 algorithm in <a href="13_Chapter04.xhtml">Chapter 4</a> has roughly <i>O</i>(<i>k</i> log<sup>3</sup> <i>n</i>) time and sample complexity. This is unfortunate, since multi-dimensional instances of DFT are often particularly sparse. This situation is somewhat alleviated by the fact that the 2D DFT over <i>p</i> &#215; <i>q</i> grids can be reduced to the 1D DFT over a signal of length <i>pq</i> [Gilbert et al. 2005a, <a id="page_60"/>Iwen 2010a]. However, the reduction applies only if <i>p</i> and <i>q</i> are relatively prime, which excludes the most typical case of <i>m</i> &#215; <i>m</i> grids where <i>m</i> is a power of 2.</p>
<p class="h2"><a id="ch5_1_1"/><b><span class="big1">5.1.1</span>&#160;&#160;&#160;&#160;Results</b></p>
<p class="noindent">In this chapter, we present sample-optimal sublinear time algorithms for the Sparse Fourier Transform over a 2D <img src="../images/in60_1.png" alt="image"/> grid. Our algorithms are analyzed in the average case. Our input distributions are natural. For the exactly sparse case, we assume the Bernoulli model: each spectrum coordinate is nonzero with probability <i>k/n</i>, in which case the entry assumes an arbitrary value predetermined for that position.<sup><a id="fn1" href="#rfn1">1</a></sup> For the approximately-sparse case, we assume that the spectrum <i>x&#770;</i> of the signal is a sum of two vectors: the signal vector, chosen from the Bernoulli distribution, and the noise vector, chosen from the Gaussian distribution (see <a href="11_Chapter02.xhtml">Chapter 2</a> Preliminaries for the complete definition). These or similar<sup><a id="fn2" href="#rfn2">2</a></sup> distributions are often used as test cases for empirical evaluations of Sparse Fourier Transform algorithms [Iwen et al. 2007, Lawlor et al. 2012] or theoretical analysis of their performance [Lawlor et al. 2012].</p>
<p class="indent">The algorithms succeed with a constant probability. The notion of success depends on the scenario considered. For the exactly sparse case, an algorithm is successful if it recovers the spectrum exactly. For the approximately sparse case, the algorithm is successful if it reports a signal with spectrum <i>z&#770;</i> such that:</p>
<p class="eqn"><a id="eq5_1"/><img src="../images/eq5_1.png" alt="image"/></p>
<p class="noindent">where <i>&#963;</i><sup>2</sup> denotes the variance of the normal distributions defining each coordinate of the noise vector, and where <i>c</i> is any constant. Note that any <i>k</i>-sparse approximation to <i>x&#770;</i> has error &#937;(<i>&#963;</i><sup>2</sup><i>n</i>) with overwhelming probability, and that the second term in the bound in <a href="#eq5_1">Equation (5.1)</a> is subsumed by the first term as long as the signal-to-noise ratio is at most polynomial, i.e., ||<i>x</i>&#770;||<sub>2</sub> &#8804; <i>n<sup>O</sup></i><sup>(1)</sup><i>&#963;</i>.</p>
<p class="indent">Assuming <img src="../images/in60_2.png" alt="image"/> is a power of 2, we present two new Sparse Fourier Transform algorithms:</p>
<p class="bull"><a id="page_61"/>&#8226;&#160;&#160;an <i>O</i>(<i>k</i> log <i>k</i>)-time algorithm that uses only <i>O</i>(<i>k</i>) samples for the exactly <i>k</i>-sparse case where <img src="../images/in61_1.png" alt="image"/>, and</p>
<p class="bull">&#8226;&#160;&#160;an <i>O</i>(<i>k</i> log<sup>2</sup> <i>n</i>)-time algorithm that uses only <i>O</i>(<i>k</i> log <i>n</i>) samples for the approximately sparse case where <i>k</i> = &#920;(<i>n</i>).</p>
<p class="noindentt">The key feature of these algorithms is that their sample complexity bounds are optimal. For the exactly sparse case, the lower bound of &#937;(<i>k</i>) is immediate. For the approximately sparse case, we note that the &#937;(<i>k</i> log(<i>n/k</i>)) lower bound of Price and Woodruff [2011] holds even if the spectrum is the sum of a <i>k</i>-sparse signal vector in {0, 1, &#8722;1}<sup><i>n</i></sup> and Gaussian noise. The latter is essentially a special case of the distributions handled by these algorithms.</p>
<p class="indent">An additional feature of the first algorithm is its simplicity and therefore its low &#8220;big-Oh&#8221; overhead. As a result, this algorithm is easy to adapt for practical applications.</p>
<p class="h2"><a id="ch5_1_2"/><b><span class="big1">5.1.2</span>&#160;&#160;&#160;&#160;Techniques</b></p>
<p class="noindent">Our first algorithm for <i>k</i>-sparse signals is based on the following observation: the aliasing filter (i.e., uniform subsampling) is one of the most efficient ways for mapping the Fourier coefficients into buckets. For 1D signals however, this filter is not amenable to randomization. Hence, when multiple nonzero Fourier coefficients collide into the same bucket, one cannot efficiently resolve the collisions by randomizing the spike-train filter. In contrast, for 2D signals, we naturally obtain two distinct spike-train filters, which correspond to subsampling the columns and sub-sampling the rows. Hence, we can resolve colliding nonzero Fourier coefficients by alternating between these two filters.</p>
<p class="indent">More specifically, recall that one way to compute the 2D DFT of a signal <i>x</i> is to apply the 1D DFT to each column and then to each row. Suppose that <img src="../images/in61_3.png" alt="image"/> for <i>a</i> &#60; 1. In this case, the expected number of nonzero entries in each row is less than 1. If <i>every</i> row contained exactly one nonzero entry, then the DFT could be computed via the following two step process. In the first step, we select the first two columns of <i>x</i>, denoted by <i>u</i><sup>(0)</sup> and <i>u</i><sup>(1)</sup>, and compute their DFTs <i>&#251;</i><sup>(0)</sup> and <i>&#251;</i><sup>(1)</sup>. Let <i>j<sub>i</sub></i> be the index of the unique nonzero entry in the <i>i</i>-th row of <i>x&#770;</i>, and let <i>a</i> be its value. Observe that <img src="../images/in61_4.png" alt="image"/> and <img src="../images/in61_5.png" alt="image"/> (where <i>&#969;</i>&#8242; is a primitive <img src="../images/in60_2.png" alt="image"/>-th root of unity), as these are the first two entries of the inverse Fourier transform of a 1-sparse signal <img src="../images/in61_6.png" alt="image"/>. Thus, in the second step, we can retrieve the value of the nonzero entry, equal to <img src="../images/in61_7.png" alt="image"/>, as well as the index <i>j<sub>i</sub></i> from the phase of the ratio <img src="../images/in61_8.png" alt="image"/>. (We first introduced this technique in <a href="13_Chapter04.xhtml">Chapter 4</a> and we referred to it as the &#8220;OFDM trick.&#8221;) The total time is dominated by the cost of the two DFTs of the columns, which is <a id="page_62"/><img src="../images/in62_1.png" alt="image"/>. Since the algorithm queries only a constant number of columns, its sample complexity is <img src="../images/in62_2.png" alt="image"/>.</p>
<p class="indent">In general, the distribution of the nonzero entries over the rows can be nonuniform, i.e., some rows may have multiple nonzero Fourier coefficients. Thus, our actual algorithm alternates the above recovery process between the columns and rows (see <a href="#fig5_1">Figure 5.1</a> for an illustration). Since the OFDM trick works only on 1-sparse columns/rows, we check the 1-sparsity of each column/row by sampling a constant number of additional entries. We then show that, as long as the sparsity constant <i>a</i> is small enough, this process recovers all entries in a logarithmic number steps with constant probability. The proof uses the fact that the probability of the existence of an &#8220;obstructing configuration&#8221; of nonzero entries which makes the process deadlocked (e.g., see <a href="#fig5_2">Figure 5.2</a>) is upper bounded by a small constant.</p>
<p class="indent">The algorithm is extended to the case of <img src="../images/in62_3.png" alt="image"/> via a reduction. Specifically, we subsample the signal <i>x</i> by the reduction ratio <img src="../images/in62_4.png" alt="image"/> for some small enough constant <i>&#945;</i> in each dimension. The subsampled signal <i>x</i>&#8242; has dimension <img src="../images/in62_5.png" alt="image"/>, where <img src="../images/in62_6.png" alt="image"/>. Since subsampling in time domain corresponds to &#8220;spectrum folding,&#8221; i.e., adding together all frequencies with indices that are equal modulo <img src="../images/in62_7.png" alt="image"/>, the nonzero entries of <i>x&#770;</i> are mapped into the entries of <i>x&#770;</i>&#8242;. It can be seen that, with constant probability, the mapping is one-to-one. If this is the case, we can use the earlier algorithm for sparse DFT to compute the nonzero frequencies in <img src="../images/in62_8.png" alt="image"/> time, using <i>O</i>(<i>k</i>) samples. We then use the OFDM trick to identify the positions of those frequencies.</p>
<p class="indent">Our second algorithm works for <i>approximately</i> sparse data, at sparsity <img src="../images/in62_9.png" alt="image"/>. Its general outline mimics that of the first algorithm. Specifically, it alternates between decoding columns and rows, assuming that they are 1-sparse. The decoding subroutine itself is similar to that of Algorithm 4.2 and uses <i>O</i>(log <i>n</i>) samples. The subroutine first checks whether the decoded entry is large; if not, the spectrum is unlikely to contain any large entry, and the subroutine terminates. The algorithm then subtracts the decoded entry from the column and checks whether the resulting signal contains no large entries in the spectrum (which would be the case if the original spectrum was approximately 1-sparse and the decoding was successful). The check is done by sampling <i>O</i>(log <i>n</i>) coordinates and checking whether their sum of squares is small. To prove that this check works with high probability, we use the fact that a collection of random rows of the Fourier matrix is likely to satisfy the Restricted Isometry Property of Candes and Tao [2006].</p>
<p class="indent">A technical difficulty in the analysis of the algorithm is that the noise accumulates in successive iterations. This means that a 1/ log<sup><i>O</i>(1)</sup> <i>n</i> fraction of the steps of the algorithm will fail. However, we show that the dependencies are &#8220;local,&#8221; which means that our analysis still applies to avast majority of the recovered entries. We continue the iterative decoding for log log <i>n</i> steps, which ensures that all but a 1/ log<sup><i>O</i>(1)</sup> <i>n</i> fraction of the large frequencies are correctly recovered. To recover the remaining frequencies, we resort to algorithms with worst-case guarantees.</p>
<div class="cap">
<p class="image"><a id="page_63"/><a id="fig5_1"/><img src="../images/fig5_1.png" alt="image"/></p>
<p class="figcaption"><b>Figure 5.1</b> An illustration of the 2D Sparse Fourier Transform algorithm. This illustration shows the &#8220;peeling&#8221; recovery process on an 8 &#215; 8 signal with 15 nonzero frequencies. In each step, the algorithm recovers all 1-sparse columns and rows (the recovered entries are depicted in red). The process converges after a few steps.</p>
</div>
<div class="cap">
<p class="image"><a id="page_64"/><a id="fig5_2"/><img src="../images/fig5_2.png" alt="image"/></p>
<p class="figcaption"><b>Figure 5.2</b> Examples of obstructing sequences of non-zero coefficients. None of the remaining rows or columns has a sparsity of 1.</p>
</div>
<p class="h2"><a id="ch5_1_3"/><b><span class="big1">5.1.3</span>&#160;&#160;&#160;&#160;Extensions</b></p>
<p class="noindent">Our algorithms have natural extensions to dimensions higher than 2. We do not include them in this chapter as the description and analysis are rather cumbersome.</p>
<p class="indent">Moreover, due to the equivalence between the 2D case and the 1D case where <i>n</i> is a product of different prime powers [Gilbert et al. 2005a, Iwen 2010a], our algorithms also give optimal sample complexity bounds for such values of <i>n</i> (e.g., <i>n</i> = 6<sup><i>t</i></sup>) in the average case.</p>
<p class="h2"><a id="ch5_1_4"/><b><span class="big1">5.1.4</span>&#160;&#160;&#160;&#160;Distributions</b></p>
<p class="noindent">In the exactly sparse case, we assume a Bernoulli model for the support of <i>x&#770;</i>. This means that for all <img src="../images/in64_1.png" alt="image"/> and thus <span class="f1">E</span>[|supp (<i>x&#770;</i>)|] = <i>k</i>. We assume an unknown predefined matrix <i>a<sub>i, j</sub></i> of values in <span class="f1">C</span>; if <i>x&#770;<sub>i, j</sub></i> is selected to be nonzero, its value is set to <i>a<sub>i, j</sub></i>.</p>
<p class="indent">In the approximately sparse case, we assume that the signal <i>x&#770;</i> is equal to <img src="../images/in64_2.png" alt="image"/>, where <i>x&#770;*<sub>i, j</sub></i> is the &#8220;signal&#8221; and <i>&#969;&#770;</i> is the &#8220;noise.&#8221; In particular, <i>x&#770;*</i> is drawn from the Bernoulli model, where <i>x&#770;</i>*<sub><i>i,j</i></sub> is drawn from {0, <i>a<sub>i,j</sub></i>} at random independently for each (<i>i, j</i>) for some values <i>a<sub>i,j</sub></i> and with <span class="f1">E</span>[| supp(<i>x</i>&#770;*) |] = <i>k</i>. We also require that |<i>a<sub>i,j</sub></i>| &#8805; <i>L</i> for some parameter <i>L. &#969;&#770;</i> is a complex Gaussian vector with variance <a id="page_65"/><i>&#963;</i><sup>2</sup> in both the real and imaginary axes independently on each coordinate; we notate this as <i>w&#770;</i> ~ <i>N</i><span class="f1"><sub>C</sub></span>(0, <i>&#963;</i><sup>2</sup><i>I<sub>n</sub></i>). We will need that <img src="../images/in65_1.png" alt="image"/> for a sufficiently large constant <i>C</i>, so that <img src="../images/in65_2.png" alt="image"/>.</p>
<p class="indent">We show in <a href="27_Appendix05.xhtml">Appendix E</a> that the sample lower bound of &#937;(<i>k</i> log (<i>n/k</i>)) on &#8467;<sub>2</sub>/&#8467;<sub>2</sub> recovery from Price and Woodruff [2011] applies to the above Bernoulli model.</p>
<p class="h1"><b><a id="ch5_2"/><span class="big">5.2</span>&#160;&#160;&#160;&#160;Algorithm for the Exactly Sparse Case</b></p>
<p class="noindent">The algorithm for the noiseless case depends on the sparsity <i>k</i> where <i>k</i> = <span class="f1">E</span>[|supp (<i>x</i>&#770;)|] for a Bernoulli distribution of the support.</p>
<p class="h2"><a id="ch5_2_1"/><b><span class="big1">5.2.1</span>&#160;&#160;&#160;&#160;Exact Algorithm:</b> <img src="../images/in65_3.png" alt="image"/></p>
<p class="noindent">In this section, we focus on the regime <img src="../images/in65_4.png" alt="image"/>. Specifically, we will assume that <img src="../images/in65_5.png" alt="image"/> for a (sufficiently small) constant <i>a</i> &#62; 0.</p>
<p class="indent">The algorithm B<small>ASIC</small>E<small>XACT2</small>DSFFT (SFT 5.0) is described as Algorithm 5.1. The key idea is to fold the spectrum into bins using the aliasing filter defined in <a href="11_Chapter02.xhtml">Chapter 2</a> and estimate frequencies which are isolated in a bin. The algorithm takes the FFT of a row and as a result frequencies in the same columns will get folded into the same row bin. It also takes the FFT of a column and consequently frequencies in the same rows will get folded into the same column bin. The algorithm then uses the OFDM trick introduced in <a href="13_Chapter04.xhtml">Chapter 4</a> to recover the columns and rows whose sparsity is 1. It iterates between the column bins and row bins, subtracting the recovered frequencies and estimating the remaining columns and rows whose sparsity is 1. An illustration of the algorithm running on an 8 &#215; 8 signal with 15 nonzero frequencies is shown in <a href="#fig5_1">Figure 5.1</a> in <a href="#ch5_1">Section 5.1</a>. The algorithm also takes a constant number of extra FFTs of columns and rows to check for collisions within a bin and avoid errors resulting from estimating bins where the sparsity is greater than 1. The algorithm uses three functions.</p>
<p class="bull">&#8226;&#160;&#160;F<small>OLD</small>T<small>O</small>B<small>INS</small>. This procedure folds the spectrum into <i>B<sub>r</sub></i> &#215; <i>B<sub>c</sub></i> bins using the aliasing filter described in <a href="11_Chapter02.xhtml">Chapter 2</a>.</p>
<p class="bull">&#8226;&#160;&#160;B<small>ASIC</small>E<small>ST</small>F<small>REQ</small>. Given the FFT of rows or columns, it estimates the frequency in the large bins. If there is no collision, i.e., if there is a single nonzero frequency in the bin, it adds this frequency to the result <i>&#969;&#770;</i> and subtracts its contribution to the row and column bins.</p>
<p class="bull">&#8226;&#160;&#160;B<small>ASIC</small>E<small>XACT2</small>DSFFT. This performs the FFT of the rows and columns and then iterates B<small>ASIC</small>E<small>ST</small>F<small>REQ</small> between the rows and columns until is recovers <i>x&#770;</i>.</p>
<p class="noindentt"><a id="page_66"/><b>Algorithm 5.1 SFT 5.0: Exact 2D Sparse Fourier Transform for</b> <img src="../images/in66_1.png" alt="image"/></p>
<p class="image"><img src="../images/pg66_1.png" alt="image"/></p>
<p class="noindentt"><a id="page_67"/><b>Analysis of</b> B<small>ASIC</small>E<small>XACT2</small>DSFFT</p>
<p class="noindent"><b>Lemma 5.1</b> For any constant <i>&#945;</i> &#62; 0, if <i>a</i> &#62; 0 is a sufficiently small constant, then assuming that all 1-sparsity tests in the procedure B<small>ASIC</small>E<small>ST</small>F<small>REQ</small> are correct, the algorithm reports the correct output with probability at least 1 &#8211; <i>O</i>(<i>&#945;</i>).</p>
<p class="noindentt"><b>Proof</b> The algorithm fails if there is a pair of nonzero entries in a column or row of <i>x&#770;</i> that &#8220;survives&#8221; <i>t<sub>max</sub></i> = <i>C</i> log <i>n</i> iterations. For this to happen there must be an &#8220;obstructing&#8221; sequence of nonzero entries <i>p</i><sub>1</sub>, <i>q</i><sub>1</sub>, <i>p</i><sub>2</sub>, <i>q</i><sub>2</sub> &#8230; <i>p<sub>t</sub></i>, 3 &#8804; <i>t</i> &#8804; <i>t<sub>max</sub></i>, such that for each <i>i</i> &#8805; 1, <i>p<sub>i</sub></i> and <i>q<sub>i</sub></i> are in the same column (&#8220;vertical collision&#8221;), while <i>q<sub>i</sub></i> and <i>p</i><sub><i>i</i>+1</sub> are in the same row (&#8220;horizontal collision&#8221;). Moreover, it must be the case that either the sequence &#8220;loops around,&#8221; i.e., <i>p</i><sub>1</sub> = <i>p<sub>t</sub></i>, or <i>t</i> &#62; <i>t<sub>max</sub></i>. We need to prove that the probability of either case is less than a. We focus on the first case; the second one is similar.</p>
<p class="indent">Assume that there is a sequence <i>p</i><sub>1</sub>, <i>q</i><sub>1</sub>, &#8230; <i>p<sub>t</sub></i><sub>&#8211;1</sub>, <i>q<sub>t</sub></i><sub>&#8211;1</sub> such that the elements in this sequence are all distinct, while <i>p</i><sub>1</sub> <b>=</b> <i>p<sub>t</sub></i>. If such a sequence exists, we say that the event <i>E<sub>t</sub></i> holds. The number of sequences satisfying <i>E<sub>t</sub></i> is at most <img src="../images/in67_1.png" alt="image"/>, while the probability that the entries corresponding to the points in a specific sequence are nonzero is at most <img src="../images/in67_2.png" alt="image"/>. Thus, the probability of <i>E<sub>t</sub></i> is at most</p>
<p class="image"><img src="../images/pg67_1.png" alt="image"/></p>
<p class="noindent">Therefore, the probability that one of the events <img src="../images/in67_3.png" alt="image"/> holds is at most <img src="../images/in67_4.png" alt="image"/>, which is smaller than <i>&#945;</i> for <i>a</i> small enough. &#9632;</p>
<p class="noindentt"><b>Lemma 5.2</b> The probability that any 1-sparsity test invoked by the algorithm is incorrect is at most <i>O</i>(1<i>/n</i><sup>(<i>c</i>&#8211;5)/2</sup>).</p>
<p class="indentt">The proof can be found in <a href="23_Appendix01.xhtml#appA_12">Appendix A.12</a>.</p>
<p class="noindentt"><b>Theorem 5.1</b> For any constant <i>&#945;</i>, the algorithm B<small>ASIC</small>E<small>XACT2</small>DSFFT uses <img src="../images/in67_5.png" alt="image"/> samples, runs in time <img src="../images/in67_6.png" alt="image"/> and returns the correct vector <i>x&#770;</i> with probability at least 1 &#8211; <i>O</i>(<i>&#945;</i>) as long as <i>a</i> is a small enough constant.</p>
<p class="noindentt"><b>Proof</b> From Lemmas 5.1 and 5.2, the algorithm returns the correct vector <i>x&#770;</i> with probability at least 1 &#8211; <i>O</i>(<i>&#945;</i>) &#8211; <i>O</i>(<i>n</i><sup>&#8212;(<i>c</i>&#8211;5)/2</sup>) = 1 &#8211; <i>O</i>(<i>&#945;</i>) for <i>c</i> &#62; 5.</p>
<p class="indent">The algorithm uses only <i>O</i>(<i>T</i>) = <i>O</i>(1) rows and columns of <i>x</i>, which yields <img src="../images/in67_5.png" alt="image"/> samples. The running time is bounded by the time needed to perform <i>O</i>(1) FFTs of rows and columns (in F<small>OLD</small>T<small>O</small>B<small>INS</small>) procedure, and <i>O</i>(log <i>n</i>) invocations of B<small>ASIC</small>E<small>ST</small>F<small>REQ</small>. Both components take time <img src="../images/in67_6.png" alt="image"/>. &#9632;</p>
<p class="h2"><a id="page_68"/><a id="ch5_2_2"/><b><span class="big1">5.2.2</span>&#160;&#160;&#160;&#160;Reduction to the Exact Algorithm:</b> <img src="../images/in68_1.png" alt="image"/></p>
<p class="noindent">Algorithm R<small>EDUCE</small>E<small>XACT2</small>DSFFT (SFT 5.1), which is for the case where <img src="../images/in68_2.png" alt="image"/>, is described in Algorithm 5.2. The key idea is to reduce the problem from the case where <img src="../images/in68_2.png" alt="image"/> to the case where <img src="../images/in65_4.png" alt="image"/>. To do that, we subsample the input time domain signal <i>x</i> by the reduction ratio <img src="../images/in68_3.png" alt="image"/> for some small enough <i>a</i>. The subsampled signal <i>x</i>&#8242; has dimension <img src="../images/in68_4.png" alt="image"/>, where <img src="../images/in68_5.png" alt="image"/>. This implies that the probability that any coefficient in <i>x&#770;</i>&#8242; is nonzero is at most <i>R</i><sup>2</sup> &#215; <i>k/n</i> = <i>a</i><sup>2</sup>/<i>k</i> = (<i>a</i><sup>2</sup>/<i>k</i>) <i>&#215;</i> (<i>k</i><sup>2</sup>/<i>a</i><sup>2</sup>)/<i>m</i> = <i>k/m</i>, since <i>m</i> = <i>k</i><sup>2</sup>/<i>a</i><sup>2</sup>. This means that we can use the algorithm B<small>ASIC</small>N<small>OISELESS2</small>DSFFT in <a href="#ch5_2_1">Section 5.2.1</a> to recover <i>x&#770;</i>&#8242;. Each of the entries of <i>x&#770;</i>&#8242; is a frequency in <i>x&#770;</i> which was folded into <i>x&#770;</i>&#8242;. We employ the same phase technique used in <a href="#ch5_2_1">Section 5.2.1</a> to recover their original frequency position in <i>x&#770;</i>.</p>
<p class="indent">The algorithm uses two functions.</p>
<p class="bullt">&#8226; R<small>EDUCE</small>T<small>O</small>B<small>ASIC</small>SFFT: This folds the spectrum into <i>O</i>(<i>k</i>) <i>&#215; O</i>(<i>k</i>) dimensions and performs the reduction to B<small>ASIC</small>E<small>XACT2</small>DSFFT. Note that only the <i>O</i>(<i>k</i>) elements of <i>x&#8217;</i> which will be used in B<small>ASIC</small>E<small>XACT2</small>DSFFT need to be computed.</p>
<p class="bull">&#8226; R<small>EDUCE</small>E<small>XACT2</small>DSFFT: This invokes the reduction as well as the phase technique to recover <i>x&#770;</i>.</p>
<p class="noindentt"><b>Analysis of</b> R<small>EDUCE</small>E<small>XACT2</small>DSFFT</p>
<p class="noindent"><b>Lemma 5.3</b> For any constant <i>&#945;</i>, for sufficiently small <i>a</i> there is a one-to-one mapping of frequency coefficients from <i>x&#770;</i> to <i>x&#770;</i>&#8242; with probability at least 1 &#8211; <i>&#945;</i>.</p>
<p class="noindentt"><b>Proof</b> The probability that there are at least two nonzero coefficients among the <i>R</i><sup>2</sup> coefficients in <i>x&#770;</i> that are folded together in <i>x&#770;</i>&#8242;, is at most</p>
<p class="image"><img src="../images/pg68_1.png" alt="image"/></p>
<p class="noindent">The probability that this event holds for any of the <i>m</i> positions in <i>x&#770;</i>&#8242; is at most <i>ma</i><sup>4</sup><i>/k</i><sup>2</sup> = (<i>k</i><sup>2</sup><i>/a</i><sup>2</sup>)<i>a</i><sup>4</sup><i>/k</i><sup>2</sup> <i>= a</i><sup>2</sup> which is less than <i>&#945;</i> for small enough <i>a</i>. Thus, with probability at least 1 &#8211; <i>&#945;</i> any nonzero coefficient in <i>x&#770;</i>&#8242; comes from only one nonzero coefficient in <i>x&#770;</i>. &#9632;</p>
<p class="noindentt"><b>Theorem 5.2</b> For any constant <i>&#945;</i> &#62; 0, there exists a constant <i>c</i> &#62; 0 such that if <img src="../images/in68_6.png" alt="image"/> then the algorithm R<small>EDUCE</small>E<small>XACT2</small>DSFFT uses <i>O</i>(<i>k</i>) samples, runs in time <i>O</i>(<i>k</i> log <i>k</i>) and returns the correct vector <i>x&#770;</i> with probability at least 1 &#8211; <i>&#945;</i>.</p>
<p class="noindentt"><b>Proof</b> By Theorem 5.1 and the fact that each coefficient in <i>x&#770;</i>&#8242; is nonzero with probability <i>O</i>(1<i>/k</i>), each invocation of the function R<small>EDUCE</small>T<small>O</small>B<small>ASIC</small>SFFT fails with probability at most <i>&#945;</i>. By Lemma 5.3, with probability at least 1 &#8211; <i>&#945;</i>, we could recover <i>x&#770;</i> correctly if each of the calls to R<small>ED</small>T<small>O</small>B<small>ASIC</small>SFFT returns the correct result. By the union bound, the algorithm R<small>EDUCE</small>E<small>XACT2</small>DSFFT fails with probability at most <i>&#945;</i> + 3 &#215; <i>&#945; = O</i>(<i>&#945;</i>).</p>
<p class="noindentt"><a id="page_69"/><b>Algorithm 5.2 SFT 5.1: Exact 2D Sparse Fourier Transform for</b> <img src="../images/in68_2.png" alt="image"/></p>
<p class="image"><img src="../images/pg69_1.png" alt="image"/></p>
<p class="indentt">The algorithm uses <i>O</i>(1) invocations of B<small>ASIC</small>E<small>XACT2</small>DSFFT on a signal of size <i>O</i>(<i>k</i>) <i>&#215; O</i>(<i>k</i>) in addition to <i>O</i>(<i>k</i>) time to recover the support using the OFDM trick. Noting that calculating the intersection <i>L</i> of supports takes <i>O</i>(<i>k</i>) time, the stated number of samples and running time then follow directly from Theorem 5.1. &#9632;</p>
<p class="h1"><b><a id="ch5_3"/><span class="big">5.3</span>&#160;&#160;&#160;&#160;Algorithm for the General Case</b></p>
<p class="noindent">The algorithm for noisy recovery R<small>OBUST2</small>DSFFT (SFT 6.0) is shown in Algorithm 5.3. The algorithm is very similar to the exactly sparse case. It first takes FFT of rows and columns using F<small>OLD</small>T<small>O</small>B<small>INS</small> procedure. It then iterates between the columns and rows, recovering frequencies in bins which are 1-sparse using the R<small>OBUST</small>E<small>STIMATE</small>C<small>OL</small> procedure. This procedure uses the function L<small>OCATE</small>S<small>IGNAL</small> from Algorithm 4.3 to make the estimation of the frequency positions robust to noise.</p>
<p class="noindentt"><a id="page_70"/><b>Algorithm 5.3 SFT 6.0: General 2D Sparse Fourier Transform for</b> <img src="../images/in65_4.png" alt="image"/></p>
<p class="image"><img src="../images/pg70_1.png" alt="image"/></p>
<p class="noindent"><a id="page_71"/><b>Preliminaries</b></p>
<p class="noindent">Following Candes and Tao [2006], we say that a matrix <i>A</i> satisfies a <i>Restricted Isometry Property</i> (RIP) of order <i>t</i> with constant <i>&#948;</i> &#62; 0 if, for all <i>t</i>-sparse vectors <i>y</i>, we have <img src="../images/in71_1.png" alt="image"/>.</p>
<p class="indent">Suppose all columns <i>A<sub>i</sub></i> of an <i>N</i> &#215; <i>M</i> matrix <i>A</i> have unit norm. Let <i>&#956;</i> = max<sub><i>i&#8800;j</i></sub> |<i>A<sub>i</sub></i> &#183; <i>A<sub>j</sub></i>| be the <i>coherence</i> of <i>A</i>. It is folklore<sup><a id="fn3" href="#rfn3">3</a></sup> that <i>A</i> satisfies the RIP of order <i>t</i> with the constant <i>&#948;</i> <b>=</b> (<i>t</i> &#8211; 1)<i>&#956;</i>.</p>
<p class="indent">Suppose that the matrix <i>A</i> is an <i>M</i> <b>&#215;</b> <i>N</i> submatrix of the <i>N</i> <b>&#215;</b> <i>N</i> Fourier matrix <i>F</i>, with each the <i>M</i> rows of <i>A</i> chosen uniformly at random from the rows of <i>F</i>. It is immediate from the Hoeffding bound that if <i>M</i> <b>=</b> <i>b&#956;</i><sup>2</sup> log(<i>N/&#947;</i>) for some large enough constant <i>b</i> &#62; 1 then the matrix <i>A</i> has coherence at most <i>&#956;</i> with probability 1 &#8211; <i>&#947;</i>. Thus, for <i>M</i> <b>=</b> &#920;(<i>t</i><sup>2</sup> &#183; <i>t</i> log <i>N</i>), <i>A</i> satisfies the RIP of order <i>t</i> with constant <i>&#948;</i> <b>=</b> 0.5 with probability 1 &#8211; 1/<i>N<sup>t</sup></i>.</p>
<p class="h2"><a id="ch5_3_1"/><b><span class="big1">5.3.1</span>&#160;&#160;&#160;&#160;Analysis of Each Stage of Recovery</b></p>
<p class="noindent">Here, we show that each step of the recovery is correct with high probability using the following two lemmas. The first lemma shows that with very low probability the R<small>OBUST</small>E<small>STIMATE</small>C<small>OL</small> procedure generates a false negative (misses a frequency), false positive (adds a fake frequency), or a bad update (wrong estimate of a frequency). The second lemma is analogous to Lemma 5.2 and shows that the probability that the 1-sparse test fails when there is noise is low.</p>
<p class="noindentt"><b>Lemma 5.4</b> Consider the recovery of a column/row <i>j</i> in R<small>OBUST</small>E<small>STIMATE</small>C<small>OL</small>, where <i>&#251;</i> and <i>v&#770;</i> are the results of F<small>OLD</small>T<small>O</small>B<small>INS</small> on <i>x&#770;</i>. Let <img src="../images/in71_2.png" alt="image"/> denote the <i>j</i>th column/row of <i>x</i>&#770;. Suppose <i>y</i> is drawn from a permutation invariant distribution <i>y</i> = <i>y</i><sup>head</sup> + <i>y</i><sup>residue</sup> + <i>y</i><sup>gauss</sup>, where <img src="../images/in71_3.png" alt="image"/>, and <i>y</i><sup>gauss</sup> is drawn from the <img src="../images/in60_2.png" alt="image"/>-dimensional normal distribution <img src="../images/in71_4.png" alt="image"/> with standard deviation <i>&#963;</i> = &#8714; <i>L/n</i><sup>1/4</sup> in each coordinate on both real and imaginary axes. We do not require that <i>y</i><sup>head</sup>, <i>y</i><sup>residue</sup>, and <i>y</i><sup>gauss</sup> are independent except for the permutation invariance of their sum.</p>
<p class="indent">Consider the following bad events.</p>
<p class="hang"><b>False negative.</b> supp(<i>y</i><sup>head</sup>) <b>=</b> {<i>i</i>} and R<small>OBUST</small>E<small>STIMATE</small>C<small>OL</small> does not update coordinate <i>i</i>.</p>
<p class="hang"><b>False positive.</b> R<small>OBUST</small>E<small>STIMATE</small>C<small>OL</small> updates some coordinate <i>i</i> but supp(<i>y</i><sup>head</sup>) <b>&#8800;</b> {<i>i</i>}.</p>
<p class="hang"><a id="page_72"/><b>Bad update.</b> supp(<i>y</i><sup>head</sup>) = {<i>i</i>} and coordinate <i>i</i> is estimated by <i>b</i> with <img src="../images/in72_1.png" alt="image"/>.</p>
<p class="indent">For any constant <i>c</i> and <i>&#8714;</i> below a sufficiently small constant, there exists a distribution over sets <i>T, T</i>&#8242; of size <i>O</i>(log <i>n</i>), such that as a distribution over <i>y</i> and <i>T, T</i>&#8242; we have</p>
<p class="bull">&#8226;&#160;&#160;The probability of a false negative is 1/ log<sup><i>c</i></sup> <i>n</i>,</p>
<p class="bull">&#8226;&#160;&#160;The probability of a false positive is 1/ <i>n<sup>c</sup></i>,</p>
<p class="bull">&#8226;&#160;&#160;The probability of a bad update is 1/ log<sup><i>c</i></sup> <i>n</i>.</p>
<p class="indentt">The proof can be found in <a href="23_Appendix01.xhtml#appA_13">Appendix A.13</a>.</p>
<p class="noindentt"><b>Lemma 5.5</b> Let <i>y</i> &#8712; <span class="f1">C</span><sup><i>m</i></sup> be drawn from a permutation invariant distribution with <i>r</i> &#8805; 2 nonzero values. Suppose that all the nonzero entries of <i>y</i> have absolute value at least <i>L</i>. Choose <i>T</i> <b>&#8834;</b> [<i>m</i>] uniformly at random with <i>t</i> &#8788; |<i>T</i><b>| =</b> <i>O</i>(<i>c</i><sup>3</sup> log <i>m</i>).</p>
<p class="indent">Then, the probability that there exists a <i>y</i>&#8217; with ||<i>y</i>&#8217;||<sub>0</sub> &#8804; 1 and</p>
<p class="image"><img src="../images/pg72_1.png" alt="image"/></p>
<p class="noindent">is at most <img src="../images/in72_2.png" alt="image"/> whenever <i>&#8714;</i> &#60; 1/8.</p>
<p class="noindentt"><b>Proof</b> The proof can be found in <a href="23_Appendix01.xhtml#appA_14">Appendix A.14</a>. &#9632;</p>
<p class="h2"><a id="ch5_3_2"/><b><span class="big1">5.3.2</span>&#160;&#160;&#160;&#160;Analysis of Overall Recovery</b></p>
<p class="noindent">Recall that we are considering the recovery of a signal <img src="../images/in72_3.png" alt="image"/>, where <i>x&#770;*</i> is drawn from the Bernoulli model with expected <img src="../images/in72_4.png" alt="image"/> nonzero entries for a sufficiently small constant <i>a</i>, and <i>&#969;&#770;</i> ~ <i>N</i><sub><span class="f1">C</span></sub>(0, <i>&#963;</i><sup>2</sup><i>I<sub>n</sub></i>) with <img src="../images/in72_5.png" alt="image"/> for sufficiently small <i>&#8712;</i>.</p>
<p class="indent">It will be useful to consider a bipartite graph representation <i>G</i> of <i>x</i>&#770;*. We construct a bipartite graph with <img src="../images/in60_2.png" alt="image"/> nodes on each side, where the left side corresponds to rows and the right side corresponds to columns. For each (<i>i, j</i>) <i><b>&#8714;</b></i> supp(<i>x&#770;</i>*), we place an edge between left node <i>i</i> and right node <i>j</i> of weight <i>x&#770;*</i><sub>(<i>i, j</i>)</sub>.</p>
<p class="indent">Our algorithm is a &#8220;peeling&#8221; procedure on this graph. It iterates over the vertices, and can with a &#8220;good probability&#8221; recover an edge if it is the only incident edge on a vertex. Once the algorithm recovers an edge, it can remove it from the graph. The algorithm will look at the column vertices, then the row vertices, then repeat; these are referred to as <i>stages</i>. Supposing that the algorithm succeeds at recovery on each vertex, this gives a canonical order to the removal of edges. Call this the <i>ideal</i> ordering.</p>
<p class="indent">In the ideal ordering, an edge <i>e</i> is removed based on one of its incident vertices <i>v</i>. This happens after all other edges reachable from <i>v</i> without passing through <i>e</i> <a id="page_73"/>are removed. Define the <i>rank</i> of <i>v</i> to be the number of such reachable edges, and rank(<i>e</i>) <i><b>=</b></i> rank(<i>v</i>) <i><b>+</b></i> 1 (with rank(<i>v</i>) undefined if <i>v</i> is not used for recovery of any edge).</p>
<p class="noindentt"><b>Lemma 5.6</b> Let <i>c, &#945;</i> be arbitrary constants, and <i>a</i> be a sufficiently small constant depending on <i>c, &#945;</i>. Then with 1 &#8211; <i>&#945;</i> probability every component in <i>G</i> is a tree and at most <i>k/</i> log<sup><i>c</i></sup> <i>n</i> edges have rank at least log log <i>n</i>.</p>
<p class="noindentt"><b>Proof</b> Each edge of <i>G</i> appears independently with probability <img src="../images/in73_1.png" alt="image"/>. There are at most <img src="../images/in73_2.png" alt="image"/> cycles of length <i>t</i>. The probability that any cycle of length <i>t</i> exists is at most <i>a<sup>t</sup></i>, so the chance any cycle exists is less than <i>a</i><sup>2</sup>/(1 &#8211; <i>a</i><sup>2</sup>) &#60; <i>&#945;</i>/2 for sufficiently small <i>a</i>.</p>
<p class="indent">Each vertex has expected degree <i>a</i> &#60; 1. Exploring the component for any vertex <i>v</i> is then a subcritical branching process, so the probability that <i>v</i>&#8217;s component has size at least log log <i>n</i> is 1/ log<sup><i>c</i></sup> <i>n</i> for sufficiently small <i>a</i>. Then for each edge, we know that removing it causes each of its two incident vertices to have component size less than log log <i>n</i> &#8211; 1 with 1 &#8211; 1/ log<sup><i>c</i></sup> <i>n</i> probability. Since the rank is one more than the size of one of these components, the rank is less than log log <i>n</i> with 1 &#8211; 2/ log<sup><i>c</i></sup> <i>n</i> probability.</p>
<p class="indent">Therefore, the expected number of edges with rank at least log log <i>n</i> is 2<i>k/</i> log<i><sup>c</sup> n</i>. Hence, with probability 1 &#8211; <i>&#945;</i>/2 there are at most (1/<i>&#945;</i>)4<i>k</i>/ log<sup><i>c</i></sup> <i>n</i> such edges; adjusting <i>c</i> gives the result. &#9632;</p>
<p class="noindentt"><b>Lemma 5.7</b> Let R<small>OBUST2</small>DSFFT&#8217; be a modified R<small>OBUST2</small>DSFFT that avoids false negatives or bad updates: whenever a false negative or bad update would occur, an oracle corrects the algorithm. With large constant probability, R<small>OBUST2</small>DSFFT&#8217; recovers <i>z&#770;</i> such that there exists a (<i>k</i>/ log<i><sup>c</sup> n</i>)-sparse <i>z&#770;</i>&#8242; satisfying</p>
<p class="image"><img src="../images/pg73_1.png" alt="image"/></p>
<p class="noindent">Furthermore, only <i>O</i>(<i>k/</i> log<sup><i>c</i></sup> <i>n</i>) false negatives or bad updates are caught by the oracle.</p>
<p class="noindentt"><b>Proof</b> The proof can be found in <a href="23_Appendix01.xhtml#appA_15">Appendix A.15</a> &#9632;</p>
<p class="noindentt"><b>Lemma 5.8</b> For any constant <i>&#945;</i> &#62; 0, the algorithm R<small>OBUST2</small>DSFFT can with probability 1 &#8211; <i>&#945;</i> recover <i>z&#770;</i> such that there exists a (<i>k</i>/ log<sup><i>c</i>&#8211;1</sup> <i>n</i>)-sparse <i>z&#770;&#8217;</i> satisfying</p>
<p class="image"><img src="../images/pg73_2.png" alt="image"/></p>
<p class="noindent">using <i>O</i>(<i>k</i> log <i>n</i>) samples and <i>O</i>(<i>k</i> log<sup>2</sup> <i>n</i>) time.</p>
<p class="noindentt"><b>Proof</b> To do this, we will show that changing the effect of a single call to R<small>OBUST</small>E<small>STIMATE</small>C<small>OL</small> can only affect log <i>n</i> positions in the output of R<small>OBUST2</small>DSFFT. By Lemma 5.7, we <a id="page_74"/>can, with large constant probability, turn R<small>OBUST2</small>DSFFT into R<small>OBUST2</small>DSFFT&#8217; with only <i>O</i>(<i>k/</i> log<sup><i>c</i></sup> <i>n</i>) changes to calls to R<small>OBUST</small>E<small>STIMATE</small>C<small>OL</small>. This means the outputs of R<small>OBUST2</small>DSFFT and of R<small>OBUST2</small>DSFFT&#8217; only differ in <i>O</i>(<i>k/</i> log<sup><i>c</i>&#8211;1</sup> <i>n</i>) positions.</p>
<p class="indent">We view R<small>OBUST</small>E<small>STIMATE</small>C<small>OL</small> as trying to estimate a vertex. Modifying it can change from recovering one edge (or none) to recovering a different edge (or none). Thus, a change can only affect at most two calls to R<small>OBUST</small>E<small>STIMATE</small>C<small>OL</small> in the next stage. Hence, in <i>r</i> stages, at most 2<sup><i>r</i>&#8211;1</sup> calls may be affected, so at most 2<sup><i>r</i></sup> edges may be recovered differently.</p>
<p class="indent">Because we refuse to recover any edge with rank at least log log <i>n</i>, the algorithm has at most log log <i>n</i> stages. Hence at most log <i>n</i> edges may be recovered differently as a result of a single change to R<small>OBUST</small>E<small>STIMATE</small>C<small>OL</small>. &#9632;</p>
<p class="noindentt"><b>Theorem 5.3</b> Our overall algorithm can recover <i>x&#770;</i>&#8242; satisfying</p>
<p class="image"><img src="../images/pg74_1.png" alt="image"/></p>
<p class="noindent">with probability 1 &#8211; <i>&#945;</i> for any constants <i>c, &#945;</i> &#62; 0 in <i>O</i>(<i>k</i> log <i>n</i>) samples and <i>O</i>(<i>k</i> log<sup>2</sup> <i>n</i>) time, where <img src="../images/in74_1.png" alt="image"/> for some constant <i>a</i> &#62; 0.</p>
<p class="noindentt"><b>Proof</b> By Lemma 5.8, we can recover an <i>O</i>(<i>k</i>)-sparse <i>z&#770;</i> such that there exists an (<i>k/</i> log<sup><i>c</i>&#8211;1</sup> <i>n</i>)-sparse <i>z&#770;</i>&#8242; with</p>
<p class="image"><img src="../images/pg74_2.png" alt="image"/></p>
<p class="noindent">with arbitrarily large constant probability for any constant <i>c</i> using <i>O</i>(<i>k</i> log<sup>2</sup> <i>n</i>) time and <i>O</i>(<i>k</i> log <i>n</i>) samples. Then by Theorem 4.3, we can recover a <i>z&#770;</i>&#8242; in <i>O</i>(<i>k</i> log<sup>2</sup> <i>n</i>) time and <i>O</i>(<i>k</i> log<sup>4&#8212;<i>c</i></sup> <i>n</i>) samples satisfying</p>
<p class="image"><img src="../images/pg74_3.png" alt="image"/></p>
<p class="noindent">and hence <i>x&#770;</i> &#8242; &#8788; <i>z&#770;</i> + <i>z&#770;</i>&#8242; is a good reconstruction for <i>x&#770;</i>. &#9632;</p>
<p class="line"/>
<p class="foot"><a id="rfn1" href="#fn1">1</a>. Note that this model subsumes the scenario where the values of the nonzero coordinates are chosen i.i.d. from some distribution.</p>
<p class="foot"><a id="rfn2" href="#fn2">2</a>. A popular alternative is to use the hypergeometric distribution over the set of nonzero entries instead of the Bernoulli distribution. The advantage of the former is that it yields vectors of sparsity <i>exactly</i> equal to <i>k</i>. In this chapter we opted for the Bernoulli model since it is simpler to analyze. However, both models are quite similar. In particular, for large enough <i>k</i>, the actual sparsity of vectors in the Bernoulli model is sharply concentrated around <i>k</i>.</p>
<p class="foot"><a id="rfn3" href="#fn3">3</a>. It is a direct corollary of Gershgorin&#8217;s theorem applied to any <i>t</i> columns of <i>A</i>.</p>
</body>
</html>