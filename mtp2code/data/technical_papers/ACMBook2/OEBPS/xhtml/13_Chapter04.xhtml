<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Sparse Fourier Transform: Theory and Practice</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<link rel="stylesheet" type="application/vnd.adobe-page-template+xml" href="../styles/page-template.xpgt"/>
</head>
<body>
<p class="chno"><a id="page_39"/><b>4</b></p>
<p class="chtitle"><b>Optimizing Runtime Complexity</b></p>
<p class="h1"><b><a id="ch4_1"/><span class="big">4.1</span>&#160;&#160;&#160;&#160;Introduction</b></p>
<p class="noindent">The algorithm presented in <a href="12_Chapter03.xhtml">Chapter 3</a> was the first algorithm to outperform FFT in practice for reasonably sparse signals. However, it has a runtime of</p>
<p class="image"><img src="../images/pg39_1.png" alt="Image"/>,</p>
<p class="noindent">which is polynomial in <i>n</i> and only outperforms FFT for <i>k</i> smaller than &#920;(<i>n</i>/ log <i>n</i>).</p>
<p class="h2"><b><a id="ch4_1_1"/><span class="big1">4.1.1</span>&#160;&#160;Results</b></p>
<p class="noindent">In this chapter, we address this limitation by presenting two new algorithms for the Sparse Fourier Transform. We show</p>
<p class="bull">&#8226;&#160;&#160;an <i>O</i>(<i>k</i> log <i>n</i>)-time algorithm for the exactly <i>k</i>-sparse case, and</p>
<p class="bull">&#8226;&#160;&#160;an <i>O</i>(<i>k</i> log <i>n</i> log(<i>n/k</i>))-time algorithm for the general case.</p>
<p class="noindentt">The key property of both algorithms is their ability to achieve <i>o</i>(<i>n</i> log <i>n</i>) time, and thus improve over the FFT, for <i>any k</i> = <i>o</i>(<i>n</i>). These algorithms are the first known algorithms that satisfy this property. Moreover, if one assumes that FFT is optimal and hence the DFT cannot be computed in less than <i>O</i>(<i>n</i> log <i>n</i>) time, the algorithm for the exactly <i>k</i>-sparse case is <i>optimal</i><sup><a id="fn1" href="#rfn1">1</a></sup> as long as <i>k</i> = <i>n</i><sup>&#937;(1)</sup>. Under the same assumption, the result for the general case is at most one log log <i>n</i> factor away from the optimal runtime for the case of &#8220;large&#8221; sparsity <img src="../images/in39_1.png" alt="Image"/>.</p>
<p class="indent">For the general case, given a signal <i>x</i>, the algorithm computes a <i>k-sparse approximation</i> <img src="../images/in39_2.png" alt="Image"/> of its Fourier transform, <i>x&#770;</i> that satisfies the following <i>guarantee</i>:</p>
<p class="eqn"><a id="eq4_1"/><img src="../images/eq4_1.png" alt="Image"/></p>
<p class="noindent"><a id="page_40"/>where <i>C</i> is some approximation factor and the minimization is over <i>k</i>-sparse signals.</p>
<p class="indent">Furthermore, our algorithm for the exactly sparse case is quite simple and has low big-Oh constants. In particular, our implementation of a variant of this algorithm, described in <a href="15_Chapter06.xhtml">Chapter 6</a>, is faster than FFTW, a highly efficient implementation of the FFT, for <i>n</i> = 2<sup>22</sup> and <i>k</i> &#8804; 2<sup>17</sup> [Hassanieh et al. 2012]. In contrast, for the same signal size, the algorithms in <a href="12_Chapter03.xhtml">Chapter 3</a> were faster than FFTW only for <i>k</i> &#8804; 2000.<sup><a id="fn2" href="#rfn2">2</a></sup></p>
<p class="indent">We complement our algorithmic results by showing that any algorithm that works for the general case must use at least &#937;(<i>k</i> log(<i>n/k</i>)/ log log <i>n</i>) samples from <i>x</i>. The proof of this lower bound can be found in <a href="25_Appendix03.xhtml">Appendix C</a>. The lower bound uses techniques from Price and Woodruff [2011], which shows a lower bound of &#937;(<i>k</i> log(<i>n/k</i>)) for the number of <i>arbitrary</i> linear measurements needed to compute the <i>k</i>-sparse approximation of an <i>n</i>-dimensional vector <i>x&#770;</i>. In comparison to Price and Woodruff [2011], our bound is slightly worse but it holds even for <i>adaptive</i> sampling, where the algorithm selects the samples based on the values of the previously sampled coordinates.<sup><a id="fn3" href="#rfn3">3</a></sup> Note that our algorithms are <i>non-adaptive</i>, and thus limited by the more stringent lower bound of Price and Woodruff [2011].</p>
<p class="h2"><b><a id="ch4_1_2"/><span class="big1">4.1.2</span>&#160;&#160;Techniques</b></p>
<p class="noindent">Recall from <a href="12_Chapter03.xhtml">Chapter 3</a> that we can use the flat window filters coupled with a random permutation of the spectrum to bin/bucketize the Fourier coefficients into a small number of buckets. We can then use that to estimate the positions and values of the large frequency coefficients that were isolated in their own bucket. Here, we use the same filters introduced in <a href="12_Chapter03.xhtml">Chapter 3</a>. In this case, a filter <i>G</i> have the property that the value of <i>&#284;</i> is &#8220;large&#8221; over a constant fraction of the pass region, referred to as the &#8220;super-pass&#8221; region. We say that a coefficient is &#8220;isolated&#8221; if it falls into a filter&#8217;s super-pass region and no other coefficient falls into filter&#8217;s pass region. <a id="page_41"/>Since the super-pass region of our filters is a constant fraction of the pass region, the probability of isolating a coefficient is constant.</p>
<p class="indent">However, the main difference in this chapter, that allows us to achieve the stated running times, is a fast method for locating and estimating isolated coefficients. Further, our algorithm is iterative, so we also provide a fast method for updating the signal so that identified coefficients are not considered in future iterations. Below, we describe these methods in more detail.</p>
<p class="noindentt"><b>New Techniques: Location and Estimation</b></p>
<p class="noindent">Our location and estimation methods depends on whether we handle the exactly sparse case or the general case. In the exactly sparse case, we show how to estimate the position of an isolated Fourier coefficient using only two samples of the filtered signal. Specifically, we show that the phase difference between the two samples is linear in the index of the coefficient, and hence we can recover the index by estimating the phases. This approach is inspired by the frequency offset estimation in orthogonal frequency division multiplexing (OFDM), which is the modulation method used in modern wireless technologies (see Heiskala and Terry [2001, chapter 2]).</p>
<p class="indent">In order to design an algorithm<sup><a id="fn4" href="#rfn4">4</a></sup> for the general case, we employ a different approach. Specifically, we can use two samples to estimate (with constant probability) individual bits of the index of an isolated coefficient. Similar approaches have been employed in prior work. However, in those papers, the index was recovered bit by bit, and one needed &#937;(log log <i>n</i>) samples per bit to recover <i>all</i> bits correctly with constant probability. In contrast, we recover the index one <i>block of bits</i> at a time, where each block consists of <i>O</i>(log log <i>n</i>) bits. This approach is inspired by the fast sparse recovery algorithm of Gilbert et al. [2010]. Applying this idea in our context, however, requires new techniques. The reason is that, unlike in Gilbert et al. [2010], we do not have the freedom of using arbitrary &#8220;linear measurements&#8221; of the vector <i>x&#770;</i>, and we can only use the measurements induced by the Fourier transform.<sup><a id="fn5" href="#rfn5">5</a></sup> As a result, the extension from &#8220;bit recovery&#8221; to &#8220;block recovery&#8221; is the most technically involved part of the algorithm. <a href="#ch4_3_1">Section 4.3.1</a> contains further intuition on this part.</p>
<p class="h4"><a id="page_42"/><b>New Techniques: Updating the Signal</b></p>
<p class="noindent">The aforementioned techniques recover the position and the value of any isolated coefficient. However, during each filtering step, each coefficient becomes isolated only with constant probability. Therefore, the filtering process needs to be repeated to ensure that each coefficient is correctly identified. In <a href="12_Chapter03.xhtml">Chapter 3</a>, the algorithm simply performs the filtering <i>O</i>(log <i>n</i>) times and uses the median estimator to identify each coefficient with high probability. This, however, would lead to a running time of <i>O</i>(<i>k</i> log<sup>2</sup> <i>n</i>) in the <i>k</i>-sparse case, since each filtering step takes <i>k</i> log <i>n</i> time.</p>
<p class="indent">One could reduce the filtering time by subtracting the identified coefficients from the signal. In this way, the number of non-zero coefficients would be reduced by a constant factor after each iteration, so the cost of the first iteration would dominate the total running time. Unfortunately, subtracting the recovered coefficients from the signal is a computationally costly operation, corresponding to a so-called <i>non-uniform</i> DFT (see Gilbert et al. [2008] for details). Its cost would override any potential savings.</p>
<p class="indent">In this chapter, we introduce a different approach: instead of subtracting the identified coefficients from the <i>signal</i>, we subtract them directly from the <i>bins</i> obtained by filtering the signal. The latter operation can be done in time linear in the number of subtracted coefficients, since each of them &#8220;falls&#8221; into only one bin. Hence, the computational costs of each iteration can be decomposed into two terms, corresponding to filtering the original signal and subtracting the coefficients. For the exactly sparse case these terms are as follows.</p>
<p class="bullt">&#8226;&#160;&#160;The cost of filtering the original signal is <i>O</i>(<i>B</i> log <i>n</i>), where <i>B</i> is the number of bins. <i>B</i> is set to <i>O</i>(<i>k</i>&#8242;), where <i>k</i>&#8242; is the number of yet-unidentified coefficients. Thus, initially <i>B</i> is equal to <i>O</i>(<i>k</i>), but its value decreases by a constant factor after each iteration.</p>
<p class="bull">&#8226;&#160;&#160;The cost of subtracting the identified coefficients from the bins is <i>O</i>(<i>k</i>).</p>
<p class="noindentt">Since the number of iterations is <i>O</i>(log <i>k</i>), and the cost of filtering is dominated by the first iteration, the total running time is <i>O</i>(<i>k</i> log <i>n</i>) for the exactly sparse case.</p>
<p class="indent">For the general case, we need to set <i>k</i>&#8242; and <i>B</i> more carefully to obtain the desired running time. The cost of each iterative step is multiplied by the number of filtering steps needed to compute the location of the coefficients, which is &#920;(log(<i>n/B</i>)).If we set <i>B</i> = &#920;(<i>k</i>&#8242;), this would be &#920;(log <i>n</i>) in most iterations, giving a &#920;(<i>k</i> log<sup>2</sup> <i>n</i>) running time. This is too slow when <i>k</i> is close to <i>n</i>. We avoid this by decreasing <i>B</i> more slowly and <i>k</i>&#8242; more quickly. In the <i>r</i>-th iteration, we set <i>B</i> = <i>k</i>/ poly(<i>r</i>). This <a id="page_43"/>allows the total number of bins to remain <i>O</i>(<i>k</i>) while keeping log(<i>n/B</i>) small&#8212;at most <i>O</i>(log log <i>k</i>) more than log (<i>n/k</i>). Then, by having <i>k</i>&#8242; decrease according to <i>k</i>&#8242; = <i>k/r</i><sup>&#920;(<i>r</i>)</sup> rather than <i>k</i>/2<sup>&#920;(<i>r</i>)</sup>, we decrease the number of rounds to <i>O</i>(log <i>k</i>/ log log <i>k</i>). Some careful analysis shows that this counteracts the log log <i>k</i> loss in the log (<i>n/B</i>) term, achieving the desired <i>O</i>(<i>k</i> log <i>n</i> log(<i>n/k</i>)) running time.</p>
<p class="h1"><b><a id="ch4_2"/><span class="big">4.2</span>&#160;&#160;&#160;&#160;Algorithm for the Exactly Sparse Case</b></p>
<p class="noindent">In this section, we assume <i>x&#770;</i><sub>i</sub> &#8712; {&#8722;<i>L</i>, &#8230;, <i>L</i>} for some precision parameter <i>L</i>. To simplify the bounds, we assume <i>L</i> &#8804; <i>n<sup>c</sup></i> for some constant <i>c</i> &#62; 0; otherwise the log <i>n</i> term in the running time bound is replaced by log <i>L</i>. We also assume that <i>x&#770;</i> is exactly <i>k</i>-sparse. We will use the filter <i>G</i> with parameter <i>&#948;</i> = 1/(4<i>n<sup>2</sup>L</i>).</p>
<p class="noindentt"><b>Definition 4.1</b> We say that <img src="../images/in43_1.png" alt="Image"/> is a <i>flat window function</i> with parameters <i>B</i> &#62; 1, <i>&#948;</i> &#62; 0, and <i>&#945;</i> &#62; 0 if <img src="../images/in43_2.png" alt="Image"/> and <img src="../images/in43_3.png" alt="Image"/> satisfies</p>
<p class="bull">&#8226;&#160;&#160;<img src="../images/in43_4.png" alt="Image"/> = 1 for |<i>i</i>| &#8804; (1 &#8722; <i>&#945;</i>)<i>n</i>/(2<i>B</i>) and <img src="../images/in43_4.png" alt="Image"/> = 0 for |<i>i</i>| &#8804; <i>n</i>/(2<i>B</i>),</p>
<p class="bull">&#8226;&#160;&#160;<img src="../images/in43_4.png" alt="Image"/> &#8712; [0,1] for all <i>i</i>, and</p>
<p class="bull">&#8226;&#160;&#160;<img src="../images/pg43_1.png" alt="Image"/>.</p>
<p class="indentt">The above notion corresponds to the (1/(2<i>B</i>), (1 &#8722; <i>&#945;</i>)/(2<i>B</i>), <i>&#948;, O</i>(<i>B/&#945;</i> log(<i>n/&#948;</i>))-flat window function. In <a href="26_Appendix04.xhtml">Appendix D</a>, we give efficient constructions of such window functions, where <i>G</i> can be computed in <img src="../images/in43_5.png" alt="Image"/> time and for each <i>i</i>, <img src="../images/in43_4.png" alt="Image"/> can be computed in <i>O</i>(log(<i>n/&#948;</i>)) time. Of course, for <i>i</i> &#8713; [(1 &#8722; <i>&#945;</i>)<i>n</i>/(2<i>B</i>), <i>n</i>/(2<i>B</i>)], <img src="../images/in43_4.png" alt="Image"/> &#8712; {0,1} can be computed in <i>O</i>(1) time. The fact that <img src="../images/in43_4.png" alt="Image"/> takes <i>&#969;</i>(1) time to compute for <i>i</i> &#8712; [(1 &#8722; <i>&#945;</i>)<i>n</i>/(2<i>B</i>), <i>n</i>/(2<i>B</i>)] will add some complexity to our algorithm and analysis. We will need to ensure that we rarely need to compute such values. A practical implementation might find it more convenient to precompute the window functions in a preprocessing stage, rather than compute them on the fly.</p>
<p class="indent">The algorithm N<small>OISELESS</small>S<small>PARSE</small>FFT (SFT 3.0) is described as Algorithm 4.1. The algorithm has three functions:</p>
<p class="bull">&#8226;&#160;&#160;H<small>ASH</small>T<small>O</small>B<small>INS</small>. This permutes the spectrum of <img src="../images/in43_6.png" alt="Image"/> with <i>P<sub>&#963;,a,b</sub></i>, then &#8220;hashes&#8221; to <i>B</i> bins. The guarantee will be described in Lemma 4.1.</p>
<p class="bull">&#8226;&#160;&#160;N<small>OISELESS</small>S<small>PARSE</small>FFTI<small>NNER</small>. Given time-domain access to <i>x</i> and a sparse vector <i>z&#770;</i> such that <img src="../images/in43_6.png" alt="Image"/> is <i>k</i>&#8242;-sparse, this function finds &#8220;most&#8221; of <img src="../images/in43_6.png" alt="Image"/>.</p>
<p class="bull">&#8226;&#160;&#160;N<small>OISELESS</small>S<small>PARSE</small>FFT. This iterates N<small>OISELESS</small>S<small>PARSE</small>FFTI<small>NNER</small> until it finds <i>x&#770;</i> exactly.</p>
<p class="noindentt"><a id="page_44"/><b>Algorithm 4.1 SFT 3.0: Exact Sparse Fourier Transform for</b> <i>k</i> = <i>o</i>(<i>n</i>)</p>
<p class="image"><img src="../images/alg4_1.png" alt="Image"/></p>
<p class="indent">We analyze the algorithm &#8220;bottom-up,&#8221; starting from the lower-level procedures.</p>
<p class="noindentt"><b>Analysis of</b> N<small>OISELESS</small>S<small>PARSE</small>FFTI<small>NNER</small> <b>and</b> H<small>ASH</small>T<small>O</small>B<small>INS</small></p>
<p class="noindent">For any execution of N<small>OISELESS</small>S<small>PARSE</small>FFTI<small>NNER</small>, define the support <i>S</i> = supp(<i>x&#770;</i> &#8722; <i>&#7825;</i>). Recall that <i>&#960;<sub>&#963;,b</sub></i>(<i>i</i>) = <i>&#963;</i>(<i>i</i> &#8722; <i>b</i>) mod <i>n</i>. Define <i>h<sub>&#963;,b</sub></i>(<i>i</i>) = round(<i>&#960;<sub>&#963;,b</sub></i>(<i>i</i>)<i>B/n</i>) and <img src="../images/in44_1.png" alt="Image"/>. Note that therefore |<i>o<sub>&#963;,b</sub></i>(<i>i</i>)| &#8804; <i>n</i>/(2<i>B</i>). We will refer <a id="page_45"/>to <i>h<sub>&#963;,b</sub></i>(<i>i</i>) as the &#8220;bin&#8221; that the frequency <i>i</i> is mapped into, and <i>o<sub>&#963;,b</sub></i>(<i>i</i>) as the &#8220;offset.&#8221; For any <i>i</i> &#8712; <i>S</i> define two types of events associated with <i>i</i> and <i>S</i> and defined over the probability space induced by <i>&#963;</i> and <i>b</i>:</p>
<p class="bullt">&#8226;&#160;&#160;&#8220;Collision&#8221; event <i>E</i><sub>coll</sub>(<i>i</i>): holds iff <i>h<sub>&#963;,b</sub></i>(<i>i</i>) &#8712; <i>h<sub>&#963;,b</sub></i>(<i>S</i>\ {<i>i</i>}), and</p>
<p class="bull">&#8226;&#160;&#160;&#8220;Large offset&#8221; event <i>E</i><sub>off</sub> (<i>i</i>): holds iff |<i>o<sub>&#963;,b</sub></i>(<i>i</i>)| &#8805; (1 &#8722; <i>&#945;</i>)<i>n</i>/(2<i>B</i>).</p>
<p class="noindentt"><b>Claim 4.1</b> For any <i>i</i> &#8712; <i>S</i>, the event <i>E</i><sub>coll</sub>(<i>i</i>) holds with probability at most 4|<i>S</i>|/<i>B</i>.</p>
<p class="noindentt"><b>Proof</b> Consider distinct <i>i, j</i> &#8712; <i>S</i>. By Lemma 2.1,</p>
<p class="image"><img src="../images/pg45_1.png" alt="Image"/></p>
<p class="noindent">By a union bound over <i>j</i> &#8712; <i>S</i>, Pr[<i>E</i><sub>coll</sub>(<i>i</i>)] &#8804; 4 |<i>S</i>| /<i>B</i>.</p>
<p class="noindentt"><b>Claim 4.2</b> For any <i>i</i> &#8712; <i>S</i>, the event <i>E</i><sub>off</sub> (<i>i</i>) holds with probability at most <i>&#945;</i>.</p>
<p class="noindentt"><b>Proof</b> Note that <i>o<sub>&#963;,b</sub></i>(<i>i</i>) &#8801; <i>&#960;<sub>&#963;,b</sub></i>(<i>i</i>) &#8801; <i>&#963;</i>(<i>i</i> &#8722; <i>b</i>) (mod <i>n/B</i>). For any odd <i>&#963;</i> and any <i>l</i> &#8712; [<i>n/B</i>], we have that Pr<sub><i>b</i></sub>[<i>&#963;</i>(<i>i</i> &#8722; <i>b</i>) &#8801; <i>l</i> (mod <i>n/B</i>)]= <i>B/n</i>. Since only <i>&#945;n/B</i> offsets <i>o<sub>&#963;,b</sub></i>(<i>i</i>) cause <i>E</i><sub>off</sub> (<i>i</i>), the claim follows.</p>
<p class="noindentt"><b>Lemma 4.1</b> Suppose <i>B</i> divides <i>n</i>. The output <i>&#251;</i> of H<small>ASH</small>T<small>O</small>B<small>INS</small> satisfies</p>
<p class="image"><img src="../images/pg45_2.png" alt="Image"/></p>
<p class="noindent">Let <img src="../images/in45_1.png" alt="Image"/> The running time of H<small>ASH</small>T<small>O</small>B<small>INS</small> is <img src="../images/in45_2.png" alt="Image"/></p>
<p class="noindentt"><b>Proof</b> The proof can be found in <a href="23_Appendix01.xhtml#appA_3">Appendix A.3</a>.</p>
<p class="noindentt"><b>Lemma 4.2</b> Consider any <i>i</i> &#8712; <i>S</i> such that neither <i>E</i><sub>coll</sub>(<i>i</i>) nor <i>E</i><sub>off</sub>(<i>i</i>) holds. Let <i>j</i> = <i>h<sub>&#963;,b</sub></i>(<i>i</i>). Then</p>
<p class="image"><img src="../images/pg45_3.png" alt="Image"/></p>
<p class="noindent">and <i>j</i> &#8712; <i>J</i>.</p>
<p class="noindentt"><b>Proof</b> The proof can be found in <a href="23_Appendix01.xhtml#appA_4">Appendix A.4</a>.</p>
<p class="indentt">For each invocation of N<small>OISELESS</small>S<small>PARSE</small>FFTI<small>NNER</small>, let <i>P</i> be the set of all pairs (<i>i, v</i>) for which the command <i>&#373;<sub>i</sub></i> &#8592; <i>v</i> was executed. Claims 4.1 and 4.2 and Lemma 4.2 <a id="page_46"/>together guarantee that for each <i>i</i> &#8712; <i>S</i> the probability that <i>P</i> does not contain the pair (<i>i</i>, (<i>x&#770;</i> &#8722; <i>&#7825;</i>)<sub><i>i</i></sub>) is at most 4|<i>S</i>|/<i>B</i> + <i>&#945;</i>. We complement this observation with the following claim.</p>
<p class="noindentt"><b>Claim 4.3</b> For any <i>j</i> &#8712; <i>J</i> we have <i>j</i> &#8712; <i>h<sub>&#963;,b</sub></i>(<i>S</i>). Therefore, |<i>J</i>| = |<i>P</i>| &#8804; |<i>S</i>|.</p>
<p class="noindentt"><b>Proof</b> Consider any <i>j</i> &#8713; <i>h<sub>&#963;,b</sub></i>(<i>S</i>). From Equation (A.1) in the proof of Lemma 4.2 it follows that |<i>&#251;<sub>j</sub></i>| &#8804; <i>&#948;nL</i> &#60; 1/2.</p>
<p class="noindentt"><b>Lemma 4.3</b> Consider an execution of N<small>OISELESS</small>S<small>PARSE</small>FFTI<small>NNER</small>, and let <i>S</i> = supp(<i>x&#770;</i> &#8722; <i>&#7825;</i>).If |<i>S</i>| &#8804; <i>k</i>&#8242;, then</p>
<p class="image"><img src="../images/pg46_1.png" alt="Image"/></p>
<p class="noindentt"><b>Proof</b> Let <i>e</i> denote the number of coordinates <i>i</i> &#8712; <i>S</i> for which either <i>E</i><sub>coll</sub>(<i>i</i>) or <i>E</i><sub>off</sub> (<i>i</i>) holds. Each such coordinate might not appear in <i>P</i> with the correct value, leading to an incorrect value of <i>&#373;<sub>i</sub></i>. In fact, it might result in an arbitrary pair (<i>i</i>&#8242;, <i>v</i>&#8242;) being added to <i>P</i>, which in turn could lead to an incorrect value of <img src="../images/in46_1.png" alt="Image"/>. By Claim 4.3 these are the only ways that <i>&#373;</i> can be assigned an incorrect value. Thus, we have</p>
<p class="image"><img src="../images/pg46_2.png" alt="Image"/></p>
<p class="noindent">Since <i>E</i>[<i>e</i>] &#8804; (4|<i>S</i>|/<i>B</i> + <i>&#945;</i>)|<i>S</i>| &#8804; (4<i>&#946;</i> + <i>&#945;</i>)|<i>S</i>|, the lemma follows.</p>
<p class="noindentt"><b>Analysis of</b> N<small>OISELESS</small>S<small>PARSE</small>FFT</p>
<p class="noindent">Consider the <i>t</i>th iteration of the procedure, and define <i>S<sub>t</sub></i> = supp(<i>x&#770;</i> &#8722; <i>&#7825;</i>) where <i>&#7825;</i> denotes the value of the variable at the beginning of loop. Note that |<i>S</i><sub>0</sub>| = |supp(<i>x&#770;</i>)| &#8804; <i>k</i>.</p>
<p class="indent">We also define an indicator variable <i>I<sub>t</sub></i> which is equal to 0 iff |<i>S<sub>t</sub></i>|/|<i>S</i><sub><i>t</i>&#8722;1</sub>| &#8804; 1/8. If <i>I<sub>t</sub></i> = 1 we say the <i>t</i>th iteration was not <i>successful</i>. Let <i>&#947;</i> = 8 &#183; 8(<i>&#946;</i> + <i>&#945;</i>). From Lemma 4.3 it follows that <img src="../images/in46_2.png" alt="Image"/>. From Claim 4.3 it follows that even if the <i>t</i>th iteration is not successful, then |<i>S<sub>t</sub></i>|/|<i>S</i><sub><i>t</i>&#8722;1</sub>| &#8804; 2.</p>
<p class="indent">For any <i>t</i> &#8804; 1, define an event <i>E</i>(<i>t</i>) that occurs iff <img src="../images/in46_3.png" alt="Image"/>. Observe that if none of the events <i>E</i>(1) &#8230; <i>E</i>(<i>t</i>) holds then |<i>S<sub>t</sub></i>| &#8804; <i>k</i>/2<sup><i>t</i></sup>.</p>
<p class="noindentt"><b>Lemma 4.4</b> Let <i>E</i> = <i>E</i>(1) &#8746; &#8230; &#8746; <i>E</i> (&#955;) for &#955; = 1 + log <i>k</i>. Assume that (4<i>&#947;</i>)<sup>1\2</sup> &#60; 1/4. Then Pr[E] &#8804; 1/3.</p>
<p class="noindentt"><b>Proof</b> Let <i>t</i>&#8242; = [<i>t</i>/2]. We have</p>
<p class="image"><img src="../images/pg46_3.png" alt="Image"/></p>
<p class="noindent"><a id="page_47"/>Therefore,</p>
<p class="image"><img src="../images/pg47_1.png" alt="Image"/></p>
<p class="noindentt"><b>Theorem 4.1</b> Suppose <i>x&#770;</i> is <i>k</i>-sparse with entries from {&#8722;<i>L</i>, &#8230;, <i>L</i>} for some known <i>L</i> = <i>n</i><sup><i>O</i>(1)</sup>. Then the algorithm N<small>OISELESS</small>S<small>PARSE</small>FFT runs in expected <i>O</i>(<i>k</i> log <i>n</i>) time and returns the correct vector <i>x&#770;</i> with probability at least 2/3.</p>
<p class="noindentt"><b>Proof</b> The correctness follows from Lemma 4.4. The running time is dominated by <i>O</i>(log <i>k</i>) executions of H<small>ASH</small>T<small>O</small>B<small>INS</small>.</p>
<p class="indent">Assuming a correct run, in every round <i>t</i> we have</p>
<p class="image"><img src="../images/pg47_2.png" alt="Image"/></p>
<p class="noindent">Therefore,</p>
<p class="image"><img src="../images/pg47_3.png" alt="Image"/></p>
<p class="noindent">so the expected running time of each execution of H<small>ASH</small>T<small>O</small>B<small>INS</small> is <img src="../images/in47_1.png" alt="Image"/>. Setting <i>&#945;</i> = &#920;(2<sup>&#8722;<i>t</i>/2</sup>) and <i>&#946;</i> = &#920;(1), the expected running time in round <i>t</i> is <img src="../images/in47_2.png" alt="Image"/>. Therefore the total expected running time is <i>O</i>(<i>k</i> log <i>n</i>). &#9632;</p>
<p class="h1"><a id="ch4_3"/><b><span class="big">4.3</span>&#160;&#160;&#160;&#160;Algorithm for the General Case</b></p>
<p class="noindent">For the general case, we only achieve <a href="#eq4_1">Equation (4.1)</a> for <img src="../images/in47_3.png" alt="Image"/>. In general, for any parameter <i>&#948;</i> &#62; 0 we can add <img src="../images/in47_4.png" alt="Image"/> to the right hand side of <a href="#eq4_1">Equation (4.1)</a> and run in time <i>O</i>(<i>k</i> log(<i>n</i>/<i>k</i>) log(<i>n</i>/<i>&#948;</i>)).</p>
<p class="indent">Pseudocode for the general algorithm SFT 4.0 is shown in Algorithms 4.2 and 4.3.</p>
<p class="h2"><b><a id="ch4_3_1"/><span class="big1">4.3.1</span>&#160;&#160;Intuition</b></p>
<p class="noindent">Let <i>S</i> denote the &#8220;heavy&#8221; <i>O</i>(<i>k</i>/<i>&#8714;</i>) coordinates of <i>x&#770;</i>. The overarching algorithm S<small>PARSE</small>FFT (SFT 4.0) works by first &#8220;locating&#8221; a set <i>L</i> containing most of <i>S</i>, then &#8220;estimating&#8221; <i>x&#770;<sub>L</sub></i> to get <i>&#7825;</i>. It then repeats on <img src="../images/in43_6.png" alt="Image"/>. We will show that each heavy coordinate has a large constant probability of both being in <i>L</i> and being estimated well. As a result, <img src="../images/in43_6.png" alt="Image"/> is probably nearly <i>k</i>/4-sparse, so we can run the next iteration with <i>k</i> &#8594; <i>k</i>/4. The later iterations then run faster and achieve a higher success probability, so the total running time is dominated by the time in the first iteration and the total error probability is bounded by a constant.</p>
<p class="noindentt"><a id="page_48"/><b>Algorithm 4.2 SFT 4.0: General Sparse Fourier Transform for</b> <i>k</i> = <i>o</i>(<i>n</i>), <b>part 1 of 2</b></p>
<p class="image"><img src="../images/alg4_2.png" alt="Image"/></p>
<p class="indent">In the rest of this intuition, we will discuss the first iteration of S<small>PARSE</small>FFT with simplified constants. In this iteration, hashes are to <i>B</i> = <i>O</i>(<i>k</i>/&#8712;) bins and, with 3/4 probability, we get <i>&#7825;</i> so <img src="../images/in43_6.png" alt="Image"/> is nearly <i>k</i>/4-sparse. The actual algorithm will involve a parameter <i>&#945;</i> in each iteration, roughly guaranteeing that with 1 &#8722; <img src="../images/in48_2.png" alt="Image"/> probability, we get <i>&#7825;</i> so <img src="../images/in43_6.png" alt="Image"/> is nearly <img src="../images/in48_2.png" alt="Image"/><i>k</i>-sparse; the formal guarantee will be given by Lemma 4.11. For this intuition we only consider the first iteration where <i>&#945;</i> is a constant.</p>
<p class="h4"><b>Location</b></p>
<p class="noindent">As in the noiseless case, to locate the &#8220;heavy&#8221; coordinates we consider the &#8220;bins&#8221; computed by H<small>ASH</small>T<small>O</small>B<small>INS</small> with <i>P<sub>&#963;,a,b</sub></i>. This roughly corresponds to first permuting the coordinates according to the (almost) pairwise independent permutation <i>P<sub>&#963;,a,b</sub></i>, partitioning the coordinates into <i>B</i> = <i>O</i>(<i>k</i>/&#8714;) &#8220;bins&#8221; of <i>n</i>/<i>B</i> consecutive indices, and observing the sum of values in each bin. We get that each heavy coordinate <i>i</i> has a large constant probability that the following two events occur: no other heavy coordinate lies in the same bin, and only a small (i.e., <i>O</i>(<i>&#8714;</i>/<i>k</i>)) fraction of the mass from non-heavy coordinates lies in the same bin. For such a &#8220;well-hashed&#8221; coordinate <i>i</i>, we would like to find its location <i>&#964;</i> = <i>&#960;<sub>&#963;,b</sub></i>(<i>i</i>) = <i>&#963;</i>(<i>i</i> &#8722; <i>b</i>) among the <i>&#8712;n</i>/<i>k</i> &#60; <i>n</i>/<i>k</i> consecutive values that hash to the same bin. Let</p>
<p class="noindentt"><a id="page_49"/><b>Algorithm 4.3 SFT 4.0: General Sparse Fourier Transform for</b> <i>k</i> = <i>o</i>(<i>n</i>), <b>part 2 of 2</b></p>
<p class="image"><img src="../images/alg4_3.png" alt="Image"/></p>
<p class="eqn"><a id="page_50"/><a id="eq4_2"/><img src="../images/eq4_2.png" alt="Image"/></p>
<p class="noindent">so <img src="../images/in50_1.png" alt="Image"/>. In the noiseless case, we showed that the difference in phase in the bin using <i>P<sub>&#963;,0,b</sub></i> and using <i>P<sub>&#963;,1,b</sub></i> is <img src="../images/in50_2.png" alt="Image"/> plus a negligible O(<i>&#948;</i>) term. With noise this may not be true; however, we can say for any <i>&#946;</i> &#8712; [<i>n</i>] that the difference in phase between using <i>P<sub>&#963;,a,b</sub></i> and <i>P<sub>&#963;,&#945;+&#946;b</sub></i>, as a distribution over uniformly random <i>&#945;</i> &#8712; [<i>n</i>], is <img src="../images/in50_3.png" alt="Image"/> with (for example) <span class="f1">E</span>[&#957;<sup>2</sup>] = 1/100 (all operations on phases modulo 2<i>&#960;</i>). We can only hope to get a constant number of bits from such a &#8220;measurement.&#8221; So our task is to find <i>&#964;</i> within a region <i>Q</i> of size <i>n/k</i> using <i>O</i>(log(<i>n/k</i>)) &#8220;measurements&#8221; of this form.</p>
<p class="indent">One method for doing so would be to simply do measurements with random <i>&#946;</i> &#8712; [<i>n</i>]. Then each measurement lies within <i>&#960;</i>/4 of <img src="../images/in50_4.png" alt="Image"/> with at least <img src="../images/in50_5.png" alt="Image"/> probability. On the other hand, for <i>j</i> &#8800; <i>&#964;</i> and as a distribution over <i>&#946;</i>, <img src="../images/in50_6.png" alt="Image"/> is roughly uniformly distributed around the circle. As a result, each measurement is probably more than <i>&#960;</i>/4 away from <img src="../images/in50_7.png" alt="Image"/>. Hence, <i>O</i>(log(<i>n</i>/<i>k</i>)) repetitions suffice to distinguish among the <i>n</i>/<i>k</i> possibilities for <i>&#964;</i>. However, while the number of measurements is small, it is not clear how to decode in polylog rather than &#937;.(<i>n/k</i>) time.</p>
<p class="indent">To solve this, we instead do a <i>t</i>-ary search on the location for <i>t</i> = &#920;(log <i>n</i>). At each of <i>O</i>(log<sub><i>t</i></sub>(<i>n/k</i>)) levels, we split our current candidate region <i>Q</i> into <i>t</i> consecutive subregions <i>Q<sub>1</sub></i>, &#8230;, <i>Q<sub>t</sub></i>, each of size <i>w</i>. Now, rather than choosing <i>&#946;</i> &#8712; [<i>n</i>], we choose <img src="../images/in50_8.png" alt="Image"/>. By the upper bound on <i>&#946;</i>, for each <i>q</i> &#8712; [<i>t</i>] the values {<img src="../images/in50_7.png" alt="Image"/> | <i>j</i> &#8712; <i>Q<sub>q</sub></i>} all lie within <img src="../images/in50_9.png" alt="Image"/> of each other on the circle. On the other hand, if |<i>j</i> &#8722; <i>&#964;</i>| &#62; 16<i>w</i>, then <img src="../images/in50_10.png" alt="Image"/> will still be roughly uniformly distributed about the circle. As a result, we can check a single candidate element <i>e<sub>q</sub></i> from each subregion: if <i>e<sub>q</sub></i> is in the same subregion as <i>&#964;</i>, each measurement usually agrees in phase; but if <i>e<sub>q</sub></i> is more than 16 subregions away, each measurement usually disagrees in phase. Hence with <i>O</i>(log <i>t</i>) measurements, we can locate <i>&#964;</i> to within <i>O</i>(1) consecutive subregions with failure probability 1/<i>t</i><sup>&#920;(1)</sup>. The decoding time is <i>O</i>(<i>t</i> log <i>t</i>).</p>
<p class="indent">This primitive L<small>OCATE</small>I<small>NNER</small> lets us narrow down the candidate region for <i>&#964;</i> to a subregion that is a <i>t</i>&#8242; = &#937;(<i>t</i>) factor smaller. By repeating L<small>OCATE</small>I<small>NNER</small> <img src="../images/in50_11.png" alt="Image"/> times, L<small>OCATE</small>S<small>IGNAL</small> can find <i>&#964;</i> precisely. The number of measurements is then <i>O</i>(log <i>t</i> log<sub><i>t</i></sub>(<i>n/k</i>)) = <i>O</i>(log(<i>n/k</i>)) and the decoding time is <i>O</i>(<i>t</i> log <i>t</i> log<sub><i>t</i></sub>(<i>n/k</i>)) = <a id="page_51"/><i>O</i>(log(<i>n/k</i>) log <i>n</i>). Furthermore, the &#8220;measurements&#8221; (which are actually calls to H<small>ASH</small>T<small>O</small>B<small>INS</small>) are non-adaptive, so we can perform them in parallel for all <i>O</i>(<i>k</i>/&#8712;) bins, with <i>O</i>(log(<i>n/&#948;</i>)) average time per measurement. This gives <i>O</i>(<i>k</i> log(<i>n/k</i>) &#183; log(<i>n/&#948;</i>)) total time for L<small>OCATE</small>S<small>IGNAL</small>.</p>
<p class="indent">This lets us locate every heavy and &#8220;well-hashed&#8221; coordinate with 1/<i>t</i><sup>&#920;(1)</sup> = <i>o</i>(1) failure probability, so every heavy coordinate is located with arbitrarily high constant probability.</p>
<p class="h4"><b>Estimation</b></p>
<p class="noindent">By contrast, estimation is fairly simple. As in Algorithm 4.1, we can estimate <img src="../images/in51_1.png" alt="Image"/> as <img src="../images/in51_2.png" alt="Image"/>, where <i>&#251;</i> is the output of H<small>ASH</small>T<small>O</small>B<small>INS</small>. Unlike in Algorithm 4.1, we now have noise that may cause a single such estimate to be poor even if <i>i</i> is &#8220;well-hashed.&#8221; However, we can show that for a random permutation <i>P<sub>&#963;,a,b</sub></i> the estimate is &#8220;good&#8221; with constant probability. E<small>STIMATE</small>V<small>ALUES</small> takes the median of <i>R<sub>est</sub></i> = <i>O</i>(log <img src="../images/in51_3.png" alt="Image"/>) such samples, getting a good estimate with 1 &#8722; <i>&#8714;</i>/64 probability. Given a candidate set <i>L</i> of size <i>k/&#8712;</i>, with 7/8 probability at most <i>k</i>/8 of the coordinates are badly estimated. On the other hand, with 7/8 probability, at least 7<i>k</i>/8 of the heavy coordinates are both located and well estimated. This suffices to show that, with 3/4 probability, the largest <i>k</i> elements <i>J</i> of our estimate <i>&#373;</i> contains good estimates of 3<i>k</i>/4 large coordinates, so <img src="../images/in51_4.png" alt="Image"/> is close to <i>k</i>/4-sparse.</p>
<p class="h2"><b><a id="ch4_3_2"/><span class="big1">4.3.2</span>&#160;&#160;Analysis</b></p>
<p class="h4a"><b>Formal Definitions</b></p>
<p class="noindent">As in the noiseless case, we define <i>&#960;<sub>&#963;,b</sub></i>(<i>i</i>) = <i>&#963;</i>(<i>i</i> &#8722; <i>b</i>) mod n, <i>h<sub>&#963;,b</sub></i>(<i>i</i>) = round <i>&#960;<sub>&#963;,b</sub></i>(<i>i</i>)<i>B/n</i>), and <i>o<sub>&#963;,b</sub></i>(<i>i</i>) = <i>&#960;<sub>&#963;,b</sub></i>(<i>i</i>) &#8722; <i>h<sub>&#963;,b</sub></i>(<i>i</i>)<i>n/B</i>. We say <i>h<sub>&#963;,b</sub></i>(<i>i</i>) is the &#8220;bin&#8221; that frequency <i>i</i> is mapped into, and <i>o<sub>&#963;,b</sub></i>(<i>i</i>) is the &#8220;offset.&#8221; We define <img src="../images/in51_5.png" alt="Image"/></p>
<p class="indent">Define</p>
<p class="image"><img src="../images/pg51_1.png" alt="Image"/></p>
<p class="noindent">In each iteration of S<small>PARSE</small>FFT, define <i>x&#770;</i>&#8242; = <i>x&#770;</i> &#8722; <i>&#7825;</i>, and let</p>
<p class="image"><img src="../images/pg51_2.png" alt="Image"/></p>
<p class="noindent"><a id="page_52"/>Then <img src="../images/in52_1.png" alt="Image"/> and <img src="../images/in52_2.png" alt="Image"/>. We will show that each <i>i</i> &#8712; <i>S</i> is found by L<small>OCATE</small>S<small>IGNAL</small> with probability <img src="../images/in52_3.png" alt="Image"/>, when <img src="../images/in52_4.png" alt="Image"/>.</p>
<p class="indent">For any <i>i</i> &#8712; <i>S</i> define three types of events associated with <i>i</i> and <i>S</i> and defined over the probability space induced by <i>&#963;</i> and <i>b</i>:</p>
<p class="bull">&#8226;&#160;&#160;&#8220;Collision&#8221; event <i>E</i><sub>coll</sub>(<i>i</i>): holds iff <img src="../images/in52_5.png" alt="Image"/>;</p>
<p class="bull">&#8226;&#160;&#160;&#8220;Large offset&#8221; event <i>E</i><sub>off</sub>(<i>i</i>): holds iff <img src="../images/in52_6.png" alt="Image"/>; and</p>
<p class="bull">&#8226;&#160;&#160;&#8220;Large noise&#8221; event <i>E</i><sub>noise</sub>(<i>i</i>): holds <img src="../images/in52_7.png" alt="Image"/>.</p>
<p class="noindentt">By Claims 4.1 and 4.2, <img src="../images/in52_8.png" alt="Image"/> and <img src="../images/in52_9.png" alt="Image"/> for any <i>i</i> &#8712; <i>S</i></p>
<p class="noindentt"><b>Claim 4.4</b> For any <i>i</i> &#8712; <i>S</i>, <img src="../images/in52_10.png" alt="Image"/>.</p>
<p class="noindentt"><b>Proof</b> For each <i>j</i> &#8800; <i>i</i>, <img src="../images/in52_11.png" alt="Image"/> by Lemma 2.1.</p>
<p class="noindent">Then</p>
<p class="image"><img src="../images/pg52_1.png" alt="Image"/></p>
<p class="noindent">The result follows by Markov&#8217;s inequality.</p>
<p class="indentt">We will show for <i>i</i> &#8712; <i>S</i> that if none of <i>E</i><sub>coll</sub>(<i>i</i>), <i>E</i><sub>off</sub>(<i>i</i>), and <i>E</i><sub>noise</sub>(<i>i</i>) hold then S<small>PARSE</small>FFTI<small>NNER</small> recovers <img src="../images/in52_12.png" alt="Image"/> with 1 &#8722; <i>O</i>(<i>&#945;</i>) probability.</p>
<p class="noindentt"><b>Lemma 4.5</b> Let <i>a</i> &#8712; [<i>n</i>] uniformly at random, <i>B</i> divide <i>n</i>, and the other parameters be arbitrary in</p>
<p class="image"><img src="../images/pg52_2.png" alt="Image"/></p>
<p class="noindent">Then for any <i>i</i> &#8712; [<i>n</i>] with <img src="../images/in52_13.png" alt="Image"/> and none of <i>E</i><sub>coll</sub>(<i>i</i>), <i>E</i><sub>off</sub>(<i>i</i>), or <i>E</i><sub>noise</sub>(<i>i</i>) holding,</p>
<p class="image"><img src="../images/pg52_3.png" alt="Image"/></p>
<p class="noindentt"><b>Proof</b> The proof can be found in <a href="23_Appendix01.xhtml#appA_5">Appendix A.5</a>.</p>
<p class="noindentt"><b>Properties of</b> L<small>OCATE</small>S<small>IGNAL</small></p>
<p class="noindent">In our intuition, we made a claim that if <img src="../images/in52_13a.png" alt="Image"/> uniformly at random, and <i>i</i> &#62; 16<i>w</i>, then <img src="../images/in52_14.png" alt="Image"/> is &#8220;roughly uniformly distributed about the circle&#8221; and hence not concentrated in any small region. This is clear if <i>&#946;</i> is chosen as a random real number; it is less clear in our setting where <i>&#946;</i> is a random integer in this range. We now prove a lemma that formalizes this claim.</p>
<p class="noindentt"><a id="page_53"/><b>Lemma 4.6</b> Let <i>T</i> &#8834; [<i>m</i>] consist of <i>t</i> consecutive integers, and suppose <i>&#946;</i> &#8712; <i>T</i> uniformly at random. Then for any <i>i</i> &#8712; [<i>n</i>] and set <i>S</i> &#8834; [<i>n</i>] of <i>l</i> consecutive integers,</p>
<p class="image"><img src="../images/pg53_1.png" alt="Image"/></p>
<p class="noindentt"><b>Proof</b> Note that any interval of length <i>l</i> can cover at most <img src="../images/in53_1.png" alt="Image"/> elements of any arithmetic sequence of common difference <i>i</i>. Then <img src="../images/in53_2.png" alt="Image"/> is such a sequence, and there are at most <img src="../images/in53_3.png" alt="Image"/> intervals <i>an</i> + <i>S</i> overlapping this sequence. Hence, at most <img src="../images/in53_4.png" alt="Image"/> of the <i>&#946;</i> &#8712; [<i>m</i>] have <i>&#946;i</i> mod <i>n</i> &#8712; <i>S</i>. Hence, <img src="../images/in53_5.png" alt="Image"/>.</p>
<p class="noindentt"><b>Lemma 4.7</b> Let <i>i</i> &#8712; <i>S</i>. Suppose none of <i>E</i><sub>coll</sub>(<i>i</i>), <i>E</i><sub>off</sub>(<i>i</i>), and <i>E</i><sub>noise</sub>(<i>i</i>) hold, and let <i>j</i> = <i>h<sub>&#963;,b</sub></i>(<i>i</i>). Consider any run of L<small>OCATE</small>I<small>NNER</small> with <img src="../images/in53_6.png" alt="Image"/>. Let <i>f</i> &#62; 0 be a parameter such that</p>
<p class="image"><img src="../images/pg53_2.png" alt="Image"/></p>
<p class="noindent">for <i>C</i> larger than some fixed constant. Then <img src="../images/in53_7.png" alt="Image"/> with probability at least <img src="../images/in53_8.png" alt="Image"/>,</p>
<p class="noindentt"><b>Proof</b> The proof can be found in <a href="23_Appendix01.xhtml#appA_6">Appendix A.6</a>.</p>
<p class="noindentt"><b>Lemma 4.8</b> Suppose <img src="../images/in53_9.png" alt="Image"/> for <i>C</i> larger than some fixed constant. The procedure L<small>OCATE</small>S<small>IGNAL</small> returns a set <i>L</i> of size |<i>L</i>| &#8804; <i>B</i> such that for any <i>i</i> &#8712; <i>S</i>, <img src="../images/in53_10.png" alt="Image"/>. Moreover the procedure runs in expected time</p>
<p class="image"><img src="../images/pg53_3.png" alt="Image"/></p>
<p class="noindentt"><b>Proof</b> The proof can be found in <a href="23_Appendix01.xhtml#appA_7">Appendix A.7</a>.</p>
<p class="noindentt"><b>Properties of</b> E<small>STIMATE</small>V<small>ALUES</small></p>
<p class="noindentt"><b>Lemma 4.9</b> For any <i>i</i> &#8712; <i>L</i>,</p>
<p class="image"><img src="../images/pg53_4.png" alt="Image"/></p>
<p class="noindent">if <img src="../images/in53_11.png" alt="Image"/> for some constant <i>C</i>.</p>
<p class="noindentt"><b>Proof</b> The proof can be found in <a href="23_Appendix01.xhtml#appA_8">Appendix A.8</a>.</p>
<p class="noindentt"><b>Lemma 4.10</b> Let <img src="../images/in53_12.png" alt="Image"/> for some constant <i>C</i> and parameters <i>&#947;, f</i> &#62; 0. Then if E<small>STIMATE</small>V<small>ALUES</small> is run with input <i>k</i>&#8242; = 3<i>k</i>, it returns <img src="../images/in53_13.png" alt="Image"/> for |<i>J</i>| = 3<i>k</i> satisfying</p>
<p class="image"><a id="page_54"/><img src="../images/pg54_1.png" alt="Image"/></p>
<p class="noindent">with probability at least 1 &#8722; <i>&#947;</i>.</p>
<p class="noindentt"><b>Proof</b> The proof can be found in <a href="23_Appendix01.xhtml#appA_9">Appendix A.9</a>.</p>
<p class="noindentt"><b>Properties of</b> S<small>PARSE</small>FFT</p>
<p class="noindent">We will show that <img src="../images/in54_1.png" alt="Image"/> gets sparser as <i>r</i> increases, with only a mild increase in the error.</p>
<p class="noindentt"><b>Lemma 4.11</b> Define <img src="../images/in54_2.png" alt="Image"/>. Consider any one loop <i>r</i> of S<small>PARSE</small>FFT, running with parameters (<i>B, k, &#945;</i>) = (<i>B<sub>r</sub>, k<sub>r</sub>, <i>&#945;</i><sub>r</sub></i>) such that <img src="../images/in54_3.png" alt="Image"/> for some <i>C</i> larger than some fixed constant. Then for any <i>f</i> &#62; 0,</p>
<p class="image"><img src="../images/pg54_2.png" alt="Image"/></p>
<p class="noindent">with probability 1 &#8722; <i>O</i>(<i>&#945;/f</i>), and the runningtime is</p>
<p class="image"><img src="../images/pg54_3.png" alt="Image"/></p>
<p class="noindentt"><b>Proof</b> The proof can be found in <a href="23_Appendix01.xhtml#appA_10">Appendix A.10</a>.</p>
<p class="indentt">Given the above, this next proof follows a similar argument to Indyk et al. [2011, Theorem 3.7].</p>
<p class="noindentt"><b>Theorem 4.2</b> With 2/3 probability, S<small>PARSE</small>FFT recovers <img src="../images/in54_4.png" alt="Image"/> such that</p>
<p class="image"><img src="../images/pg54_4.png" alt="Image"/></p>
<p class="indent">in <img src="../images/in54_5.png" alt="Image"/> time.</p>
<p class="noindentt"><b>Proof</b> The proof can be found in <a href="23_Appendix01.xhtml#appA_11">Appendix A.11</a>.</p>
<p class="h1"><b><a id="ch4_4"/><span class="big">4.4</span>&#160;&#160;&#160;&#160;Extension to Two Dimensions</b></p>
<p class="noindent">This section outlines the straightforward generalization of SFT 4.0 shown in Algorithm 4.2 to two dimensions. We will refer to this algorithm as SFT 4.1.</p>
<p class="noindentt"><b>Theorem 4.3</b> There is a variant of Algorithm 4.2 that will, given <i>x</i>, <img src="../images/in54_6.png" alt="Image"/>, return <img src="../images/in54_7.png" alt="Image"/> with</p>
<p class="image"><img src="../images/pg54_5.png" alt="Image"/></p>
<p class="noindent">with probability 1 &#8722; <i>&#945;</i> for any constants <i>c, &#945;</i> &#62; 0 in time</p>
<p class="image"><img src="../images/pg54_6.png" alt="Image"/></p>
<p class="noindent"><a id="page_55"/>using <i>O</i>(<i>k</i> log(<i>n/k</i>) log<sup>2</sup> <i>n</i>) samples of <i>x</i>.</p>
<p class="noindentt"><b>Proof</b> We need to modify Algorithm 4.2 in two ways: by extending it to two dimensions and by allowing the parameter <i>&#7825;</i>.<sup><a id="fn6" href="#rfn6">6</a></sup> We will start by describing the adaptation to two dimensions.</p>
<p class="indent">The basic idea of Algorithm 4.2 is to construct from Fourier measurements a way to &#8220;hash&#8221; the coordinates in <i>B</i> = <i>O</i>(<i>k</i>) bins. There are three basic components that are needed: a <i>permutation</i> that gives nearly pairwise independent hashing to bins; a <i>filter</i> that allows for computing the sum of bins using Fourier measurements; and the <i>location</i> estimation needs to search in both axes. The permutation is the main subtlety.</p>
<p class="noindentt"><b>Permutation.</b> Let <img src="../images/in55_1.png" alt="Image"/> be the set of matrices with odd determinant. For notational purposes, for <i>&#957;</i> = (<i>i, j</i>) we define <i>x<sub>&#957;</sub></i> := <i>x<sub>i,j</sub></i>.</p>
<p class="noindentt"><b>Definition 4.2</b> For <i>M</i> &#8712; <span class="f2">M</span> and <i>a, b</i> &#8712; <img src="../images/in55_2.png" alt="Image"/> we define the permutation <img src="../images/in55_3.png" alt="Image"/> by</p>
<p class="image"><img src="../images/pg55_1.png" alt="Image"/></p>
<p class="indent">We also define <img src="../images/in55_4.png" alt="Image"/> mod <img src="../images/in55_5.png" alt="Image"/>.</p>
<p class="noindentt"><b>Claim 4.5</b> <img src="../images/in55_6.png" alt="Image"/></p>
<p class="noindentt"><b>Proof</b></p>
<p class="image"><img src="../images/pg55_2.png" alt="Image"/></p>
<p class="noindent">where we used that <i>M<sup>T</sup></i> is a bijection over <img src="../images/in55_7.png" alt="Image"/> because det(<i>M</i>) is odd. &#9632;</p>
<p class="indentt">This gives a lemma analogous to Lemma 2.1 from <a href="12_Chapter03.xhtml">Chapter 3</a>.</p>
<p class="noindent"><a id="page_56"/><b>Lemma 4.12</b> Suppose <img src="../images/in56_1.png" alt="Image"/> is not 0. Then</p>
<p class="image"><img src="../images/pg56_1.png" alt="Image"/></p>
<p class="noindentt"><b>Proof</b> For any <i>u</i>, define <i>G</i>(<i>u</i>) to be the largest power of 2 that divides both <i>u</i><sub>0</sub> and <i>u</i><sub>1</sub>. Define <i>g</i> = <i>G</i>(<i>v</i>), and let <img src="../images/in56_2.png" alt="Image"/>. We have that <i>Mv</i> is uniform over <i>S</i>: <i><span class="f2">M</span></i> is a group and <i>S</i> is the orbit of (0, <i>g</i>).</p>
<p class="indent">Because <i>S</i> lies on a lattice of distance <i>g</i> and does not include the origin, there are at most <img src="../images/in56_3.png" alt="Image"/> elements in <i>S</i> &#8745; [&#8722;<i>C, C</i>]<sup>2</sup>, and (3/4)<i>n/g</i><sup>2</sup> total elements in <i>S</i>. Hence, the probability is at most (32/3)<i>C</i><sup>2</sup>/<i>n</i>. &#9632;</p>
<p class="noindentt"><b>Proof</b> Original, continued. We can then define the &#8220;hash function&#8221; <img src="../images/in56_4.png" alt="Image"/> given by (<i>h<sub>M,b</sub></i>(<i>u</i>)) = round<img src="../images/in56_5.png" alt="Image"/>, i.e., round to the nearest multiple of <img src="../images/in56_6.png" alt="Image"/> in each coordinate and scale down. We also define the &#8220;offset&#8221; <img src="../images/in56_7.png" alt="Image"/>. This lets us give results analogous to Claims 4.1 and 4.2.</p>
<p class="bull">&#8226;&#160;&#160;<img src="../images/in56_8.png" alt="Image"/> for <i>u</i> &#8800; <i>v</i>. In order for <i>h</i>(<i>u</i>) = <i>h</i>(<i>v</i>), we need that <img src="../images/in56_9.png" alt="Image"/>. But Lemma 4.12 implies this probability is <i>O</i>(1/<i>B</i>).</p>
<p class="bull">&#8226;&#160;&#160;<img src="../images/in56_10.png" alt="Image"/> for any <i>&#945;</i> &#62; 0. Because of the offset <i>b, o<sub>M,b</sub></i>(<i>u</i>) is uniform over <img src="../images/in56_11.png" alt="Image"/>. Hence the probability is 2<i>&#945;</i> &#8722; <i>&#945;</i><sup>2</sup> + <i>o</i>(1) by a volume argument.</p>
<p class="noindentt">These results are all we need of the hash function.</p>
<p class="noindentt"><b>Filter.</b> Modifying the filter is pretty simple. Specifically, we define a flat-window filter <img src="../images/in56_12.png" alt="Image"/> with support size <img src="../images/in56_13.png" alt="Image"/> such that <img src="../images/in56_14.png" alt="Image"/> is essentially zero outsize <img src="../images/in56_15.png" alt="Image"/> and is essentially 1 inside <img src="../images/in56_16.png" alt="Image"/> for constant <i>&#945;</i>. We compute the <img src="../images/in56_17.png" alt="Image"/> 2-dimensional DFT of <img src="../images/in56_18.png" alt="Image"/> to sum up the element in each bin. This takes <i>B</i> log<sup>2</sup> <i>n</i> samples and time rather than <i>B</i> log <i>n</i>, which is the reason for the extra log <i>n</i> factor compared to the 1D case.</p>
<p class="noindentt"><b>Location.</b> Location is easy to modify; we simply run it twice to find the row and column separately. We will see how to do this in the following chapter as well.</p>
<p class="indent">In summary, the aforementioned adaptations leads to a variant of Algorithm 4.2 that works in two dimensions, with running time <i>O</i>(<i>k</i> log(<i>n/k</i>) log<sup>2</sup> <i>n</i>), using <i>O</i>(<i>k</i> log(<i>n/k</i>) log<sup>2</sup> <i>n</i>) samples.</p>
<p class="noindentt"><b>Adding extra coefficient list</b> <i>&#7825;</i>. The modification of the Algorithm 4.2 (as well as its variant above) is straightforward. The algorithm performs a sequence of iterations, <a id="page_57"/>where each iteration involves hashing the frequencies of the signal into bins, followed by subtracting the already recovered coefficients from the bins. Since the algorithm recovers &#920;(<i>k</i>) coefficients in the first iteration, the subtracted list is always of size &#920;(<i>k</i>).</p>
<p class="indent">Given the extra coefficient list, the only modification to the algorithm is that the list of the subtracted coefficients needs to be appended with coefficients in <i>&#7825;</i>. Since this step does not affect the samples taken by the algorithm, the sample bound remains unchanged. To analyze the running time, let <i>k</i>&#8242; be the number of nonzero coefficients in <i>&#7825;</i>. Observe that the total time of the original algorithm spent on subtracting the coefficients from a list of size &#920;(<i>k</i>) was <i>O</i>(<i>k</i> log(<i>n/k</i>) log <i>n</i>), or <i>O</i>(log(<i>n/k</i>) log <i>n</i>) per list coefficient. Since in our case the number of coefficients in the list is increased from &#920;(<i>k</i>) to <i>k</i>&#8242; + &#920;(<i>k</i>), the running time is increased by an additive factor of <i>O</i>(<i>k</i>&#8242; log(<i>n/k</i>) log <i>n</i>). &#9632;</p>
<p class="line"/>
<p class="foot"><a id="rfn1" href="#fn1">1</a>. One also needs to assume that <i>k</i> divides <i>n</i>. See <a href="24_Appendix02.xhtml">Appendix B</a> for more details</p>
<p class="foot"><a id="rfn2" href="#fn2">2</a>. Note that both numbers (<i>k</i> &#8804; 2<sup>17</sup> and <i>k</i> &#8804; 2,000) are for the exactly <i>k</i>-sparse case. The algorithm in <a href="12_Chapter03.xhtml">Chapter 3</a> can deal with the general case, but the empirical runtimes are higher.</p>
<p class="foot"><a id="rfn3" href="#fn3">3</a>. Note that if we allow <i>arbitrary</i> adaptive linear measurements of a vector <i>x&#770;</i>, then its <i>k</i>-sparse approximation can be computed using only <i>O</i>(<i>k</i> log log(<i>n/k</i>)) samples [Indyketal. 2011]. Therefore, our lower bound holds only where the measurements, although adaptive, are limited to those induced by the Fourier matrix. This is the case when we want to compute a sparse approximation to <i>x&#770;</i> from samples of <i>x</i>.</p>
<p class="foot"><a id="rfn4" href="#fn4">4</a>. We note that although the two-sample approach employed in our algorithm works in theory only for the exactly <i>k</i>-sparse case, our preliminary experiments show that using a few more samples to estimate the phase works surprisingly well even for general signals.</p>
<p class="foot"><a id="rfn5" href="#fn5">5</a>. In particular, the method of Gilbert et al. [2010] uses measurements corresponding to a random error correcting code.</p>
<p class="foot"><a id="rfn6" href="#fn6">6</a>. We include <i>&#7825;</i> so we can use this a subroutine for partial recovery in the following chapter. However, the input <i>&#7825;</i> can be set to zero in which case we get the <i>l</i><sub>2</sub>/<i>l</i><sub>2</sub> guarantee for a 2D version of SFT 4.0.</p>
</body>
</html>