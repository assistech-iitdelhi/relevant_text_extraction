Figure 4.3 shows the high-level architecture of an event mining platform. The main components of this platform are as follows:

Data sources.  All the data available for analysis from different sources, such as environmental, traffic, wearable, mobile apps, and so on, come into the system in a structured, semi-structured, or unstructured format. The speed that the data arrive and the rate at which they are delivered varies according to the data source. At the collection point, data are collected directly or through data providers in real time or in batch mode.

Data ingestion.  Although data ingestion is not necessarily an internal part of the event mining platform, it is the beginning of the data pipeline and the focus of data transport from its source to everywhere else. Some data sources have tables or databases that are frequently updated to reveal certain sensory data collected by different agencies. Data can be streamed in real time or ingested in batches. Real-time data are consumed by an online event processing component as soon as new data are available. When data are ingested in batches, data items are ingested in chunks at periodic intervals. With the availability of numerous data sources, data exist in different formats, which is a big challenge for applications. They have to ingest data at a reasonable speed and convert data to a correct format.
Figure 4.3The high-level conceptual architecture of an event mining framework.

Event recognition and event management.  Most of the data coming from sensors (physical, social, etc.) are in the form of data streams. The event mining platform should handle and aggregate multiple heterogeneous streams, synchronize them, and provide stream processing operations. To support effective and efficient model building, streaming data need to be managed at an event level. Primitive sensor data also need to be filtered, aggregated, and correlated to generate more semantically meaningful complex events with a rich predefined event schema. Different recognition techniques can be utilized to convert raw sensor data to events, ranging from simple time-series discretization to complex machine learning methods.

Model building.  Hypothesis formulation, hypothesis evaluation, and event mining are important subtasks in an event mining system. The end goal is to build an explanatory model. One way to formulate this model is by converting extracted patterns to explainable rules. These pattern are either verified hypotheses (generated by applying hypothesis formulation and evaluation) or machine-generated patterns that are novel, interesting, and human-understandable. In the model building component, different pattern mining and machine-learning techniques should be included to detect co-occurrences and causal explanations among different events. Human-in-the-loop for hypotheses formulation, hypotheses verification, and to verify hypotheses and examine investigation of machine-generated patterns is only possible with user-friendly dashboards and expressive visualizations.

Model deployment.  Deployment is a process in which a trained model is integrated into the environment for data-based decision-making. Many applications require continuous data monitoring and processing in a timely fashion as data flows from source to system. The key requirement here is early identification of a situation of interest and quick response generation. The model generated in the previous step can be considered as a set of causal explanation rules, and might be deployed in a similar setting as complex event processing (CEP) systems. CEP technology is addressing continuous data monitoring requirements through an infrastructure that consists of three main components: a set of event sources, a rule engine, and a set of event sinks or event listeners. The rule engine consists of a set of rules that act as continuous queries or pattern signatures that filter, aggregate, and correlate events as they flow into the system. Conventional CEP rules are manually defined by domain experts. In the event mining framework, the rule set is extracted by pattern mining and hypothesis verification. The rule set is configured once but might change later as pattern mining algorithms run in the background and more interesting patterns are discovered.