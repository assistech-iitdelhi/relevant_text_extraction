<?xml version="1.0" encoding="utf-8"?>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Chapter 03</title>
<link rel="stylesheet" href="../css/stylesheet.css" type="text/css"/>
</head>
<body epub:type="bodymatter">
<div epub:type="chapter" role="doc-chapter" aria-labelledby="ch3">
<h1 class="ct" id="ch3"><span class="cn" id="b_9781450384827-chapter-003">3</span><span class="ct1">Event Mining and Pattern Discovery</span></h1>
<p class="tx"><a id="page_19"/>In the last few years, progress in sensors, wearable devices, the Internet of Things, and smartphones has transformed both the nature of data and the emerging applications resulting from it. In this setting, data are usually part of a data stream that is tied to a location or a person or some other device. By reviewing data streams in full, we can start to predict certain scenarios and make appropriate decisions in real time. Increasingly autonomous systems are being developed to make these sensitive decisions in critical situations. Big data has ushered in a clear departure from the earlier use of data streams, which was primarily for understanding consumer behavior or for helping people to purchase stocks. </p>
<p class="txi">An implicit assumption in data systems in the last century was that objects are primary and events are just the properties of an object. In contrast, this century&#x2019;s data systems place events at the same level as objects. What ancient philosophers [<a href="bib.xhtml#b_9781450384827_ref_029">Casati and Varzi 2015</a>] believed about the world being represented by objects and events is finally coming to computers and cyberspaces. With the increasing availability of sensor data streams that represent diverse attributes for objects and locations, events are becoming as important as objects.</p>
<p class="txi">As discussed in Chapter <a href="b_9781450384827-chapter-002.xhtml">2</a>, an event has multiple properties, usually recognized at different levels of granularity, and represented with an event model. Depending on the complexity of an application, the event model might either contain all facets (i.e., informational, structural, experiential, spatial, temporal, and causal), or only a few of them. In its simplest form, however, an event model must contain informational and temporal facets: the event type or name is needed as a human- and machine-understandable label, and a timestamp is needed because events occur at a certain moment in time or span an interval. Event streams have two main dimensions: (1) temporal sequence, where data are indexed by time, and (2) informational segment, where data are encapsulated in events&#x2019; properties (such as type, name, location, participants, etc.). As shown in Figure <a href="../xhtml/b_9781450384827-chapter-002.xhtml#b_9781450384827-002_fig_003">2.3</a>, event streams contain a <a id="page_20"/>lot more information than merely a sequence of symbols (i.e., event names) on a timeline. Consequently, algorithms for mining event streams with the objective of explanatory modeling need to handle complexities of event information in addition to complexities of time. </p>
<p class="txi">As we get deeper into the concept of event mining, it is important to make a distinction between <i>sequential data</i>, <i>temporal sequence</i>, and <i>time series</i>. In sequential data the order of the data matters, but the timestamp is irrelevant or does not matter. For example, DNA sequences or a sequence of events when we only care about before/after relations and the exact timestamp of events do not matter. In temporal sequences, in addition to the order of data, the timestamp also matters. In time series the data are in order, with a fixed time-difference between the occurrence of successive data points; for example, time series of environmental particulate matter (PM2.5) being recorded hourly. In one application, we might face multiple data streams in which each belong to one of the categories mentioned. </p>
<p class="txi">Over the last two decades, many interesting techniques of temporal data mining were proposed and shown to be useful in many applications. Temporal data mining is defined as &#x201C;the process of knowledge discovery in temporal databases that enumerates structures (temporal patterns or models) over the temporal data, and any algorithm that enumerates temporal patterns from, or fits models to, temporal data is a temporal data mining algorithm&#x201D; [<a href="bib.xhtml#b_9781450384827_ref_105">Lin et al. 2002</a>]. Event mining, however, is of a more recent origin with somewhat different constraints and objectives. One main difference lies in the nature of data and the manner in which the data are collected. In temporal data mining, data types are simple and sequences may either be nominal-valued or symbolic. Event mining, on the other hand, needs to handle complexities that arise from heterogeneity of data. </p>
<p class="txi">Another difference is centered around the design principles of temporal data mining and event mining. The focus of temporal data mining techniques is on handling the complexities of time for a large volume of data. Therefore, the main center of attention has been on designing algorithms that improve time and space complexities for simple data types stored in a temporal database. In event mining, however, consuming multiple streams of data and incorporating informational aspects of events at different granularity levels are a main priority.</p>
<p class="txi">In this chapter, we overview state-of-the-art techniques related to mining temporal data to investigate which algorithms are capable of building explanatory models for emerging applications. The literature related to temporal data mining is scattered across different domains. We characterized state-of-the-art based on algorithm objectives and input data types to four broad categories shown in Figure <a href="#b_9781450384827-003_fig_001">3.1</a>. Algorithms are either descriptive or predictive. Pattern discovery algorithms belong to the descriptive class and are designed with the goals of inferring <a id="page_21"/>and investigating associations in the data. Classification and clustering algorithms belong to the prediction class and have the goals of predicting and forecasting unseen data. However, there are some cases where such distinctions between algorithms become blurred due to the inherent multidisciplinary nature of temporal data mining. </p>
<figure class="split" id="b_9781450384827-003_fig_001">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-003_fig_001.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 3.1</span>Categorizing the space of state-of-art into multiple categories depending on the input type and algorithms.</p></figcaption>
</figure>
<p class="txi">In the following sections, we start with an example from an emerging application to clarify the types of local structures that a pattern discovery algorithm should handle. Then we discuss how interval-based temporal logic is represented and examine the characteristics of commonly used representations. Next, we explore some of the common descriptive and predictive algorithms related to temporal data mining. Whenever possible, we explain the algorithm in the context of the running example explained in Section <a href="#b_9781450384827-003_s_001">3.1</a>.</p>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-003_s_001">3.1</span>An Example of Asthma Risk Factor Patterns</h2>
<p class="tx">Targeting care for those at highest risk of asthma attack is an important application and helps a large number of people all over the world. Asthma attacks are at best unpleasant, and at worst catastrophic and even fatal. Asthma risk management is growing in importance, both at an individual level and at the public health level. Exposure to air pollution, specifically PM2.5, is linked with asthma exacerbation; however, the role played by meteorological factors such as temperature, humidity, rainfall, wind, and so on, and the complicated interrelations between these factors <a id="page_22"/>and asthma outbreaks are not well understood. A comprehensive understanding of asthma risk factors requires building an explanatory model on event streams that facilitates studying the interrelations among multiple risk factors. Figure <a href="#b_9781450384827-003_fig_002">3.2</a> shows a simplified example of three event streams: an activities of daily living (ADL) stream and two environmental streams&#x2014;temperature and pollution. To control this chronic disease, a doctor might be interested in studying specific risk factors of asthma in a patient. For instance, does the patient suffer from exercise-induced asthma? Or is exercise <i>followed by</i> an asthma attack? If yes, <i>within</i> how many minutes after starting to exercise does the asthma attack happen? And, how do the seasons and air pollution affect this? Or is an asthma attack more probable <i>while</i> temperature is low and/or pollution is high? </p>
<figure class="split" id="b_9781450384827-003_fig_002">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-003_fig_002.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 3.2</span>A toy example including three event streams. Each event is specified with a name and its timestamp(s). The ADL stream shows a person was in a meeting followed by a break. Start home and leave home are considered as point events. Asthma attack happened after starting exercise, while temperature was low and pollution was high in the surrounding environment. ADL, Activities of daily living.</p></figcaption>
</figure>
<p class="txi">In the following sections, we investigate which pattern representations and pattern mining algorithms are capable of formulating three main statements: <i>followed by</i>, <i>within</i>, and <i>while</i> required in the above risk factor patterns.</p>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-003_s_002">3.2</span>Temporal Knowledge Representation</h2>
<p class="tx">In general, temporal data are considered as a sequence or chronicle of a primary data type, usually numerical or categorical values but sometimes multivariate information as well. Examples of temporal data are numerical time series (e.g., electroencephalography [EEG] signals, stock prices), event sequences (e.g., sensor readings, online click streams, medical records), and temporal databases (e.g., timestamped records). The problem of representing temporal knowledge and temporal reasoning arises in a wide range of disciplines, including philosophy, physics, and linguistics. In computer science, temporal knowledge representation is a core <a id="page_23"/>problem in information systems, artificial intelligence (AI), and other areas that involve process modeling. The development of temporal reasoning systems in AI has matured to the point that temporal knowledge is fully integrated into many industrial applications, primarily in planning and scheduling. </p>
<p class="txi">Time is a major dimension in analyzing event-related information. Temporal logic is a formal system for reasoning about time with extensive application in computer science. In 1977, <a href="bib.xhtml#b_9781450384827_ref_146">Pnueli</a> [<a href="bib.xhtml#b_9781450384827_ref_146">1977</a>] introduced linear-time temporal logic, which promoted temporal logic as a specification language. In linear time, every moment has a unique successor. In 1981, <a href="bib.xhtml#b_9781450384827_ref_033">Clarke and Emerson</a> [<a href="bib.xhtml#b_9781450384827_ref_033">1981</a>] presented branching-time temporal logic where every moment has several successors and model of time is a tree-like structure. Time points are assumed to be basic temporal entities, but instantaneous points in time are not suitable for inference about real-world events that span a time interval. One of the first formalisms of interval-based logic is interval temporal logic (ITL), introduced by <a href="bib.xhtml#b_9781450384827_ref_126">Moszkowski</a> [<a href="bib.xhtml#b_9781450384827_ref_126">1983</a>]. ITL is similar to discrete linear-time temporal logic, but it also includes time intervals. Moszkowski proposed this logic to investigate the behavior of programs and hardware devices. Duration calculus (DC) [<a href="bib.xhtml#b_9781450384827_ref_031">Chaochen et al. 1991</a>], an extension of ITL, uses the integrated duration of states within two given intervals of time to describe the requirements of real-time embedded systems. While DC is one of the most popular interval-based logical formalisms, its semantics are built on point-based temporal characteristics.</p>
<p class="txi">The most famous work in the formal study of purely interval-based temporal logic and reasoning in AI is by Allen [1981], who formalized temporal logic on intervals by specifying 13 interval relations and showing their completeness. Any two intervals are related by exactly one of the relations through operators (<i>before</i>, <i>meets</i>, <i>overlaps</i>, <i>starts</i>, <i>during</i>, <i>finishes</i>), corresponding inverses (<i>after</i>, <i>met by</i>, <i>overlapped by</i>, <i>started by</i>, <i>contains</i>, <i>finished by</i>), and <i>equals</i> (see Figure <a href="#b_9781450384827-003_fig_003">3.3</a>). Allen argued against using time points for event representation, stating instead that logical inconsistencies arise when events are allowed to have zero duration. This representation is not appropriate for modeling events from a cognitive perspective either. Hence, events have to have a certain duration in time and extend in space in order to be perceivable. <a href="bib.xhtml#b_9781450384827_ref_070">Halpern and Shoham</a> [<a href="bib.xhtml#b_9781450384827_ref_070">1991</a>] introduced modal temporal logic based on time intervals, a generalization of point-based modal temporal logic. They expressed all 13 of Allen&#x2019;s relations between two distinct intervals with six modal operators <span class="inline-math" id="b_9781450384827-003_ineq_0001"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>&lt;</m:mo><m:mi>B</m:mi><m:mo>&gt;</m:mo></m:mrow></m:math></span>, <span class="inline-math" id="b_9781450384827-003_ineq_0002"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>&lt;</m:mo><m:mi>E</m:mi><m:mo>&gt;</m:mo></m:mrow></m:math></span>, and <span class="inline-math" id="b_9781450384827-003_ineq_0003"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>&lt;</m:mo><m:mi>A</m:mi><m:mo>&gt;</m:mo></m:mrow></m:math></span> (for <i>begin</i>, <i>end</i>, and <i>after</i>) and their transposes <span class="inline-math" id="b_9781450384827-003_ineq_0004"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>&lt;</m:mo><m:mover accent="true"><m:mi>B</m:mi><m:mo>&#x00AF;</m:mo></m:mover><m:mo>&gt;</m:mo></m:mrow></m:math></span>, <span class="inline-math" id="b_9781450384827-003_ineq_0005"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>&lt;</m:mo><m:mover accent="true"><m:mi>E</m:mi><m:mo>&#x00AF;</m:mo></m:mover><m:mo>&gt;</m:mo></m:mrow></m:math></span>, and <span class="inline-math" id="b_9781450384827-003_ineq_0006"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>&lt;</m:mo><m:mover accent="true"><m:mi>A</m:mi><m:mo>&#x00AF;</m:mo></m:mover><m:mo>&gt;</m:mo></m:mrow></m:math></span>. In this semantics, time could be discrete, continuous, linear, or branching.</p>
<p class="txi">Allen&#x2019;s interval relations have been used by most event mining algorithms [<a href="bib.xhtml#b_9781450384827_ref_034">Cohen 2001</a>, <a href="bib.xhtml#b_9781450384827_ref_124">Mooney and Roddick 2004</a>, <a href="bib.xhtml#b_9781450384827_ref_159">Shan Kam and Fu 2000</a>], but researchers <a id="page_24"/>have identified multiple disadvantages when these relations are used for pattern mining from interval time series. <a href="bib.xhtml#b_9781450384827_ref_121">Moerchen</a> [<a href="bib.xhtml#b_9781450384827_ref_121">2006</a>] summarized these issues in three categories: (1) the relations are not robust to noise because small shifts in time points lead to different patterns describing similar situations observed in the data; (2) pattern representation is ambiguous because the same pattern can describe different situations in the data; (3) pattern representation is not easily comprehensible, and the representation grows quickly with the increasing number of intervals.</p>
<figure class="split" id="b_9781450384827-003_fig_003">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-003_fig_003.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 3.3</span>Allen&#x2019;s interval relations between the intervals X and Y.</p></figcaption>
</figure>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_050">Freksa</a> [<a href="bib.xhtml#b_9781450384827_ref_050">1992</a>] revisited interval relationships at the semi-interval level, agreeing with Allen that qualitative knowledge about temporal affairs can be based on events. However, he also argued that intervals should not be used as representational primitives for reasoning about events&#x2014;in a real-world event we have to deal with incomplete knowledge, and we typically do not have complete information about temporal relations between events to begin with. Freksa used <i>beginnings</i> and <i>endings</i> of intervals as representational primitives because we can only know the temporal beginning or ending of an event. For example, we may know that a certain event X did not start before a given event Y, but we do not know if X and Y started simultaneously, or if X started after Y. As shown in Figure <a href="#b_9781450384827-003_fig_004">3.4</a>, Freksa generalized interval relations by using semi-intervals with 11 operators: <i>older</i>, <i>younger</i>, <i>head to head</i>, <i>survives</i>, <i>survived by</i>, <i>tail to tail</i>, <i>precedes</i>, <i>succeeds</i>, <i>contemporary</i>, <i>born <a id="page_25"/>before death</i> and <i>died before birth</i>. Semi-intervals allow for a flexible representation when partial or incomplete knowledge needs to be handled because operations are applied on part of an interval and not the whole. </p>
<figure class="split" id="b_9781450384827-003_fig_004">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-003_fig_004.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 3.4</span>Eleven semi-interval relationships. Question marks (?) in the pictorial illustration stand for either the symbol denoting the event depicted in the same line (X or Y) or for a blank. The number of question marks reflects the number of qualitatively alternative implementations of the given relation [<a href="bib.xhtml#b_9781450384827_ref_050">Freksa 1992</a>].</p></figcaption>
</figure>
<p class="txi">In temporal data mining very little work has focused on using semi-intervals. The most notable research was conducted by <a href="bib.xhtml#b_9781450384827_ref_122">Moerchen and Fradkin</a> [<a href="bib.xhtml#b_9781450384827_ref_122">2010</a>], which explored semi-intervals for unsupervised pattern mining and proved that semi-interval patterns are more flexible than patterns over full intervals.</p>
<p class="txi">Regardless of the exact representation of temporal entities in event mining tasks, a good representation should have the following characteristics:</p>
<p class="numeric1"><span class="nspace">1.</span><a id="page_26"/>Temporal knowledge is relative (e.g., event <i>A</i> happens after event <i>B</i>), so the representation should have very little relation to absolute dates. </p>
<p class="numeric"><span class="nspace">2.</span>The exact relationship between two times is not always known, even though some constraint on how they are related is known, so the representation should allow uncertainty of information.</p>
<p class="numeric2"><span class="nspace">3.</span>The representation should allow various granularity of temporal reasoning. For example, when modeling knowledge of a person&#x2019;s lifestyle, we may only need to consider time in terms of days. When modeling knowledge of a factory machine, we may need to consider times on the order of minutes or less.</p>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-003_s_003">3.3</span>Temporal Data Prediction</h2>
<h3 class="h3afterh2"><span class="h3space" id="b_9781450384827-003_s_003_s_001">3.3.1</span>Sequence Classification</h3>
<p class="tx">Classification is the most typical supervised learning technique, but it has not received much attention for temporal data. In sequence classification, each sequence is assumed to belong to one of the many predefined classes and the goal is to automatically determine the corresponding category for a given input sequence. For example, a time series of electrocardiogram data can be classified as healthy or not healthy. Sequence classification is challenging because sequences do not have explicit features and even with sophisticated feature selection techniques the dimensionality of potential features may still be very high. One way to classify sequences is by using a distance function that measures the similarity between a pair of sequences. Euclidean distance is a widely adopted option for calculating similarities between two sequences. However, it requires two time series to have the same length and is sensitive to distortions in time dimension. Hence, dynamic time warping distance (DTW) is utilized to overcome this problem. DTW is a generalization of classical algorithms for comparing discrete sequences to sequences of continuous values. Given two time series, <span class="inline-math" id="b_9781450384827-003_ineq_0007"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>X</m:mi><m:mo>=</m:mo><m:mrow><m:msub><m:mi>x</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:msub><m:mi>x</m:mi><m:mn>2</m:mn></m:msub><m:mo>,</m:mo><m:mi mathvariant="normal">&#x2026;</m:mi><m:mo>,</m:mo><m:msub><m:mi>x</m:mi><m:mi>i</m:mi></m:msub><m:mo>,</m:mo><m:mi mathvariant="normal">&#x2026;</m:mi><m:mo>,</m:mo><m:msub><m:mi>x</m:mi><m:mi>n</m:mi></m:msub></m:mrow></m:mrow></m:math></span> and s, DTW aligns the two series so that their difference is minimized. To this end, an <span class="inline-math" id="b_9781450384827-003_ineq_0008"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>n</m:mi><m:mi>x</m:mi><m:mi>m</m:mi></m:mrow></m:math></span> matrix is computed where the (<i>i</i>, <i>j</i>) element of the matrix contains the distance <span class="inline-math" id="b_9781450384827-003_ineq_0009"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>d</m:mi><m:mrow><m:mo>(</m:mo><m:msub><m:mi>x</m:mi><m:mi>i</m:mi></m:msub><m:mo>,</m:mo><m:msub><m:mi>y</m:mi><m:mi>j</m:mi></m:msub><m:mo>)</m:mo></m:mrow></m:mrow></m:math></span> between two points <i>x<sub>i</sub></i> and <span class="inline-math" id="b_9781450384827-003_ineq_0010"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:msub><m:mi>y</m:mi><m:mi>j</m:mi></m:msub><m:mo>.</m:mo></m:mrow></m:math></span> Dynamic programming is usually used to compute DTW. Therefore, it is a costly computation on large datasets. </p>
<p class="txi">Alignment-based distances are popular for symbolic sequences, such as DNA or event sequences. Global alignments attempt to align every residue in every sequence and are most useful when the sequences are similar and of roughly equal size. The <a href="bib.xhtml#b_9781450384827_ref_128">Needleman&#x2013;Wunsch</a> [<a href="bib.xhtml#b_9781450384827_ref_128">1970</a>] algorithm is a general global alignment <a id="page_27"/>technique that computes an optimum global alignment score between two sequences through dynamic programming. In contrast to global alignment algorithms, local alignment algorithms, such as the <a href="bib.xhtml#b_9781450384827_ref_164">Smith&#x2013;Waterman</a> [<a href="bib.xhtml#b_9781450384827_ref_164">1981</a>] algorithm and BLAST [<a href="bib.xhtml#b_9781450384827_ref_007">Altschul et al. 1990</a>], measure the similarity between two sequences by considering the most similar regions but not enforcing the alignments on full length. </p>
<p class="txi">Building an interpretable sequence classifier using distance function is difficult since there are no explicit features. One way to solve the problem of sequence classification is to transform a sequence into a vector of features through feature selections. For a symbolic sequence, the simplest way is to treat each symbol as a feature. For example, an event sequence <span class="inline-math" id="b_9781450384827-003_ineq_0011"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:msub><m:mi>E</m:mi><m:mn>2</m:mn></m:msub><m:msub><m:mi>E</m:mi><m:mn>1</m:mn></m:msub><m:msub><m:mi>E</m:mi><m:mn>2</m:mn></m:msub><m:msub><m:mi>E</m:mi><m:mn>3</m:mn></m:msub><m:msub><m:mi>E</m:mi><m:mn>1</m:mn></m:msub><m:msub><m:mi>E</m:mi><m:mn>2</m:mn></m:msub></m:mrow></m:math></span> can be transformed to a vector <span class="inline-math" id="b_9781450384827-003_ineq_0012"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi/><m:mo>&lt;</m:mo><m:mrow><m:msub><m:mi>E</m:mi><m:mn>1</m:mn></m:msub><m:msub><m:mi>E</m:mi><m:mn>2</m:mn></m:msub><m:msub><m:mi>E</m:mi><m:mn>3</m:mn></m:msub></m:mrow></m:mrow></m:math></span> However, the order between elements of the sequence cannot be captured by this transformation. To solve that, a short sequence segment of <i>n</i> consecutive symbols called <i>n</i>-grams is selected as a feature. Given a set of <i>n</i>-grams, a sequence can be represented as a vector of the presence and the absence of the <i>n</i>-grams or as a vector of the frequencies of the <i>n</i>-grams. After the transformation, sequences can be classified by a conventional classification method. For feature extraction, <a href="bib.xhtml#b_9781450384827_ref_101">Lesh et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_101">1999</a>] proposed <i>FeatureMine</i> that serves as a pre-processor to a classification algorithm. The goal of <i>FeatureMine</i> is to reduce the number of potentially useful features to be considered in the classification phase, where a feature in this context is a sequence of items.</p>
<p class="txi">Apart from sequence classification, there is a body of research on classification of numeric time series data. <a href="bib.xhtml#b_9781450384827_ref_084">Keogh and Pazzani</a> [<a href="bib.xhtml#b_9781450384827_ref_084">1998</a>] introduced a method that utilizes an extended representation of time series along with a <i>merge</i> operator. The representation consists of piecewise linear segments to represent shape and a weight vector that contains the relative importance of each individual linear segment. In a classification context, the weights are learned automatically as part of the training cycle, and the merge operator lets us combine information from two sequences; repeated application of the merge operator allows for combining information from multiple sequences. The basic idea is that the merge operator takes two sequences as input and returns a single sequence whose shape is a compromise between the two original sequences and whose weight vector reflects how much the corresponding segments in each sequence agree. Syntactic representations of signals have also been explored in temporal classification. <a href="bib.xhtml#b_9781450384827_ref_049">Firschein</a> [<a href="bib.xhtml#b_9781450384827_ref_049">1983</a>] looked at hand-constructing grammars for syntactic parsing (with error correction) of signals. In a syntactic pattern recognition approach, a time series is converted to a sequence of symbols or terminals, and a grammar is then defined to allow or disallow particular sequences of terminals corresponding to each class.</p>
<p class="txi"><a id="page_28"/>Another interesting method is applying dynamic Bayesian networks to temporal classification problems. While they are not specifically designed for temporal classification (they are more commonly used for prediction or to estimate current state, given a previous state estimate), <a href="bib.xhtml#b_9781450384827_ref_190">Zweig</a> [<a href="bib.xhtml#b_9781450384827_ref_190">1998</a>] applied them to a speech recognition task. The main problem with using dynamic Bayesian networks is that while algorithms for learning the parameters of a Bayes net are well-advanced, learning the structure of Bayes nets has proved more difficult. <a href="bib.xhtml#b_9781450384827_ref_051">Friedman et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_051">1998</a>] developed techniques for learning the structure of dynamic Bayes networks, but it remains to be seen whether these techniques can be applied in temporal classification domains.</p>
<h3 class="h3"><span class="h3space" id="b_9781450384827-003_s_003_s_002">3.3.2</span>Sequence Clustering</h3>
<p class="tx">With the amount of data generated from today&#x2019;s sensors, there is a need for unsupervised learning of sequences due to the lack of expert human-generated labels. Hence, there is an increasing interest in sequence clustering as part of temporal data mining research. The goal of clustering is to identify structure in an unlabeled dataset by organizing data into homogeneous groups where the intragroup similarity is minimized and the intergroup dissimilarity is maximized. If data does not change with time, or the change is negligible, it is called static. The bulk of clustering analyses has been performed on static data. <a href="bib.xhtml#b_9781450384827_ref_075">Han et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_075">2011</a>] classified different clustering methods for static data into five major methods: partitioning, hierarchical, density-based, grid-based, and model-based. Unlike static data, time-series data comprise values that change with time. Given a set of unlabeled time series, it is often desirable to determine groups of similar time series.</p>
<p class="txi">In temporal data streams, data can be discrete- or real-valued, uniformly or non-uniformly sampled, or univariate or multivariate. Putting aside the differences between the various algorithms that cluster data streams, there are two approaches to temporal clustering: clustering algorithms for static data are modified in such a way that temporal components can be handled, or temporal data are converted into static data so that existing algorithms can be used directly. The latter approach first converts a raw data stream either into a feature vector of lower dimension or a number of model parameters and then applies a conventional clustering algorithm to the extracted feature vectors/model parameters. The former approach replaces the distance/similarity measure for static data with an appropriate one for temporal data.</p>
<p class="txi">Similar to sequence classification, a key component in clustering is the metric used to measure the similarity between two data points being compared. These data could be in various forms, including raw values of equal or unequal length, vectors of feature&#x2013;value pairs, symbols in a specific order, and so on. The choice <a id="page_29"/>of a proper similarity or distance metric depends on the characteristic of the time series, the representation method, and the objective or the particular application domain. The objectives of similarity measures can be classified into three broad categories: finding similar time series in time; finding similar time series in shape; and finding similar time series in change (structural similarity). </p>
<p class="txi">With this categorization, it is easier to classify different similarity/distance metrics under the correct category. For instance, Euclidean, root-mean-square, Minkowski, and correlation-based distances can be applied to find similar time series in time. Also, time series can be transformed using fast Fourier transforms (FFTs), wavelets, or piecewise aggregate approximation (PAA) to reduce the cost of similarity computation. <a href="bib.xhtml#b_9781450384827_ref_085">Keogh and Kasetty</a> [<a href="bib.xhtml#b_9781450384827_ref_085">2003</a>] did a comprehensive review on this subject. When the occurrence time of patterns is not the main concern but the overall similarity in shape is important, other techniques such as DTW and short time series (STS) distance can be used. </p>
<p class="txi">Considering each time series as a piecewise linear function, <a href="bib.xhtml#b_9781450384827_ref_123">M&#x00F6;ller-Levet et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_123">2003</a>] proposed the STS distance as the sum of the squared differences of the slopes in two time series being compared. Mathematically, the STS distance between two time series <i>x<sub>i</sub></i> and <i>v<sub>j</sub></i> is defined as:</p>
<p class="display"><disp-formula id="b_9781450384827-003_eq_001">
<m:math display="block" xmlns:m="http://www.w3.org/1998/Math/MathML" alttext="" altimg="../images/b_9781450384827-003_eq_0001.png"><m:mrow><m:msub><m:mi>d</m:mi><m:mrow><m:mi>S</m:mi><m:mi>T</m:mi><m:mi>S</m:mi></m:mrow></m:msub><m:mo>=</m:mo><m:msqrt><m:mrow><m:munderover><m:mo largeop="true" movablelimits="false" symmetric="true">&#x2211;</m:mo><m:mrow><m:mi>k</m:mi><m:mo>=</m:mo><m:mn>1</m:mn></m:mrow><m:mi>P</m:mi></m:munderover><m:msup><m:mrow><m:mo>(</m:mo><m:mrow><m:mfrac><m:mrow><m:msub><m:mi>v</m:mi><m:mrow><m:mi>j</m:mi><m:mrow><m:mo>(</m:mo><m:mrow><m:mi>k</m:mi><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow><m:mo>)</m:mo></m:mrow></m:mrow></m:msub><m:mo>&#x2212;</m:mo><m:msub><m:mi>v</m:mi><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub></m:mrow><m:mrow><m:msub><m:mi>t</m:mi><m:mrow><m:mi>k</m:mi><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:msub><m:mo>&#x2212;</m:mo><m:msub><m:mi>t</m:mi><m:mi>k</m:mi></m:msub></m:mrow></m:mfrac><m:mo>&#x2212;</m:mo><m:mfrac><m:mrow><m:msub><m:mi>x</m:mi><m:mrow><m:mi>i</m:mi><m:mrow><m:mo>(</m:mo><m:mrow><m:mi>k</m:mi><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow><m:mo>)</m:mo></m:mrow></m:mrow></m:msub><m:mo>&#x2212;</m:mo><m:msub><m:mi>x</m:mi><m:mrow><m:mi>i</m:mi><m:mi>k</m:mi></m:mrow></m:msub></m:mrow><m:mrow><m:msub><m:mi>t</m:mi><m:mrow><m:mi>k</m:mi><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:msub><m:mo>&#x2212;</m:mo><m:msub><m:mi>t</m:mi><m:mi>k</m:mi></m:msub></m:mrow></m:mfrac></m:mrow><m:mo>)</m:mo></m:mrow><m:mn>2</m:mn></m:msup></m:mrow></m:msqrt></m:mrow></m:math>
</disp-formula></p>
<p class="float">(3.1)</p>
<p class="tx">where <i>t<sub>k</sub></i> is the time point for data point <span class="inline-math" id="b_9781450384827-003_ineq_0013"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:msub><m:mi>x</m:mi><m:mrow><m:mi>i</m:mi><m:mi>k</m:mi></m:mrow></m:msub></m:math></span> and <span class="inline-math" id="b_9781450384827-003_ineq_0014"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:msub><m:mi>v</m:mi><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub><m:mo>.</m:mo></m:mrow></m:math></span> To remove the effect of scale, <i>z</i> standardization of the series is recommended.</p>
<p class="txi">For structural similarity we can use modeling methods such as a hidden Markov model (HMM) [<a href="bib.xhtml#b_9781450384827_ref_165">Smyth 1997</a>] or an autoregressive&#x2013;moving-average model (ARMA) process [<a href="bib.xhtml#b_9781450384827_ref_186">Xiong and Yeung 2002</a>] and measure similarity on the parameters of the fitted model to the time series. It is worth noting that the shape level is usually utilized to measure similarity in short-length time series such as individual heartbeats by comparing their local patterns, whereas the structure level measures similarity based on global and high-level structures and is used for long-length time-series data such as yearly meteorological data. Hence, DTW, Euclidean, and longest common subsequence [<a href="bib.xhtml#b_9781450384827_ref_175">Vlachos et al. 2002</a>] are appropriate for shape-level similarity, and HMM and ARMA are more suitable for structure-level similarity. <a href="bib.xhtml#b_9781450384827_ref_097">Last et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_097">2004</a>] documented a survey on various methods for efficient retrieval of similar time series.</p>
<p class="txi">In temporal data mining, there is a notion of <i>subsequence clustering</i> that states, given a single time series, individual time series (subsequences) can be extracted <a id="page_30"/>with a sliding window. Clustering is then performed on the extracted subsequences. With growing interest in streaming data and online algorithms, subsequence clustering has attracted much attention in recent years. <a href="bib.xhtml#b_9781450384827_ref_086">Keogh and Lin</a> [<a href="bib.xhtml#b_9781450384827_ref_086">2005</a>] argued that clusters extracted from subsequences are forced to obey a certain constraint that is pathologically unlikely to be satisfied by any dataset, and because of this, the clusters extracted by any clustering algorithm are essentially random. The researchers experimentally confirmed that STS clustering produces sine wave clusters, regardless of the dataset used or the parameter setting, and they provided a tentative solution based on motif mining. Although the time complexity of the algorithm is untenable for massive datasets, it is simply offered as an existence proof that such an algorithm for subsequence clustering exists. </p>
<p class="txi">Most algorithms for time-series clustering are developed for continuous data. Despite growing interest in discrete-valued time series, especially related to counting specific objects or the occurrence of events at specified times, most time-series algorithms give very little attention to this increasingly important subject area. <a href="bib.xhtml#b_9781450384827_ref_150">Ramoni et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_150">2002</a>] presented BCD: a Bayesian algorithm for clustering by dynamics. Given a set <i>S</i> of <i>n</i> univariate discrete-valued time series, BCD transforms each series into a Markov chain (MC) and then clusters similar MCs to discover the most probable set of generating processes. BCD is basically an unsupervised agglomerative clustering method. Considering a partition as a hidden discrete variable <i>C</i>, each state <i>C<sub>k</sub></i> of <i>C</i> represents a cluster of time series and hence determines a transition matrix. The task of clustering is regarded as a Bayesian model selection problem, with the objective being to select the model with the maximum posterior probability. Since the same data are used to compare all models, and all models are equally likely, the comparison can be based on the marginal likelihood <span class="inline-math" id="b_9781450384827-003_ineq_0015"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>p</m:mi><m:mrow><m:mo>(</m:mo><m:mi>S</m:mi><m:mo>|</m:mo><m:mi>M</m:mi><m:mi>C</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math></span>, which is a measure of how likely the data were if the model <i>MC</i> is true. The similarity between two estimated transition matrices is measured as an average of the symmetrized Kullback&#x2013;Liebler distance between corresponding rows in the matrices. The clustering result is evaluated mainly by measuring the loss of data information induced by clustering, which is specific to the proposed clustering method. <a href="bib.xhtml#b_9781450384827_ref_149">Ramoni et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_149">2000</a>] also presented a Bayesian clustering algorithm for multivariate time series that searches for the most probable set of clusters given the data using a similarity-based heuristic search method. The measure of similarity is an average of the Kullback&#x2013;Liebler distances between comparable transition probability tables, with the similarity measure serving as a heuristic guide for the search process rather than a grouping criterion. Both the grouping and stopping criteria are based on the posterior probability of the obtained clustering. The objective is to find a maximum posterior probability partition of a set of MCs. </p>
<p class="txi"><a id="page_31"/></p>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-003_s_004">3.4</span>Pattern Discovery</h2>
<p class="tx">Once database applications for shopping, potential outbreaks of infectious diseases, stock trading, and other time-related scenarios became widespread, temporal data mining became popular. Applying traditional data mining techniques by treating the temporal aspects of data as static, and ignoring its temporal structure, is not adequate in harnessing the true value of data and building appropriate models of dynamic systems. The scope of temporal data mining and pattern discovery extends beyond the standard forecast or prediction applications of time series analysis. Very often, in temporal data mining one does not even know which variables in the data are expected to exhibit any correlations or causal relationships. The objective of pattern discovery is unearthing useful and unexpected trends or patterns in the data usually by exploratory and unsupervised means. These patterns signify a local structure in the sequence and are much more readily interpretable by the data owner. There is no universal criteria in the literature for measuring the <i>interestingness</i> of a pattern, but pattern <i>frequency</i> is a useful criteria for this purpose. Much of data mining literature is concerned with formulating useful pattern structures and developing efficient algorithms to discover frequently occurring patterns.</p>
<h3 class="h3"><span class="h3space" id="b_9781450384827-003_s_004_s_001">3.4.1</span>Association Rule Mining</h3>
<p class="tx">The first step toward understanding causal relationships is manifested in the form of association rule mining. In this approach, the goal is to find which event may be causing other events. The relationships among these events are called <i>association rules</i>. From the time when association rule mining was first introduced by <a href="bib.xhtml#b_9781450384827_ref_004">Agrawal et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_004">1993b</a>], it has received a great deal of attention in the knowledge discovery in databases (KDD) community. Following the original definition by <a href="bib.xhtml#b_9781450384827_ref_002">Agrawal and Srikant</a> [<a href="bib.xhtml#b_9781450384827_ref_002">1995</a>], let <span class="inline-math" id="b_9781450384827-003_ineq_0016"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>I</m:mi><m:mo>=</m:mo><m:mrow><m:mo>{</m:mo><m:msub><m:mi>i</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:msub><m:mi>i</m:mi><m:mn>2</m:mn></m:msub><m:mo>,</m:mo><m:mi mathvariant="normal">&#x2026;</m:mi><m:mo>,</m:mo><m:msub><m:mi>i</m:mi><m:mi>n</m:mi></m:msub><m:mo>}</m:mo></m:mrow></m:mrow></m:math></span> be a set of <i>n</i> binary attributes called items and <span class="inline-math" id="b_9781450384827-003_ineq_0017"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>T</m:mi><m:mo>=</m:mo><m:mrow><m:mo>{</m:mo><m:msub><m:mi>t</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:msub><m:mi>t</m:mi><m:mn>2</m:mn></m:msub><m:mo>,</m:mo><m:mi mathvariant="normal">&#x2026;</m:mi><m:mo>,</m:mo><m:msub><m:mi>t</m:mi><m:mi>m</m:mi></m:msub><m:mo>}</m:mo></m:mrow></m:mrow></m:math></span> be a set of <i>m</i> transactions in the database. An association rule is an expression <i>X</i> &#x21D2; <i>Y</i>, where <span class="inline-math" id="b_9781450384827-003_ineq_0018"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mrow><m:mrow><m:mi>X</m:mi><m:mo>,</m:mo><m:mi>Y</m:mi></m:mrow><m:mo>&#x2286;</m:mo><m:mi>I</m:mi></m:mrow><m:mo>.</m:mo></m:mrow></m:math></span> Intuitively, the rule means whenever attribute <i>X</i> is present in the transaction attribute <i>Y</i> is probably present as well. The probability or rule confidence is defined as the percentage of transactions containing <i>Y</i> in addition to <i>X</i> with regard to the overall number of transactions containing <i>X</i>. In conditional probability language, rule confidence can be expressed as <span class="inline-math" id="b_9781450384827-003_ineq_0019"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>p</m:mi><m:mrow><m:mo>(</m:mo><m:mi>Y</m:mi><m:mo>&#x2286;</m:mo><m:mi>I</m:mi><m:mo>|</m:mo><m:mi>X</m:mi><m:mo>&#x2286;</m:mo><m:mi>I</m:mi><m:mo>)</m:mo></m:mrow><m:mo>.</m:mo></m:mrow></m:math></span> Association rules are often explained in the context of a supermarket basket, where <i>I</i> is the list of all items in a supermarket and <i>i<sub>n</sub></i> is an entry with binary value, where 0 means the item is absent in its corresponding transaction and 1 means the item is present in that transaction.</p>
<p class="txi">To select interesting rules from the set of all possible rules, various significance metrics might be used. For example, <i>support</i> indicates how frequently the itemset <a id="page_32"/>appears in the database. For a given rule <i>X</i> &#x21D2; <i>Y</i>, support of <i>X</i> with respect to <i>T</i> is defined as: </p>
<p class="display"><disp-formula id="b_9781450384827-003_eq_002">
<m:math display="block" xmlns:m="http://www.w3.org/1998/Math/MathML" alttext="" altimg="../images/b_9781450384827-003_eq_0002.png"><m:mrow><m:mrow><m:mi>s</m:mi><m:mi>u</m:mi><m:mi>p</m:mi><m:mi>p</m:mi><m:mrow><m:mo>(</m:mo><m:mi>X</m:mi><m:mo>)</m:mo></m:mrow></m:mrow><m:mo>=</m:mo><m:mfrac><m:mrow><m:mo>|</m:mo><m:mi>t</m:mi><m:mo>&#x2208;</m:mo><m:mi>T</m:mi><m:mo>;</m:mo><m:mi>X</m:mi><m:mo>&#x2286;</m:mo><m:mi>t</m:mi><m:mo>|</m:mo></m:mrow><m:mrow><m:mo>|</m:mo><m:mi>T</m:mi><m:mo>|</m:mo></m:mrow></m:mfrac></m:mrow></m:math>
</disp-formula></p>
<p class="float">(3.2)</p>
<p class="txi">Also, as mentioned above, <i>confidence</i> is an indication of how often the rule has been found true.</p>
<p class="display"><disp-formula id="b_9781450384827-003_eq_003">
<m:math display="block" xmlns:m="http://www.w3.org/1998/Math/MathML" alttext="" altimg="../images/b_9781450384827-003_eq_0003.png"><m:mrow><m:mi>c</m:mi><m:mi>o</m:mi><m:mi>n</m:mi><m:mi>f</m:mi><m:mrow><m:mo>(</m:mo><m:mi>X</m:mi><m:mo>&#x21D2;</m:mo><m:mi>Y</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mfrac><m:mrow><m:mi>s</m:mi><m:mi>u</m:mi><m:mi>p</m:mi><m:mi>p</m:mi><m:mrow><m:mo>(</m:mo><m:mrow><m:mi>X</m:mi><m:mo>&#x222A;</m:mo><m:mi>Y</m:mi></m:mrow><m:mo>)</m:mo></m:mrow></m:mrow><m:mrow><m:mi>s</m:mi><m:mi>u</m:mi><m:mi>p</m:mi><m:mi>p</m:mi><m:mrow><m:mo>(</m:mo><m:mi>X</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:mfrac></m:mrow></m:math>
</disp-formula></p>
<p class="float">(3.3)</p>
<p class="txi">Usually, the problem of association rule mining can be reduced to the problem of finding all itemsets that are frequent with respect to a predefined support threshold. However, the use of a brute-force technique to calculate the support of all possible itemsets is not practical&#x2014;as the number of items increases, the number of itemsets grows exponentially. </p>
<p class="txi">Multiple algorithms have been proposed for association rule mining, including Apriori [<a href="bib.xhtml#b_9781450384827_ref_076">Hipp et al. 2000</a>], FP-growth [<a href="bib.xhtml#b_9781450384827_ref_073">Han et al. 2000b</a>], and Eclat [<a href="bib.xhtml#b_9781450384827_ref_189">Zaki 2000</a>]. Apriori is based on the downward closure property of itemset support values; it prunes the candidate rules that have an infrequent subset before computing their support value by using breadth first search (BFS) to determine the support value of all (<i>k</i>-1)-itemsets before counting the support value of <i>k</i>-itemsets. The critical part is finding candidate itemsets of different sizes. For this purpose, <a href="bib.xhtml#b_9781450384827_ref_001">Agarwal and Srikant</a> [<a href="bib.xhtml#b_9781450384827_ref_001">1994</a>] introduced a hash-tree structure in which the items in each transaction are used to descend the hash tree. Whenever we reach one of the tree&#x2019;s leaves, we find a set of candidates that have a common prefix in the transaction. AprioriTID is an extension of the basic Apriori algorithm that uses a raw database to count the supports. AprioriTID internally represents each transaction as <span class="inline-math" id="b_9781450384827-003_ineq_0020"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>&lt;</m:mo><m:mi>T</m:mi><m:mi>I</m:mi><m:mi>D</m:mi><m:mo>,</m:mo><m:msub><m:mi>X</m:mi><m:mi>k</m:mi></m:msub><m:mo>&gt;</m:mo></m:mrow></m:math></span>, where <i>X<sub>k</sub></i> is a potentially large <i>k</i>-itemset in the transaction identified by <i>TID</i>.</p>
<p class="txi">In association rule mining items in an itemset do not have any specific order. Hence, this approach cannot formulate <i>followed by</i> or <i>within</i> statements. Moreover, <i>while</i> statements cannot be formulated because duration of events are not considered at all.</p>
<h3 class="h3"><span class="h2space" id="b_9781450384827-003_s_004_s_002">3.4.2</span>Sequence Mining</h3>
<p class="tx">Sequence mining was first proposed by <a href="bib.xhtml#b_9781450384827_ref_002">Agrawal and Srikant</a> [<a href="bib.xhtml#b_9781450384827_ref_002">1995</a>] to discover the sequential patterns that occur frequently in a database. It assumes that the customer transaction database is given, where each transaction includes the customer ID, the transaction time, and the itemset purchased in the transaction. An <i>itemset</i> is a nonempty set of items and a <i>sequence</i> is an ordered list of itemsets. <a id="page_33"/>For example, an itemset <i>i</i> denoted as <span class="inline-math" id="b_9781450384827-003_ineq_0021"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>(</m:mo><m:msub><m:mi>i</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:msub><m:mi>i</m:mi><m:mn>2</m:mn></m:msub><m:mo>,</m:mo><m:mi mathvariant="normal">&#x2026;</m:mi><m:mo>,</m:mo><m:msub><m:mi>i</m:mi><m:mi>k</m:mi></m:msub><m:mo>)</m:mo></m:mrow></m:math></span>, where <i>i<sub>k</sub></i> is an item in the itemset, and sequence <i>s</i> denoted as <span class="inline-math" id="b_9781450384827-003_ineq_0022"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>s</m:mi><m:mo>=</m:mo><m:mo>&lt;</m:mo><m:msub><m:mi>s</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:mi mathvariant="normal">&#x2026;</m:mi><m:mo>,</m:mo><m:msub><m:mi>s</m:mi><m:mi>n</m:mi></m:msub><m:mo>&gt;</m:mo></m:mrow></m:math></span>, where <i>s<sub>j</sub></i> is an itemset. A sequence <span class="inline-math" id="b_9781450384827-003_ineq_0023"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>a</m:mi><m:mo>=</m:mo><m:mo>&lt;</m:mo><m:msub><m:mi>a</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:mi mathvariant="normal">&#x2026;</m:mi><m:mo>,</m:mo><m:msub><m:mi>a</m:mi><m:mi>n</m:mi></m:msub><m:mo>&gt;</m:mo></m:mrow></m:math></span> is <i>contained</i> in another sequence <span class="inline-math" id="b_9781450384827-003_ineq_0024"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>b</m:mi><m:mo>=</m:mo><m:mo>&lt;</m:mo><m:msub><m:mi>b</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:mi mathvariant="normal">&#x2026;</m:mi><m:mo>,</m:mo><m:msub><m:mi>b</m:mi><m:mi>m</m:mi></m:msub><m:mo>&gt;</m:mo></m:mrow></m:math></span> if there exist integers <span class="inline-math" id="b_9781450384827-003_ineq_0025"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:msub><m:mi>i</m:mi><m:mn>1</m:mn></m:msub><m:mo>&#x2264;</m:mo><m:msub><m:mi>i</m:mi><m:mn>2</m:mn></m:msub><m:mo>&#x2264;</m:mo><m:mi mathvariant="normal">&#x2026;</m:mi><m:mo>&#x2264;</m:mo><m:msub><m:mi>i</m:mi><m:mi>n</m:mi></m:msub></m:mrow></m:math></span> such that <span class="inline-math" id="b_9781450384827-003_ineq_0026"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mrow><m:msub><m:mi>a</m:mi><m:mn>1</m:mn></m:msub><m:mo>&#x2286;</m:mo><m:msub><m:mi>b</m:mi><m:msub><m:mi>i</m:mi><m:mn>1</m:mn></m:msub></m:msub></m:mrow><m:mo>,</m:mo></m:mrow></m:math></span> <span class="inline-math" id="b_9781450384827-003_ineq_0027"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:msub><m:mi>a</m:mi><m:mn>2</m:mn></m:msub><m:mo>&#x2286;</m:mo><m:msub><m:mi>b</m:mi><m:msub><m:mi>i</m:mi><m:mn>2</m:mn></m:msub></m:msub></m:mrow></m:math></span>,&#x2026;, <span class="inline-math" id="b_9781450384827-003_ineq_0028"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mrow><m:msub><m:mi>a</m:mi><m:mi>n</m:mi></m:msub><m:mo>&#x2286;</m:mo><m:msub><m:mi>b</m:mi><m:msub><m:mi>i</m:mi><m:mi>n</m:mi></m:msub></m:msub></m:mrow><m:mo>.</m:mo></m:mrow></m:math></span> For example, the sequence <span class="inline-math" id="b_9781450384827-003_ineq_0029"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>a</m:mi><m:mo>=</m:mo><m:mrow><m:mo>&lt;</m:mo><m:mrow><m:mrow><m:mo>(</m:mo><m:mn>3</m:mn><m:mo>)</m:mo></m:mrow><m:mrow><m:mo>(</m:mo><m:mn>4</m:mn><m:mo>,</m:mo><m:mn>6</m:mn><m:mo>)</m:mo></m:mrow><m:mrow><m:mo>(</m:mo><m:mn>8</m:mn><m:mo>)</m:mo></m:mrow></m:mrow><m:mo>&gt;</m:mo></m:mrow></m:mrow></m:math></span> is contained in <span class="inline-math" id="b_9781450384827-003_ineq_0030"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>b</m:mi><m:mo>=</m:mo><m:mrow><m:mo>&lt;</m:mo><m:mrow><m:mrow><m:mo>(</m:mo><m:mn>3</m:mn><m:mo>,</m:mo><m:mn>2</m:mn><m:mo>)</m:mo></m:mrow><m:mrow><m:mo>(</m:mo><m:mn>4</m:mn><m:mo>,</m:mo><m:mn>7</m:mn><m:mo>,</m:mo><m:mn>6</m:mn><m:mo>)</m:mo></m:mrow><m:mrow><m:mo>(</m:mo><m:mn>12</m:mn><m:mo>,</m:mo><m:mn>8</m:mn><m:mo>)</m:mo></m:mrow></m:mrow><m:mo>&gt;</m:mo></m:mrow></m:mrow></m:math></span>. Sequence <i>a</i> shows that item 3 is bought, and then items 4 and 6 bought together. In a set of sequences, a sequence <i>s</i> is <i>maximal</i> if <i>s</i> is not contained in any other sequence. All the transactions of a customer can together be viewed as a sequence where each transaction corresponds to a set of items and the list of transactions, ordered by increasing transaction time, corresponds to a sequence that is called a customer-sequence.</p>
<p class="txi">The support for any arbitrary sequence <i>s</i> of itemsets is the fraction of customer transaction sequences in the database <i>D</i> which contain <i>s</i>. The user can specify a minimum support threshold &#x03B8;; any sequences whose support thresholds are greater than &#x03B8; are called a <i>large sequence</i>. If sequence <i>a</i> is large and maximal (among the set of all large sequences), it is regarded as a <i>sequential pattern</i>. To find all such sequential patterns in <i>D</i>, <a href="bib.xhtml#b_9781450384827_ref_002">Agrawal and Srikant</a> [<a href="bib.xhtml#b_9781450384827_ref_002">1995</a>] presented two general algorithms: AprioriAll and AprioriSome. AprioriAll counts all the large sequences and prunes out the non-maximal ones in a post-processing step. In its first pass through the data, it obtains large 1-sequence patterns and then constructs candidate 2-sequences by combining the large 1-sequence itemsets in all possible combinations. The next pass identifies the large 2-sequences, obtains large 3-sequences from them, and so on. AprioriSome intelligently exploits the maximality constraint. The idea behind these algorithms is that since we are only interested in maximal sequences, if we first count longer sequences, we can avoid counting sequences that are contained in a longer sequence. However, we have to be careful not to count longer sequences that do not have minimum support. The algorithm has a forward phase that finds all the frequent sequences of certain lengths and a backward phase that discovers all the remaining frequent sequences. </p>
<p class="txi">Although sequence mining considers an order between itemsets and is capable of formulating <i>followed by</i> statements, it cannot formulate <i>within</i> and <i>while</i> statements needed for asthma risk application. Hence, questions such as <i>how long after starting to exercise asthma attacks happen?</i> cannot be answered because sequence mining does not consider the start and/or end time of events, or any time lag between events.</p>
<p class="txi">Since sequence mining was first introduced 1995, it has drawn a great deal of attention, and many extensions have been proposed in various directions:</p>
<p class="numeric1"><span class="nspace">1.</span>Different variants of sequential patterns including maximum patterns [<a href="bib.xhtml#b_9781450384827_ref_002">Agrawal and Srikant 1995</a>], similar patterns [<a href="bib.xhtml#b_9781450384827_ref_003">Agrawal et al. 1993a</a>, <a href="bib.xhtml#b_9781450384827_ref_103">Li et al.</a> <a href="bib.xhtml#b_9781450384827_ref_103">1996</a>], <a id="page_34"/>cyclic/periodic patterns [<a href="bib.xhtml#b_9781450384827_ref_009">Amphawan et al. 2009</a>, <a href="bib.xhtml#b_9781450384827_ref_114">Ma and Hellerstein 2001</a>], and traversal patterns [<a href="bib.xhtml#b_9781450384827_ref_140">Pei et al. 2000</a>].</p>
<p class="numeric"><span class="nspace">2.</span>Improved algorithms for mining sequential patterns [<a href="bib.xhtml#b_9781450384827_ref_072">Han et al. 2000a</a>, <a href="bib.xhtml#b_9781450384827_ref_012">Ayres et al. 2002</a>, <a href="bib.xhtml#b_9781450384827_ref_141">Pei et al. 2001</a>].</p>
<p class="numeric2"><span class="nspace">3.</span>Mining sequential patterns with constraints [<a href="bib.xhtml#b_9781450384827_ref_125">Morzy et al. 2002</a>, <a href="bib.xhtml#b_9781450384827_ref_142">Pei et al. 2002</a>, <a href="bib.xhtml#b_9781450384827_ref_166">Srikant and Agrawal 1996</a>].</p>
<p class="txi">Some of these methods aim to improve the performance of the algorithm by Agrawal and Srikant. For example, <a href="bib.xhtml#b_9781450384827_ref_104">Lin and Lee</a> [<a href="bib.xhtml#b_9781450384827_ref_104">2003</a>] introduced a framework for interactive sequential pattern discovery in which the user generates queries with several minimum support thresholds iteratively and discovers a set of patterns corresponding to the last threshold.</p>
<p class="txi">Another well-known method is the PrefixSpan algorithm [<a href="bib.xhtml#b_9781450384827_ref_141">Pei et al. 2001</a>], which uses a divide-and-conquer strategy to solve the sequence mining problem for point events. First, it scans the database to find the frequent 1-patterns <span class="inline-math" id="b_9781450384827-003_ineq_0031"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mrow><m:mo>(</m:mo><m:msub><m:mi>L</m:mi><m:mn>1</m:mn></m:msub><m:mo>)</m:mo></m:mrow><m:mo>.</m:mo></m:mrow></m:math></span> Suppose there are <span class="inline-math" id="b_9781450384827-003_ineq_0032"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>|</m:mo><m:msub><m:mi>L</m:mi><m:mn>1</m:mn></m:msub><m:mo>|</m:mo></m:mrow></m:math></span> patterns in <span class="inline-math" id="b_9781450384827-003_ineq_0033"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:msub><m:mi>L</m:mi><m:mn>1</m:mn></m:msub><m:mo>.</m:mo></m:mrow></m:math></span> The original database, therefore, is divided into <span class="inline-math" id="b_9781450384827-003_ineq_0034"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>|</m:mo><m:msub><m:mi>L</m:mi><m:mn>1</m:mn></m:msub><m:mo>|</m:mo></m:mrow></m:math></span> partitions, where each partition is the projection of the sequence database with respect to the corresponding 1-patterns. Second, each partition is treated as the original one to find all large 1-patterns within it. Appending these large 1-patterns will generate frequent patterns with a length increased by 1, successfully extending the prefixes. By recursively running these steps until the prefixes cannot be extended reveals all the frequent sequential patterns. SPAM [<a href="bib.xhtml#b_9781450384827_ref_012">Ayres et al. 2002</a>] is another algorithm based on depth-first search that uses a memory-resident bitmap representation to achieve a high speedup (compared to PrefixSpan) for large datasets.</p>
<h3 class="h3"><span class="h2space" id="b_9781450384827-003_s_004_s_003">3.4.3</span>Frequent Episode Mining</h3>
<p class="tx">Another approach for temporal pattern discovery in sequential data is frequent episode mining [<a href="bib.xhtml#b_9781450384827_ref_118">Mannila et al. 1997</a>]. In the sequence mining explained in Section <a href="#b_9781450384827-003_s_004_s_002">3.4.2</a>, a collection of sequences is given, and the task is to discover the ordered sequences of items that occur repeatedly. In frequent episode mining, the data are given in a single long sequence, and the task is to discover the temporal patterns, called episodes, that occur frequently in that sequence.</p>
<p class="txi">The data are referred to as an <i>event sequence</i>, denoted by <span class="inline-math" id="b_9781450384827-003_ineq_0035"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>&lt;</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mi>E</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:msub><m:mi>t</m:mi><m:mn>1</m:mn></m:msub><m:mo>)</m:mo></m:mrow><m:mo>,</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mi>E</m:mi><m:mn>2</m:mn></m:msub><m:mo>,</m:mo><m:msub><m:mi>t</m:mi><m:mn>2</m:mn></m:msub><m:mo>)</m:mo></m:mrow><m:mo>,</m:mo><m:mi mathvariant="normal">&#x2026;</m:mi><m:mo>&gt;</m:mo></m:mrow></m:math></span>, where <i>E<sub>i</sub></i> is a point event with timestamp <i>t<sub>i</sub></i>. An episode is a partially ordered set of events. When the order among the event types of an episode is total it is called a <i>serial episode</i>, and when there is no order at all the episode is called a <i>parallel episode</i>. Parallel episodes are similar to itemsets where the order between items does not matter. For example, <span class="inline-math" id="b_9781450384827-003_ineq_0036"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>(</m:mo><m:mrow><m:mi>A</m:mi><m:mo>&#x2192;</m:mo><m:mi>B</m:mi><m:mo>&#x2192;</m:mo><m:mi>C</m:mi></m:mrow><m:mo>)</m:mo></m:mrow></m:math></span> is a 3-node serial episode and (<i>ABC</i>) is a <a id="page_35"/>3-node parallel episode. Given a pre-defined frequency threshold &#x03B8;, all episodes with frequency higher than &#x03B8; are considered frequent. The process of frequent episode discovery is a level-wise algorithm that starts with discovering frequent 1-node episodes. These are then combined to form candidate 2-node episodes, and by counting their frequencies, 2-node frequent episodes are obtained, and so on. Thus far, for the asthma risks example, episode mining is the most appropriate algorithm. We can not only represent sequential patterns like <span class="inline-math" id="b_9781450384827-003_ineq_0037"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>(</m:mo><m:mrow><m:mrow><m:mi>E</m:mi><m:mi>x</m:mi><m:mi>e</m:mi><m:mi>r</m:mi><m:mi>c</m:mi><m:mi>i</m:mi><m:mi>s</m:mi><m:mi>e</m:mi></m:mrow><m:mo>&#x2192;</m:mo><m:mrow><m:mrow><m:mi>A</m:mi><m:mi>s</m:mi><m:mi>t</m:mi><m:mi>h</m:mi><m:mi>m</m:mi><m:mi>a</m:mi></m:mrow><m:mo>&#x2212;</m:mo><m:mrow><m:mi>A</m:mi><m:mi>t</m:mi><m:mi>t</m:mi><m:mi>a</m:mi><m:mi>c</m:mi><m:mi>k</m:mi></m:mrow></m:mrow></m:mrow><m:mo>)</m:mo></m:mrow></m:math></span>, but also parallel patterns like <span class="inline-math" id="b_9781450384827-003_ineq_0038"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mrow><m:mo>(</m:mo><m:mrow><m:mrow><m:mi>P</m:mi><m:mi>o</m:mi><m:mi>l</m:mi><m:mi>l</m:mi><m:mi>u</m:mi><m:mi>t</m:mi><m:mi>i</m:mi><m:mi>o</m:mi><m:mi>n</m:mi></m:mrow><m:mo>&#x2212;</m:mo><m:mrow><m:mi>H</m:mi><m:mi>i</m:mi><m:mi>g</m:mi><m:mi>h</m:mi></m:mrow></m:mrow><m:mo mathvariant="italic" separator="true">&#x2003;</m:mo><m:mrow><m:mrow><m:mi>T</m:mi><m:mi>e</m:mi><m:mi>m</m:mi><m:mi>p</m:mi><m:mi>e</m:mi><m:mi>r</m:mi><m:mi>a</m:mi><m:mi>t</m:mi><m:mi>u</m:mi><m:mi>r</m:mi><m:mi>e</m:mi></m:mrow><m:mo>&#x2212;</m:mo><m:mrow><m:mi>L</m:mi><m:mi>o</m:mi><m:mi>w</m:mi></m:mrow></m:mrow><m:mo>)</m:mo></m:mrow><m:mo>.</m:mo></m:mrow></m:math></span> However, we still cannot formulate <i>within</i> statement between events, for instance, Asthma-Attack followed by Exercise <i>within</i> 10 minutes.</p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_118">Mannila et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_118">1997</a>] applied a windows-based frequency measure to count episodes, whereas <a href="bib.xhtml#b_9781450384827_ref_028">Casas-Garriga</a> [<a href="bib.xhtml#b_9781450384827_ref_028">2003</a>] proposed a frequency measure in which the user chooses the maximum inter-node distance allowed and the algorithm automatically adjusts window width based on the length of the episodes being counted. <a href="bib.xhtml#b_9781450384827_ref_098">Laxman et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_098">2005</a>] proposed frequency counts for non-overlapped episode occurrence, with non-interleaved episode occurrence based on directly counting a suitable subset of episode occurrences. Graph theory approaches have also been explored to locate episodes in a sequence [<a href="bib.xhtml#b_9781450384827_ref_077">Hirao et al. 2001</a>, <a href="bib.xhtml#b_9781450384827_ref_172">Tron&#x00ED;&#x010D;ek 2001</a>], typically employing a pre-processing step that builds an automaton called a directed acyclic subsequence graph (DASG) that accepts a string if and only if it is a subsequence of the given input sequence. Once the DASG is constructed an episode can be located in the sequence in linear time. These algorithms, however, are more suited for pattern matching applications than for the discovery of frequent episodes. </p>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-003_s_005">3.5</span>Different Types of Patterns</h2>
<p class="tx">Frequent pattern mining often generates a huge number of patterns, the majority of which might be insignificant depending on application or user requirements. When confronted with this problem in real-world tasks, researchers have tried to reduce the desired set by finding user interest-based patterns such as maximal frequent [<a href="bib.xhtml#b_9781450384827_ref_065">Gouda and Zaki 2001</a>], demand-driven, utility [<a href="bib.xhtml#b_9781450384827_ref_188">Yao et al. 2004</a>], constraint-based [<a href="bib.xhtml#b_9781450384827_ref_143">Pei et al. 2004</a>], top-k [<a href="bib.xhtml#b_9781450384827_ref_074">Han et al. 2002</a>], and cyclic patterns [<a href="bib.xhtml#b_9781450384827_ref_169">Tanbeer et al. 2009b</a>].</p>
<h3 class="h3"><span class="h2space" id="b_9781450384827-003_s_005_s_001">3.5.1</span>T-Patterns</h3>
<p class="tx">T-patterns, as defined by <a href="bib.xhtml#b_9781450384827_ref_115">Magnusson</a> [<a href="bib.xhtml#b_9781450384827_ref_115">2000</a>], are used to detect sequential patterns in the behavioral sciences. In the T-pattern approach, symbolic time series are investigated, with each symbol representing the occurrence of a particular event. The principal goal is to find possible relationships between pairs of symbols and <a id="page_36"/>then to build trees of temporal dependencies in a hierarchical fashion. A complete search is conducted on the training sequence for symbols in an ever-growing dictionary. As the algorithm proceeds, pairs of strongly correlated events are joined into new events, and the search is resumed with the expanded dictionary.</p>
<p class="txi">Magnusson informally defined T-patterns as a recurring ordered set <span class="inline-math" id="b_9781450384827-003_ineq_0039"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>Q</m:mi><m:mo>=</m:mo><m:msub><m:mi>X</m:mi><m:mn>1</m:mn></m:msub><m:mo>&#x2248;</m:mo><m:mrow><m:mi>d</m:mi><m:msub><m:mi>t</m:mi><m:mn>1</m:mn></m:msub></m:mrow><m:mo>&#x2248;</m:mo><m:msub><m:mi>X</m:mi><m:mn>2</m:mn></m:msub><m:mo>&#x2248;</m:mo><m:mrow><m:mi>d</m:mi><m:msub><m:mi>t</m:mi><m:mn>2</m:mn></m:msub></m:mrow><m:mo>&#x2248;</m:mo><m:mrow><m:mi mathvariant="normal">&#x2026;</m:mi><m:msub><m:mi>X</m:mi><m:mi>i</m:mi></m:msub></m:mrow><m:mo>&#x2248;</m:mo><m:mrow><m:mi>d</m:mi><m:msub><m:mi>t</m:mi><m:mi>i</m:mi></m:msub><m:mi mathvariant="normal">&#x2026;</m:mi><m:msub><m:mi>X</m:mi><m:mi>m</m:mi></m:msub></m:mrow></m:mrow></m:math></span>, where <i>X</i> is either an event symbol or a simpler T-patterns, and <span class="inline-math" id="b_9781450384827-003_ineq_0040"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi/><m:mo>&#x2248;</m:mo><m:mrow><m:mi>d</m:mi><m:mi>t</m:mi></m:mrow></m:mrow></m:math></span> is a relatively invariant time distance separating the consecutive <i>X<sub>i</sub></i> and <i>X</i><sub><i>i</i> + 1</sub> events within occurrences of <i>Q</i>. Magnusson also introduced the notion of a critical interval <span class="inline-math" id="b_9781450384827-003_ineq_0041"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mrow><m:mo>(</m:mo><m:mrow><m:mi>C</m:mi><m:mi>I</m:mi></m:mrow><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mrow><m:mo>[</m:mo><m:mrow><m:mi>d</m:mi><m:mn>1</m:mn></m:mrow><m:mo>,</m:mo><m:mrow><m:mi>d</m:mi><m:mn>2</m:mn></m:mrow><m:mo>]</m:mo></m:mrow></m:mrow></m:math></span> for a pair of events (<i>A</i>, <i>B</i>). if <i>A</i> is an earlier and <i>B</i> is a later component of the same recurring T-pattern, then, after an occurrence of <i>A</i> at <i>t</i>, there is an interval <span class="inline-math" id="b_9781450384827-003_ineq_0042"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>[</m:mo><m:mrow><m:mi>t</m:mi><m:mo>+</m:mo><m:mrow><m:mi>d</m:mi><m:mn>1</m:mn></m:mrow></m:mrow><m:mo>,</m:mo><m:mrow><m:mi>t</m:mi><m:mo>+</m:mo><m:mrow><m:mi>d</m:mi><m:mn>2</m:mn></m:mrow></m:mrow><m:mo>]</m:mo></m:mrow></m:math></span> <span class="inline-math" id="b_9781450384827-003_ineq_0043"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>(</m:mo><m:mrow><m:mrow><m:mi>d</m:mi><m:mn>2</m:mn></m:mrow><m:mo>&#x2265;</m:mo><m:mrow><m:mi>d</m:mi><m:mn>1</m:mn></m:mrow><m:mo>&#x2265;</m:mo><m:mn>0</m:mn></m:mrow><m:mo>)</m:mo></m:mrow></m:math></span> that tends to contain at least one occurrence of <i>B</i> more often than would be expected by chance. Magnusson suggests using the standard p-value to judge how exceptional the observed frequency of the combination is. More precisely, suppose that within an observation period <span class="inline-math" id="b_9781450384827-003_ineq_0044"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>[</m:mo><m:mn>1</m:mn><m:mo>,</m:mo><m:msub><m:mi>N</m:mi><m:mi>T</m:mi></m:msub><m:mo>]</m:mo></m:mrow></m:math></span>, <i>a<sub>i</sub></i> and <i>b<sub>j</sub></i> are two event occurrences of event types <i>A</i> and <i>B</i> with <span class="inline-math" id="b_9781450384827-003_ineq_0045"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>i</m:mi><m:mo>=</m:mo><m:mrow><m:mn>1</m:mn><m:mi mathvariant="normal">&#x2026;</m:mi><m:msub><m:mi>N</m:mi><m:mi>A</m:mi></m:msub></m:mrow></m:mrow></m:math></span> and <span class="inline-math" id="b_9781450384827-003_ineq_0046"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>j</m:mi><m:mo>=</m:mo><m:mrow><m:mn>1</m:mn><m:mi mathvariant="normal">&#x2026;</m:mi><m:msub><m:mi>N</m:mi><m:mi>B</m:mi></m:msub></m:mrow></m:mrow></m:math></span> with <i>N<sub>A</sub></i> and <i>N<sub>B</sub></i> the number of occurrences of events <i>A</i> and <i>B</i> within <span class="inline-math" id="b_9781450384827-003_ineq_0047"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mrow><m:mo>[</m:mo><m:mn>1</m:mn><m:mo>,</m:mo><m:msub><m:mi>N</m:mi><m:mi>T</m:mi></m:msub><m:mo>]</m:mo></m:mrow><m:mo>.</m:mo></m:mrow></m:math></span> We assume as null-hypothesis that <i>A</i> and <i>B</i> are independent Poisson processes with probability of event <i>B</i> occurring within the basic time unit of <span class="inline-math" id="b_9781450384827-003_ineq_0048"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mrow><m:mi>P</m:mi><m:mrow><m:mo>(</m:mo><m:mi>B</m:mi><m:mo>)</m:mo></m:mrow></m:mrow><m:mo>=</m:mo><m:mrow><m:msub><m:mi>N</m:mi><m:mi>B</m:mi></m:msub><m:mo>/</m:mo><m:msub><m:mi>N</m:mi><m:mi>T</m:mi></m:msub></m:mrow></m:mrow></m:math></span>, and thus the probability of <i>B</i> not occurring <span class="inline-math" id="b_9781450384827-003_ineq_0049"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mrow><m:mrow><m:mi>P</m:mi><m:mspace/><m:mrow><m:mo>(</m:mo><m:mrow><m:mi/><m:mo>&#x223C;</m:mo><m:mi>B</m:mi></m:mrow><m:mo>)</m:mo></m:mrow></m:mrow><m:mo>=</m:mo><m:mrow><m:mn>1</m:mn><m:mo>&#x2212;</m:mo><m:mrow><m:mi>P</m:mi><m:mrow><m:mo>(</m:mo><m:mi>B</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:mrow></m:mrow><m:mo>.</m:mo></m:mrow></m:math></span> Now, given a CI [<i>d</i>1, <i>d</i>2] we find all the times <i>t<sub>i</sub></i> at which <i>A</i> occurs and then cumulatively collect all the <i>B</i> events that occur in the intervals <span class="inline-math" id="b_9781450384827-003_ineq_0050"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>[</m:mo><m:mrow><m:msub><m:mi>t</m:mi><m:mi>i</m:mi></m:msub><m:mo>+</m:mo><m:mrow><m:mi>d</m:mi><m:mn>1</m:mn></m:mrow></m:mrow><m:mo>,</m:mo><m:mrow><m:msub><m:mi>t</m:mi><m:mi>i</m:mi></m:msub><m:mo>+</m:mo><m:mrow><m:mi>d</m:mi><m:mn>2</m:mn></m:mrow></m:mrow><m:mo>]</m:mo></m:mrow></m:math></span>, thus arriving at a number <span class="inline-math" id="b_9781450384827-003_ineq_0051"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:msub><m:mi>N</m:mi><m:mrow><m:mi>A</m:mi><m:mi>B</m:mi></m:mrow></m:msub><m:mo>.</m:mo></m:mrow></m:math></span> Under the null-hypothesis, the expected number of <i>B</i> events in a time interval of length <i>d</i> equals <span class="inline-math" id="b_9781450384827-003_ineq_0052"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>P</m:mi><m:msup><m:mrow><m:mo>(</m:mo><m:mo>&#x223C;</m:mo><m:mi>B</m:mi><m:mo>)</m:mo></m:mrow><m:mi>d</m:mi></m:msup><m:mo>,</m:mo></m:mrow></m:math></span> and the probability of at least one occurrence of <i>B</i> within an interval of length <i>d</i> is <span class="inline-math" id="b_9781450384827-003_ineq_0053"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mn>1</m:mn><m:mo>&#x2212;</m:mo><m:mi>P</m:mi><m:msup><m:mrow><m:mo>(</m:mo><m:mo>&#x223C;</m:mo><m:mi>B</m:mi><m:mo>)</m:mo></m:mrow><m:mi>d</m:mi></m:msup><m:mo>.</m:mo></m:mrow></m:math></span> The above-mentioned p-value is then computed as the probability of observing at least <span class="inline-math" id="b_9781450384827-003_ineq_0054"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:msub><m:mi>N</m:mi><m:mrow><m:mi>A</m:mi><m:mi>B</m:mi></m:mrow></m:msub></m:math></span> occurrences of <i>B</i> in the CI, if we assume that <i>A</i> and <i>B</i> are independent. If it is calculated by using the binomial distribution with <i>N<sub>A</sub></i> as the number of trials and <span class="inline-math" id="b_9781450384827-003_ineq_0055"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mn>1</m:mn><m:mo>&#x2212;</m:mo><m:mi>P</m:mi><m:msup><m:mrow><m:mo>(</m:mo><m:mo>&#x223C;</m:mo><m:mi>B</m:mi><m:mo>)</m:mo></m:mrow><m:mi>d</m:mi></m:msup></m:mrow></m:math></span> as the probability of success (i.e., of one or more occurrences of <i>B</i> within each of the <i>N<sub>A</sub></i> intervals of length <i>d</i>). This probability will be one minus the sum of the probabilities of all possible lower values of <span class="inline-math" id="b_9781450384827-003_ineq_0056"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:msub><m:mi>N</m:mi><m:mrow><m:mi>A</m:mi><m:mi>B</m:mi></m:mrow></m:msub></m:math></span>, that is, <span class="inline-math" id="b_9781450384827-003_ineq_0057"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>p</m:mi><m:mo>=</m:mo><m:mn>1</m:mn><m:mo>&#x2212;</m:mo><m:mo largeop="true" symmetric="true">&#x2211;</m:mo><m:mrow><m:mo>(</m:mo><m:mi>b</m:mi><m:mi>i</m:mi><m:mi>n</m:mi><m:mi>o</m:mi><m:mi>m</m:mi><m:mi>i</m:mi><m:mi>a</m:mi><m:mi>l</m:mi><m:mrow><m:mo>(</m:mo><m:msub><m:mi>N</m:mi><m:mi>A</m:mi></m:msub><m:mo>,</m:mo><m:mi>i</m:mi><m:mo>,</m:mo><m:mn>1</m:mn><m:mo>&#x2212;</m:mo><m:mi>P</m:mi><m:msup><m:mrow><m:mo>(</m:mo><m:mo>&#x223C;</m:mo><m:mi>B</m:mi><m:mo>)</m:mo></m:mrow><m:mi>d</m:mi></m:msup><m:mo>)</m:mo></m:mrow><m:mo>)</m:mo></m:mrow><m:mo>.</m:mo></m:mrow></m:math></span> Consequently, <i>p</i> is compared with the specified significance level.</p>
<p class="txi">As a T-pattern detection scheme, Magnusson suggested that for every possible pair of symbols of the form (<i>A</i>, <i>B</i>), we should test every possible CI, from the largest to the smallest, until the <i>p</i>-value is sufficiently small, indicating significance (0.005 is a typical upper bound). Note that <i>p</i> will be high for high values of <i>d</i>, which means that short intervals will be favored. T-patterns were previously used to model complex interactions in behavioral studies and sporting events [<a href="bib.xhtml#b_9781450384827_ref_020">Borrie et al. 2002</a>], and the core algorithm is commercialized. A related software package, called C-quence <a id="page_37"/>[<a href="bib.xhtml#b_9781450384827_ref_042">Duncan and Collier 2002</a>], allows searching for user-defined event sequences with varying inter-event timings.</p>
<p class="txi">T-patterns are a sufficient representation for formulating sequential patterns with desired <i>within</i> statements. The general T-pattern term <span class="inline-math" id="b_9781450384827-003_ineq_0058"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:msub><m:mi>X</m:mi><m:mi>i</m:mi></m:msub><m:mo>&#x2248;</m:mo><m:mrow><m:mi>d</m:mi><m:mpadded><m:msub><m:mi>t</m:mi><m:mi>i</m:mi></m:msub></m:mpadded><m:msub><m:mi>X</m:mi><m:mrow><m:mi>i</m:mi><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:msub></m:mrow></m:mrow></m:math></span> reads: when pattern <i>X<sub>i</sub></i> ends it is followed <span class="inline-math" id="b_9781450384827-003_ineq_0059"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi/><m:mo>&#x2248;</m:mo><m:mrow><m:mi>d</m:mi><m:msub><m:mi>t</m:mi><m:mi>i</m:mi></m:msub></m:mrow></m:mrow></m:math></span> time units later by the beginning of pattern <span class="inline-math" id="b_9781450384827-003_ineq_0060"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:msub><m:mi>X</m:mi><m:mrow><m:mi>i</m:mi><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:msub><m:mo>.</m:mo></m:mrow></m:math></span> Hence, our sequential asthma risk pattern can be written as (<i>Exercise</i> &#x2248; 10<i>min</i> <i>Asthma</i> &#x2212; <i>Attack</i>). However, parallel patterns such as <i>asthma attack happened while pollution was high and temperature was low</i> cannot be formulated since T-pattern represents non-overlapping events.</p>
<h3 class="h3"><span class="h2space" id="b_9781450384827-003_s_005_s_002">3.5.2</span>Cyclic Patterns</h3>
<p class="tx">As explained throughout this chapter, pattern mining plays an important role in data mining, including association rule mining, sequential pattern and episode mining, and inter-transaction rules. Temporal regularity also plays a crucial role. For instance, via association rule mining we might find a frequent pattern of <i>milk and bread</i> bought together with a support level of 20% and a confidence level of 75%. However, by investigating temporal regularities we might further determine that the pattern of <i>milk and bread</i> bought together occurs each Sunday for multiple weeks in a row. This association rule is a kind of periodic or cyclic pattern that can be applied in specific predictions. Finding cyclic patterns in DNA and protein sequences, for example, is an important problem in biology [<a href="bib.xhtml#b_9781450384827_ref_036">Coward and Drablos 1998</a>]. Cyclic pattern mining on, say, web log data can find sets of webpages that are not only visited heavily but also regularly.</p>
<p class="txi">There are a number of methods for finding patterns in large datasets, most of which are designed to mine full cyclic patterns. In full cyclic every point in time contributes to the cyclic behavior of the time series (e.g., each second in a day contributes to the day/night cycle). <a href="bib.xhtml#b_9781450384827_ref_134">Ozden et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_134">1998</a>] defined the problem of discovering cyclic association rules that display regular variation over time. An association rule might not have user-specified minimum confidence or support over the entire time spectrum, but it could be above the minimum threshold within certain time intervals. The authors considered partial cyclic patterns with perfect periodicity in the sense that the pattern reoccurs in every cycle with 100% confidence. The perfect periodicity assumption leads to a key idea in designing efficient cyclic association rule mining algorithms: as soon as the algorithm finds that an association rule does not hold at a particular instant in time, it can infer that the association rule cannot have periods in this specific time instant. By studying the interaction between association rules and time, researchers applied three heuristics: cycle pruning, cycle skipping, and cycle elimination. In their approach, a database is fragmented into non-overlapping subsets with respect to time, and <a id="page_38"/>the association rules that appear in at least a certain number of subsets are discovered as cyclic association rules. Fragmenting the data and counting the number of subsets in which a pattern occurs greatly simplifies the design of the mining algorithm, but the drawback is that patterns (or association rules) that span multiple windows cannot be discovered. In real-life applications patterns rarely obey perfect periodicity, thereby it is crucial to define partial cyclic patterns in which the behavior of a time series is periodic at some intervals only. The problem of finding partial cyclic patterns has received a great deal of attention in time-series data analysis [<a href="bib.xhtml#b_9781450384827_ref_011">Aref et al. 2004</a>, <a href="bib.xhtml#b_9781450384827_ref_027">Cao et al. 2004</a>, <a href="bib.xhtml#b_9781450384827_ref_032">Chen et al. 2011</a>, <a href="bib.xhtml#b_9781450384827_ref_071">Han et al. 1998</a>].</p>
<p class="txi">Despite minor differences between these algorithms, the basic approach remains the same:</p>
<p class="numeric1"><span class="nspace">1.</span>Partition the time series into distinct segments of a fixed length so that each segment resembles a subsequence.</p>
<p class="numeric2"><span class="nspace">2.</span>Discover all partial cyclic patterns that satisfy a user-specified minimum support value. This value controls the minimum number of segments that a pattern must cover. </p>
<p class="txi">For example, given the time series <span class="inline-math" id="b_9781450384827-003_ineq_0061"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>T</m:mi><m:mo>=</m:mo><m:mrow><m:mi>A</m:mi><m:mi>B</m:mi><m:mi>D</m:mi><m:mi>B</m:mi><m:mi>C</m:mi><m:mi>D</m:mi><m:mi>D</m:mi><m:mi>C</m:mi><m:mi>A</m:mi></m:mrow></m:mrow></m:math></span> and the user-specified window = 3, <i>T</i> is divided into three segments: <span class="inline-math" id="b_9781450384827-003_ineq_0062"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:msub><m:mi>T</m:mi><m:mn>1</m:mn></m:msub><m:mo>=</m:mo><m:mrow><m:mi>A</m:mi><m:mi>B</m:mi><m:mi>D</m:mi></m:mrow></m:mrow></m:math></span>, <span class="inline-math" id="b_9781450384827-003_ineq_0063"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:msub><m:mi>T</m:mi><m:mn>2</m:mn></m:msub><m:mo>=</m:mo><m:mrow><m:mi>B</m:mi><m:mi>C</m:mi><m:mi>D</m:mi></m:mrow></m:mrow></m:math></span>, and <span class="inline-math" id="b_9781450384827-003_ineq_0064"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mrow><m:msub><m:mi>T</m:mi><m:mn>3</m:mn></m:msub><m:mo>=</m:mo><m:mrow><m:mi>D</m:mi><m:mi>C</m:mi><m:mi>A</m:mi></m:mrow></m:mrow><m:mo>.</m:mo></m:mrow></m:math></span> Let <i>a</i> * <i>b</i> be a pattern, where &#x201C;*&#x201D; represents any single event character. This pattern appears in the <i>T</i><sub>1</sub> and <i>T</i><sub>2</sub> segments. Therefore, its support count is 2. If user specifies minimum support value = 2, then <i>a</i> * <i>b</i> represents a partial cyclic pattern in <i>T</i>. The major drawback of this approach is that it does not take into account the actual temporal information (i.e., occurrence time and time lag between events) within a sequence. <a href="bib.xhtml#b_9781450384827_ref_168">Tanbeer et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_168">2009a</a>] described an algorithm to find frequent cyclic patterns in a transactional database using a tree-based data structure called a periodic-frequent pattern tree to capture database contents in a highly compact manner and use a pattern growth mining technique to generate the complete set of frequent cyclic patterns in a database for user-given periodicity and support thresholds. Without any need for data fragmentation, this approach discovers all frequent patterns that have exhibited full cyclic repetitions in a database. <a href="bib.xhtml#b_9781450384827_ref_009">Amphawan et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_009">2009</a>] investigated the problem of finding top-k frequent cyclic patterns in a transactional database using a best-first search strategy without a support threshold.</p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_114">Ma and Hellerstein</a> [<a href="bib.xhtml#b_9781450384827_ref_114">2001</a>] proposed a partial cyclic pattern mining algorithm by considering the temporal information of items within a series. This approach considers time series as a time-based sequence, specifies a period for each pattern using FFT, and discovers all patterns that satisfy a user-defined minimum support <a id="page_39"/>(called p-patterns). However, the algorithm is computationally expensive because the cost of FFT to find a pattern&#x2019;s period is <span class="inline-math" id="b_9781450384827-003_ineq_0065"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>O</m:mi><m:mrow><m:mo>(</m:mo><m:mrow><m:mi>n</m:mi><m:mi>l</m:mi><m:mi>o</m:mi><m:mi>g</m:mi><m:mi>n</m:mi></m:mrow><m:mo>)</m:mo></m:mrow></m:mrow></m:math></span>, where <i>n</i> represents the number of time units. As <i>n</i> can be a very large number in big data applications, the algorithm becomes infeasible in real life. <a href="bib.xhtml#b_9781450384827_ref_027">Cao et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_027">2004</a>] investigated the problem of partial cyclic pattern mining in a discrete data sequence and mentioned two essential sub-problems: efficient mining of frequent patterns and automatic discovery of periods that correspond to these patterns. They proposed a structure for the abbreviated list table (ALT) that maintains the occurrence counts of all distinct events in the sequence and facilitates the mining of periods and frequent patterns. They also presented an <span class="inline-math" id="b_9781450384827-003_ineq_0066"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>O</m:mi><m:mrow><m:mo>(</m:mo><m:mi>n</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math></span> algorithm to identify periods from ALT.</p>
<h3 class="h3"><span class="h2space" id="b_9781450384827-003_s_005_s_003">3.5.3</span>Sequential Patterns with Time Constraints</h3>
<p class="tx">The basic formulation of the pattern discovery and sequence mining tasks assumes that the only constraint that needs to be satisfied is the minimum support threshold. However, many applications require extra constraints on the structure of discovered patterns. Generalized sequential pattern (GSP) [<a href="bib.xhtml#b_9781450384827_ref_166">Srikant and Agrawal 1996</a>] is the first and most well-known algorithm that allows users to specify desirable time constraints when mining a given sequence for frequent subsequences. In GSP, time constraints can include minimum gap, maximum gap, and a sliding time window within the Apriori framework. GSP solves the problem by generating and testing candidate patterns in multiple database scans, and reviewing the database <i>k</i> times to discover patterns of size <i>k</i>. As the algorithm makes multiple passes over the data, the first one determines the support of each item, that is, the number of data sequences that include the item. At the end of this first pass the algorithm knows which items are frequent and each such item yields a 1-element frequent sequence consisting of that item. Each subsequent pass starts with a seed set, that is, the frequent sequences found in the previous pass. The seed set is used to generate new potentially frequent sequences called candidate sequences, each of which has one more item than the seed sequence, so all candidate sequences in a pass will have the same number of items. The support for these candidate sequences is found during the pass over the data, with the algorithm determining which candidate sequences are actually frequent; these frequent candidates then become the seed for the next pass. The algorithm terminates when there are no more frequent sequences at the end of a pass or when there are no more candidate sequences generated. Since the start and end times of an element must be considered and could comprise several transactions, GSP defines the <i>contiguous subsequence</i> for candidate generation and moves between the forward and backward phases to check whether a data sequence contains a certain candidate.</p>
<p class="txi"><a id="page_40"/>Algorithms from the SPIRIT family [<a href="bib.xhtml#b_9781450384827_ref_057">Garofalakis et al. 1999</a>] exploit pattern structure constraints to improve performance. These algorithms can be seen as extensions of GSP because they use advanced candidate generation and pruning techniques. In the SPIRIT framework, pattern constraints are specified as regular expressions, which is an especially convenient method if a user wants to significantly restrict the structure of patterns to be discovered. </p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_142">Pei et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_142">2002</a>] defined seven types of constraints: (1) an item constraint specifies the particular individual or groups of items that should or should not be present in the patterns; (2) a <i>length constraint</i> specifies the requirement on the length of patterns, where the length can be either the number of occurrences of items or the number of transactions (length constraints can also be specified as the number of distinct items or even the maximal number of items per transactions); (3) a <i>super-pattern</i> constraint is given a set of patterns and finds patterns that contain those particular set of patterns as sub-patterns; (4) an <i>aggregate constraint</i> on an aggregate of items in a pattern can use sum, avg, max, min, standard deviation, and so on, functions; (5) a <i>regular expression</i> constraint is a regular expression over the set of items using the established set of regular expression operators, such as disjunction and Kleene closure (a sequential pattern satisfies a constraint if and only if the pattern is accepted by its equivalent deterministic finite automata); (6) a <i>duration constraint</i> is defined only in sequence databases where each transaction in every sequence has a timestamp (it requires that the pattern appears frequently in the sequence database such that the timestamp difference between the first and last transactions in the pattern must be longer or shorter than a given period); and (7) a <i>gap constraint</i> is defined only in sequence databases where each transaction in every sequence has a timestamp. It requires that the pattern appears frequently in the sequence database such that the timestamp difference between every two adjacent transactions must be longer or shorter than a given gap.</p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_142">Pei et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_142">2002</a>] introduced a prefix-growth algorithm that takes PrefixSpan as the basic sequential pattern mining algorithm (introduced in Section <a href="#b_9781450384827-003_s_004_s_002">3.4.2</a>) and pushed prefix-monotone constraints deeply into the PrefixSpan mining process. Although prefix-growth algorithms handle time constraints, they prune patterns and choose only those that comply with the constraint. <a href="bib.xhtml#b_9781450384827_ref_119">Masseglia et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_119">2009</a>] raised an interesting question: Is it possible to consider time constraints directly in the mining process rather than a post-processing step? In an attempt to answer that question, they introduced an algorithm called graph for time constraints (GTC) in which time constraints are considered during the mining process and handled prior to and separately from the counting step of a data sequence. </p>
<p class="txi"><a id="page_41"/>Sequential patterns with time constraints can formulate <i>followed by</i> and <i>within</i> statements needed in asthma risk application (e.g., <i>asthma attack followed by exercise within 10 minutes</i>). However, the <i>while</i> statement cannot be formulated because commonly used sequential pattern mining algorithms are only applicable on non-overlapping sequence patterns. </p>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-003_s_006">3.6</span>Revisiting Asthma Risk Factor Patterns</h2>
<p class="tx">One important issue in pattern discovery is that of what constitutes an interesting pattern in data. The notions of sequential patterns or frequent episodes represent only one popular structure for patterns. Experience with different applications would give rise to other useful notions and the problem of defining other structures for interesting patterns. For instance, in the asthma risk scenario, we are interested in what is happening in the environment of the patient while the patient engages in her daily activities. So new pattern mining techniques are needed to explore other structural relation patterns, such as concurrent patterns and a mixture of concurrent and sequential patterns, to discover more complex local structures in data for emerging applications. </p>
<p class="txi">State-of-the-art pattern discovery algorithms contained sequences of nominal-value or symbolic stored in a relational or temporal database. The main implementation focus has been on time and space improvements. In the asthma risk factor example, in addition to temporal aspects we are interested in incorporating informational aspects of events in a pattern as well. This requires an expressive event language to precisely articulate relationships among events. Moreover, discovered models and extracted patterns have to be explicitly represented and stored to allow the progressive mining and reuse of extracted models with different data.<a id="page_42"/> </p>
</section>
</div>
</body>
</html>
