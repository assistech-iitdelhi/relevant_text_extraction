<?xml version="1.0" encoding="utf-8"?>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Chapter 07</title>
<link rel="stylesheet" href="../css/stylesheet.css" type="text/css"/>
</head>
<body epub:type="bodymatter">
<div epub:type="chapter" role="doc-chapter" aria-labelledby="ch7">
<h1 class="ct" id="ch7"><span class="cn" id="b_9781450384827-chapter-007">7</span><span class="ct1">Conclusion and Future Direction</span></h1>
<p class="tx"><a id="page_117"/>One notable form of progress in the natural sciences and engineering fields over the past century has been the development of more rigorous and detailed models of the phenomena being studied, based on a large volume of data. Studying a model often yields insight into the phenomena it models. When a model is replaced by another one that captures more details about how the phenomena work, science progresses. Models have been the principal motivation for scientific study. Models come in different forms not only based on their use but also based on the modeling technology available at that time. </p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_139">Pearl</a> [<a href="bib.xhtml#b_9781450384827_ref_139">2019</a>] outlined a three-level modeling hierarchy: (1) association, (2) intervention, and (3) counterfactual. Association modeling is purely based on statistical relationship and generates predictive models. Machine learning and data mining algorithms are very advanced in creating complex predictive models. Intervention and counterfactual are generating explanatory models that aim to understand cause-and-effect connections. The goal of explanatory models is to provide causal explanations. Given a causal theoretical model, causal inference methods are applied to data in order to test causal hypotheses. In explanatory modeling, researchers carefully design experiments to control the risk and confounding factors, define causal hypotheses, and justify the merits of the hypotheses. Often, proper causal inference methods such as structural causal models (SCMs), causal diagrams, propensity score, and graphical modeling need to be applied for testing causality. </p>
<p class="txi">In practice, exploratory models derived from causal inference impose major analysis limitations and are not applicable in many fields of science. Some limitations are the following: (1) Causal models are typically designed to test an association between only a single exposure and an outcome. This criterion significantly limits the number of variables in the study, and relationships between multiple <a id="page_118"/>risk factors and an outcome cannot be investigated simultaneously. (2) Collecting enough experimental data is subject to availability and resource constraints. (3) Overly clean experimental data might not be available and cost-effective in many fields. As a result, in practice, association-based statistical models are more commonly applied to observational data to create predictive models. </p>
<p class="txi">Data-driven machine learning techniques can find associations that a human is not able to observe. However, the black box nature of machine learning is an obstacle in utilizing the predictions in sensitive domains. In an attempt to make machine learning more suitable for decision-making, and provide explanations for predictive models, there has recently been a surge of research in eXplainable artificial intelligence (XAI). XAI is the ability to provide an explanation on why a machine decision has been reached. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems are based on finding a feature importance score for each prediction value, rather than focusing on bringing in the experience, expertise, knowledge, and intuition of an expert in the modeling process.</p>
<p class="txi">This book was about advancing the emerging techniques in AI and machine learning to detect events in data streams and then build models by discovering relationships among events to recognize specific points of interest. We called this explanatory process event mining. It has two distinct phases: hypothesis formation and hypothesis testing. Hypothesis formation requires the analysis of a good deal of event data to find strong candidate hypotheses. Based on expert knowledge and judgment, one or more hypotheses are then formed for further testing via relevant data analysis. Thus, event mining relies on contextual data analysis that is guided by the knowledge of the domain expert. The result of an event mining process is an <i>explainable</i> model that <i>explains</i> causal relationships in the form of structural and temporal local patterns in the data. Event mining allows the experts to decide when and how to question the explanations and steer their discovery by themselves.</p>
<p class="txi">We also introduced EventMiner, an event mining framework with two distinct phases: hypothesis formation and hypothesis testing. Hypothesis formation requires the analysis of a good deal of event data to find strong candidate hypotheses. Based on an expert&#x2019;s knowledge and judgment, one or more hypotheses are then selected for further testing. We emphasized the framework&#x2019;s capabilities in fusing and analyzing temporal data to capture dynamic characteristics of complex systems. The framework is built on a high-level descriptive pattern formulation and pattern mining language. The language is composed of a well-defined set of operators that facilitate pattern analysis. Data-driven operators bring hidden interesting patterns to the surface and hypothesis-driven operators facilitate knowledge <a id="page_119"/>formulation by a domain expert to guide the modeling process. Finally, we discussed two use cases of EventMiner in chronic disease management and behavioral analysis applications. </p>
<p class="txi">In the future, there will be some obvious trends in data and computing. Among the huge amount of data we collect and analyze on a daily basis, geospatial data has a unique place. As a result of the emerging Internet of Things, smartphones, and GPS satellites, we are able to collect immense amount of real-world data streams in very precise ways that were not possible until recently. This geospatial data can be utilized for predicting situations [<a href="bib.xhtml#b_9781450384827_ref_161">Singh and Jain 2016</a>] to make the right decisions promptly and for maximizing quality of life while minimizing damage. The right actions at the right time and in the right situations require predictive models. These predictive models have been the subject of much of the scientific research in almost all fields ranging from biology to weather prediction. </p>
<p class="txi">Food is central to everyday life. For humans, food provides energy and the foundational building blocks for our bodies and is a major source of joy in personal and social settings. People love to eat what they and their friends and families enjoy. Recently, however, due to the rapid increase in chronic diseases and our relationship with food, people are becoming sensitive to the effects food has on our health. This leads to a battle between what a person enjoys eating and what their body wants to ingest. This is an important problem because it is central to our lifestyle and health.</p>
<p class="txi">A significant part of the global economy is related to food. Starting with the production of food items through its agriculture to its distribution, processing, preparation for consumption, and delivery, much of our economy depends on food-related activities. Central to this is understanding what each individual likes to eat and what they should eat to care for their health. This information needs to be aggregated to understand the food ecosystem and its development and management.</p>
<p class="txi">Each person is unique due to genetic, environmental, and lifestyle factors. It is commonly believed that genetics only controls less than 20% of a person&#x2019;s health and development, and the rest is due to the environment and their lifestyle. To estimate a person&#x2019;s health, to help them maintain a desirable healthly state, and even to help them in a disease state, it is important to understand their unique body. This could be done by logging data about them to analyze different relevant events. Here, we present an example application of event mining in the context of sleep quality. Similar efforts will be required to complete models related to other health problems as well as to what a person likes to eat.</p>
<p class="txi">Many scientific studies have been performed to understand the effects of different food items, exercise, and stress on the duration, quality, and latency of sleep. <a id="page_120"/>These scientific studies control a particular variable and perform randomized studies on a population to understand the effect of that variable on different aspects of sleep. Such results are available in scientific publications. These findings are represented in observations such as:</p>
<p class="bullet1"><span class="bspace">&#x25CF;</span>O1: Highly spicy food reduces slow-wave and stage 2 sleep and increases total time awake.</p>
<p class="bullet"><span class="bspace">&#x25CF;</span>O2: High sugar and carbohydrates increase arousal at night.</p>
<p class="bullet2"><span class="bspace">&#x25CF;</span>O3: Adequate physical activity decreases total sleep time and increases sleep efficiency.</p>
<p class="txi">All such observations are derived from population statistics and may be collected in a knowledge base of condition&#x2013;action rules. Clearly, these observations are valid for a percentage of population as reported in research reports but are not necessarily applicable to a specific individual. Since each individual is unique, they have similar specific personalized rules as noted above that need to be discovered. Person-specific rules may be discovered using a similar scientific process as is used in the case of a particular population. All relevant data streams for a person should be collected for as long a period as possible. These data streams may be converted to event streams. Such a stream is shown in Figure <a href="#b_9781450384827-007_fig_001">7.1</a> for four different data streams that we will use to illustrate our thinking. Many more such streams may be collected for a person. The goal of collecting such streams is to build a personal model using this data. Event mining is discussed in this book as a concrete example of how to build such a model.</p>
<p class="txi">Based on the high-level knowledge about the conditions responsible for poor sleep, potential hypotheses are formed to be tested using data. These hypotheses are expressed using event algebra and are evaluated using the relevant data in event streams. The expert selects a candidate hypothesis <i>H<sub>c</sub></i> that has a high likelihood to be valid and refines it based on the contextual information and the population-based knowledge. The refinement process may use existing knowledge, related context, and even other event streams. The refinement process is akin to specifying confounding variables that need to be controlled; however, this is done using already collected data but using as precise conditions as possible. The refined hypothesis <i>H<sub>r</sub></i> is now expressed using event algebra and is evaluated for verification. The verification process may require causality analysis or strong correlation-based observation that in turn may require selecting specific event sequences from the event stream of the person. Based on such filtered event data selected using the new hypothesis expressed precisely using event algebra, the hypothesis may be verified. This hypothesis <i>H<sub>vn</sub></i> is then made part of the personal model and may <a id="page_121"/>be further used for recommending correct lifestyle recommendations to the person in right context. In this case, we are considering food as the main lifestyle factor; thus, we are interested in the enjoyment aspect of food for a person as well as the health aspect of food for the person. For the enjoyment aspect, we will need to know how much a person enjoyed the food and under what conditions. For the health aspect, we will need to know how different biomarkers reflected on the effect of food under different contexts and the time relationship of the ingestion event and the health effect event as indicated by the biomarker.<a id="page_122"/></p>
<figure class="split" id="b_9781450384827-007_fig_001">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-007_fig_001.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 7.1</span>Four different data streams that are converted to different types of events related to food, stress, activity, and sleep. These event streams are used both for hypotheses formation as well as hypothesis verification by an expert who has access to domain knowledge sources. After a hypothesis is verified, it is added to the personal model, which plays an important role in all lifestyle recommendations.</p></figcaption>
</figure>
</div>
</body>
</html>
