<?xml version="1.0" encoding="utf-8"?>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Chapter 05</title>
<link rel="stylesheet" href="../css/stylesheet.css" type="text/css"/>
</head>
<body epub:type="bodymatter">
<div epub:type="chapter" role="doc-chapter" aria-labelledby="ch5">
<h1 class="ct" id="ch5"><span class="cn" id="b_9781450384827-chapter-005">5</span><span class="ct1">Event Mining Applications</span></h1>
<p class="tx"><a id="page_55"/>Event mining algorithms and tools can be utilized in a wide range of applications in engineering (e.g., intrusion detection and network security and flow classification), business (e.g., fraud detection, decision support systems, and forecasting market trends), environmental science (e.g., flood prediction and urban environmental analysis), and medicine and population health (e.g., study of drug implications and disease outbreak). Event mining can be utilized to extract knowledge from a complex system where events can denote any of the system&#x2019;s activities. Anything can be defined as a system, from a single component in a jet engine manufacturing line to ubiquitous computing. As the system operates, events can be extracted and subsequently mined through a variety of methods. Unwanted events can be filtered, anomaly events can be detected, complex event processing (CEP) can be applied to aggregate and monitor the occurrence of correspondence between multiple events, significant patterns of multiple events can be extracted, and so on. </p>
<p class="txi">To understand complex systems and deal with complex patterns, logging systems&#x2019; events with only an ID, name, and timestamp is not enough. As mentioned in Chapter <a href="b_9781450384827-chapter-002.xhtml">2</a>, events need to be stored as a complex object with all their properties and relationships rather than a relational tuple. Event mining algorithms not only need to support symbolic representation of events but also complex relationships such as causality, where one event is the root cause of another.</p>
<p class="txi">In this chapter, we discuss how event mining can be applied in different domains and overview general requirements and high-level workflow in each domain.</p>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-005_s_001">5.1</span>Healthcare and Medicine</h2>
<p class="tx">Through the application of knowledge extraction tools and techniques there is a wealth of data available within the healthcare industry that could provide valuable insights. Many providers use electronic medical records (EMRs) to store large quantities of patients&#x2019; data related to test results, medications, prior diagnoses, and other medical history. Also, with the advent of wearable sensors and the <a id="page_56"/>widespread use of smartphone apps, a copious amount of longitudinal data can be collected from groups of patients, thereby permitting examination and analysis of patterns and trends over time. The variety of healthcare data including EMRs, genetic data, biomedical data, and data for public health impose challenges not only on event mining techniques but also on understanding the relationships between these data points that are spread thinly across several dimensions. </p>
<p class="txi">Analyzing relations between temporal events in EMR data has many applications, from risk factor analysis (e.g., risk of readmission or risk of hospital infections) to understanding complex relationships between different diseases and the causes for their comorbidities. The primary information embedded in an EMR is referred to as the International Classification of Diseases and Related Health Problems (ICD) and Current Procedural Terminology (CPT) codes. ICD-9 and ICD-10 (versions 9 and 10, respectively) are popular coding systems used by healthcare organizations. They are primarily intended to code symptoms, diseases, procedures, and other conditions. CPT is used to categorize the medical procedures performed on a patient. The purpose of these coding systems is to provide a uniform language that accurately describes medical, surgical, diagnostic, and treatment services provided by physicians and other healthcare professionals. Despite the abundance of temporal data and events that are encapsulated in EMRs, research on temporal data mining and event mining in medical records is scarce. </p>
<p class="txi">Regardless of the nature of the medical data, many of the typical problems in this domain can be classified into two main categories:</p>
<p class="bullet1"><span class="bspace">&#x25CF;</span>Exploratory techniques</p>
<p class="bullet2"><span class="bspace">&#x25CF;</span>Predictive techniques</p>
<p class="txi">Next we will explain each category in more detail.</p>
<h3 class="h3"><span class="h3space" id="b_9781450384827-005_s_001_s_001">5.1.1</span>Exploratory Techniques</h3>
<p class="tx">In this approach, the focus is on finding pre-existing medical patterns in the data with no apriori knowledge of what those patterns might be. Visual analytics techniques are especially valuable during the discovery process due to the richness of medical data and the depth of domain knowledge that might be needed.</p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_022">Boytcheva et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_022">2019</a>] designed a system with the aim of examining the comorbidity of diseases and the effect of different treatments on them. For example, how the treatment of one disease can affect other co-existing conditions. In order to solve this problem, the authors defined three main tasks: (1) Frequent pattern mining and association rules for chronic diseases and related treatments. (2) Frequent sequence mining to analyze complex sequences of events with different time <a id="page_57"/>spans between them. (3) Periodic event mining for finding periodical sequences of certain diseases. </p>
<figure class="split" id="b_9781450384827-005_fig_001">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-005_fig_001.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 5.1</span>High-level architecture of a pattern mining system for understanding comorbidities of multiple chronic diseases. Reproduced from <a href="bib.xhtml#b_9781450384827_ref_022">Boytcheva et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_022">2019</a>].</p></figcaption>
</figure>
<p class="txi">Although the authors did not provide any specific algorithm for each task, they applied their system to find associations among schizophrenia, diabetes mellitus type 2, and hyperprolactinemia. Figure <a href="#b_9781450384827-005_fig_001">5.1</a> shows their proposed high-level architecture.</p>
<p class="txi">Many researchers have developed algorithms for exploring temporal events in the healthcare domain with a specific focus on visualization. Early works focused mostly on visualizing an individual&#x2019;s medical records, for example, LifeLines [<a href="bib.xhtml#b_9781450384827_ref_145">Plaisant et al. 1998</a>, <a href="bib.xhtml#b_9781450384827_ref_005">Aigner and Miksch 2006</a>], or an individual&#x2019;s care plan [<a href="bib.xhtml#b_9781450384827_ref_063">Gotz and Wongsuphasawat 2012</a>]. Such tools typically organize data hierarchically to summarize the complex set of values associated with an individual patient. More recently, attention has shifted to visualizations of information related to a group of patients. This includes a range of tools for visualizing, querying, and sorting through groups of patients&#x2019; event data [<a href="bib.xhtml#b_9781450384827_ref_182">Wongsuphasawat and Gotz 2012</a>, <a href="bib.xhtml#b_9781450384827_ref_180">Wongsuphasawat and Shneiderman 2009</a>].</p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_181">Wongsuphasawat and Gotz</a> [<a href="bib.xhtml#b_9781450384827_ref_181">2011</a>] introduced <i>Outflow</i>, a visualization framework summarizing temporal event data extracted from multiple patient medical <a id="page_58"/>records to show aggregate disease evolution statistics for a group of patients. Outflow aggregates event sequences into an <i>Outflow graph</i> which is analogous to a state diagram or state transition graph. The states are the unique combinations of symptoms that were observed in the data and the edges capture symptom transitions. User interaction capabilities such as panning and zooming, symptom selection, filtering, and brushing are incorporated into the framework. Although the visualization aspects of the framework are interesting, it lacks pattern mining techniques and user interaction is limited to relatively basic capabilities. Futhermore, when the number of event types exceeds a certain threshold, it can lead to an extremely complex web of event pathways.</p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_047">Fails et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_047">2006</a>] created an interface, called <i>PatternFinder</i>, for visual query and result visualization for searching temporal patterns in multivariate and categorical clinical datasets. A dashboard user interface can be used to select an arbitrary number of events and timelags between events to create a pattern. The pattern is then translated into an SQL query and the results are shown as a timeline for each patient. Events are considered as instantaneous points where day level is the lowest time granularity. Each event has a type and values. These values can be numeric, such as for a systolic blood pressure reading, or categorical, such as for normal/abnormal blood sugar. Although this interface provides a comprehensive dashboard in terms of pattern formulation and query, it is restricted to sequential patterns and it lacks pattern discovery techniques to help analysts bring hidden patterns to the surface. Also, the framework lacks data fusion and data transformation modules.</p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_064">Gotz et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_064">2014</a>] presented a methodology for interactive mining and visual analysis of clinical event patterns using electronic health record data. They applied a conventional pattern mining technique, Sequential PAttern Miner (SPAM) [<a href="bib.xhtml#b_9781450384827_ref_012">Ayres et al. 2002</a>], for pattern discovery and display an ad-hoc visualization of discovered patterns. The framework addressed three issues: visual query capabilities to interactively specify pattern definitions, pattern mining techniques to discover important patterns, and interactive visualization techniques which help uncover events that most impact the outcome. One major problem with the framework is that events are point events and there is no notion of temporal constraint between events.</p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_135">Pastrello et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_135">2014</a>] noted the importance and the challenges involved in integrating heterogeneous data from multiple experiments. They focused on network analysis as a key technique to integrate, visualize, and extrapolate relevant information from diverse data. The authors considered networks as nodes and edges, where individual entities (e.g., genes, proteins, drugs) are represented by nodes and relationships between components (e.g., drug-targeting) are represented by edges. They mentioned that network visualization and analysis can be utilized to <a id="page_59"/>integrate diverse data, which results in an insightful representation of the system being studied. They integrated different datasets from the literature in one network to obtain insight into the gastric cancer problem. </p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_091">K&#x00F6;lling et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_091">2012</a>] introduced a web-based tool for visual data mining colocation patterns in multivariate bioimages. The system is called Web-based Hyperbolic Image Data Explorer (WHIDE). The authors emphasized that bioimaging techniques rapidly develop toward higher resolution and higher dimension. The analysis of such multivariate bio-images requires new techniques to support end users in the analysis of both aspects of such images: space and molecular colocation or interaction. Their approach combines principles from machine learning, dimension reduction, and visualization. </p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_021">Bowman et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_021">2012</a>] introduced a neuroimaging visualization framework, called <i>INVIZIAN</i>, for the graphical rendering and the dynamic interaction with the contents of large-scale neuroimaging data sets. Their system graphically displays brain surfaces as points in a coordinate space. The user can interact with the elements in this space, search over meta-data features, select one or more brain surfaces, and create groups to generate new hypotheses that are worth new experimentation. This process enables the user to rapidly explore large collections of neuroimaging data for identifying interesting trends across features and attributes.</p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_136">Patnaik et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_136">2011</a>, <a href="bib.xhtml#b_9781450384827_ref_137">2012</a>] introduced a system called <i>EMRView</i>, which is one of the first attempts for mining patients&#x2019; history at the big data scale by processing millions of patient records. EMRView mines the precedence relationships between events to identify and visualize partially ordered information encoded in key classes of patients. The high-level workflow of EMRView is shown in Figure <a href="#b_9781450384827-005_fig_002">5.2</a>, and it involves discovering frequent episodes using itemset mining techniques, tracking the order of the diagnostic codes in the discovered episodes, and learning partial orders using PQtree algorithm. </p>
<h3 class="h3"><span class="h3space" id="b_9781450384827-005_s_001_s_002">5.1.2</span>Predictive Techniques</h3>
<p class="tx">The predictive techniques of classification and regression can help forecast some types of outcome. In healthcare, just like other industries, predictors are only useful when the insights they provide can be translated into action. This requires a willingness from the healthcare provider&#x2019;s side to intervene at the right time given the prediction results. In this section, we explain some use cases of predictions in the healthcare domain. </p>
<p class="txi">Hospital readmissions is an area where hospitals have great incentive to use predictive analytics to improve patient outcome and avoid penalties. Generally speaking, a hospital readmission occurs when a patient is admitted to a hospital <a id="page_60"/>within a specified time period after being discharged from an earlier (initial) hospitalization. Medicare under the Hospital Readmission Reduction Program (HRRP) financially penalizes hospitals with relatively high rates of Medicare readmissions [<a href="bib.xhtml#b_9781450384827_ref_019">Boccuti and Casillas 2017</a>]. Under the HRRP, hospitals with readmission rates that exceed the national average are penalized through a reduction in payments across all of their Medicare admissions, not just those which resulted in readmissions. Highlighting the rapidly growing interest in this problem, some recent papers focused on predicting readmission from EMR data. In 2015, the number of papers on hospital readmissions prediction approximately doubled compared to the previous year (Figure <a href="#b_9781450384827-005_fig_003">5.3</a>). The motivation behind the research efforts in using predictive analytics in healthcare is improving outcomes while reducing costs and avoiding reimbursement penalties for hospitals. </p>
<figure class="split" id="b_9781450384827-005_fig_002">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-005_fig_002.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 5.2</span>Workflow of EMRView system. The three tasks of (i) mining parallel episodes, (ii) tracking serial extensions, and (iii) learning partial orders are performed to uncover partially ordered information in EMR data. Reproduced from <a href="bib.xhtml#b_9781450384827_ref_136">Patnaik et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_136">2011</a>].</p></figcaption>
</figure>
<figure class="split" id="b_9781450384827-005_fig_003">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-005_fig_003.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 5.3</span>Opportunities for improving patient care while reducing costs and avoiding penalties have resulted in rapidly growing interest in predicting hospital readmissions.</p></figcaption>
</figure>
<p class="txi"><a id="page_61"/><a href="bib.xhtml#b_9781450384827_ref_017">Ben-Chetrit et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_017">2012</a>] performed a retrospective observational study on physician discharge notes and used univariate and multivariate logistic regression analyses to find factors that were significantly associated with readmission <span class="inline-math" id="b_9781450384827-005_ineq_0001"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>(</m:mo><m:mrow><m:mi>P</m:mi><m:mo>&lt;</m:mo><m:mn>0.05</m:mn></m:mrow><m:mo>)</m:mo></m:mrow></m:math></span>.</p>
<p class="txi">Using machine learning techniques, <a href="bib.xhtml#b_9781450384827_ref_157">Shameer et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_157">2017</a>] designed a multi-step modeling strategy, using the naive Bayes algorithm, for readmissions prediction in heart failure patients. First, they created individual models for Boolean classification of readmitted versus non-readmitted. Then, features contributing to predictive risk from the model in the previous stage were combined into a composite model using a correlation-based feature selection (CFS) method. With this approach, authors could reach an area under the curve (AUC) of 78% and accuracy of 83.19%. Some other studies utilized artificial neural networks [<a href="bib.xhtml#b_9781450384827_ref_081">Jamei et al. 2017</a>] or decision trees [<a href="bib.xhtml#b_9781450384827_ref_158">Shams et al. 2015</a>] for this problem and concluded that factors such as longer length of stay, disease severity index, being discharged from a hospital, and primary language other than English were associated with increased risks of being readmitted within 30 days.</p>
<p class="txi">Predictive techniques have been used in other domains such as mortality prediction [<a href="bib.xhtml#b_9781450384827_ref_152">Rose 2013</a>, <a href="bib.xhtml#b_9781450384827_ref_184">Wu et al. 2010</a>], intensive care unit admission [<a href="bib.xhtml#b_9781450384827_ref_041">Desautels et al. 2017</a>, <a href="bib.xhtml#b_9781450384827_ref_177">Wengerter et al. 2018</a>], predicting if a particular treatment is beneficial for a specific patient, and disease prognoses prediction [<a href="bib.xhtml#b_9781450384827_ref_037">Cruz and Wishart 2006</a>].</p>
<p class="txi">Aside from research in academia, the common challenge in healthcare applications is: How can hospitals successfully apply promising ideas from academic research to a fully developed system in a hospital information technology (IT) environment, given the inherent problems of predictive models (e.g., the need for retraining) and data privacy issues? </p>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-005_s_002">5.2</span>Biological Data Analysis</h2>
<p class="tx">In recent years, rapid developments in genomics and proteomics have generated a large amount of biological data. Genomics can be broadly defined as the systematic study of genes, their functions, and their interactions. Proteomics is the study of proteins, protein complexes, and their interactions. With the advent of high-throughput technologies in biology and biotechnology, we are witnessing a paradigm shift from traditional hypothesis-driven to data-driven analysis for understanding large amounts of biological data. One major challenge, however, is managing the variety and complexity of data types, the hierarchy, and the need to acquire data that spans many modalities. Each data type requires a unique analytical approach for understanding and inferring structure or patterns from them. </p>
<p class="txi"><a id="page_62"/>Sequence data that is associated with the DNA of various species have grown enormously with the development of automated sequencing technology. DNA is a long, chainlike molecule which has two strands twisted into a double helix. The two strands are made up of simpler molecules called <i>nucleotides</i>. Each nucleotide is composed of one of the four possible letters: A, C, G, and T. DNA sequencing is the process of determining the nucleotide sequence of a given DNA fragment. DNA sequencing is being used to refine the human genome sequence and to provide information regarding normal and disease-associated variations in the human genome. Two other central molecular building blocks are RNA and proteins. RNA molecules are very similar to DNA and are also composed of four nucleotides (A, C, G, and U). Proteins are chains of 20 different basic units called amino acids. In computational biology, data presents itself in the form of DNA, RNA, and protein sequences. This gives rise to an opportunity to employ unsupervised event mining techniques to discover recurrent patterns in these sequences. </p>
<p class="txi">Generally in event mining, given two sequences of equal length we can define a measure of similarity by considering distances between corresponding elements of the two sequences. When the sequences consist of symbolic data, for example, in DNA sequences, we need to define the similarity between every pair of symbols. When sequences are of different lengths, it is not possible to accumulate distances between corresponding elements of the sequences. This motivates an important subproblem of sequence matching, which is <i>sequence alignment</i>. Dynamic time warping and its variations are a well-known technique for finding an optimal alignment between two temporal sequences, and it has been used for sequence classification and matching [<a href="bib.xhtml#b_9781450384827_ref_127">M&#x00FC;ller 2007</a>, <a href="bib.xhtml#b_9781450384827_ref_163">Skutkova et al. 2013</a>, <a href="bib.xhtml#b_9781450384827_ref_079">Hou et al. 2017</a>]. The detailed analysis of DNA or protein sequences is beyond the scope of this chapter. Interested readers are encouraged to refer to <a href="bib.xhtml#b_9781450384827_ref_046">Ewens and Grant</a> [<a href="bib.xhtml#b_9781450384827_ref_046">2006</a>] and <a href="bib.xhtml#b_9781450384827_ref_035">Cohen</a> [<a href="bib.xhtml#b_9781450384827_ref_035">2004</a>] for more details.</p>
<p class="txi">In addition to sequence matching and sequence alignment in biological sequences, motif discovery has interesting biological applications. A sequence motif is a nucleotide or amino acid sequence pattern that is widespread and has been proven or assumed to have biological significance. In general, motifs could represent patterns in any kind of biological sequences such as DNA sequences, RNA sequences, and protein sequences. In DNA, a motif may correspond to a protein binding site. In proteins, a motif may correspond to the active site of an enzyme or a structural unit necessary for proper folding of the protein. Motifs can be represented as regular expressions, hidden Markov models, or stochastic context-free grammars (for RNA or DNA sequences). For example, a motif in a protein can be: <i>N</i>, followed by anything except <i>P</i>, followed by either <i>S</i> or <i>T</i>, followed by anything <a id="page_63"/>except <i>P</i>. This motif can be written as <span class="inline-math" id="b_9781450384827-005_ineq_0002"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mi>N</m:mi><m:mi>P</m:mi><m:mrow><m:mo>[</m:mo><m:mrow><m:mi>S</m:mi><m:mi>T</m:mi></m:mrow><m:mo>]</m:mo></m:mrow><m:mi>P</m:mi></m:mrow></m:math></span>, where <i>X</i> means any amino acid except <i>X</i>, and <span class="inline-math" id="b_9781450384827-005_ineq_0003"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mo>[</m:mo><m:mrow><m:mi>X</m:mi><m:mi>Y</m:mi><m:mi>Z</m:mi></m:mrow><m:mo>]</m:mo></m:mrow></m:math></span> means either <i>X</i> or <i>Y</i> or <i>Z</i>.</p>
<p class="txi">In bioinformatics, motif discovery is becoming very important because they represent conserved sequences that can be biologically meaningful. Many algorithms have been developed for discovering motifs and searching databases to find matches to a given motif. Some of these algorithms include anchored gapless local alignment of multiple sequences (A-GLAM) [<a href="bib.xhtml#b_9781450384827_ref_088">Kim et al. 2006</a>], BioProspector [<a href="bib.xhtml#b_9781450384827_ref_108">Liu et al. 2000</a>], MDscan [<a href="bib.xhtml#b_9781450384827_ref_109">Liu et al. 2002</a>], and MEME [<a href="bib.xhtml#b_9781450384827_ref_015">Bailey and Elkan 1994</a>]. Most motif discovery algorithms are limited to gapless motifs. The main reason is that by allowing gaps the possible number of potential variations increase exponentially. Regular expressions have been used to find motifs with gaps. However, they have trouble capturing subtle motifs because they specify exactly what residues and spacers are allowed at each position, and do not allow a better match in one part to compensate for a worse match in another part. Since they make very detailed specifications, it may also be hard to discover accurate regular expressions from small numbers of examples. Gapped local alignment of motifs (GLAM2) is another algorithm proposed for discovering motifs that allows insertion or deletion of bases in a genome. A companion method called GLAM2SCAN searches sequence databases using such motifs [<a href="bib.xhtml#b_9781450384827_ref_052">Frith et al. 2008</a>]. A motif in GLAM2 has a certain number, <i>W</i>, of key positions. The idea is that the key positions hold amino acids or nucleotides that are important for the motif&#x2019;s function. An instance of the motif is a string of residues (amino acids or nucleotides), where each residue either occupies one of the key positions or is inserted between key positions. GLAM2 searches for an alignment of substrings of the input sequences to a series of key positions. The number of key positions is optimized by the algorithm. In order to define the strongest motif, GLAM2 uses a formula to assign a numeric score to any given alignment. Then, the task is to find the alignment with the maximum score. GLAM2 utilizes a simple statistical model of a motif, with position-specific residue probabilities, position-specific deletion probabilities, and position-specific insertion probabilities. If each key position has characteristic probabilities, <i>&#x03B8;<sub>i</sub></i>, containing the <span class="inline-math" id="b_9781450384827-005_ineq_0004"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:msup><m:mi>i</m:mi><m:mrow><m:mi>t</m:mi><m:mi>h</m:mi></m:mrow></m:msup></m:math></span> residue type, &#x03D5; is the probability of each key position being deleted and <span class="inline-math" id="b_9781450384827-005_ineq_0005"><m:math alttext="alt_math" display="inline" xmlns:m="http://www.w3.org/1998/Math/MathML"><m:mrow><m:msup><m:mi mathvariant="normal">&#x03C8;</m:mi><m:mi>x</m:mi></m:msup><m:mrow><m:mo>(</m:mo><m:mrow><m:mn>1</m:mn><m:mo>&#x2212;</m:mo><m:mi mathvariant="normal">&#x03C8;</m:mi></m:mrow><m:mo>)</m:mo></m:mrow></m:mrow></m:math></span> is the probability that <i>x</i> residues are inserted between two key positions, This model can be regarded as a hidden Markov model, where <i>&#x03B8;<sub>i</sub></i> are emission probabilities and &#x03D5; and &#x03C8; are transition probabilities. Figure <a href="#b_9781450384827-005_fig_004">5.4</a> shows motif alignment for multiple segments of sequences, where key positions are denoted by the * symbol.</p>
<p class="txi">To analyze the functionality of the algorithm, the authors applied GLAM2 to discover a model of a bipartite DNA-binding motif associated with the <i>LMO2</i> gene. Figure <a href="#b_9781450384827-005_fig_005">5.5</a> shows two motifs, a <i>CAGGTG</i> and a <i>GATA</i>, separated by a spacer of length 8 to 10 basepairs determined using the CAST-ing technique [<a href="bib.xhtml#b_9781450384827_ref_176">Wadman et al. 1997</a>]. <a id="page_64"/>CASTing is a procedure for identification of consensus sequences of DNA to which a protein, for example, a transcription factor, may bind. In Figure <a href="#b_9781450384827-005_fig_006">5.6</a>, when 31 random protein complexes are given as input, GLAM2 discovers the <i>CAGGTG</i> motif and extends the <i>GATA</i> motif on the right by three columns. </p>
<figure class="split" id="b_9781450384827-005_fig_004">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-005_fig_004.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 5.4</span>A typical motif alignment from GLAM2. The stars indicate the key positions. The residues inserted between key positions are not considered aligned to each other; their column placement is arbitrary. The numbers on either side of the aligned segments indicate the coordinates of each segment within the sequence. The decimal numbers on the right are the marginal scores of each aligned segment. Reproduced from <a href="bib.xhtml#b_9781450384827_ref_052">Frith et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_052">2008</a>].</p></figcaption>
</figure>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-005_s_003">5.3</span>Predictive Maintenance</h2>
<p class="tx">Manufacturing and industrial operations can obtain great benefits by analyzing event-driven information, sensor data streams, and machine/maintenance logs to provide proactive recommendations. Manufacturing failures cause significant problems in the reliability of industrial processes and associated costs with downtime of equipment. Thus, there is an increasing demand for reducing unexpected failures, eliminating unscheduled downtimes, and minimizing maintenance-related costs. Proactive decisions regarding the replacement of a part in predictive maintenance requires a balance between the cost due to premature replacement of the part and the cost of unexpected failure. Hence, remaining useful life (RUL) estimation using real-time data (e.g., through sensors and event logs) is an emerging area of research [<a href="bib.xhtml#b_9781450384827_ref_160">Si et al. 2011</a>]. Moreover, the use of sensors that gather huge amounts of data from industrial business processes leads to the evolution of Internet of Things (IoTs), which in turn enables the identification and prediction of deviations in the production process in comparison to the scheduled performance and the recommendation of the appropriate actions at the appropriate time [<a href="bib.xhtml#b_9781450384827_ref_048">Feldman et al. 2013</a>]. Doing so requires the development of event-driven data processing systems that are able to handle very high frequency events from <a id="page_65"/>various sources, and coupling prognostic-based decision methods with sensor-based, event-driven architectures that can support efficient processing of events and improve scalability. </p>
<figure class="split" id="b_9781450384827-005_fig_005">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-005_fig_005.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 5.5</span>Fifteen clones identified by LMO2 CASTing data. The majority of clones contained the <i>CAGGTG</i> motif. Reproduced from <a href="bib.xhtml#b_9781450384827_ref_176">Wadman et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_176">1997</a>].</p></figcaption>
</figure>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_068">Gupta and Farahat</a> [<a href="bib.xhtml#b_9781450384827_ref_068">2020</a>] presented a taxonomy of artificial intelligence (AI) and machine learning problems in the industrial domain. On one axis of the taxonomy there are the horizontals such as maintenance, operations, quality and safety, and on the other axis there are descriptive, predictive, and prescriptive analytics. Among these verticals, there has been more research on maintenance. Maintenance is not a siloed situation. It impacts the whole value chain, when maintenance improves, quality improves, as does customer satisfaction. <a href="bib.xhtml#b_9781450384827_ref_068">Gupta and Farahat</a> [<a href="bib.xhtml#b_9781450384827_ref_068">2020</a>] argues that from the research perspective the feasibility of a solution is the key. They emphasize that repeatable building blocks are needed to solve specific business problems under specific conditions, and can be applied across verticals. They call these building blocks <i>solution cores</i>. All of the solution cores are repeatable analytics and are developed to meet the needs of the customer. So <a id="page_66"/><a id="page_67"/>they try to identify and develop the underlying analytics required, and then extract and abstract repeatable components. Doing this systematically will help solve the problems of operations, maintenance, and quality.</p>
<figure class="split" id="b_9781450384827-005_fig_006">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-005_fig_006.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 5.6</span>The GLAM2 alignment applied on clones shown in Figure <a href="#b_9781450384827-005_fig_005">5.5</a> is demonstrated on the top, and the information content LOGO corresponding to the alignment is shown on the bottom. Reproduced from <a href="bib.xhtml#b_9781450384827_ref_052">Frith et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_052">2008</a>].</p></figcaption>
</figure>
<figure class="split" id="b_9781450384827-005_fig_007">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-005_fig_007.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 5.7</span>Overall predictive maintenance workflow. Reproduced from <a href="bib.xhtml#b_9781450384827_ref_162">Sipos et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_162">2014</a>].</p></figcaption>
</figure>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_162">Sipos et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_162">2014</a>] presented a data-driven approach, based on multiple-instance learning, for predicting equipment failures by mining equipment event logs, which contain rich operational information. The authors stated that the use of equipment logs to predict failures is complicated due to heterogeneous data that include symbolic and numeric time series, categorical variables, and unstructured text. Also, massive amounts of data pose computational challenges. The overall predictive maintenance workflow and a piece of a log file containing event information are represented in Figures <a href="#b_9781450384827-005_fig_007">5.7</a> and <a href="#b_9781450384827-005_fig_008">5.8</a>, respectively. An event consists of a timestamp (indicating when the event occurred), a message text (either fully unstructured or generated from a template) describing the event, an event code (representing the category of a group of similar message text), and event severity. Logs are usually broken into day-long pieces that contain tens of thousands of events. The authors applied multi-instance learning (MIL) on multiple instances (daily logs) that are labeled positive or negative. If a batch of logs within a certain time interval is labeled negative, it means all the instances in it are negative and no failure has occurred according to the service notifications. A positive batch indicates that it contains at least one positive log. The results are then used to build a classifier that is used to label the logs. </p>
<figure class="split" id="b_9781450384827-005_fig_008">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-005_fig_008.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 5.8</span>A sample log file from a medical scanner. Reproduced from <a href="bib.xhtml#b_9781450384827_ref_162">Sipos et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_162">2014</a>].</p></figcaption>
</figure>
<p class="txi"><a id="page_68"/><a href="bib.xhtml#b_9781450384827_ref_016">Baptista et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_016">2018</a>] performed prediction of the next fault event in the industrial case of unscheduled removals of a critical valve in an aircraft engine. They applied auto-regressive moving average to capture the trends from time series data and used that as a predictive feature in different machine learning techniques such as generalized linear regression, k-nearest neighbors, neural networks, support vector regression, and random forests. </p>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-005_s_004">5.4</span>Business Intelligence</h2>
<p class="tx"><i>Forrester Research</i> defined business intelligence (BI) as &#x201C;a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making&#x201D; [<a href="bib.xhtml#b_9781450384827_ref_045">Evelson and Norman 2008</a>]. BI has many dimensions including predictive analytics, data mining, text mining, process mining, CEP, business performance management, and benchmarking. The aim of BI is to identify new opportunities and to implement an effective strategy based on insights that provide businesses with a competitive market advantage and long-term stability [<a href="bib.xhtml#b_9781450384827_ref_153">Rud 2009</a>].</p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_099">Leat</a> [<a href="bib.xhtml#b_9781450384827_ref_099">2007</a>] defined the evolutionary steps in adoption of BI analytics in five levels, with the evolution of event-based analysis appearing in the higher levels of the hierarchy:</p>
<p class="bullet1"><span class="bspace">&#x25CF;</span>Level 1, awareness: This level shows businesses are aware of the importance of BI in their decision-making but there are few BI capabilities. BI is primarily based on batched reports with some online analytical processing (OLAP). </p>
<p class="bullet"><span class="bspace">&#x25CF;</span><a id="page_69"/>Level 2, development: There is basic and non-integrated BI capabilities in place. Still, BI is mostly based on batched reports with some OLAP and predictive modeling and data mining. </p>
<p class="bullet"><span class="bspace">&#x25CF;</span>Level 3, practice: This level implements basic BI capabilities. Analytical and predictive modeling and mining grows.</p>
<p class="bullet"><span class="bspace">&#x25CF;</span>Level 4, optimization: This level integrates BI practices into daily operations. BI is mostly based on data mining and adapts event-based analysis to some extent.</p>
<p class="bullet2"><span class="bspace">&#x25CF;</span>Level 5, leadership: In this level, the organization is led based on predictive models deployed in an event-triggered environment.</p>
<p class="txi">Recently, the application of CEP in BI has gained some attention. Generally, BI platforms provide companies with the ability to model, manage, and predict business processes. CEP is a technology for tracking and analyzing streams of data from multiple sources to infer patterns that suggest complicated situations. When CEP is applied in the context of BI, events coming from different event sources can trigger a business process or influence the execution of the process. Moreover, complex events (e.g., the relation between multiple business level events) can also be relevant for the execution of other business processes. One important aspect of CEP in BI is the use of technology to proactively define and analyze the most critical opportunities and risks in an enterprise.</p>
<p class="txi"><a href="bib.xhtml#b_9781450384827_ref_008">Ammon et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_008">2008</a>] proposed a high-level reference architecture for using CEP in business process management. The architecture has two separate sets of components. Business processes are demonstrated with workflow components. The main elements of an event processing system are a different set of components that accompany workflow components. The role of business analysts or workflow modelers is to identify, analyze, and optimize business processes, while the role of an event modeler is to define which business level events need to be monitored in a dashboard, which alerts should be sent to which roles in the organization, and which actions should be started automatically if a certain event pattern occurs. </p>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-005_s_005">5.5</span>Computer Networks</h2>
<p class="tx">Network infrastructure is the hardware and software resources of an entire network that enable network connectivity, communication, operations, and management of an enterprise network. A typical network infrastructure includes hardware (e.g., wireless routers, switches, and LAN cards), software (e.g., network operations and management, firewall, and network security applications), and services (e.g., DSL, wireless protocols, and IP addressing). The network infrastructure of companies like Amazon is even more complex since they have several international <a id="page_70"/>data centers. These large-scale complex networks introduce many difficulties in design, operation, and maintenance. Equipment failure, communication error, and system misconfiguration are typical problems that need to be addressed by system administrators. Additionally, complex network infrastructure are heterogeneous, consisting of components made by various manufacturers. Also, software components running on the network often generate a huge number of messages and alerts that cannot be processed and addressed manually. </p>
<p class="txi">Different network elements in large complex networks collect alerts and messages in log files, indicating the equipment status in real time. Mostly, the contents of these log files are unstructured and system events are stored as short messages in plain text. The log files therefore need to be analyzed and appropriate events need to be extracted from the logs. Converting log messages to events provides the capability of canonically describing the semantics of log data and improves the ability of correlating and root cause analysis across multiple log components. The task of event recognition from log files is beyond the scope of this chapter. There are different classification [<a href="bib.xhtml#b_9781450384827_ref_010">Androulidakis et al. 2009</a>, <a href="bib.xhtml#b_9781450384827_ref_092">Kruegel et al. 2003</a>], clustering [<a href="bib.xhtml#b_9781450384827_ref_170">Tang and Li 2010</a>, <a href="bib.xhtml#b_9781450384827_ref_171">Tang et al. 2011</a>, <a href="bib.xhtml#b_9781450384827_ref_116">Makanju et al. 2009</a>], and parsing techniques that can be applied to extract events from logs.</p>
<p class="txi">Once events are extracted from log files, different event mining and pattern discovery techniques (see Chapter <a href="b_9781450384827-chapter-003.xhtml">3</a> for in depth explanation) can be utilized for automatic root cause analysis of failures in the networks. <a href="bib.xhtml#b_9781450384827_ref_111">Lou et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_111">2010</a>] proposed a framework to find intercomponent dependencies from unstructured logs. First, log messages are parsed to find log key pairs and some parameters. Then, dependent log key pairs belonging to different components are found by co-occurrence analysis and parameter correspondence. To estimate the dependency direction of each dependent log key pair, Bayesian decision theory is utilized at the end. <a href="bib.xhtml#b_9781450384827_ref_087">Khan et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_087">2008</a>] presented a tool called <i>Dustminer</i> that finds complex and unexpected interactions between multiple, often individually non-faulty, components in a sensor network that lead to a highly degraded mode of operation. Another body of research uses temporal pattern mining from sequential log data for root cause analysis. Unlike traditional methods that use predefined time windows, these techniques mine temporal dependencies between events to find fluctuating and interleaved time lags. <a href="bib.xhtml#b_9781450384827_ref_187">Yan et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_187">2012</a>] described a root cause analysis platform for service quality management in large IP networks. The platform collects data from various logging and performance monitoring systems at different network layers in the ISP, and includes a library of event definitions, network topology, service dependencies, and dependency relationship rules. For root cause analysis, the platform utilizes spatiotemporal correlation, rule-based reasoning, and Bayesian inference. </p>
</section>
</div>
</body>
</html>
