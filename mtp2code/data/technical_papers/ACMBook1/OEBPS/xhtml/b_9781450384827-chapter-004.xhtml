<?xml version="1.0" encoding="utf-8"?>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Chapter 04</title>
<link rel="stylesheet" href="../css/stylesheet.css" type="text/css"/>
</head>
<body epub:type="bodymatter">
<div epub:type="chapter" role="doc-chapter" aria-labelledby="ch4">
<h1 class="ct" id="ch4"><span class="cn" id="b_9781450384827-chapter-004">4</span><span class="ct1">Design Principles of Event Mining Systems</span></h1>
<p class="tx"><a id="page_43"/>We live in an era of data abundance. The availability of large volumes of multimodal data streams, ranging from videos, tweets, and other social media content to human activity, location, and environmental variables, can now be used to solve many critical societal problems. But as the emphasis on collecting data increases, there is a real need for a new generation of techniques, frameworks, and algorithms to assist researchers, analysts, and decision makers in extracting knowledge from such a variety of data. As the speed and scale of this data generation will increase even further in the future, we require new frameworks that support high-performance computing, processing techniques that fuse heterogeneous data and uncover hidden patterns and unknown correlations, scalable software tools, and useful visualizations that help analysts and decision makers understand the results better. </p>
<p class="txi">Modeling complex, mysterious, and at least partly unknowable systems involves many complicated decisions such as determining a model selection strategy, defining a model structure, defining a criteria for model goodness, selecting data and the transformations applied to the data, tuning learning parameters, and so on. Most of these decisions involve a reliance on theoretical or empirical results, that is, expert domain knowledge, and cannot be learned by a system itself solely from available input data. Moreover, many spurious associations might arise from learned models, resulting in false scientific discoveries and false statistical inferences [<a href="bib.xhtml#b_9781450384827_ref_026">Calude and Longo 2017</a>]. A promising approach for modeling complex phenomenon is to adopt a human-in-the-loop approach in the data processing step. This integrates high-level expert knowledge into the modeling process by acquiring an expert&#x2019;s relevance judgments regarding a set of initial retrieval results. Despite the apparent benefits of such a perspective, frameworks that facilitate a seamless interaction between a domain expert and a traditional knowledge discovery process are not well studied. Figure <a href="#b_9781450384827-004_fig_001">4.1</a> shows the human-in-the-loop in a modeling process rooted in event mining.</p>
<p class="txi"><a id="page_44"/>The ultimate goal of event mining is to extract knowledge from data and create explanatory models. Knowledge is either a verified <i>hypothesis</i> or machine-generated <i>patterns</i> that are novel, interesting, and human-understandable. We emphasize that knowledge is required to be representable in a linguistic form, that is understandable by humans and automatically usable by knowledge-based systems. The conversion of sub-symbolic patterns and trends in data to a symbolic human-understandable form is the most critical part of such interactive modeling. In this chapter, we review some of the most important characteristics of an event mining system, especially the need for a common event mining language that facilitates hypothesis formation, testing, and expansion. </p>
<figure class="split" id="b_9781450384827-004_fig_001">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-004_fig_001.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 4.1</span>The workflow of an event mining system.</p></figcaption>
</figure>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-004_s_001">4.1</span>Data Fusion and Transformation</h2>
<p class="tx">Data fusion is the process of combining data to estimate or predict the state of some aspect of the world. This process usually combines data from multiple sensors to achieve more specific inferences than could be achieved by using a single, independent sensor. While the concept of multisensor data fusion is not new, the emergence of new sensors and advanced processing techniques has made real-time fusion of data increasingly viable. Techniques to fuse data are drawn from a diverse set of disciplines, including multimedia computing and communications, digital signal processing, statistical estimation, control theory, and machine learning. </p>
<p class="txi">Fused data from multiple sensors provide several advantages over data from a single sensor. First, if several identical sensors are used (e.g., identical accelerometer sensors attached to different body parts of a subject), combining the observations will result in an improved estimate of the subject&#x2019;s activities and movements. <a id="page_45"/>A second advantage involves using the relative placement or motion of multiple sensors to improve the observation process. A third advantage of using multiple sensors is observability improvement. For example, combining GPS readings with accelerometer measurements can lead to a better recognition and differentiation between activities such as cycling and driving. </p>
<p class="txi">One area that benefits immensely from sensor fusion is the monitoring of complex mechanical equipment such as turbomachinery, helicopter gear trains, and industrial manufacturing equipment. For instance, in a motor vehicle monitoring system, sensor data can be obtained from accelerometers, temperature gauges, oil debris monitors, acoustic sensors, and infrared measurements. A predictive maintenance system would seek to combine these observations to identify precursors to failure, such as an abnormal gear wear, shaft misalignment, or bearing failure. The use of such monitoring is expected to reduce maintenance costs and improve safety and reliability. In addition to mechanical equipment, the human body is a complex system that can benefit from continuous monitoring, fusion of observations, and higher-level decision-making. Health professionals are trying to develop more personalized predictive techniques to prevent diseases by taking appropriate lifestyle and medical actions.</p>
<p class="txi">There are three basic approaches to fusing multisensor data:</p>
<p class="bullet1"><span class="bspace">&#x25CF;</span>low-level, or the direct fusion of raw sensor data;</p>
<p class="bullet"><span class="bspace">&#x25CF;</span>feature-level, or converting raw sensor data to features (first-level abstraction), and fusion of the feature vectors usually by applying statistical estimation and machine learning techniques; and</p>
<p class="bullet2"><span class="bspace">&#x25CF;</span>decision-level, or processing the output of each classification result separately to achieve a high inference (second-level abstraction).</p>
<p class="txi">If the sensor data measures similar (commensurate) information from the same physical phenomena, such as two visual image sensors or two accelerometer sensors on different limbs, then the raw sensor data can be directly combined. Techniques for raw data fusion typically involve classic estimation methods, such as Kalman filtering. If the sensor data is non-commensurate (i.e., sensors observing the same physical phenomena with dissimilar information), then the data must be fused at the feature vector level. Feature-level fusion involves the extraction of representative features from sensor data. In this case, features are extracted from multiple sensor observations and combined into a single concatenated feature vector. Fusion of principal component analysis (PCA) and linear discriminant analysis (LDA) coefficients of face images and the fusion of face and hand modalities are examples of fusion at the feature level. Feature-level fusion has also been used in <a id="page_46"/>medical imaging where multiple image types are utilized together to take advantage of &#x201C;cross-information.&#x201D; This proves helpful in recognizing a change in the functional magnetic resonance imaging (fMRI) activation maps associated with a change in the brain structure [<a href="bib.xhtml#b_9781450384827_ref_025">Calhoun and Adali 2009</a>].</p>
<p class="txi">Decision-level fusion is the most appropriate fusion technique for non-commensurate data that is sampled at different time points. <a href="bib.xhtml#b_9781450384827_ref_117">Mangai et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_117">2010</a>] defined three categories for decision-level fusion: abstract, rank, and measurement levels. In a nutshell, the majority voting [<a href="bib.xhtml#b_9781450384827_ref_089">Kittler et al. 1998</a>], weighted majority voting [<a href="bib.xhtml#b_9781450384827_ref_107">Littlestone and Warmuth 1994</a>], and naive-Bayes combination [<a href="bib.xhtml#b_9781450384827_ref_093">Kuncheva 2004</a>] methods belong to the abstract level; the class set reduction (including intersection of neighborhoods and union of neighborhoods) and the class set reordering methods (including highest rank method, Borda count method, and logistic regression method) [<a href="bib.xhtml#b_9781450384827_ref_078">Ho et al. 1994</a>] belong to the rank level; and the class-conscious methods [<a href="bib.xhtml#b_9781450384827_ref_096">Lam and Suen 1995</a>] and the class-indifferent methods [<a href="bib.xhtml#b_9781450384827_ref_094">Kuncheva et al. 2001</a>] belong to the measurement level.</p>
<p class="txi">In the literature [<a href="bib.xhtml#b_9781450384827_ref_040">Dasarathy 1994</a>], decision-level fusion is considered as the highest level of abstraction. As mentioned, it is a combination of classifiers to achieve better classification accuracy in pattern recognition problems. We can clearly see that decision-level fusion is indeed helpful in aggregating data from asynchronous sources of information. However, for longitudinal analysis of the behavior of a system, and based on discussions around event models in Chapter <a href="b_9781450384827-chapter-002.xhtml">2</a>, we need an event-level fusion.</p>
<p class="txi">In various disciplines, information about an underlying phenomenon might be acquired from different types of sensors. Rarely does a single modality provide complete knowledge of the phenomenon of interest due to the rich characteristics and complexity of that phenomenon. Hence, sensor fusion is an inevitable pre-processing step in any event mining framework. However, a main challenge is how to fuse these modalities into a human-understandable abstraction signal that not only preserves the semantics of the underlying system but also facilitates modeling. Data from different sources usually results in different silos that do not communicate with one another and are indexed using data-centric approaches. In the current form, it is not possible to make sense of these diverse sources of data. One way to address this issue is by organizing all this data around meaningful events so that we can overcome heterogeneity and variety issues in the data and fuse non-commensurate observations in a meaningful way. </p>
<p class="txi">Event models help us to understand the underlying heterogeneous real-life events in multimodal content and use them to better organize, retrieve, and discover knowledge in any possible way. Event models assign different properties to an event, and different observation sources help in our interpretation and <a id="page_47"/>modification of these event properties. For instance, an exercise event can have multiple properties such as heart rate, calorie consumption, speed, distance traveled, and number of floors climbed. Some of these properties can be recognized from motion sensors, and some can be recognized from physiological sensors. Figure <a href="#b_9781450384827-004_fig_002">4.2</a> shows multiple data streams from various sources being stored in data silos and consumed by applications separately. However, using an effective event model helps fuse these data streams at the event level and creates multiple event streams. The properties of each event in the stream might be populated from disparate sensor measurements.</p>
<figure class="split" id="b_9781450384827-004_fig_002">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-004_fig_002.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 4.2</span>Multiple data streams from various sources are usually stored in data silos and they are consumed by applications separately. Using an effective event model helps in fusing these data streams at the event level.</p></figcaption>
</figure>
<p class="txi">In summary, data transformation and event-level fusion provides an abstraction layer for the essential structural and functional properties of an event mining system, hiding the varieties of input data streams and complexities of low-level event extraction algorithms. An event is an abstract bundle that has meaningful attributes for specific applications, and these specified attributes play a key role in event mining. </p>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-004_s_002">4.2</span>Extensibility and Reusability</h2>
<p class="tx">Extensibility is a measure of the ability to extend a system with new functionalities with minimal or no effects on its internal structure and data flow. An event mining framework needs to provide an easy way to integrate new kinds of data streams, data processing operators, and visualization techniques. Different event <a id="page_48"/>mining applications may share similar conceptual and functional models or implementation details. The availability of reusable design and processing components may facilitate the rapid development of such applications by enabling reconfiguration and composition of available building blocks. The system needs to provide capabilities for specifying and mining patterns of events at different layers of abstraction that regulate the creation of higher level pattern instances according to pattern matching conditions on lower-level events. Each application has its own requirement regarding the definition of patterns and the relationships between events. High-level abstraction operators that are easily extensible and reusable are essential components of a well-designed event mining system. In summary, a highly modular tool with its main functional units well isolated in order to ease maintenance and extension of operations is the core principle of an event mining system. </p>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-004_s_003">4.3</span>Interactive Process</h2>
<p class="tx">As shown in Figure <a href="#b_9781450384827-004_fig_001">4.1</a>, the event mining process for explanatory modeling is interactive. It is a closed-loop system that involves extracting patterns, formulating, refining, and validating a hypothesis and extracting patterns again in the light of new knowledge. Hence, the event mining system should incorporate an iterative query processing capability that both allows the analyst to apply data-driven algorithms to discover interesting patterns and formulate hypotheses to investigate these patterns further for a deeper analysis. The hypothesis formulation step is very important in creating hypotheses that are not only based on current observations but also accumulated domain knowledge. This is an iterative process that allows the analyst to use the models as static knowledge to be presented as results as well as an active element of the process used to go deeper into data understanding.</p>
<p class="txi">Interestingly, even data-driven hypothesis formation may not be completely data driven. In most cases, the data streams that should be used in correlation and the other factors that should be considered come from some domain knowledge or prior experience. Therefore, this process is actually a knowledge-mediated event mining process for modeling. The ability to precisely articulate relationships among events becomes very important, which is where an expressive event language can play a huge role. Moreover, discovered models and extracted patterns have to be explicitly represented and stored to allow the progressive mining and reuse of extracted models with different data.</p>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-004_s_004">4.4</span>Human-centered Analysis</h2>
<p class="tx">Traditionally, having a human-in-the-loop in AI leverages both human and machine intelligence to create machine learning models. Humans label data, and a <a id="page_49"/>machine learning algorithm learns to make decisions from this data. Humans also tune, test, and validate the model as well as adjust it in situations where an algorithm is not confident about a judgment or overly confident about an incorrect decision. In real-world data analysis problems, domain experts typically use visualization techniques from common software/machine learning tools to visualize model results. Often, the underlying computations are not transparent and comprehensive enough to provide the feedback needed to guide the model refinement process. So, by integrating machine learning algorithms with interactive visualization, visual analytics aims to provide visual platforms for analysts to interact directly with data and models. In a case study, <a href="bib.xhtml#b_9781450384827_ref_167">Tam et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_167">2017</a>] illustrated that human-centric machine learning can produce better results than purely machine-centric methods. By combining visual analytics and machine learning, an analyst can steer the computation and interact with the model and data through an interactive visual interface.</p>
<p class="txi">Visual approaches alone are limited by data size and representation design, as well as how observant a user is. Therefore, automatic identification of patterns is important in enhancing the model building task, eliminating noise, and bringing interesting patterns into focus. Most often frequency is specified as the characterizing attribute of an identified pattern. This is, however, not necessarily the desired feature in all applications. Allowing a user to specify the characteristics of the <i>interestingness</i> of patterns can instead lead to the discovery of patterns that are important with respect to other factors. Furthermore, even constraining the event mining process by specifying the desired attributes can still lead to a huge number of resulting patterns. This enhances the benefit of introducing interactivity in the search for patterns. Adopting such a method eliminates the need to identify a large number of patterns first and then filter these patterns to retain the interesting ones. Interactivity allows the user to select and pursue only directions of interest during the model building process.</p>
<p class="txi">With increased computing power and speed, deep learning has made significant advancements in terms of efficiency and speed and has had a considerable impact on various long-running AI problems, including computer vision, speech recognition, and natural language understanding. As humans increasingly rely on artificial intelligence (AI) techniques, the interpretability of their decisions and the control over their internal processes are becoming a serious concern for sensitive domains such as precision medicine and bioinformatics. In domains where the costs of false positives and false negatives tend to be higher, it is important to leverage the knowledge of subject matter experts to the greatest extent and with the utmost precision. The cost of misclassification of a cat image versus a dog image is not high. However, the cost of misdiagnosing a deadly disease is <a id="page_50"/>irreparable. <a href="bib.xhtml#b_9781450384827_ref_061">Girardi et al.</a> [<a href="bib.xhtml#b_9781450384827_ref_061">2016</a>] emphasized the importance of a doctor-in-the-loop concept in knowledge discovery and proposed an interactive data mining process in biomedical research. </p>
<p class="txi">In an effort to increase explainability in machine learning models, researchers have started to look inside black-box machine learning models and emphasize the importance of understanding why an algorithm acts a certain way. The Defense Advanced Research Projects Agency (DARPA) in the United States is launching an initiative called eXplainable artificial intelligence (XAI) to bring attention to the importance of understanding, trusting, and effectively managing AI models [<a href="bib.xhtml#b_9781450384827_ref_066">Gunning 2017a</a>].</p>
<p class="txi">One way to look inside a machine learning black box is to provide visualizations of what is happening in an algorithm because it opens up interesting challenges at the intersection of machine learning and visual analytics. In this direction, a number of conceptual frameworks have been proposed, such as iPCA [<a href="bib.xhtml#b_9781450384827_ref_082">Jeong et al. 2009</a>], which lets analysts visualize the results of PCA by using multiple coordinated views and a set of user interactions. iPCA provides four views, each representing a specific aspect of the input data either in data space or eigenspace, and is coordinated in such a way that any interaction with one view is immediately reflected in all the other views. ForceSPIRE [<a href="bib.xhtml#b_9781450384827_ref_043">Endert et al. 2012</a>] introduces the concept of semantic interaction, which leads to a design space for user interaction with textual information. Using the system, analysts can add textual annotations to different documents and include those annotations in the analysis process. To capture domain expert knowledge in the model building process, ForceSPIRE allows users to lay out documents visually such that the layout reflects meaningful notions of similarity and distance between the documents. </p>
</section>
<section epub:type="division">
<h2 class="h2"><span class="h2space" id="b_9781450384827-004_s_005">4.5</span>Event Mining Architecture</h2>
<p class="tx">The goal of reviewing design principles in an event mining framework is to propose a truly extensible, reusable, flexible, human-centered, event-driven architecture for explanatory modeling. This architecture should provide seamless connectivity between different components, fluid communication between different levels (batch processing and real-time analysis), and manage the recognition and indexing of different types of events and their properties. In various disciplines, not only does information need to be captured from different sensor modalities, which in return results in event-level fusion in the system, but also decisions should be made depending on more than one event. The event mining platform needs to support two separate processing environments: offline model building and online model deployment. The offline model building environment is similar to a training/test phase in machine learning, where a model is trained and evaluated on <a id="page_51"/>historical data. The model building phase in an event mining platform involves applying event mining algorithms on longitudinal event streams to extract significant patterns from data. This process is iterative, interactive, and involves humans in hypothesis formulation, evaluation, and refinement. The algorithms need to be extremely efficient in processing the large amounts of data collected over a period of time. The end result of this process is a set of rules (i.e., extracted patterns) that encode complex interactions between a set of events (e.g., the cause) that induce a target event (e.g., the effect). For example, to understand the risk factors of an asthma attack in a patient, we might find significant patterns such as: </p>
<p class="bullet1"><span class="bspace">&#x25CF;</span>PM2.5-inc <i>followed by</i> asthma attack</p>
<p class="bullet2"><span class="bspace">&#x25CF;</span>(PM2.5-inc <i>and</i> temperature high) <i>followed by</i> asthma attack</p>
<p class="txi">These patterns can be encoded as rules and utilized in an online stream processing task for recommendation and feedback.</p>
<p class="txi">Online model deployment is similar to complex event processing environments, where streams of data flow into the system, and a set of predefined rules act as standing queries. If a rule is triggered, an alert will be generated. Any architecture for complex event processing should have the ability to import data from multiple sources, apply rules, and derive outbound actions. A rule engine can vary from a simple domain-specific language to a complex business rule management system. However, regardless of the complexity of the rules, they are predefined by an expert. An event mining framework aims to extract these rules in a model building environment by applying a combination of data-driven and hypothesis-driven event mining operators. </p>
<p class="txi">Figure <a href="#b_9781450384827-004_fig_003">4.3</a> shows the high-level architecture of an event mining platform. The main components of this platform are as follows:</p>
<p class="tx1"><b>Data sources.&#x2002;</b> All the data available for analysis from different sources, such as environmental, traffic, wearable, mobile apps, and so on, come into the system in a structured, semi-structured, or unstructured format. The speed that the data arrive and the rate at which they are delivered varies according to the data source. At the collection point, data are collected directly or through data providers in real time or in batch mode.</p>
<p class="tx1"><b>Data ingestion.&#x2002;</b> Although data ingestion is not necessarily an internal part of the event mining platform, it is the beginning of the data pipeline and the focus of data transport from its source to everywhere else. Some data sources have tables or databases that are frequently updated to reveal certain sensory data collected by <a id="page_52"/>different agencies. Data can be streamed in real time or ingested in batches. Real-time data are consumed by an online event processing component as soon as new data are available. When data are ingested in batches, data items are ingested in chunks at periodic intervals. With the availability of numerous data sources, data exist in different formats, which is a big challenge for applications. They have to ingest data at a reasonable speed and convert data to a correct format. </p>
<figure class="split" id="b_9781450384827-004_fig_003">
<p class="figure-img"><img role="presentation" alt="figure" src="../images/b_9781450384827-004_fig_003.jpg"/></p>
<figcaption><p class="figcaption"><span class="figspace">Figure 4.3</span>The high-level conceptual architecture of an event mining framework.</p></figcaption>
</figure>
<p class="tx1"><b>Event recognition and event management.&#x2002;</b> Most of the data coming from sensors (physical, social, etc.) are in the form of data streams. The event mining platform should handle and aggregate multiple heterogeneous streams, synchronize them, and provide stream processing operations. To support effective and efficient model building, streaming data need to be managed at an event level. Primitive sensor data also need to be filtered, aggregated, and correlated to generate more semantically meaningful complex events with a rich predefined event schema. Different recognition techniques can be utilized to convert raw sensor data to events, ranging from simple time-series discretization to complex machine learning methods. </p>
<p class="tx1"><b>Model building.&#x2002;</b> Hypothesis formulation, hypothesis evaluation, and event mining are important subtasks in an event mining system. The end goal is to build an <a id="page_53"/>explanatory model. One way to formulate this model is by converting extracted patterns to explainable rules. These pattern are either verified hypotheses (generated by applying hypothesis formulation and evaluation) or machine-generated patterns that are novel, interesting, and human-understandable. In the model building component, different pattern mining and machine-learning techniques should be included to detect co-occurrences and causal explanations among different events. Human-in-the-loop for hypotheses formulation, hypotheses verification, and to verify hypotheses and examine investigation of machine-generated patterns is only possible with user-friendly dashboards and expressive visualizations. </p>
<p class="tx1"><b>Model deployment.&#x2002;</b> Deployment is a process in which a trained model is integrated into the environment for data-based decision-making. Many applications require continuous data monitoring and processing in a timely fashion as data flows from source to system. The key requirement here is early identification of a situation of interest and quick response generation. The model generated in the previous step can be considered as a set of causal explanation rules, and might be deployed in a similar setting as complex event processing (CEP) systems. CEP technology is addressing continuous data monitoring requirements through an infrastructure that consists of three main components: a set of event sources, a rule engine, and a set of event sinks or event listeners. The rule engine consists of a set of rules that act as continuous queries or pattern signatures that filter, aggregate, and correlate events as they flow into the system. Conventional CEP rules are manually defined by domain experts. In the event mining framework, the rule set is extracted by pattern mining and hypothesis verification. The rule set is configured once but might change later as pattern mining algorithms run in the background and more interesting patterns are discovered.<a id="page_54"/> </p>
</section>
</div>
</body>
</html>
