<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" >
<head>
<title>The VR Book</title>
<link rel="stylesheet" type="text/css" href="../styles/9781970001143.css"/>
</head>
<body>
<h2 class="h2"><a id="page_307"></a><a id="ch27"></a><span class="blue1">27</span></h2>
<h2 class="h2b"><span class="blue">Input Devices</span></h2>
<p class="noindent"><strong>Input devices</strong> are the physical tools/hardware used to convey information to the application and to interact with the virtual environment. Some interaction techniques work more or less across different input devices whereas other techniques work only with input devices that have specific characteristics. Thus, appropriately choosing input hardware that best fits the application&#8217;s interaction techniques is an important design decision (or, conversely, designing and implementing interaction techniques depends upon available input hardware).</p>
<p class="indent"><a id="pg307lev1"></a>This chapter describes some general characteristics of input devices and then describes the primary classes of input devices.</p>
<h3 class="h3"><a id="lev27.1"></a><span class="font">27.1</span> <strong>Input Device Characteristics</strong></h3>
<p class="noindent">Input devices can be very different, and the characteristics of each should be considered when choosing hardware and designing interactions.</p>
<h4 class="h4"><a id="lev27.1.1"></a><span class="font1">27.1.1</span> Size and Shape</h4>
<p class="noindent">The most obvious characteristics to a new VR user are the basic shape and size of the input device. The shape and size has more to do with just how the controller looks and feels in the hand. Large hand-held devices are primarily controlled by large muscle groups of the shoulder, elbow, and wrist, whereas smaller hand-held devices utilize smaller and faster muscle groups in the fingers [<a href="reference.html#ref24"><span class="blue">Bowman et al. 2004</span></a>]. Smaller devices can also decrease <strong>clutching</strong>&#8212;the releasing and regrasping of an object in order to complete a task due to not being able to complete it in a single motion (such as a wrench). Gloves also use these smaller muscle groups and have the advantage of being able to freely touch and feel other items.</p>
<h4 class="h4"><a id="lev27.1.2"></a><span class="font1">27.1.2</span> Degrees of Freedom</h4>
<p class="noindent">Input devices are often classified by the number of degrees of freedom (DoF) they report. <strong>Degrees of freedom (DoF)</strong> are the number of dimensions that an input device <a id="page_308"></a>is capable of manipulating (also see Section <a href="chapter25.html#lev25.2.3"><span class="blue">25.2.3</span></a>). Devices range from a single DoF (e.g., an analog trigger), to 6 DoF that measure full 3D translation (up/down, left/right, forward/backward) and rotation (roll, pitch, and yaw), to full hand or body tracking with many DoFs. A traditional mouse, joystick, trackball, and touchpad (a rotatable ball that is essentially an upside-down mouse) are examples of 2 DoF devices. VR hand tracking should have a minimum of 6 DoF (multiple points tracked on the hand have more than 6 DoF). For a majority of active VR experiences, one or more 6 DoF hand-held controllers is often the most appropriate choice. For some simple tasks only requiring navigation and no direct interaction, a non-tracked hand-held controller is good enough.</p>
<h4 class="h4"><a id="lev27.1.3"></a><span class="font1">27.1.3</span> Relative vs. Absolute</h4>
<p class="noindent"><strong>Relative input devices</strong> measure the differences between the current and last measurement. Mice, trackballs, and inertial trackers are examples of relative devices. Relative devices drift over time and thus are not nulling compliant (Section <a href="chapter25.html#lev25.2.5"><span class="blue">25.2.5</span></a>). Although limited, the Nintendo Wii proved relative devices can work well for natural interactions under some circumstances if the applications are carefully designed. VR relative devices typically use inertial measurement units (IMUs) that have the advantage of having a higher update rate (e.g., 1,000 Hz) and faster response (e.g., 1 ms) than absolute measurements. <strong>Absolute input devices</strong> sense pose relative to a constant point of reference independent of past measurements and are nulling compliant. <strong>Hybrid tracking systems</strong> fuse both relative and absolute trackers to provide the advantages of both. VR head and hand tracking should sense pose via absolute measurements (although relative devices can arguably estimate absolute pose of the hands from modeling the constraints imposed by the physicality of the arms and hand).</p>
<h4 class="h4"><a id="lev27.1.4"></a><span class="font1">27.1.4</span> Separable vs. Integral</h4>
<p class="noindent"><strong>Integral input devices</strong> enable users to control all DoFs simultaneously from a single motion (a single composition) whereas <strong>separable input devices</strong> contain at least one DoF that cannot be controlled simultaneously from a single motion (two or more distinct compositions). A gamepad with two different analog sticks is an example of a separable device. A 2D device that enables control of more than two dimensions via mode switching is also an example of a separable device. VR hand tracking should be integral.</p>
<h4 class="h4"><a id="lev27.1.5"></a><span class="font1">27.1.5</span> Isometric vs. Isotonic</h4>
<p class="noindent"><strong>Isometric input devices</strong> measure pressure or force that contains no or little actual movement. <strong>Isotonic input devices</strong> measure deflection from a center point and may <a id="page_309"></a>or may not have some resistance. Mice are isotonic input devices. Joysticks can be either isotonic or isometric. Isotonic input devices are best for controlling position, whereas isometric input devices are best for controlling rates such as navigation speed. An isometric joystick, for example, works well for controlling velocity (i.e., hold to continue moving).</p>
<h4 class="h4"><a id="lev27.1.6"></a><span class="font1">27.1.6</span> Buttons</h4>
<p class="noindent"><strong>Buttons</strong> control one DoF via pushing with a finger and typically take on one of two states (e.g., pushed or not pushed) although some buttons can take on analog values (also known as analog triggers). Buttons are often used to change modes, to select and object, or to start an action. Although buttons can be useful for VR applications, too many buttons can lead to confusion and error&#8212;especially when the button-mapping functionality is unclear or inconsistent (although attaching labels to virtual controllers can help&#8212;see Figure <a href="chapter26.html#fig26.3"><span class="blue">26.3</span></a> as an example). Consider the capabilities and intuitiveness of desktop applications that are controlled via no more than three buttons on a mouse.</p>
<p class="indent">There is a debate between bare-hand system (Section <a href="chapter27.html#lev27.2.5"><span class="blue">27.2.5</span></a>) and hand-held controller <a id="pg309lev1"></a>(Sections <a href="chapter27.html#lev27.2.2"><span class="blue">27.2.2</span></a> and <a href="chapter27.html#lev27.2.3"><span class="blue">27.2.3</span></a>) advocates as to the utility of buttons. For example, Microsoft Kinect and Leap Motion developers believe buttons are a primitive and unnatural form of input whereas Playstation Move and Sixense Stem developers believe buttons are an essential part of game play. Like any great debate, the answer is &#8220;it depends&#8221; [<a href="reference.html#ref146"><span class="blue">Jerald et al. 2012</span></a>]. Buttons can be abstracted to indirectly trigger nearly any action, but this abstraction can cause a disconnect between the user and the application. Buttons are most effective when an action is binary, when the action needs to occur often, when reliability is required, and when physical feedback to the user is essential. Buttons are also ideal for time-sensitive actions, since very little time is required for the physical action (i.e., an entire dynamic gesture does not need to be completed in order to register the action). Gestures can be slower and more fatiguing than button presses, particularly in command-intensive tasks such as modeling or radiology. Natural buttonless hand manipulation is most effective for providing a sense of realism and presence, when abstraction is not appropriate, or when detailed tracking of the entire hand is required.</p>
<h4 class="h4"><a id="lev27.1.7"></a><span class="font1">27.1.7</span> Encumbrance</h4>
<p class="noindent"><strong>Unencumbered input devices</strong> do not require physical hardware to be held or worn. Such systems are implemented with camera systems. Thus no &#8220;suit-up&#8221; time is required (although calibration might be necessary). Unencumbered systems can also reduce hygiene issues (Section <a href="chapter14.html#lev14.4"><span class="blue">14.4</span></a>) since a physical device is not passed between users. Non-encumbrance is not always a design goal; holding something physically in <a id="page_310"></a>the hand can add to the sense of presence (Section <a href="chapter03.html#lev3.2.3"><span class="blue">3.2.3</span></a>); consider holding a controller in the hand vs. not holding a controller in the hand for a shooting or golf experience.</p>
<h4 class="h4"><a id="lev27.1.8"></a><span class="font1">27.1.8</span> Ability to Fully Interact with Physical Objects</h4>
<p class="noindent">Some devices enable one to touch the real world in natural ways without the device getting in the way. Bare-hand tracking systems (i.e., camera systems) and gloves are the most common examples of this. Held devices and world-grounded devices (Section <a href="chapter27.html#lev27.2.1"><span class="blue">27.2.1</span></a>) must be released before the hand can fully interact with other physical objects. In such cases, the physical devices should be tracked and rendered in the virtual world so the user can reach out to grab them again.</p>
<h4 class="h4"><a id="lev27.1.9"></a><span class="font1">27.1.9</span> Device Reliability</h4>
<p class="noindent">Device <strong>reliability</strong> is the extent to which an input device can consistently work within the user&#8217;s entire personal space (and a larger volume if the user is expected to physically move around). Devices should ideally have 100% reliability anywhere the user can reach with no loss in tracking acquisition. Reliability should carefully be considered when choosing an input device as unreliable devices can result in frustration, fatigue (e.g., due to having to hold the hand high and in front of the body; Section <a href="chapter14.html#lev14.1"><span class="blue">14.1</span></a>), increased cognitive load (e.g., because the user must think about holding the device in a certain way), breaks-in-presence (Section <a href="chapter04.html#lev4.2"><span class="blue">4.2</span></a>), and reduced performance.</p>
<p class="indent">Unreliable tracking can be due to multiple reasons and divided into two sets of factors: (1) implementation limitations and (2) inherent physical limitations. An inherent physical limitation is the best a device will ever be able to achieve given an optimal engineering solution/implementation. Some devices cannot possibly provide 100% reliability no matter what the engineering effort. A system that requires line of sight from a sensor to the tracked device can be occluded by another physical object (such as a hand or torso), in which case there is no way for the system to reliably determine the pose of the device (although the state of the device can be estimated for short periods of time). VR devices should ideally work in all orientations and hand postures (e.g., hands covering sensors or tracking the fingers when a fist is made).</p>
<p class="indent">Another challenge of reliability occurs for users attempting to work in a tracked volume smaller than the user&#8217;s personal space. An example is a vision-based system with a limited field of view. Lighting can also be a challenge for some vision-based systems, especially in uncontrolled environments outside of the laboratory or some other highly controlled space. Other systems only recognize gestures when the hand is oriented or held in a certain way. Many camera-based hand tracking systems only reliably recognize poses and gestures when the hands are perpendicular to the camera with the fingers visible.</p>
<h4 class="h4"><a id="page_311"></a><a id="lev27.1.10"></a><span class="font1">27.1.10</span> Haptics Capable</h4>
<p class="noindent">Active haptics can easily be added to physical devices that are worn or held. However, the degree of haptics might be limited depending on the size of the device and if attached to the world in some way (Section <a href="chapter03.html#lev3.2.3"><span class="blue">3.2.3</span></a>).</p>
<h3 class="h3"><a id="lev27.2"></a><span class="font">27.2</span> <strong>Classes of Hand Input Devices</strong></h3>
<p class="noindent">The most important VR input devices are the human hands, and this section explores how different types of devices can integrate the hands into VR. An <strong>input device class</strong> is a set of input devices that share the same essential characteristics that are crucial to interaction. Input device classes described in this section focus on the hands and are classified as world-grounded input devices, non-tracked hand-held controllers, tracked hand-held controllers, hand-worn devices, and bare-hand controllers.</p>
<p class="indent">Table <a href="chapter27.html#tab27.1"><span class="blue">27.1</span></a> summarizes some of the most essential characteristics of the hand input device classes described in this section and non-hand input device classes described in Section <a href="chapter27.html#lev27.3"><span class="blue">27.3</span></a>. As can be seen, no single input device class is universally advantageous although some classes can be combined to create a hybrid system with more advantages. For example, a bare-hands camera-based system might be used with tracked hand-held controllers (although simultaneous usage can be difficult due to different characteristics, so bare hands might be used until the controller is picked up). Note the table is based upon inherent physical limitations rather than today&#8217;s existing implementations. Technical specifications such as update rate, latency, etc. are not included here as such specs are independent of the hardware (e.g., a fast update rate could potentially be implemented on any device class).</p>
<h4 class="h4"><a id="lev27.2.1"></a><span class="font1">27.2.1</span> World-Grounded Input Devices</h4>
<p class="noindent"><strong>World-grounded input devices,</strong> like world-grounded haptics (Section <a href="chapter03.html#lev3.2.3"><span class="blue">3.2.3</span></a>), are designed to be constrained or fixed in the real world and are most often used to interact with desktop systems.</p>
<p class="indent">Keyboards and mice are considered to be world-grounded devices and are the most popular form of input that works extremely well for its intended task&#8212;2D desktop manipulation. However, such input is not a good way of interacting for most all immersive applications (with the possible exception of video-see-through augmented reality where users can see the mouse and keyboard).</p>
<p class="indent">Trackballs and joysticks mounted to a permanent location are world-grounded devices. Other devices offer up to 6 DoF through pushing, pulling, twisting, and/or buttons for mode changes. However, these devices suffer from similar limitations for VR as the mouse due to not being designed to be held comfortably and freely <a id="page_312"></a>in the hands. There are exceptions (e.g., to mount a joystick on the arm of a chair or if simulating a desktop environment where the physical controls precisely match a virtual desktop), but from a human-centered design perspective there should be a solid reason to choose such devices, instead of &#8220;because it is available&#8221; or &#8220;it is what someone else is using.&#8221;</p>
<p class="tabcaption"><a id="tab27.1"></a><span class="blue"><strong>Table 27.1</strong></span> Comparison of hand and non-hand input device classes.</p>
<p class="tabimage"><img src="../images/f0312-01.png" alt="image"/></p>
<p class="indentt">World-grounded devices that work extremely well for VR are specialty devices such as handlebars, steering wheels, gas and brake pedals, cockpits, and automotive interior controls. Such devices can be especially good for travel, as most users already <a id="page_313"></a><a id="pg313lev1"></a>have real-world experience with devices. Even if world-grounded devices aren&#8217;t actually used in the real world, they can still be quite effective and presence inducing if designed well. For example, Disney&#8217;s Aladdin Magic Carpet Ride, 3 DoF controls (Figure <a href="chapter27.html#fig27.1"><span class="blue">27.1</span></a>) provides an intuitive physical interface for travel. One reason such controls are so effective is due to having physical signifiers, affordances, and feedback, e.g., the user feels what can be done and how she is doing it. Some challenges of these devices are that creators can&#8217;t assume there is a wide user base that owns such hardware and it can be difficult to generalize the devices to work with a wide range of tasks. Thus, such devices are more commonly used at location-based entertainment venues where large groups of people use the same device(s) and the device can be designed or modified for the particular VR experience.</p>
<p class="image"><a id="fig27.1"></a><img src="../images/f0313-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 27.1</span> The Disney Aladdin world-grounded input device along with its mapping for viewpoint control.</strong> (From <a href="reference.html#ref237"><span class="blue">Pausch et al.</span></a> [<a href="reference.html#ref237"><span class="blue">1996</span></a>])</p>
<h4 class="h4"><a id="lev27.2.2"></a><span class="font1">27.2.2</span> Non-Tracked Hand-Held Controllers</h4>
<p class="noindent"><strong>Non-tracked hand-held controllers</strong> are devices held in the hand that include buttons, joysticks/analog sticks, triggers, etc. but are not tracked in 3D space. Traditional video game input devices such as joysticks and gamepads are the most common form of non-tracked hand-held controllers (Figure <a href="chapter27.html#fig27.2"><span class="blue">27.2</span></a>). Many VR applications are starting to support such game controllers. These controllers work much better than the mouse and keyboard since the controller can be held comfortably in the lap where users can continually hold the controller. Many gamers have an intuitive feel of where the <a id="page_314"></a>buttons are through years of use. Controllers with analog sticks work surprisingly well for navigating within VR (Section <a href="chapter28.html#lev28.3.2"><span class="blue">28.3.2</span></a>).</p>
<p class="image"><a id="fig27.2"></a><img src="../images/f0314-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 27.2</span> The Xbox One controller is an example of a non-tracked hand-held controller.</strong></p>
<p class="indent">Although not tracked, such controllers can increase presence for seated experiences by placing visual hands and controllers at the approximate location of the user&#8217;s lap since most users hold the controller in such a position. The visual controller and hands have also been found to cause users to subconsciously move the hands and controller to the visual controller (Andrew Robinson and Sigurdur Gunnarsson, personal communication, May 11, 2015). Unfortunately, when a user moves his hands away from the assumed position, a break-in-presence typically occurs if the user sees the virtual hands stay in place.</p>
<h4 class="h4"><a id="lev27.2.3"></a><span class="font1">27.2.3</span> Tracked Hand-Held Controllers</h4>
<p class="noindent"><strong>Tracked hand-held controllers</strong> are typically 6 DoF devices (known as &#8220;wands&#8221; in the VR research community where they have been used for decades) and can also contain functionality offered by non-tracked hand-held controllers. Tracked hand-held controllers are currently the best option for a majority of interactive VR applications. Tracked hand-held controllers are easy to use for many 3D tasks due to their natural, direct mapping to hand motion. Because the controllers are tracked, they can <a id="page_315"></a>be visually co-located with the real hands (i.e., spatially and temporally compliant; Section <a href="chapter25.html#lev25.2.5"><span class="blue">25.2.5</span></a>) as well as physically felt, providing proprioceptive and passive haptics/<a id="pg315lev1"></a>touch cues. Labels can also be attached to the virtual representation to provide immediate instruction of what the buttons do by simply looking at where the hands are physically located (Section <a href="chapter26.html#lev26.3.4"><span class="blue">26.3.4</span></a> and Figure <a href="chapter26.html#fig26.3"><span class="blue">26.3</span></a>), adding a big advantage over traditional desktop and gamepad input. Viewpoint manipulation with these devices is typically achieved using buttons and a trackball, integrated analog sticks (as on the Sixense STEM and Oculus Touch controllers as shown in Figure <a href="chapter27.html#fig27.3"><span class="blue">27.3</span></a>), or by flying with the hands. Such techniques are described in Section <a href="chapter28.html#lev28.3"><span class="blue">28.3</span></a>. Other types of physical controls and feedback can also be added to these devices, such as trackpads and active haptics (e.g., vibration).</p>
<p class="image"><a id="fig27.3"></a><img src="../images/f0315-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 27.3</span> The Sixense STEM (left) and Oculus Touch (right) tracked hand-held controllers.</strong> (Courtesy of Sixense (left) and Oculus (right))</p>
<p class="indent">Tracked hand-held controllers have the advantage of acting as a physical prop, which enhances presence through physical touch. Not only do such controls facilitate communication with the virtual world, but they also help to make spatial relationships seem more concrete to the user [<a href="reference.html#ref121"><span class="blue">Hinckley et al. 1998</span></a>]. However, such props come at the cost of not being able to directly/fully touch and feel other passive objects in the world and world-grounded input devices, such as seats, handlebars, and cockpit controls (Section <a href="chapter27.html#lev27.2"><span class="blue">27.2</span></a>), without first setting down the controller/prop.</p>
<p class="indent">Tracked hand-held devices typically use inertial, electromagnetic, ultrasonic, or optical (camera) technologies. Each of these technologies has advantages and disadvantages, and hand-held trackers ideally use some hybrid method of integrating multiple technologies together (sensor fusion) to provide both high precision and accuracy.</p>
<h4 class="h4"><a id="page_316"></a><a id="lev27.2.4"></a><span class="font1">27.2.4</span> Hand-Worn Devices</h4>
<p class="noindent"><strong>Hand-worn input devices</strong> include gloves, muscle-tension sensors (electromyographic or EMG sensors), such as what Thalmic Labs recently made popular with the Myo (which is worn on the arm but measures hand motion), and rings.</p>
<p class="indent">Many believe gloves (Figure <a href="chapter27.html#fig27.4"><span class="blue">27.4</span></a>) to be the ultimate VR interface as they theoretically have many advantages, such as not having line-of-sight, sensor field-of-view, or lighting requirements so the hands can be held comfortably to the side or in the lap with no concern of losing tracking, resulting in less gorilla arm if the interaction techniques are designed well (Section <a href="chapter18.html#lev18.9"><span class="blue">18.9</span></a>). Like bare hands, gloves also have the advantage that the hands and fingers can still fully interact with other physical objects.</p>
<p class="indent">Unfortunately, like bare-hand systems, full-hand tracked gloves are lacking in their current form and will require dramatic improvement to be used by the masses. Consistent recognition of more than a few gestures is still challenging due to the lack of consistent finger tracking accuracy. Recognizing more than a few gestures requires the user to recalibrate often due to the glove moving on the hand. Gloves must also <a id="pg316lev1"></a>be put on and worn, which can become uncomfortable and result in sweaty hands. There is also a risk of social resistance to wearing gloves similar to the resistance of Google Glass&#8212;although those willing to wear an HMD on their face are unlikely to care what others think of wearing gloves. If such challenges can be solved, then gloves may eventually become the input device of choice for VR.</p>
<p class="indent">The Fakespace Pinch Gloves have functions more like buttons, with near 100% consistent recognition, more than typical gloves that do full hand and finger tracking. They work via a conductive cloth sewn into the tip of each finger. When two or more fingers touch then the circuit closes resulting in a signal. This simple design provides the capability for a large number of pinch gestures; combinations range from two to ten fingers touching each other plus poses involving separate but simultaneous pinches (e.g., left thumb to left index finger and right thumb to right index finger pinched at the same time). In practice, due to the usability of physical hand constraints and users&#8217; willingness to memorize gestures, applications will only use so many of these gestures, similar to how too many buttons on a hand controller would be too confusing. Pinch gloves can be used quite well with many of the example techniques described in Chapter <a href="chapter28.html#ch28"><span class="blue">28</span></a>.</p>
<p class="indent">Perhaps one of the most significant advantages of gloves is that both full hand tracking and button simulation via pinch gestures can be combined such as demonstrated by <a href="reference.html#ref176"><span class="blue">LaViola and Zeleznik</span></a> [<a href="reference.html#ref176"><span class="blue">1999</span></a>]. Haptics can also be used with gloves, such as is done with the CyberGlove CyberTouch (as shown in Figure <a href="chapter27.html#fig27.4"><span class="blue">27.4</span></a> but with buzzers added to provide a sense of haptics).</p>
<p class="image"><a id="page_317"></a><a id="fig27.4"></a><img src="../images/f0317-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 27.4</span> The CyberGlove is an example of a hand-worn device.</strong> (Courtesy of CyberGlove Systems LLC)</p>
<p class="indent">If EMG sensors and rings can be made more accurate, then they too might become an ideal fit for many applications.</p>
<h4 class="h4"><a id="lev27.2.5"></a><span class="font1">27.2.5</span> Bare Hands</h4>
<p class="noindent"><strong>Bare-hand input devices</strong> work via sensors aimed at the hands (mounted in the world or on the HMD). Figure <a href="chapter27.html#fig27.5"><span class="blue">27.5</span></a> shows the hands and a skeletal model fit to the hands, as seen by the user. The obvious major advantage is that the user&#8217;s hands are completely unencumbered. Many believe bare-hand systems will ultimately be the ideal VR interface. Although the bare hands work extremely well in the real world, it turns out consistent interaction with the bare hands in VR is an enormous challenge. Challenges include not having a sense of touch, fatigue from holding the hands in front of the sensor, line-of-sight requirements, and consistent recognition of gestures across a wide range of users. Such technical challenges lead to usability challenges, such as being able to comfortably work with the hands in the lap without concern for where the sensor(s) is located. The bare hands also lack physical buttons, which is important for some applications but not important for other applications (Section <a href="chapter27.html#lev27.1.6"><span class="blue">27.1.6</span></a>).</p>
<p class="indent">Regardless of the challenges of effectively working with the bare hands in VR, seeing the entirety of one&#8217;s hands in 3D is extremely compelling and provides a nice sense of presence for the periods of time when the tracking does consistently work. It remains to be seen if such challenges will be overcome and/or accepted by a wide range of users.</p>
<h3 class="h3"><a id="lev27.3"></a><span class="font">27.3</span> <strong>Classes of Non-hand Input Devices</strong></h3>
<p class="noindent">VR input can occur through more than just the hands. This section describes head tracking, eye tracking, microphones, and full-body tracking.</p>
<p class="image"><a id="page_318"></a><a id="fig27.5"></a><img src="../images/f0318-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 27.5</span> A depth-sensing camera on the HMD looking out enables one to see her own hands in <a id="pg318lev1"></a>detail. Here a skeleton model is also fit to the hands.</strong> (Courtesy of Leap Motion)</p>
<h4 class="h4"><a id="lev27.3.1"></a><span class="font1">27.3.1</span> Head Tracking Input</h4>
<p class="noindent">Head tracking must be accurate, precise, fast, and well calibrated for the virtual world to appear stable. World stability is essential for VR and assumed to work well, but is not the focus of this chapter. Here, <strong>head-tracking input</strong> refers to interaction that modifies or provides feedback beyond just seeing the virtual environment. The most common form of head-tracking interaction is to aim by looking. One way of doing this is to provide a reticle or pointer in the middle of the screen (Section <a href="chapter26.html#lev26.3.5"><span class="blue">26.3.5</span></a>) that is triggered by a button press, firing in the direction of the reticle, or selecting an option the reticle projects onto. Other more subtle interactions can be used, such as having an action occur in the direction the user is looking, having characters respond when looked at, or simple head gestures such as nodding the head yes or no.</p>
<h4 class="h4"><a id="lev27.3.2"></a><span class="font1">27.3.2</span> Eye Tracking Input</h4>
<p class="noindent">An <strong>eye-tracking input device</strong> tracks where the eyes are looking. Eye-tracking input for VR is a largely unexplored topic other than the obvious of selecting (Section <a href="chapter28.html#lev28.1.2"><span class="blue">28.1.2</span></a>) or firing where one is looking as mostly demonstrated with interactive eye-tracking systems that are integrated within some of today&#8217;s HMDs.</p>
<p class="indent">The <strong>Midas Touch problem</strong> refers to the fact that people expect to look at things without that look &#8220;meaning&#8221; something [<a href="reference.html#ref141"><span class="blue">Jacob 1991</span></a>]. Interacting via eye tracking <a id="page_319"></a>alone is usually not a good idea&#8212;eye tracking works better with multimodal input. For example, signaling via a clutch (e.g., push a button, blink, or say &#8220;select&#8221;) typically works better than signaling via a dwell time. Even with a clutch, designing interactions that work well with gaze is a challenge. Straightforward feedback in the form of a pointer/reticle that moves with the eyes can be annoying to users and occludes viewing in the part of vision with the highest visual acuity. Eye saccades can also make the pointer jitter or jump in unintended ways. These problems can be mitigated by only turning on the pointer when a button is held down and filtering out high-frequency motions.</p>
<p class="indent">Eye tracking can be more effective for specialized tasks and for subtle interactions, such as how a character responds when looked at. The following guidelines are useful to consider when designing eye-tracking interactions [<a href="reference.html#ref168"><span class="blue">Kumar 2007</span></a>].</p>
<p class="hang1"><strong>Maintain the natural function of the eyes.</strong> Our eyes are meant for looking, and interaction designers should maintain the natural function of the eye. Using the <a id="pg319ev1"></a>eyes for other purposes overloads the visual channel.</p>
<p class="hanga"><strong>Augment rather than replace.</strong> Attempting to replace existing interfaces with eye tracking is typically not appropriate. Instead, think about how to add functionality onto already existing and newley created interfaces. Gaze can provide context and inform the system that the user is paying attention to a specific object or area in the scene.</p>
<p class="hanga"><strong>Focus on interaction design.</strong> Focus on the overall experience instead of eye tracking alone. Consider the number of steps in the interaction, the amount of time it takes, the cost of an error/failure, cognitive load, and fatigue.</p>
<p class="hanga"><strong>Improve the interpretation of eye movements.</strong> Gaze data is noisy. Consider how to best filter eye movements, classify gaze data, recognize gaze patterns, and take other input modalities into account.</p>
<p class="hanga"><strong>Choose appropriate tasks.</strong> Don&#8217;t try to force gaze to solve every problem. Eye tracking is not appropriate for all tasks. Consider the task and scenario before choosing gaze.</p>
<p class="hanga"><strong>Use passive gaze over active gaze.</strong> Consider ways in which gaze can be used more passively so the eyes can better maintain their natural function.</p>
<p class="hanga"><strong>Leverage gaze information for other interactions.</strong> Leverage the system&#8217;s knowledge of where the user is paying attention in order to provide context for non-gaze interactions.</p>
<p class="indentt"><a id="page_320"></a>Although typically not interactive, eye tracking will have significant benefit for VR usability by providing clues to content creators of what most engages a user&#8217;s attention (see attention maps in Section <a href="chapter10.html#lev10.3.2"><span class="blue">10.3.2</span></a>), similar to the use of eye tracking to inform and enhance website design.</p>
<h4 class="h4"><a id="lev27.3.3"></a><span class="font1">27.3.3</span> Microphones</h4>
<p class="noindent">A <strong>microphone</strong> is an acoustic sensor that transforms physical sound into an electric signal. Use a headset microphone specifically designed for speech recognition that includes noise-canceling features that remove/reduce ambient noise. The microphone should be able to be easily adjusted/positioned while the HMD is on and while not being able to see it. Accuracy improves as the microphone comes closer to and in front of the mouth. Microphones should be comfortable and light.</p>
<p class="indent">To prevent speech recognition errors (Section <a href="chapter26.html#lev26.4.2"><span class="blue">26.4.2</span></a>), such as sounds picked up when thinking aloud or another person&#8217;s voice, use a push-to-talk interface. If hand-held controllers are used, then the push-to-talk button should be located on the controller.</p>
<h4 class="h4"><a id="lev27.3.4"></a><span class="font1">27.3.4</span> Full-Body Tracking</h4>
<p class="noindent"><strong>Full-body tracking</strong> consists of tracking more than just the head and hands. Full-body tracking can significantly add to the illusion of self-embodiment as well as the illusion of social presence (Section <a href="chapter04.html#lev4.3"><span class="blue">4.3</span></a>). Tracking a large number of features can also be used to enhance interaction (e.g., a game that allows kicking a ball).</p>
<p class="indent">VR full-body tracking is typically done with a motion capture suit, similar to what is used in the film industry. Different suits utilize different technologies such as electromagnetic sensors, retro-reflective markers, and inertial sensors. Depth cameras such as Microsoft Kinect can theoretically track the entire body, but it can be difficult to capture the entire body unless multiple cameras are used. Regardless, full-body camera capture systems, like Microsoft Kinect, can be used for creating extremely compelling experiences, even though resolution is not yet great and much of the body might go in and out of view. Figure <a href="chapter27.html#fig27.6"><span class="blue">27.6</span></a> shows real-time capture of the real world and display of the resulting point-cloud data within an HMD.</p>
<p class="image"><a id="page_321"></a><a id="fig27.6"></a><img src="../images/f0321-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 27.6</span> Depth cameras enable users to see their own body, the real world, and/or other people from the real world.</strong> (Courtesy of Dassault Syst&#232;mes, iV Lab)</p>
</body>
</html>
