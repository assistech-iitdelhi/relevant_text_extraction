<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" >
<head>
<title>The VR Book</title>
<link rel="stylesheet" type="text/css" href="../styles/9781970001143.css"/>
</head>
<body>
<h2 class="h2"><a id="page_28"></a><a id="page_29"></a><a id="ch3"></a><span class="blue1">3</span></h2>
<h2 class="h2b"><span class="blue">An Overview of Various Realities</span></h2>
<p class="noindent">This chapter aims to provide a basic high-level overview of various forms of reality, as well as different hardware options to build systems supporting those forms of reality. Whereas most of the book focuses on fully immersive VR, this chapter takes a broader view; its aim is to put fully immersive VR in the context of the larger array of options.</p>
<h3 class="h3"><a id="lev3.1"></a><a id="pg29lev1"></a><span class="blue"><span class="font">3.1</span></span> Forms of Reality</h3>
<p class="noindent">Reality takes many forms and can be considered to range on a virtuality continuum from the real environment to virtual environments [<a href="reference.html#ref214"><span class="blue">Milgram and Kishino 1994</span></a>]. Figure <a href="chapter03.html#fig3.1"><span class="blue">3.1</span></a> shows various forms along that continuum. These forms, which are somewhere between virtual and augmented reality, are broadly defined as &#8220;mixed reality,&#8221; which can be further broken down into &#8220;augmented reality&#8221; and &#8220;augmented virtuality.&#8221; This book focuses on the right side of the continuum from augmented virtuality to virtual environments.</p>
<p class="indent">The <strong>real environment</strong> is the real world that we live in. Although creating real-world experiences is not always the goal of VR, it is still important to understand the real world and how we perceive and interact with it in order to replicate relevant functionality into VR experiences. What is relevant depends on the goals of the application. Section <a href="chapter04.html#lev4.4"><span class="blue">4.4</span></a> further discusses trade-offs of realism vs. more abstract implementations of reality. Part <a href="part02.html#part2"><span class="blue">II</span></a> discusses how we perceive real environments in order to help build better fully immersive virtual environments.</p>
<p class="indent">Instead of replacing reality, <strong>augmented reality</strong> (AR) adds cues onto the already existing real world, and ideally the human mind would not be able to distinguish between computer-generated stimuli and the real world. This can take various forms, some of which are described in Section <a href="chapter03.html#lev3.2"><span class="blue">3.2</span></a>.</p>
<p class="image"><a id="page_30"></a><a id="fig3.1"></a><img src="../images/f0030-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.1</span> The virtuality continuum.</strong> (Adapted from <a href="reference.html#ref214"><span class="blue">Milgram and Kishino</span></a> [<a href="reference.html#ref214"><span class="blue">1994</span></a>])</p>
<p class="indent"><strong>Augmented virtuality</strong> (AV) is the result of capturing real-world content and bringing that content into VR. Immersive film is an example of augmented virtuality. In the simplest case, the capture is taken from a single viewpoint, but in other cases, real-world capture can consist of light fields or geometry, where users can freely move about the environment, perceiving it from any perspective. Section <a href="chapter21.html#lev21.6"><span class="blue">21.6</span></a> provides some examples of augmented virtuality.</p>
<p class="indent">True <strong>virtual environments</strong> are artificially created without capturing any content from the real world. The goal of virtual environments is to completely engage a user in an experience so that she feels as if she is present (Chapter <a href="chapter04.html#ch4"><span class="blue">4</span></a>) in another world such that the real world is temporarily forgotten, while minimizing any adverse effects (Part <a href="part03.html#part3"><span class="blue">III</span></a>).</p>
<h3 class="h3"><a id="lev3.2"></a><a id="pg30lev1"></a><span class="blue"><span class="font">3.2</span></span> Reality Systems</h3>
<p class="chblock"><em>The screen is a window through which one sees a virtual world. The challenge is to make that world look real, act real, sound real, feel real</em>.</p>
<p class="chattribute">&#8212;<a href="reference.html#ref302"><span class="blue">Sutherland</span></a> [<a href="reference.html#ref302"><span class="blue">1965</span></a>]</p>
<p class="noindent">A <strong>reality system</strong> is the hardware and operating system that full sensory experiences are built upon. The reality system&#8217;s job is to effectively communicate the application content to and from the user in an intuitive way as if the user is interacting with the real world. Humans and computers do not speak the same language so the reality system must act as a translator or intermediary between them (note the reality system also includes the computer). It is the VR creator&#8217;s obligation to integrate content with the system so the intermediary is transparent and to ensure objects and system behaviors are consistent with the intended experience. Ideally, the technology will not be perceived so that users forget about the interface and experience the artificial reality as if it is real.</p>
<p class="indent">Communication between the human and system is achieved via hardware devices. These devices serve as input and/or output. A transfer function, as it relates to interaction, is a conversion from human output to digital input or from digital output to human input. What is output and what is input depends on whether it is from the point of view of the system or the human. For consistency, input is considered information traveling from the user into the system and output is feedback that goes from the system back to the user. This forms a cycle of input/output that continuously occurs for as long as the VR experience lasts. This loop can be thought of as occurring between the action and distal stimulus stages of the perceptual process (Figure <a href="chapter07.html#fig7.2"><span class="blue">7.2</span></a>) where the user is the perceptual process.</p>
<p class="image"><a id="page_31"></a><a id="fig3.2"></a><img src="../images/f0031-01.png" alt="image"/></p>
<p class="caption"><a id="pg31lev1"></a><strong><span class="blue">Figure 3.2</span> A VR system consists of input from the user, the application, rendering, and output to the user.</strong> (Adapted from <a href="reference.html#ref143"><span class="blue">Jerald</span></a> [<a href="reference.html#ref143"><span class="blue">2009</span></a>])</p>
<p class="indent">Figure <a href="chapter03.html#fig3.2"><span class="blue">3.2</span></a> shows a user and a VR system divided into their primary components of input, application, rendering, and output. <strong>Input</strong> collects data from the user such as where the user&#8217;s eyes are located, where the hands are located, button presses, etc. The <strong>application</strong> includes non-rendering aspects of the virtual world including updating dynamic geometry, user interaction, physics simulation, etc. <strong>Rendering</strong> is the transformation of a computer-friendly format to a user-friendly format that gives the illusion of some form of reality and includes visual rendering, auditory <a id="page_32"></a>rendering (called auralization), and haptic (the sense of touch) rendering. An example of rendering is drawing a sphere. Rendering is already well defined (e.g., <a href="reference.html#ref87"><span class="blue">Foley et al. 1995</span></a>) and other than high-level descriptions and elements that directly affect the user experience the technical details are not the focus of this book. <strong>Output</strong> is the physical representation directly perceived by the user (e.g., a display with pixels or headphones with sound waves).</p>
<p class="indent">The primary output devices used for VR are visual displays, speakers, haptics, and motion platforms. More exotic displays include olfactory (smell), wind, heat, and even taste displays. Input devices are only briefly mentioned in this chapter as they are described in detail in Chapter <a href="chapter27.html#ch27"><span class="blue">27</span></a>. Selecting appropriate hardware is an essential part of designing VR experiences. Some hardware may be more appropriate for some designs than others. For example, large screens are more appropriate than head-mounted displays for large audiences located at the same physical location. The following sections provide an overview of some commonly used VR hardware.</p>
<h4 class="h4"><a id="lev3.2.1"></a><span class="font1">3.2.1</span> Visual Displays</h4>
<p class="noindent">Today&#8217;s reality systems are implemented in one of three ways: head-mounted displays, world-fixed displays, and hand-held displays.</p>
<h5 class="h5"><a id="pg32lev1"></a>Head-Mounted Displays</h5>
<p class="noindent">A <strong>head-mounted display</strong> (HMD) is a visual display that is more or less rigidly attached to the head. Figure <a href="chapter03.html#fig3.3"><span class="blue">3.3</span></a> shows some examples of different HMDs. Position and orientation tracking of HMDs is essential for VR because the display and earphones move with the head. For a virtual object to appear stable in space, the display must be appropriately updated as a function of the current pose of the head; for example, as the user rotates his head to the left, the computer-generated image on the display should move to the right so that the image of the virtual objects appears stable in space, just as real-world objects are stable in space as people turn their heads. Well-implemented HMDs typically provide the greatest amount of immersion. However, doing this well consists of many challenges such as accurate tracking, low latency, and careful calibration.</p>
<p class="indent">HMDs can be further broken down into three types: non-see-through HMDs, video-see-through HMDs, and optical-see-through HMDs. <strong>Non-see-through HMDs</strong> block out all cues from the real world and provide optimal full immersion conditions for VR. <strong>Optical-see-through HMDs</strong> enable computer-generated cues to be overlaid onto the visual field and provide the ideal augmented reality experience. Conveying the ideal augmented reality experience using optical-see-through head-mounted displays is extremely challenging due to various requirements (extremely low latency, extremely accurate tracking, optics, etc.). Due to these challenges, <strong>video-see-through HMDs</strong> are sometimes used. Video-see-through HMDs are often considered to be augmented virtuality (Section <a href="chapter03.html#lev3.1"><span class="blue">3.1</span></a>), and have some advantages and disadvantages of both augmented reality and virtual reality.</p>
<p class="image"><a id="page_33"></a><a id="pg33lev1"></a><a id="fig3.3"></a><img src="../images/f0033-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.3</span> The Oculus Rift (upper left; courtesy of Oculus VR), CastAR (upper right; courtesy of CastAR), the Joint-Force Fighter Helmet (lower left; courtesy of Marines Magazine), and a custom built/modified HMD (lower right; from <a href="reference.html#ref145"><span class="blue">Jerald et al.</span></a> [<a href="reference.html#ref145"><span class="blue">2007</span></a>]).</strong></p>
<h5 class="h5">World-Fixed Displays</h5>
<p class="noindent"><strong>World-fixed displays</strong> render graphics onto surfaces and audio through speakers that do not move with the head. Displays take many forms, ranging from a standard monitor (also known as fish-tank VR) to displays that completely surround the user (e.g., CAVEs and CAVE-like displays as shown in Figures <a href="chapter03.html#fig3.4"><span class="blue">3.4</span></a> and <a href="chapter03.html#fig3.5"><span class="blue">3.5</span></a>). Display surfaces are typically flat surfaces, although more complex shapes can be used if those shapes are well defined or known, as shown in Figure <a href="chapter03.html#fig3.6"><span class="blue">3.6</span></a>. Head tracking is important for world-fixed displays, but accuracy and latency requirements are typically not as critical <a id="pg34lev1"></a>as they are for head-mounted displays because stimuli are not as dependent upon head motion. High-end world-fixed displays with multiple surfaces and projectors can be highly immersive but are more expensive in dollars and space.</p>
<p class="image"><a id="page_34"></a><a id="fig3.4"></a><img src="../images/f0034-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.4</span> Conceptual drawing of a CAVE (left). Users are surrounded with stereoscopic perspective-correct images displayed on the floor and walls that they interact with. The CABANA (right) has movable walls so that the display can be configured into different display shapes such as a wall or L-shape.</strong> (From <a href="reference.html#ref52"><span class="blue">Cruz et al.</span></a> [<a href="reference.html#ref52"><span class="blue">1992</span></a>] (left) and <a href="reference.html#ref60"><span class="blue">Daily et al.</span></a> [<a href="reference.html#ref60"><span class="blue">1999</span></a>] (right))</p>
<p class="indent">World-fixed displays typically are considered to be part virtual reality and part augmented reality. This is because real-world objects are easily integrated into the experience, such as the physical chair shown in Figure <a href="chapter03.html#fig3.7"><span class="blue">3.7</span></a>. However, it is often the intent that the user&#8217;s body is the only visible real-world cue.</p>
<h5 class="h5">Hand-Held Displays</h5>
<p class="noindent"><strong>Hand-held displays</strong> are output devices that can be held with the hand(s) and do not require precise tracking or alignment with the head/eyes (in fact the head is rarely tracked for hand-held displays). Hand-held augmented reality, also called indirect augmented reality, has recently become popular due to the ease of access and improvements in smartphones/tablets (Figure <a href="chapter03.html#fig3.8"><span class="blue">3.8</span></a>). In addition, system requirements are much less since viewing is indirect&#8212;rendering is independent of the user&#8217;s head and eyes.</p>
<h4 class="h4"><a id="lev3.2.2"></a><span class="font1">3.2.2</span> Audio</h4>
<p class="noindent"><strong>Spatialized audio</strong> provides a sense of where sounds are coming from in 3D space. Speakers can be fixed in space or move with the head. Headphones are preferred for a fully immersive system as they block out more of the real world. How the ears and brain perceive sound is discussed in Section <a href="chapter08.html#lev8.2"><span class="blue">8.2</span></a>. Audio more specific to how content is created for VR is discussed in Section <a href="chapter21.html#lev21.3"><span class="blue">21.3</span></a>.</p>
<p class="image"><a id="page_35"></a><a id="fig3.5"></a><img src="../images/f0035-01.jpg" alt="image"/></p>
<p class="caption"><a id="pg35lev1"></a><strong><span class="blue">Figure 3.5</span> The author interacting with desktop applications within the CABANA.</strong> (From <a href="reference.html#ref144"><span class="blue">Jerald et al.</span></a> [<a href="reference.html#ref144"><span class="blue">2001</span></a>])</p>
<p class="image"><a id="fig3.6"></a><img src="../images/f0035-02.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.6</span> Display surfaces do not necessarily need to be planar.</strong> (From <a href="reference.html#ref167"><span class="blue">Krum et al.</span></a> [<a href="reference.html#ref167"><span class="blue">2012</span></a>])</p>
<p class="image"><a id="page_36"></a><a id="fig3.7"></a><img src="../images/f0036-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.7</span> The University of Southern California&#8217;s Gunslinger uses a mix of the real world along with world-fixed displays.</strong> (Courtesy of USC Institute for Creative Technologies)</p>
<p class="image"><a id="fig3.8"></a><img src="../images/f0036-02.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.8</span> Zoo-AR from GeoMedia and a virtual assistant that appears on a business card from NextGen Interactions.</strong> (Courtesy of Geomedia (left) and NextGen Interactions (right))</p>
<h4 class="h4"><a id="lev3.2.3"></a><span class="font1">3.2.3</span> Haptics</h4>
<p class="noindent"><strong>Haptics</strong> are artificial forces between virtual objects and the user&#8217;s body. Haptics can be classified as passive (static physical objects) or active (physical feedback controlled by <a id="page_37"></a>the computer), tactile (through skin) or proprioceptive force (through joints/muscles), and self-grounded (worn) or world-grounded (attached to real world). Many haptic systems also serve as input devices.</p>
<h5 class="h5">Passive vs. Active Haptics</h5>
<p class="noindent"><strong>Passive haptics</strong> provide a sense of touch in VR at a low cost&#8212;one simply creates a real-world physical object and matches that object to the shape of a virtual object [<a href="reference.html#ref186"><span class="blue">Lindeman et al. 1999</span></a>]. These physical objects can be hand-held props or larger objects in the world that can be touched. Passive haptics increases presence, improves cognitive mapping of the environment, and improves training performance [<a href="reference.html#ref137"><span class="blue">Insko 2001</span></a>].</p>
<p class="indent">Touching a few objects with passive haptics can make everything else seem more real. Perhaps the most compelling VR experience to this day is the legendary UNC-Chapel Hill Pit demo [<a href="reference.html#ref211"><span class="blue">Meehan et al. 2002</span></a>]. Users first experience a virtual room that includes passive haptics made from Styrofoam blocks and other real-world material to match the visual VR environment. After touching different parts of the room, users walk into a second room and see a pit in the floor. The pit is quite compelling (in fact, heart rate increases) because everything else they have touched up to this point <a id="pg37lev1"></a>has physically felt real, thus users assume the pit is physically real as well. There is an even more startling response from many users when they put their toe over the virtual ledge and feel a real ledge. What they don&#8217;t realize is the physical ledge is only a 1.5 inch drop-off compared to the visual pit that is 20 feet deep.</p>
<p class="indent"><strong>Active haptics</strong> are controlled by a computer and are the most common form of haptics. Active haptics have the advantage that forces can be dynamically controlled to provide a feeling of a wide range of simulated virtual objects. The remainder of this section focuses on active haptics.</p>
<h5 class="h5">Tactile vs. Proprioceptive Force Haptics</h5>
<p class="noindent"><strong>Tactile haptics</strong> provide a sense of touch through the skin. Vibrotactile stimulation evokes tactile sensations using mechanical vibration of the skin. Electrotactile stimulation evokes tactile sensation via an electrode passing current through the skin.</p>
<p class="indent">Figure <a href="chapter03.html#fig3.9"><span class="blue">3.9</span></a> shows Tactical Haptics&#8217; Reactive Grip technology, which provides a sense of tactile feedback that is surprisingly compelling, especially when combined with fully immersive visual displays [<a href="reference.html#ref252"><span class="blue">Provancher 2014</span></a>]. The system utilizes sliding skin-contact plates that can be added to any hand-held controller. Translational motions and forces are portrayed along the length of the grip by moving the plates in unison. Opposing motion and forces from different plates creates the feeling of a virtual object wrenching within the user&#8217;s grasp.</p>
<p class="image"><a id="page_38"></a><a id="fig3.9"></a><img src="../images/f0038-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.9</span> Tactical Haptics technology uses sliding plates to provide a sense of up or down force as well as rotational torque. The rightmost image shows the sliding plates integrated into the latest controller design.</strong> (Courtesy of Tactical Haptics)</p>
<p class="image"><a id="fig3.10"></a><img src="../images/f0038-02.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.10</span> The Dexta Robotics Dexmo F2 device provides both finger tracking and force feedback.</strong> (Courtesy of Dexta Robotics)</p>
<p class="indent"><strong>Proprioceptive force</strong> provides a sense of limb movement and muscular resistance. Proprioceptive haptics can be self-grounded or world-grounded.</p>
<h5 class="h5">Self-Grounded vs. World-Grounded Haptics</h5>
<p class="noindent"><strong>Self-grounded haptics</strong> are worn/held by and move with the user. The forces applied are relative to the user. Gloves with exoskeletons or buzzers are examples of self-grounded haptics. Figure <a href="chapter03.html#fig3.10"><span class="blue">3.10</span></a> shows an exoskeleton glove. Hand-held controllers are also examples of self-grounded haptics. Such controllers might be simply a passive <a id="pg39lev1"></a>prop that acts as a handle to virtual objects or might be rumble controllers that vibrate to provide feedback to the user (e.g., to signify the user has put his hand through a virtual object).</p>
<p class="image"><a id="page_39"></a><a id="fig3.11"></a><img src="../images/f0039-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.11</span> Sensable&#8217;s Phantom haptics system.</strong> (Courtesy of INITION)</p>
<p class="indent"><strong>World-grounded haptics</strong> are physically attached to the real world and can provide a true sense of fully solid objects that don&#8217;t move because the position of the virtual object providing the force can remain stable relative to the world. The ease of which the object can be moved can also be felt, providing a sense of weight and friction [<a href="reference.html#ref51"><span class="blue">Craig et al. 2009</span></a>].</p>
<p class="indent">Figure <a href="chapter03.html#fig3.11"><span class="blue">3.11</span></a> shows Sensable&#8217;s Phantom haptic device, which provides stable force feedback for a single point in space (the tip of the stylus). Figure <a href="chapter03.html#fig3.12"><span class="blue">3.12</span></a> shows Cyberglove&#8217;s CyberForce glove that provides the sense of touching real objects with the entire hand as if the objects were stationary in the world.</p>
<h4 class="h4"><a id="lev3.2.4"></a><span class="font1">3.2.4</span> Motion Platforms</h4>
<p class="noindent">A <strong>motion platform</strong> is a hardware device that moves the entire body resulting in a sense of physical motion and gravity. Such motions can help to convey a sense of orientation, vibration, acceleration, and jerking. Common uses of platforms are for racing games, flight simulation, and location-based entertainment. When integrated well with the rest of a VR application, motion sickness can be reduced by decreasing the conflict between visual motion and felt motion. Section <a href="chapter18.html#lev18.8"><span class="blue">18.8</span></a> discusses how motion platforms can be used to reduce motion sickness.</p>
<p class="image"><a id="page_40"></a><a id="fig3.12"></a><img src="../images/f0040-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.12</span> Cyberglove&#8217;s Cyberforce Immersive Workstation.</strong> (Courtesy of Haptic Workstation with HMD at VRLab in EPFL, Lausanne, 2005)</p>
<p class="indent">Motion platforms can be active or passive. An <strong>active motion platform</strong> is controlled by the computer simulation. Figure <a href="chapter03.html#fig3.13"><span class="blue">3.13</span></a> shows an example of an active motion platform that moves a base platform via hydraulic actuators. A <strong>passive motion platform</strong> is controlled by the user. For example, the tilting of a passive motion platform might be achieved by leaning forward, such as used with Birdly shown in Figure <a href="chapter03.html#fig3.14"><span class="blue">3.14</span></a>. Note active and passive as described here are from the point of view of the motion platform and system. When describing motion from the point of view of the user, passive implies the user is passively along for the ride, with no way to influence the experience, and active implies the user is actively influencing the experience.</p>
<h4 class="h4"><a id="lev3.2.5"></a><span class="font1">3.2.5</span> Treadmills</h4>
<p class="noindent"><strong>Treadmills</strong> provide a sense that one is walking or running while actually staying in one place. Variable-incline treadmills, individual foot platforms, and mechanical tethers <a id="pg42lev1"></a>providing restraint can convey hills by manipulating the physical effort required to travel forward.</p>
<p class="image"><a id="page_41"></a><a id="fig3.13"></a><img src="../images/f0041-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.13</span> An active motion platform that moves via hydraulic actuators. A chair can be attached to the top of the platform.</strong> (Courtesy of Shifz, Syntharturalist Art Association)</p>
<p class="image"><a id="fig3.14"></a><img src="../images/f0041-02.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.14</span> Birdly by Somniacs. In addition to providing visual, auditory, and motion cues, this VR experience provides a sense of taste and smell.</strong> (Courtesy of Swissnex San Francisco and Myleen Hollero)</p>
<p class="image"><a id="page_42"></a><a id="fig3.15"></a><img src="../images/f0042-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 3.15</span> The Virtuix Omni.</strong> (Courtesy of Virtuix)</p>
<p class="indent"><strong>Omnidirectional treadmills</strong> enable simulation of physical travel in any direction and can be active or passive.</p>
<p class="indent"><strong>Active omnidirectional treadmills</strong> have computer-controlled mechanically moving parts. These treadmills move the treadmill surface in order to recenter the user on the treadmill (e.g., <a href="reference.html#ref62"><span class="blue">Darken et al. 1997</span></a> and <a href="reference.html#ref140"><span class="blue">Iwata 1999</span></a>). Unfortunately, such recentering can cause the user to lose balance.</p>
<p class="indent"><strong>Passive omnidirectional treadmills</strong> contain no computer-controlled mechanically moving parts. For example, the feet might slide along a low-friction surface (e.g., the Virtuix Omni as shown in Figure <a href="chapter03.html#fig3.15"><span class="blue">3.15</span></a>). A harness and surrounding encasing keeps the user from falling. Like other forms of non-real walking, the experience of walking on a passive treadmill does not perfectly match the real world (it feels more like walking on slippery ice), but can add a significant amount of presence and reduce motion sickness.</p>
<h4 class="h4"><a id="lev3.2.6"></a><span class="font1"><span class="blue">3.2.6</span></span> Other Sensory Output</h4>
<p class="noindent">VR largely focuses on the display component, but other components such as taste, smell, and wind can more fully immerse users and add to a VR experience. Figure <a href="chapter03.html#fig3.14"><span class="blue">3.14</span></a> <a id="page_43"></a>shows Birdly by Somniacs, a system that adds smells and wind to a VR experience. Section <a href="chapter08.html#lev8.6"><span class="blue">8.6</span></a> discusses the senses of taste and smell.</p>
<h4 class="h4"><a id="lev3.2.7"></a><span class="font1"><span class="blue">3.2.7</span></span> Input</h4>
<p class="noindent">A fully immersive VR experience is more than simply presenting content. The more a user physically interacts with a virtual world using his own body in intuitive ways, the more that user feels engaged and present in that virtual world. VR interactions consist of both hardware and software working closely together in complex ways, yet the best interaction techniques are simple and intuitive to use. Designers must take into account input device capabilities when designing experiences&#8212;one input device might work well for one type of interaction but be inappropriate for another. Other interactions might work across a wider range of input devices. Part <a href="part05.html#part5"><span class="blue">V</span></a> contains multiple chapters on interaction with Chapter <a href="chapter27.html#ch27"><span class="blue">27</span></a> focusing exclusively on input devices.</p>
<h4 class="h4"><a id="lev3.2.8"></a><span class="font1"><span class="blue">3.2.8</span></span> Content</h4>
<p class="noindent">VR cannot exist without content. The more compelling the content, the more interesting and engaging the experience. Content includes not only the individual pieces of media and their perceptual cues, but also the conceptual arc of the story, the design/layout of the environment, and computer- or user-controlled characters. Part <a href="part04.html#part4"><span class="blue">IV</span></a> contains multiple chapters dedicated to content creation.</p>
</body>
</html>
