<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" >
<head>
<title>The VR Book</title>
<link rel="stylesheet" type="text/css" href="../styles/9781970001143.css"/>
</head>
<body>
<h2 class="h2"><a id="page_110"></a><a id="page_111"></a><a id="ch9"></a><span class="blue1">9</span></h2>
<h2 class="h2b"><span class="blue">Perception of Space and Time</span></h2>
<p class="noindent">How do we perceive the layout, timing, and motion of objects in the world around us? This question is not new, as it has fascinated artists, philosophers, and psychologists for centuries. The success of painting, photography, cinema, video games, and now VR is largely contingent upon the convincing portrayal of spatial 3D relationships and their timings.</p>
<h3 class="h3"><a id="pg111lev1"></a><a id="lev9.1"></a><strong><span class="blue"><span class="font">9.1</span></span> Space Perception</strong></h3>
<p class="noindent">Trompe-l&#8217;&#339;il art (Figure II.1) and the Ames room (Figure <a href="chapter06.html#fig6.9"><span class="blue">6.9</span></a>) prove the eye can be fooled when it comes to space perception. However, these illusions are very constrained and only work when viewed from a specific viewpoint&#8212;when the observer moves and explores resulting in the perception of more information, then the illusion goes away and the truth becomes more apparent. It is relatively easy to fool observers from a single perspective, compared to fooling VR users into believing they are present in another world.</p>
<p class="indent">If the senses can be deceived to misinterpret space, then how can we be sure the senses do not deceive us all the time? Perhaps we cannot be 100% sure of the space around us. However, evolution tells us that our perceptions must be truthful to some degree&#8212;otherwise if our perceptions were overly deceived, we would have become extinct long ago [<a href="reference.html#ref57"><span class="blue">Cutting and Vishton 1995</span></a>]. Perhaps deception of space is the goal of VR, and if we can someday completely overcome the challenges of VR, then we can defeat the argument from evolution, and hopefully not extinguish ourselves from existence in the process.</p>
<p class="indent">Multiple sensory modalities often work together to give us spatial location. Vision is the superior spatial modality as it is more precise, is faster for perceiving location (although audition is faster when the task is detection), is accurate for all three <a id="page_112"></a>directions (whereas audition, for example, is reasonably good only in the lateral direction), and works well at long distances [<a href="reference.html#ref329"><span class="blue">Welch and Warren 1986</span></a>].</p>
<p class="indent">Perceiving the space around us is not just about passive perception but is important for actively moving through the world (Section <a href="chapter10.html#lev10.4.3"><span class="blue">10.4.3</span></a>) and interaction (Section <a href="chapter26.html#lev26.2"><span class="blue">26.2</span></a>).</p>
<h4 class="h4"><a id="lev9.1.1"></a><strong><span class="blue"><span class="font1">9.1.1</span></span> Exocentric and Egocentric Judgments</strong></h4>
<p class="noindent"><strong>Exocentric judgments,</strong> also known as object-relative judgments, are the sense of where objects are relative to other objects, the world, or some other external reference. Exocentric judgments include the sense of gravity, geographic direction (e.g., north), and distance between two objects. Gogel&#8217;s principle of adjacency states &#8220;The effectiveness of relative cues decreases as the perceived distance between objects producing the relative cues increases&#8221; [<a href="reference.html#ref193"><span class="blue">Mack 1986</span></a>]. That is, objects are more difficult to compare the further they are apart.</p>
<p class="indent">The perception of the location of objects also includes the direction and distance relative to our bodies. <strong>Egocentric judgments,</strong> also known as subject-relative judgments, are the sense of where (direction and distance) cues are relative to the observer. Left/right and forward/backward judgments are egocentric. Egocentric judgments can be performed by the primary sensory modalities of vision, audition, proprioception, <a id="pg112lev1"></a>and touch.</p>
<p class="indent">Several variables affect our egocentric perception. The more stimuli available, the more stable our judgments; our judgments are much less stable in the dark as demonstrated by the autokinetic effect (Section <a href="chapter06.html#lev6.2.7"><span class="blue">6.2.7</span></a>). A stimulus that is not centered in the visual field (e.g., an offset square) can result in the straight-ahead being perceived as biased toward that off-centered stimulus. The <strong>dominant eye,</strong> the eye that is used for sighting tasks, also influences direction judgments. Eye movement (Section <a href="chapter08.html#lev8.1.5"><span class="blue">8.1.5</span></a>) also affect our judgment of the straight-ahead.</p>
<p class="indent">Reference frames of exocentric and egocentric space as they relate to VR are discussed in Section <a href="chapter26.html#lev26.3"><span class="blue">26.3</span></a>. Such reference frames are especially important for designing VR interfaces.</p>
<h4 class="h4"><a id="lev9.1.2"></a><strong><span class="blue"><span class="font1">9.1.2</span></span> Segmenting the Space around Us</strong></h4>
<p class="noindent">The layout of the space around each of us can be divided into three circular egocentric regions: personal space, action space, and vista space [<a href="reference.html#ref57"><span class="blue">Cutting and Vishton 1995</span></a>].</p>
<p class="indent">Our <strong>personal space</strong> is generally considered to be the natural working volume within arm&#8217;s reach and slightly beyond. We typically are only comfortable with others in this space in intimate situations or due to public necessity (e.g., a subway or concert). This space is within 2 meters of the person&#8217;s viewpoint, which includes users&#8217; feet while standing and the space users can physically manipulate without requiring locomotion. <a id="page_113"></a>Working within personal space offers advantages of proprioceptive cues, more direct mapping between hand motion and object motion, stronger stereopsis and head motion parallax cues, and finer angular precision of motion [<a href="reference.html#ref219"><span class="blue">Min&#233; et al. 1997</span></a>].</p>
<p class="indent">Beyond personal space, <strong>action space</strong> is the space of public action. In action space, we can move relatively quickly, speak to others, and toss objects. This space starts at about 2 meters and extends to about 20 meters from the user.</p>
<p class="indent">Beyond 20 meters is <strong>vista space,</strong> where we have little immediate control and perceptual cues are fairly consistent (e.g., binocular vision is almost non-existent). Because effective depth cues are pictorial at this range, large 2D trompe-l&#8217;&#339;il paintings at this range are most effective in deceiving the eye that the space is real, as demonstrated in Figure <a href="chapter09.html#fig9.1"><span class="blue">9.1</span></a> showing the ceiling of the Jesuit Church in Vienna painted by Andrea Pozzo.</p>
<p class="image"><a id="pg113lev1"></a><a id="fig9.1"></a><img src="../images/f0113-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 9.1</span> This trompe-l&#8217;&#339;il art on the dome of the Jesuit Church in Vienna demonstrates that at a distance it can be difficult to distinguish 2D painting from 3D architecture.</strong></p>
<h4 class="h4"><a id="page_114"></a><a id="lev9.1.3"></a><strong><span class="blue"><span class="font1">9.1.3</span></span> Depth Perception</strong></h4>
<p class="noindent">This page you are reading is likely less than a meter away and the words and lettering are less than a centimeter in height. If you look further up you might see walls, windows, and various objects further than a meter. You can intuitively estimate these distances and interact with objects without thinking much about distance even though the projection of those objects onto the retinas of your eyes are 2D images. Based on these projected scenes of the world onto the surface of the retinas alone, we somehow are able to intuitively perceive how far away objects are.</p>
<p class="indent">How is it that we can take individual photons that fall onto the retina&#8217;s surface and re-create from them an understanding of a 3D world? If spatial perception was solely a function of a 2D image on each retina (Section <a href="chapter08.html#lev8.1.1"><span class="blue">8.1.1</span></a>), then there would be no way to know if a particular visual stimulus was a foot away or a mile away. Clearly, there must be more going on than simply detecting points of light on the retina.</p>
<p class="indent">Higher-level processing occurs that results in the perception of a 3D world. Human perception relies on a large number of ways we perceive space, shapes, and distance. In this section, several cues are described that help us perceive egocentric depth or distance. The more cues available and the more consistent they are with each other, the stronger the perception of depth. If implemented well in a VR system, such depth cues can significantly add to presence.</p>
<p class="indent"><strong>Depth cues</strong> can be organized into four categories: pictorial cues, motion cues, binocular cues, and oculomotor cues. The context or psychological state of the user can also affect a person&#8217;s judgment of depth/distance. Table <a href="chapter09.html#tab9.1"><span class="blue">9.1</span></a> shows an organization of the cues and factors described in this section, and Table <a href="chapter09.html#tab9.2"><span class="blue">9.2</span></a> shows the approximate order of importance for several of the specific cues.</p>
<p class="noindentt"><strong>Pictorial Depth Cues</strong></p>
<p class="noindent"><strong>Pictorial depth cues</strong> come from distal stimuli projecting photons onto the retina (proximal stimuli), resulting in 2D images. These pictorial cues are present even when viewed with only a single eye and when that eye or anything in the view is not moving. Described below are pictorial cues listed in approximate declining order of importance.</p>
<p class="noindentt"><strong>Occlusion. Occlusion</strong>&#160;&#160;&#160;is the strongest depth cue as it is always the case for close opaque objects to hide other more distant objects and is effective over the entire range of perceivable distances. Although occlusion is an important depth cue, it only provides relative depth information, and thus precision is dependent upon the number of objects in the scene.</p>
<p class="indent">Occlusion is an especially important concept when designing VR displays. Many heads-up displays found in 2D video games do not work well with VR displays due to conflicts with occlusion, motion, physiological, and binocular cues. Because of these conflicts, 2D heads-up displays should be transformed into 3D geometry that has a distance from the viewpoint so that occlusion is properly handled. Otherwise users will get confused when the text or geometry appears at a distance but is not occluded by closer objects and can lead to eye strain and headaches (Section <a href="chapter13.html#lev13.2"><span class="blue">13.2</span></a>).</p>
<p class="tabcaption"><a id="page_115"></a><a id="tab9.1"></a><strong><span class="blue">Table 9.1</span> Organization of depth cues and factors presented in this chapter.</strong></p>
<div class="bgcolor2">
<p class="boxbullet">&#8226; Pictorial Depth Cues</p>
</div>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Occlusion</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Linear perspective</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Relative/familiar size</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Shadows/shading</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Texture gradient</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Height relative to horizon</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Aerial perspective</p>
<div class="bgcolor2">
<p class="boxbullet">&#8226; Motion Depth Cues</p>
</div>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Motion parallax</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Kinetic depth effect</p>
<div class="bgcolor2">
<p class="boxbullet">&#8226; Binocular Depth Cues</p>
</div>
<div class="bgcolor2">
<p class="boxbullet">&#8226; Oculomotor Depth Cues</p>
</div>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Vergence</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Accommodation</p>
<div class="bgcolor2">
<p class="boxbullet">&#8226; Contextual Distance Factors</p>
</div>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Intended action</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Fear</p>
<p class="tabcaption"><a id="tab9.2"></a><strong><span class="blue">Table 9.2</span> Approximate order of importance (with 1 being most important) of visual cues for perceiving egocentric distance in personal space, action space, and vista space.</strong></p>
<p class="tabimage"><img src="../images/f0115-01.png" alt="image"/></p>
<p class="image"><a id="page_116"></a><a id="fig9.2"></a><img src="../images/f0116-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 9.2</span> Linear perspective causes parallel lines receding into the distance to appear to converge at the vanishing point.</strong></p>
<p class="noindentt"><strong>Linear perspective. Linear perspective</strong>&#160;&#160;&#160;causes parallel lines that recede into the distance to appear to converge together at a single point called the vanishing point (Figure <a href="chapter09.html#fig9.2"><span class="blue">9.2</span></a>). The Ponzo railroad illusion in Figure <a href="chapter06.html#fig6.8"><span class="blue">6.8</span></a> demonstrates how strong of a cue linear perspective is. The sense of depth provided by linear perspective is so strong that non-realistic grid patterns may be more effective to conveying spatial presence than more common textures with little detail such as carpet or flat walls.</p>
<p class="noindentt"><strong>Relative/familiar size.</strong>&#160;&#160;&#160;When two objects are of equal size, the further object&#8217;s projected image on the retina will take up less visual angle than the closer object. <strong>Relative/familiar size</strong> enables us to estimate distance when we know the size of an object, whether due to prior knowledge or comparison to other objects in the scene (Figure <a href="chapter09.html#fig9.3"><span class="blue">9.3</span></a>). Seeing a representation of one&#8217;s own body not only adds presence (Section <a href="chapter04.html#lev4.3"><span class="blue">4.3</span></a>) but can also serve as a relative-size depth cue since users know the size of their own body.</p>
<p class="noindentt"><strong>Shadows/shading. Shadows and shading</strong>&#160;&#160;&#160;can provide information regarding the location of objects. Shadows can immediately tell us if an object is lying on the ground or floating above the ground (Figure <a href="chapter09.html#fig9.4"><span class="blue">9.4</span></a>). Shadows can also give us clues about the shape (and different depths of a single object).</p>
<p class="image"><a id="page_117"></a><a id="fig9.3"></a><img src="../images/f0117-01.jpg" alt="image"/></p>
<p class="caption"><a id="pg117lev1"></a><strong><span class="blue">Figure 9.3</span> Relative/familiar size. The visual angle of an object of known size provides a sense of distance to that object. Two identical objects having different angular sizes implies the smaller object is at a further distance.</strong></p>
<p class="image"><a id="fig9.4"></a><img src="../images/f0117-02.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 9.4</span> Example of how objects can appear at different depth and heights depending on where their shadows lie.</strong> (Based on <a href="reference.html#ref159"><span class="blue">Kersten et al.</span></a> [<a href="reference.html#ref159"><span class="blue">1997</span></a>])</p>
<p class="noindentt"><strong>Texture gradient.</strong>&#160;&#160;&#160;Most natural objects contain visual texture. <strong>Texture gradient</strong> is similar to linear perspective; texture density increases with distance between the eye and the object surface (Figure <a href="chapter09.html#fig9.5"><span class="blue">9.5</span></a>).</p>
<p class="image"><a id="page_118"></a><a id="fig9.5"></a><img src="../images/f0118-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 9.5</span> The textures of objects can provide a sense of shape and depth. The center of the image seems closer than the edges due to texture gradient.</strong></p>
<p class="noindentt"><strong>Height relative to horizon. Height relative to the horizon</strong>&#160;&#160;&#160;makes objects appear to be further away when they are closer to the horizon (Figure <a href="chapter09.html#fig9.6"><span class="blue">9.6</span></a>).</p>
<p class="noindentt"><strong>Aerial perspective.</strong>&#160;&#160;&#160;Aerial perspective, also called atmospheric perspective, is a depth cue where objects with more contrast (e.g., brighter, sharper, and more colorful) appear closer than duller objects (Figure <a href="chapter09.html#fig9.7"><span class="blue">9.7</span></a>). This results from scattering of light by particles in the atmosphere.</p>
<p class="noindentt"><strong>Motion Depth Cues</strong></p>
<p class="noindent"><strong>Motion cues</strong> are relative movements on the retina.</p>
<p class="noindentt"><strong>Motion parallax. Motion parallax</strong>&#160;&#160;&#160;is a depth cue where the images of distal stimuli projected onto the retina (proximal stimuli) move at different rates depending on their distance. For example, when you look outside the side window of a moving car, nearby objects appear to speed by in a blur, whereas distant objects appear to move more slowly. If while moving, you focus on a single point called the fixation point, objects further and closer than that fixation point appear to move in opposite directions (closer objects move against the direction of physical motion and further objects appear to move with the direction of physical motion). Motion parallax enables us to look around objects and is a strong cue for relative perception of depth. Motion parallax works equally in all directions, whereas binocular disparity only occurs in head-horizontal directions. Even small motions of the head due to breathing can result in subtle motion parallax that adds to presence (Andrew Robinson and Sigurdur Gunnarsson, personal communication, May 11, 2015).</p>
<p class="image"><a id="page_119"></a><a id="fig9.6"></a><img src="../images/f0119-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 9.6</span> Objects closer to the horizon result in the perception that they are further away.</strong></p>
<p class="indent">Motion parallax is a problem for world-fixed displays (Section <a href="chapter03.html#lev3.2.1"><span class="blue">3.2.1</span></a>) when there are multiple viewers but only one person&#8217;s viewpoint is being tracked. The shape and motion of the scene is correct for the person being tracked but appears as distortion and objects swaying for the non-tracked viewers.</p>
<p class="image"><a id="page_120"></a><a id="fig9.7"></a><img src="../images/f0120-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 9.7</span> Objects further away have less color and detail due to aerial perspective.</strong> (Courtesy of NextGen Interactions)</p>
<p class="noindentt"><strong>Kinetic depth effect.</strong>&#160;&#160;&#160;The <strong>kinetic depth effect</strong> is a special form of motion parallax that describes how 3D structural form can be perceived when an object moves. For example, observers can perceive the 3D structure of a wire simply by the motion and deformation of the shadow cast by that wire as it is rotated.</p>
<p class="noindentt"><strong>Binocular Depth Cues</strong></p>
<p class="noindent">Those of us with two healthy eyes receive slightly different views of the world on each of our retinas, with the differences being a function of distance. <strong>Binocular disparity</strong> is the difference in image location of a stimulus seen by the left and right eyes resulting from the eyes&#8217; horizontal separation and the stimulus distance. <strong>Stereopsis</strong> (also known as binocular fusion) occurs when the brain combines separate and slightly different images from each eye to form a single percept that contains a vivid sense of depth.</p>
<p class="indent">Stimuli at a distance resulting in no disparity form a surface in space called the <strong>horopter</strong> (Figure <a href="chapter09.html#fig9.8"><span class="blue">9.8</span></a>). The surrounding space in front of and behind the horopter where we can perceive stereopsis is called <strong>Panum&#8217;s fusional area.</strong> The area outside Panum&#8217;s fusional area contains disparity that is too great to fuse, causing double images resulting in diplopia.</p>
<p class="image"><a id="page_121"></a><a id="fig9.8"></a><img src="../images/f0121-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 9.8</span> The horopter and Panum&#8217;s fusional area.</strong> (Adapted from <a href="reference.html#ref220"><span class="blue">Mirzaie</span></a> [<a href="reference.html#ref220"><span class="blue">2009</span></a>])</p>
<p class="indent"><a id="pg121lev1"></a>HMDs can be <strong>monocular</strong> (one image for a single eye), <strong>biocular</strong> (two identical images, one image for each eye), or <strong>binocular</strong> (two different images for each eye, providing a sense of stereopsis). Binocular images can cause conflicts with accommodation, vergence, and disparity (Section <a href="chapter13.html#lev13.1"><span class="blue">13.1</span></a>). Biocular HMDs cause a conflict with motion parallax, but there are few complaints of the conflict.</p>
<p class="indent">The value of binocular cues is often debated for stereoscopic displays and VR. Part of this is likely due to some proportion of the population being <strong>stereoblind</strong>&#8212;it is estimated that 3&#8211;5% of the population is unable to extract depth signaled purely by disparity differences in modern 3D displays [<a href="reference.html#ref12"><span class="blue">Badcock et al. 2014</span></a>]. The incidence of such stereoblindness increases with age, and there are likely different levels of stereoblindness beyond the 3&#8211;5%, resulting in disagreement on the value of binocular displays.</p>
<p class="indent">For most people, stereopsis becomes important when interacting in personal space through the use of the hands, especially when other depth cues are not strong. We do perceive some binocular depth cues at further distances (up to ~40 meters under ideal conditions&#8212;see Section <a href="chapter08.html#lev8.1.4"><span class="blue">8.1.4</span></a>), but stereopsis is much less important at such distance.</p>
<h5 class="h5"><a id="page_122"></a><strong>Oculomotor Depth Cues</strong></h5>
<p class="noindent">The musculature of the eyes, specifically those dealing with accommodation and vergence, can provide subtle depth cues at distances up to 2 meters. Under normal real-world conditions, accommodation and vergence automatically work together when looking at objects at different distances, known as the accommodation-vergence reflex. With VR, accommodation and vergence can be in conflict with each other (as well as with other binocular cues) when the display focus is set at a distance different than where the eyes feel like they are looking (Section <a href="chapter13.html#lev13.1"><span class="blue">13.1</span></a>).</p>
<p class="noindentt"><strong>Vergence.</strong>&#160;&#160;&#160;As discussed in Section <a href="chapter08.html#lev8.1.5"><span class="blue">8.1.5</span></a>, <strong>vergence</strong> is the simultaneous rotation of the eyes in opposite directions, triggered by retinal disparity, with the primary purpose being to obtain or maintain both sharp and comfortable binocular vision [<a href="reference.html#ref263"><span class="blue">Reichelt et al. 2010</span></a>]. We can feel inward (<strong>convergence</strong>) and outward (<strong>divergence</strong>) eye movements that provide depth cues. Evidence suggests the effectiveness of convergence alone as a source of information for depth is confined to a range of up to about 2 meters [<a href="reference.html#ref57"><span class="blue">Cutting and Vishton 1995</span></a>]. Convergence is a more effective cue than accommodation, described below.</p>
<p class="noindentt"><a id="pg122lev1"></a><strong>Accommodation.</strong>&#160;&#160;&#160;Our eyes contain elastic lenses that change curvature, enabling us to focus the image of objects sharply on the retina. <strong>Accommodation</strong> is the mechanism by which the eye alters its optical power to hold objects at different distances into focus on the retina. We can feel the tightening of eye muscles that change the shape of the lens to focus on nearby objects, and this muscular contraction can serve as a cue for distance for up to about 2 meters [<a href="reference.html#ref57"><span class="blue">Cutting and Vishton 1995</span></a>]. Full accommodation response requires a minimum fixation time of one second or longer and is triggered primarily by retinal blur [<a href="reference.html#ref263"><span class="blue">Reichelt et al. 2010</span></a>]. Once we have accommodated to a specific distance, objects much closer or further from that distance are out of focus. This blurriness can also provide subtle depth cues, but it is ambiguous if the blurred object is closer or further away from the object that is in focus. Objects having low contrast represent a weak stimulus for accommodation.</p>
<p class="indent">The ability to accommodate decreases with age, and most people cannot maintain close-up accommodation for any more than a short period of time. VR designers should not place essential stimuli that must be consistently viewed close to the eye.</p>
<h5 class="h5"><strong>Contextual Distance Factors</strong></h5>
<p class="noindent">Although research on visual depth cues has been around for centuries, more recent research suggests distance perception is influenced not only by visual cues but also by environmental context and personal variables. For example, the accuracy of distance perception has been found to differ depending on whether objects being judged are <a id="page_123"></a>indoors or outdoors, or even in different types of indoor environments [<a href="reference.html#ref264"><span class="blue">Renner et al. 2013</span></a>].</p>
<p class="indent"><strong>Future effect distance cues</strong> are internal psychological beliefs of how distance may personally affect the viewer in the future. Such cues include one&#8217;s intended actions and fear.</p>
<p class="noindentt"><strong>Intended action.</strong>&#160;&#160;&#160;Perception and action are closely intertwined (Section <a href="chapter10.html#lev10.4"><span class="blue">10.4</span></a>). <strong>Action-intended distance cues</strong> are psychological factors of future actions that influence distance perception. People in the same situation perceive the world differently depending upon the actions they intend to perform and benefits/costs of those actions [<a href="reference.html#ref249"><span class="blue">Proffitt 2008</span></a>]. We see the world as &#8220;reachers&#8221; only if we intend to reach, as &#8220;throwers&#8221; only if we intend to throw, and as &#8220;walkers&#8221; only if we intend to walk. Objects within reach appear closer than those out of reach, and when reachability is extended by holding a tool that is intended to be used, apparent distances are diminished. Hills appear to be steeper than they actually are. Hills of 5&#176; are typically judged to be about 20&#176;, and 10&#176; hills appear to be 30&#176; [<a href="reference.html#ref250"><span class="blue">Proffitt et al. 1995</span></a>]. A manipulation that influences the effort to walk will influence people&#8217;s perception of distance if they intend to walk. <a id="pg123lev1"></a>This overestimation increases for those encumbered by wearing a heavy backpack, fatigue, low physical health, old age, and/or declining health [<a href="reference.html#ref16"><span class="blue">Bhalla and Proffitt 1999</span></a>]. Hills appear steeper and egocentric distances farther, as the anticipated metabolic energy costs associated with walking increase.</p>
<p class="noindentt"><strong>Fear.&#160;&#160;&#160;Fear-based distance cues</strong> are fear-causing situations that increase the sense of height. We overestimate vertical distance, and this overestimation is much greater when the height is viewed from above than from below. A sidewalk appears steeper for those fearfully standing on a skateboard compared to those standing fearlessly on a box [<a href="reference.html#ref296"><span class="blue">Stefanucci et al. 2008</span></a>]. This is likely due to an evolved adaptation&#8212;enhancing the perception of vertical height motivates people to avoid falling. Fear influences spatial perception.</p>
<h4 class="h4"><a id="lev9.1.4"></a><strong><span class="blue"><span class="font1">9.1.4</span></span> Measuring Depth Perception</strong></h4>
<p class="noindent">There are several methods to test distance perception and those methods can be divided into three categories: (1) verbal estimation, (2) perceptual matching, and (3) visually directed actions [<a href="reference.html#ref264"><span class="blue">Renner et al. 2013</span></a>]. The different methods often result in different estimates. Even the phrasing of instruction within the same method can influence how distance estimates are made. For example, &#8220;how far away does the object visually appear to be&#8221; or &#8220;how far away the object really is&#8221; may result in <a id="page_124"></a>different estimates. With carefully designed methods, depth perception can be quite accurate for distances of up to 20 meters.</p>
<h4 class="h4"><a id="lev9.1.5"></a><strong><span class="blue"><span class="font1">9.1.5</span></span> VR Challenges of Space Perception</strong></h4>
<p class="noindent">Natural perception of space within VR still remains a challenge for display technology as well as content creation. For example, whereas egocentric distance perception can be quite good for the real world, VR egocentric distance estimates are reported to be compressed [<a href="reference.html#ref189"><span class="blue">Loomis and Knapp 2003</span></a>, <a href="reference.html#ref264"><span class="blue">Renner et al. 2013</span></a>]. Veridical spatial perception is not essential for all VR applications, but it is extremely important where perception of distance and object size is essential, such as architecture, automotive, and military applications.</p>
<h3 class="h3"><a id="lev9.2"></a><span class="font">9.2</span> <strong>Time Perception</strong></h3>
<p class="noindent">Time can be thought of as a person&#8217;s own perception or experience of successive change. Unlike objectively measured time, <strong>time perception</strong> is a construction of the brain that can be manipulated and distorted under certain circumstances. The concept <a id="pg124lev1"></a>of time is essential to most cultures, with languages having distinct tenses of past, present, and future, plus innumerable modifiers to specify the &#8220;when&#8221; more precisely.</p>
<h4 class="h4"><a id="lev9.2.1"></a><strong><span class="blue"><span class="font1">9.2.1</span></span> A Breakdown of Time</strong></h4>
<p class="noindent">The <strong>subjective present</strong> is always with us as the precious few seconds of our ongoing conscious experience. Everything else does not exist, but is either past or future. The subjective present consists of (1) the &#8220;now&#8221;&#8212;i.e., the current moment, and (2) the experience of passing time [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. The experience of passing time can further be broken down into (a) duration estimation, (b) order/sequence (consisting of simultaneity and successiveness), and (c) anticipation/planning for the immediate future (e.g., knowing what note to play next with a musical instrument).</p>
<p class="indent">Psychological time can be considered a sequence of moments instead of a continuous experience. A <strong>perceptual moment</strong> is the smallest psychological unit of time that an observer can sense. Stimuli presented within the same perceptual moment are perceived as occurring simultaneously. Based on several research findings, <a href="reference.html#ref301"><span class="blue">Stroud</span></a> [<a href="reference.html#ref301"><span class="blue">1986</span></a>] estimated that perceptual moments are in the 100 ms range in duration. The length of these perceptual moments ranges from about 25 to 150 ms depending upon the sensory modality, stimulus, task, etc. Evidence suggests perceptual moments become unstable when one judges with more than one sensory modality, such as when judging the order of presentation of a sound and a light [<a href="reference.html#ref311"><span class="blue">Ulrich 1987</span></a>].</p>
<p class="indent"><a id="page_125"></a>An <strong>event</strong> is a segment of time at a particular location that is perceived to have a beginning and an end or a series of perceptual moments that unfolds over time. Our ability to understand relationships between actions and objects depends on this unfolding. Events are similar to sentences and phrases in language, where the sequence and timing of nouns and verbs gives meaning. There is a big difference between &#8220;bear eats man&#8221; and &#8220;man eats bear.&#8221; The two phrases contain the same elements, but it is the sequence and timing of these stimuli at the eye or ear that gives meaning.</p>
<h4 class="h4"><a id="lev9.2.2"></a><strong><span class="blue"><span class="font1">9.2.2</span></span> The Brain and Time</strong></h4>
<h5 class="h5a"><strong>Delayed Perception</strong></h5>
<p class="noindent">Despite what most of us would like to believe, we all live in the past with our consciousness always lagging behind reality. It takes between 10 ms and 50 ms for visual signals to reach the LGN (Section <a href="chapter08.html#lev8.1.1"><span class="blue">8.1.1</span></a>), another 20&#8211;50 ms for those signals to reach the visual cortex, and an additional 50&#8211;100 ms to reach parts of the brain that are responsible for action and planning [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. This can cause problems when response time is essential. For example, it takes a driver over 200 ms to perceive a stimulus <a id="pg125lev1"></a>that requires the decision to apply the brakes. This number can be even worse if the driver is dark adapted (Section <a href="chapter10.html#lev10.2.1"><span class="blue">10.2.1</span></a>). After perceiving the stimulus, it takes a relatively short amount of time at 10&#8211;15 ms to initiate muscle responses. For a driver traveling at 100 kph (62 mph), the car will have traveled 6 m (20 ft) before the foot starts moving toward the brakes. For cases where we can anticipate events, this delay can be reduced via top-down processing that primes cells to respond faster.</p>
<p class="indent">Although we live in the past couple hundred milliseconds, this does not mean it is okay to have a couple hundred milliseconds of VR system delay. Such additional external-to-the-body latency is a major cause of motion sickness as discussed in Chapter <a href="chapter15.html#ch15"><span class="blue">15</span></a>.</p>
<h5 class="h5"><strong>Persistence</strong></h5>
<p class="noindent">In addition to delayed perception, the length of an experience depends on neural activity, not necessarily the duration of a stimulus. <strong>Persistence</strong> is the phenomenon by which a positive afterimage (Section <a href="chapter06.html#lev6.2.6"><span class="blue">6.2.6</span></a>) seemingly persists visually. A single 1 ms flash of light is often experienced as a 100 ms perceptual moment when the observer is light adapted and as long as 400 ms when dark adapted [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>].</p>
<p class="indent">When two stimuli are close in time and space, the two stimuli can be perceived as a single moving stimulus (Section <a href="chapter09.html#lev9.3.6"><span class="blue">9.3.6</span></a>). For example, pixels displayed in different locations at different times can appear as a single moving stimulus. <strong>Masking</strong> can also occur where one of two stimuli (usually visuals or sounds) are not perceived. The <a id="page_126"></a>second stimulus is typically perceived more accurately (backward masking) as if the second stimulus goes back in time and erases the first stimulus.</p>
<h5 class="h5"><strong>Perceptual Continuity</strong></h5>
<p class="noindent"><strong>Perceptual continuity</strong> is the illusion of continuity and completeness, even in cases when at any given moment only parts of stimuli from the world reach our sensory receptors [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. The brain often constructs events after multiple stimuli have been presented, helping to provide continuity for the perception of a single event across breaks or interruptions that occur because of extraneous stimuli in the environment.</p>
<p class="indent">The phonemic restoration effect (Section <a href="chapter08.html#lev8.2.3"><span class="blue">8.2.3</span></a>) causes us to perceive missing parts of speech, where what we hear in place of the missing parts is dependent on context. The visual system behaves in a similar way. When an object moves behind other occluding objects, then we perceive the entire object even when there is no single moment in time when the occluded object is completely seen. This can be seen by looking through a narrow slit created by a partially opened door (e.g., even for an opening of only 1 mm). As the head is moved or an object is moved behind the slit, we perceive the entire scene or object to be present due to the brain stitching together the <a id="pg126lev1"></a>stimuli over time. Perceptual continuity is one reason why we don&#8217;t normally perceive the blind spot (Section <a href="chapter06.html#lev6.2.3"><span class="blue">6.2.3</span></a>).</p>
<h4 class="h4"><a id="lev9.2.3"></a><strong><span class="blue"><span class="font1">9.2.3</span></span> Perceiving the Passage of Time</strong></h4>
<p class="noindent">Unlike the sensory modalities, there is no obvious time organ that enables us to perceive the passage of time. Instead it is through change that we perceive time. Two theories that describe how we are able to perceive time are biological clocks and cognitive clocks [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>].</p>
<h5 class="h5"><strong>Biological Clocks</strong></h5>
<p class="noindent">The world and its inhabitants have their own rhythms of change, including day-night cycles, moon cycles, seasonal cycles, and physiological changes. <strong>Biological clocks</strong> are body mechanisms that act in periodic manners, with each period serving as a tick of the clock, which enable us to sense the passing of time. Altering the speed of our physiological rhythm alters the perception of the speed of time.</p>
<p class="indent">A <strong>circadian rhythm</strong> (circadian comes from the root circa, meaning &#8220;approximate&#8221; and dies, meaning &#8220;days&#8221;) is a biologically recurring natural 24-hour oscillation. Circadian rhythms are largely controlled by the supra chiasmatic nucleus (SCN), a small organ near the optic chiasm of only a few thousand cells. Circadian rhythms result in sleep-wakefulness timed behaviors that run through a day-night cycle. Although circadian rhythms are endogenous, they are entrained by external cues called zeitgebers <a id="page_127"></a>(German for &#8220;time-givers&#8221;), the most important being light. Without light, most of us actually have a rhythm of 25 hours.</p>
<p class="indent">Biological processes such as pulse, blood pressure, and body temperature vary as a function of the day-night cycle. Other effects on our mind and body include changes to mood, alertness, memory, perceptual motor tasks, cognition, spatial orientation, postural stability, and overall health. The experiences of VR users may not be as good at times when they are normally sleeping.</p>
<p class="indent">Slow circadian rhythms do not help us with our perception of short time periods. We have several biological clocks for judgments of different amounts of time and different behavioral aspects. Short-term biological timers that have been suggested for internal biological timing mechanisms include heartbeats, electrical activity in the brain, breathing, metabolic activities, and walking steps.</p>
<p class="indent">Our perception of time also seems to change when our biological clocks change. Durations seem to be longer when our biological clocks tick faster due to more biological ticks per unit time than normally occur. Conversely, time seems to whiz by when our internal clock is too slow. Evidence demonstrates body temperature, fatigue, and drugs cause change in time duration estimates.</p>
<h5 class="h5x"><strong>Cognitive Clocks</strong></h5>
<p class="noindent"><strong>Cognitive clocks</strong> are based on mental processes that occur during an interval. Time is not directly perceived but rather is constructed or inferred. The tasks a person engages in will influence the perception of the passage of time.</p>
<p class="indent">Cognitive clock factors affecting time perception are (1) the amount of change, (2) the complexity of stimulus events that are processed, (3) the amount of attention given to the passage of time, and (4) age. All these affect the perception of the passage of time, and all are consistent with the idea that the rate at which the cognitive clock ticks is affected by how internal events are processed.</p>
<p class="noindent3"><strong>Change.</strong>&#160;&#160;&#160;The ticking rate of cognitive clocks is dependent on the number of changes that occur. The more changes that occur during an interval, the faster the clock ticks, and thus the longer the estimate of the amount of time passed.</p>
<p class="indent"><a id="page_128"></a>The <strong>filled duration illusion</strong> is the perception that a duration filled with stimuli is perceived as longer than an identical time period empty of stimuli. For example, an interval filled with tones is perceived as being longer than the same interval with fewer tones. This is also the case for light flashes, words, and drawings. Conversely, those in a soundproof darkened chamber tend to underestimate the amount of time they have spent in the chamber.</p>
<p class="noindentt"><strong>Processing effort.</strong>&#160;&#160;&#160;The ticking rate of cognitive clocks is also dependent on cognitive activity. Stimuli that are more difficult to process result in judgments of longer durations. Similarly, increasing the amount of memory storage required to process information results in judgments of longer durations.</p>
<p class="noindentt"><strong>Temporal attention.</strong>&#160;&#160;&#160;Time perception is complicated by the way observers attend to a task. The more attention one pays to the passage of time, the longer the time interval seems to be. Paying attention to time may even reverse the filled duration illusion mentioned above because it is difficult to process many events while attending at the same time to the passage of time.</p>
<p class="indent">When observers are told in advance that they will have to judge the time a task takes, <a id="pg128lev1"></a>they tend to judge the duration as being longer than they do when they are unexpectedly asked to judge the duration after the task is completed. This is presumably due to drawing attention to time.</p>
<p class="indent">Conversely, anything that draws attention away from the passage of time results in a faster perception of time. Duration estimates are typically shorter for difficult tasks than for easy tasks because it is more difficult to attend to both time and a difficult task.</p>
<p class="noindentt"><strong>Age.</strong>&#160;&#160;&#160;As people age, the passage of larger units of time (i.e., days, months, or even years) seems to speed up. One explanation for this is that the total amount of time someone has experienced serves as a reference. One year for a 4-year old is 20% of his life so time seems to drag by. One year for a 50-year old is 2% of his life, so time seems to move more quickly.</p>
<h4 class="h4"><a id="lev9.2.4"></a><strong><span class="blue"><span class="font1">9.2.4</span></span> Flicker</strong></h4>
<p class="noindent"><strong>Flicker</strong> is the flashing or repeating of alternating visual intensities. Retinal receptors respond to flickering light at up to several hundred cycles per second, although sensitivity in the visual cortex is much less [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. Perception of flicker covers a wide range depending on many factors including dark adaptation, light intensity, retinal location, stimulus size, blanking time between stimuli, time of day <a id="page_129"></a>(due to circadian rhythms), gender, age, bodily functions, and wavelength. We are most sensitive to a light-adapted eye with a high light intensity over a wide range of the visual periphery when the body is most awake. The <strong>flicker-fusion frequency threshold</strong> is the flicker frequency where flicker becomes visually perceptible.</p>
<h3 class="h3"><a id="lev9.3"></a><span class="font">9.3</span> <strong>Motion Perception</strong></h3>
<p class="noindent">Perception is not static but varies continuously over time. Perception of motion at first seems quite trivial. For example, visual motion simply seems as a sense of stimuli moving across the retina. However, motion perception is a complex process that involves a number of sensory systems and physiological components [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>].</p>
<p class="indent">We can perceive movement even when visual stimuli are not moving on the retina, such as when tracking a moving object with our eyes (even when there are no other contextual stimuli to compare to). At other times, stimuli move on the retina yet we do not perceive movement; for example, when the eyes move to examine different objects. Despite the image of the world moving on our retinas, we perceive a stable world (Section <a href="chapter10.html#lev10.1.3"><span class="blue">10.1.3</span></a>).</p>
<p class="indent"><a id="pg129lev1"></a>A world perceived without motion can be both disturbing and dangerous. A person who has akinetopsia (blindness to motion) sees people and objects suddenly appearing or disappearing due to not being able to see them approach. Pouring a cup of coffee becomes nearly impossible. The lack of motion perception is not just a social inconvenience but can be quite dangerous; consider crossing a street where a car seems far away and then suddenly appears very close.</p>
<h4 class="h4"><a id="lev9.3.1"></a><strong><span class="blue"><span class="font1">9.3.1</span></span> Physiology</strong></h4>
<p class="noindent">The primitive visual pathway through the brain (Section <a href="chapter08.html#lev8.1.1"><span class="blue">8.1.1</span></a>) is largely specialized for motion perception along with the control of reflexive responses such as eye and head movements. The primary visual pathway also contributes to motion detection through magno cells (and to a lesser extent parvo cells) as well as through both the ventral pathway (the &#8220;what&#8221; pathway) and the dorsal pathway (the &#8220;where/how/action&#8221; pathway).</p>
<p class="indent">The primate brain has visual receptor systems dedicated to motion detection [<a href="reference.html#ref188"><span class="blue">Lisberger and Movshon 1999</span></a>], which humans use to perceive movement [<a href="reference.html#ref226"><span class="blue">Nakayama and Tyler 1981</span></a>]. Whereas visual velocity (i.e., speed and direction of a visual stimulus on the retina) is sensed directly in primates, visual acceleration is not, but is instead inferred through processing of velocity signals [<a href="reference.html#ref188"><span class="blue">Lisberger and Movshon 1999</span></a>]. <a id="page_130"></a>Most visual perception scientists agree that for perception of most motion, visual acceleration is not as important as visual velocity [<a href="reference.html#ref260"><span class="blue">Regan et al. 1986</span></a>]. However, visual linear acceleration can evoke motion sickness more so than visual linear velocity when there is a mismatch with linear acceleration detected by the vestibular system (Section <a href="chapter18.html#lev18.5"><span class="blue">18.5</span></a>).</p>
<h4 class="h4"><a id="lev9.3.2"></a><strong><span class="blue"><span class="font1">9.3.2</span></span> Object-Relative vs. Subject-Relative Motion</strong></h4>
<p class="noindent"><strong>Object-relative motion</strong> (similar to exocentric judgments; Section <a href="chapter09.html#lev9.1.1"><span class="blue">9.1.1</span></a>) occurs when the spatial relationship between stimuli changes. Because the relationship is relative, which object is moving is ambiguous. Visual object-relative judgments depend solely upon stimuli on the retina and do not take into account extra-retinal information such as eye or head motion.</p>
<p class="indent"><strong>Subject-relative motion</strong> (similar to egocentric judgments; Section <a href="chapter09.html#lev9.1.1"><span class="blue">9.1.1</span></a>) occurs when the spatial relationship between a stimulus and the observer changes. This subject-relative frame of reference for visual perception is provided by extra-retinal information such as vestibular input. Even when the head is not moving the vestibular <a id="pg130lev1"></a>system still receives input. A purely subject-relative cue without any object-relative cues occurs only when a single visual stimulus is visible or all visual stimuli have the same motion.</p>
<p class="indent">People use both object-relative and subject-relative cues to judge motion of the external world. For all but the shortest intervals, humans are much more sensitive to object-relative motion than subject-relative motion [<a href="reference.html#ref193"><span class="blue">Mack 1986</span></a>] due to perceiving displacement relative to a visual context rather than velocity. For example, one study found that observers can detect a luminous dot moving in the dark (subject-relative) at about 0.2&#176;/s whereas observers can detect a moving target in the presence of a stationary visual context (object-relative) at about 0.03&#176;/s.</p>
<p class="indent">Visuals in augmented reality are considered to be mostly object-relative to real-world cues, as the visuals can be directly compared with the real world. Incorrectly moving visuals are more easily noticed in optical-see-through HMDs (i.e., augmented reality) than non-see-through HMDs [<a href="reference.html#ref10"><span class="blue">Azuma 1997</span></a>], since users can directly compare rendered visuals relative to the real world and hence see spurious object-relative motion. Visual error in VR due to problems such as latency and miscalibration is usually subject-relative, as such error causes the entire scene to move as the user moves the head (other than for motion parallax, where visuals move differently as a function of viewpoint translation and depth).</p>
<h4 class="h4"><a id="page_131"></a><a id="lev9.3.3"></a><strong><span class="blue"><span class="font1">9.3.3</span></span> Optic Flow</strong></h4>
<p class="noindent"><strong>Optic flow</strong> is the pattern of visual motion on the retina&#8212;the motion pattern of objects, surfaces, and edges on the retina caused by the relative motion between a person and the scene. As one looks to the left, optic flow moves to the right. As one moves foward, optic flow expands in a radial pattern. <strong>Gradient flow</strong> is the different speed of flow for different parts of the retinal image&#8212;fast near the observer and slower further away. The <strong>focus of expansion</strong> is the point in space around which all other stimuli seem to expand as one moves forward toward it.</p>
<h4 class="h4"><a id="lev9.3.4"></a><strong><span class="blue"><span class="font1">9.3.4</span></span> Factors of Motion Perception</strong></h4>
<p class="noindent">Many factors influence our perception of motion. For example, we are more sensitive to visual motion as contrast increases. Some other factors are discussed below.</p>
<h5 class="h5"><strong>Motion Perception in Peripheral vs. Central Vision</strong></h5>
<p class="noindent">The literature conflicts as to whether sensitivity to motion increases or decreases with eye eccentricity (the distance from the stimulus on the retina to the center of the fovea). These conflicting claims are likely due to differences of experimental conditions as <a id="pg131lev1"></a>well as interpretations.</p>
<p class="indent"><a href="reference.html#ref6"><span class="blue">Anstis</span></a> [<a href="reference.html#ref6"><span class="blue">1986</span></a>] states that it is sometimes mistakenly claimed that peripheral vision is more sensitive to motion than central vision. In fact, the ability to detect slow-moving stimuli actually decreases steadily with eye eccentricity. However, since sensitivity to static detail decreases even faster, peripheral vision is relatively better at detecting motion than form. A moving object seen in the periphery is perceived as something moving, but it is more difficult to see what that something is.</p>
<p class="indent"><a href="reference.html#ref49"><span class="blue">Coren et al.</span></a> [<a href="reference.html#ref49"><span class="blue">1999</span></a>] states that the detection of movement depends on both the speed of the moving stimulus and eye eccentricity. A person&#8217;s ability to detect slow-moving stimuli (less than 1.5&#176;/s) decreases with eye eccentricity, which is consistent with Anstis. For faster-moving stimuli, however, the ability to detect moving stimuli increases with eye eccentricity. These differences are due to the dominance in the periphery of the dorsal or &#8220;action&#8221; pathway (Section <a href="chapter08.html#lev8.1.1"><span class="blue">8.1.1</span></a>), which consists of more transient cells (cells that respond best to fast-changing stimuli). Scene motion in the peripheral visual field is also important in sensing self-motion (Sections <a href="chapter09.html#lev9.3.10"><span class="blue">9.3.10</span></a> and <a href="chapter10.html#lev10.4.3"><span class="blue">10.4.3</span></a>).</p>
<p class="indent">Judging motion consists of more than just looking for moving stimuli. Qualitative evidence suggests slight visual motion in the periphery may more easily be judged by &#8220;feeling&#8221; (e.g., vection as discussed in Section <a href="chapter09.html#lev9.3.10"><span class="blue">9.3.10</span></a>) whereas slight motion in central <a id="page_132"></a>vision may more easily be judged by direct visual and logical thinking. For example, two subjects judging a VR scene motion perception task [<a href="reference.html#ref148"><span class="blue">Jerald et al. 2008</span></a>] stated:</p>
<p class="blockquotet">Most of the times I detected the scene with motion by noticing a slight dizziness sensation. On the very slight motion scenes there was an eerie dwell or &#8216;suspension&#8217; of the scene; it would be still but had a floating quality.</p>
<p class="blockquote1t">At the beginning of each trial [sic: session&#8212;when scene velocities were greatest because of the adaptive staircases], I used visual judgments of motion; further along [later in sessions] I relied on the feeling of motion in the pit of my stomach when it became difficult to discern the motion visually.</p>
<p class="indentt">These different ways of judging visual motion are likely due to the different visual pathways (Section <a href="chapter08.html#lev8.1.1"><span class="blue">8.1.1</span></a>).</p>
<h5 class="h5"><strong>Head Motion Suppresses Motion Perception</strong></h5>
<p class="noindent">Although head motion suppresses perception of visual motion, perception of visual motion while the head is moving can be surprisingly accurate. <a href="reference.html#ref190"><span class="blue">Loose and Probst</span></a> [<a href="reference.html#ref190"><span class="blue">2001</span></a>] found that increasing the angular velocity of the head significantly suppresses the ability <a id="pg132lev1"></a>to detect visual motion when the visual motion moves relative to the head. <a href="reference.html#ref4"><span class="blue">Adelstein et al.</span></a> [<a href="reference.html#ref4"><span class="blue">2006</span></a>] and <a href="reference.html#ref181"><span class="blue">Li et al.</span></a> [<a href="reference.html#ref181"><span class="blue">2006</span></a>] also showed that head motion suppresses perception of visual motion. They used an HMD without head tracking where the slight motion to detect was relative to the head. <a href="reference.html#ref143"><span class="blue">Jerald</span></a> [<a href="reference.html#ref143"><span class="blue">2009</span></a>] measured how head motion suppresses perception of visual motion where the slight motion to detect was relative to the real world, and then mathematically related that suppression to the perception of latency.</p>
<h5 class="h5"><strong>Depth Perception Affects Motion Perception</strong></h5>
<p class="noindent">The <strong>pivot hypothesis</strong> [<a href="reference.html#ref97"><span class="blue">Gogel 1990</span></a>] states that a point stimulus at a distance will appear to move as the head moves if its perceived distance differs from its actual distance. A related effect is demonstrated by focusing on a finger held in front of the eyes and noticing that the background further in the distance seems to move with the head. Likewise if one focuses on the background then the finger seems to move against the direction of the head.</p>
<p class="indent">Objects in HMDs tend to appear closer to users than their intended distance (Section <a href="chapter09.html#lev9.1.5"><span class="blue">9.1.5</span></a>). According to the pivot hypothesis, if a user looks at an object as if it is close, but the system moves the object&#8217;s image in the HMD as if it is further away when turning the head, then the object will appear to move in the same direction as the head (similar to the finger example above).</p>
<h4 class="h4"><a id="page_133"></a><a id="lev9.3.5"></a><strong><span class="blue"><span class="font1">9.3.5</span></span> Induced Motion</strong></h4>
<p class="noindent"><strong>Induced motion</strong> occurs when motion of one object induces the perception of motion in another object.</p>
<p class="indent">When a dot moves near an existing dot in an otherwise uniform visual field, it is clear one of the dots is moving but often it is difficult to identify which of the two dots is moving (due to the object-relative cues telling us how objects move relative to each other, but not relative to the world). However, when a rectangular frame and dot are observed, we tend to perceive the dot to be moving even if the dot is stable and the frame is what is moving. This is because the mind assumes smaller objects are more likely to move than larger surround stimuli and the larger surround stimuli serve as a stable context (see the moon-cloud illusion and the autokinetic effect in Section <a href="chapter06.html#lev6.2.7"><span class="blue">6.2.7</span></a> and the rest frame hypothesis in Section <a href="chapter12.html#lev12.3.4"><span class="blue">12.3.4</span></a>).</p>
<p class="indent">Induced motion effects are most dramatic when the context is moving slowly, the context is square vs. circular, the surround is larger, and the target and context are the same distance from the observer.</p>
<h4 class="h4"><a id="lev9.3.6"></a><strong><span class="blue"><span class="font1">9.3.6</span></span> Apparent Motion</strong></h4>
<p class="noindent"><a id="pg133lev1"></a>Perception of motion does not require continuously moving stimuli. <strong>Apparent motion</strong> (also known as stroboscopic motion) is the perception of visual movement that results from appropriately displaced stimuli in time and space even though nothing is actually moving [<a href="reference.html#ref6"><span class="blue">Anstis 1986</span></a>, <a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. It&#8217;s as if the brain is unwilling to conclude two similar-looking stimuli happened to disappear and appear next to each other close in time. Therefore, the original stimulus must have moved to the new location.</p>
<p class="indent">The ability to see discontinuous changes as if they were continuous motion has benefited technology since the 1800s and continues to this day, ranging from flashing neon signs (e.g., older Las Vegas signs giving the illusion of movement) to rolling text displays, television, movies, computer graphics, and VR. For VR creators, understanding apparent motion and the brain&#8217;s rules of visual motion perception for filling in physically absent motion information is important for conveying not only a sense of motion from an array of pixels, but a sense of stability since the pixels must change appropriately as the head moves in order to appear stable in space.</p>
<p class="indent">The mind most often perceives apparent motion by following the shortest distance between two neighboring stimuli. However, in some cases the perceived path of motion can also seem to deflect around an object located between the two flashing stimuli. It is as if the brain tries to figure out how the object could have gotten from point A to point B. Two stimuli of different shapes, orientations, brightnesses, and colors can also be perceived to be a single stimulus that transforms while moving.</p>
<h5 class="h5"><a id="page_134"></a><strong>Strobing and Judder</strong></h5>
<p class="noindent">Strobing and judder can be a problem for HMDs due to movement of the head and movement of the eyes relative to the screen and stimulus.</p>
<p class="indent"><strong>Strobing</strong> is the perception of multiple stimuli appearing simultaneously, even though they are separated in time, due to the stimuli persisting on the retina. If two adjacent stimuli are flashed too quickly, then strobing can result. Stimuli separated by larger distances require longer time intervals for motion to be perceived.</p>
<p class="indent"><strong>Judder</strong> is the appearance of jerky or unsmooth visual motion. If adjacent stimuli are flashed too slowly, then judder can result (for traditional video, judder can also result from converting between different timing formats).</p>
<h5 class="h5"><strong>Factors</strong></h5>
<p class="noindent">Three variables, as shown in Figure <a href="chapter09.html#fig9.9"><span class="blue">9.9</span></a>, are involved with the temporal aspects of apparent motion.</p>
<p class="hangt"><strong>Interstimulus interval:</strong> The blanking time between stimuli. Increasing the blanking time increases strobing and decreases judder.</p>
<p class="hanga"><a id="pg134lev1"></a><strong>Stimulus duration:</strong> The amount of time each stimulus is displayed. Increasing the stimulus duration reduces strobing and increases judder.</p>
<p class="hanga"><strong>Stimulus onset asynchrony:</strong> The total time that elapses between the onset of one stimulus and the onset of the next stimulus (interstimulus interval + stimulus duration). This is the inverse of the display refresh rate.</p>
<p class="image"><a id="fig9.9"></a><img src="../images/f0134-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 9.9</span> Three variables affecting apparent motion.</strong> (Adapted from <a href="reference.html#ref49"><span class="blue">Coren et al.</span></a> [<a href="reference.html#ref49"><span class="blue">1999</span></a>])</p>
<p class="indent"><a id="page_135"></a>The timings required to not notice judder or strobing depend not only on timings but on the spatial properties of the stimuli such as contrast and distance between stimuli. Filmmakers control many of these challenges through limited camera motion, object movement, lighting, and motion blur. The stimulus onset asynchrony required to perceive motion ranges from 200 ms for a flashing Vegas casino sign to ~10 ms for smooth motion in a high-quality HMD. VR is much more challenging, partly because the user controls the viewpoint that often is quite fast. The best way to minimize judder for VR is to minimize the stimulus onset asynchrony and stimulus duration, although strobing can appear if the stimulus duration is too short [<a href="reference.html#ref1"><span class="blue">Abrash 2013</span></a>].</p>
<h4 class="h4"><a id="lev9.3.7"></a><strong><span class="blue"><span class="font1">9.3.7</span></span> Motion Coherence</strong></h4>
<p class="noindent">Seemingly randomly placed dots in individual frames can be perceived as having form and motion if those dots move in a coherent manner across frames. <strong>Motion coherence</strong> is the correlation between movements of dots in successive images. Complete random motion of the dots between frames has 0% motion coherence, and if all dots move in the same way there is 100% motion coherence. Humans are highly sensitive to motion <a id="pg135lev1"></a>coherence, and we can see form and motion with as little as 3% coherence [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. The most optimal condition of perceiving motion coherence is when the dots move at 2&#176;/s. Sensitivity to motion coherence increases as the visual field size increases up to 20&#176; in diameter.</p>
<h4 class="h4"><a id="lev9.3.8"></a><strong><span class="blue"><span class="font1">9.3.8</span></span> Motion Smear</strong></h4>
<p class="noindent"><strong>Motion smear</strong> is the trail of perceived persistence that is left by an object in motion [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. The persistence of the streak is long enough in duration to allow the path to be seen. Although motion can be seen when moving a small lit cigarette or hot campfire ember, motion smear does not require either dark conditions or bright stimuli to be seen. For example, motion smear can be seen by waving the hand back and forth in front of the eyes, resulting in the hand appearing to be in multiple locations simultaneously. Research has shown there is substantial motion smear suppression when a moving stimulus follows a predictable path, so motion smear cannot be due to persistence on the retina alone.</p>
<h4 class="h4"><a id="lev9.3.9"></a><strong><span class="blue"><span class="font1">9.3.9</span></span> Biological Motion</strong></h4>
<p class="noindent">Humans have especially fine-tuned perception skills when it comes to recognizing human motion. <strong>Biological motion perception</strong> refers to the ability to detect such motion.</p>
<p class="indent"><a id="page_136"></a><a href="reference.html#ref150"><span class="blue">Johansson</span></a> [<a href="reference.html#ref150"><span class="blue">1976</span></a>] found that some observers were able to identify human motion represented by 10 moving point lights in durations as short as 200 ms and all observers tested were able to perfectly recognize 9 different motion patterns for durations of 400 ms or more.</p>
<p class="indent">In another study, people who were well acquainted with each other were filmed with only lit portions of several of their joints. Several months later, the participants were able to identify themselves and others on many trials just from viewing the moving lit joints [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. When asked how they made their judgments, they mentioned a variety of motion components such as speed, bounciness, rhythm, arm swing, and length of steps. In other studies, observers could determine above chance whether moving dot patterns were from a male or female within 5 seconds of viewing even when only the ankles were represented!</p>
<p class="indent">These studies imply avatar motion in VR is extremely important, even when the avatar is not seen well or when presented over a short period of time. Any motion simulation of humans must be accurate in order to be believed.</p>
<h4 class="h4"><a id="lev9.3.10"></a><strong><span class="blue"><span class="font1">9.3.10</span></span> Vection</strong></h4>
<p class="noindent"><a id="pg136lev1"></a><strong>Vection</strong> is an illusion of self-motion when one is not actually physically moving in the perceived manner. Vection is similar to induced motion (Section <a href="chapter09.html#lev9.3.5"><span class="blue">9.3.5</span></a>), but instead of inducing an illusion of a small visual stimulus motion, vection induces an illusion of self-motion. In the real world, vection can occur when one is seated in a car and an adjacent stopped car pulls away. One experiences the car one is seated in, which is actually stationary, to move in the opposite direction of the moving car.</p>
<p class="indent">Vection often occurs in VR due to the entire visual world moving around the user even though the user is not physically moving. This is experienced as a compelling illusion of self-motion in the direction opposite the moving stimuli. If the visual scene is moved to the left, the user may feel that she is moving or leaning to the right and will compensate by leaning into the direction of the moving scene (the left). In fact, vection in carefully designed VR experiences can be indistinguishable from actual self-motion (e.g., the Haunted Swing described in Section <a href="chapter02.html#lev2.1"><span class="blue">2.1</span></a>).</p>
<p class="indent">Although not nearly as strong as vision, other sensory modalities can contribute to vection [<a href="reference.html#ref119"><span class="blue">Hettinger et al. 2014</span></a>]. Auditory vection can produce vection by itself in the absence of other sensory stimuli or can enhance vection provided by other sensory stimuli. Biomechanical vection can occur when one is standing or seated and repeatedly steps on a treadmill or an even, static surface. Haptic/tactile cues via touching a rotating drum surrounding the user or airflow via a fan can also add to the sense of vection.</p>
<h5 class="h5"><a id="page_137"></a><strong>Factors</strong></h5>
<p class="noindent">Vection is more likely to occur for large stimuli moving in peripheral vision. Increasing visual motions up to about 90&#176;/s result in increased vection [<a href="reference.html#ref130"><span class="blue">Howard 1986a</span></a>]. Visual stimuli with high spatial frequencies also increase vection.</p>
<p class="indent">Normally, users correctly perceive themselves to be stable and the visuals to be moving before the onset of vection. This delay of vection can last several seconds. However, at stimulus accelerations of less than about 5&#176;/s<sup>2</sup>, vection is not preceded by a period of perceived stimulus motion as it is at higher rates of acceleration [<a href="reference.html#ref131"><span class="blue">Howard 1986b</span></a>].</p>
<p class="indent">Vection is not only attributed to low-level bottom-up processing. Research suggests higher-level top-down vection processing of globally consistent natural scenes with a convincing rest frame (Section <a href="chapter12.html#lev12.3.4"><span class="blue">12.3.4</span></a>) dominates low-level bottom-up vection processing [<a href="reference.html#ref266"><span class="blue">Riecke et al. 2005</span></a>]. Stimuli perceived to be further away cause more vection than closer stimuli, presumably because our experience with real life tells us background objects are typically more stable. Moving sounds that are normally perceived as landmarks in the real world (e.g., church bells) can cause greater vection than from artificial sounds or sounds typically generated from moving objects.</p>
<p class="indent"><a id="pg137lev1"></a>If vection depends on the assumption of a stable environment, then one would expect the sensation of vection would be enhanced if the stimulus is accepted as stationary. Conversely, if we can reverse that assumption by thinking of the world as an object that can be manipulated (e.g., grabbing the world and moving it toward us as done in the 3D Multi-Touch Pattern; Section <a href="chapter28.html#lev28.3.3"><span class="blue">28.3.3</span></a>), then vection and motion sickness can be reduced (Section <a href="chapter18.html#lev18.3"><span class="blue">18.3</span></a>). Informal feedback from users of such manipulable worlds indeed suggests this to be the case. However, further research is required to determine to what degree.</p>
</body>
</html>
