<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" >
<head>
<title>The VR Book</title>
<link rel="stylesheet" type="text/css" href="../styles/9781970001143.css"/>
</head>
<body>
<h2 class="h2"><a id="page_182"></a><a id="page_183"></a><a id="ch15"></a><span class="blue1">15</span></h2>
<h2 class="h2b"><span class="blue">Latency</span></h2>
<p class="noindent">A fundamental task of a VR system is to present worlds that have no unintended scene motion even as the user&#8217;s head moves. Today&#8217;s VR applications often produce spatially unstable scenes, most notably due to latency.</p>
<p class="indent"><strong>Latency</strong> is the time a system takes to respond to a user&#8217;s action, the true time from the start of movement to the time a pixel resulting from that movement responds [<a href="reference.html#ref143"><span class="blue">Jerald 2009</span></a>]. Note depending on display technology different pixels can show at different times and with different rise and fall times (Section <a href="chapter15.html#lev15.4"><span class="blue">15.4</span></a>). Prediction and warping (Section <a href="chapter18.html#lev18.7"><span class="blue">18.7</span></a>) can reduce effective latency where the goal is to stabilize the scene in real-world space although the actual latency is greater than zero.</p>
<h3 class="h3"><a id="pg183lev1"></a><a id="lev15.1"></a><span class="font">15.1</span> Negative Effects of Latency</h3>
<p class="noindent">For low latencies below ~100 ms, users do not perceive latency directly, but rather the consequences of latency&#8212;a static virtual scene appears to be unstable in space when users move their heads [<a href="reference.html#ref143"><span class="blue">Jerald 2009</span></a>]. Latency in an HMD-based system causes visual cues to lag behind other perceptual cues (e.g., visual cues get out of phase with vestibular cues), creating sensory conflict. With latency and some head motion, the visual scene presented to a subject in an HMD moves incorrectly. This unintended scene motion (Section <a href="chapter12.html#lev12.1"><span class="blue">12.1</span></a>) due to latency is known as &#8220;swimming&#8221; and has serious usability consequences. VR latency is a major contributor to motion sickness, and it is essential for VR creators to understand latency in order to minimize it.</p>
<p class="indent">In addition to causing scene motion and being a primary cause of motion sickness, latency has other negative effects as described below.</p>
<h4 class="h4"><a id="lev15.1.1"></a><span class="font1">15.1.1</span> Degraded Visual Acuity</h4>
<p class="noindent">Latency can cause degraded vision. Given some latency, as an HMD user moves her head then stops, the scene&#8217;s image relative to the head is still moving when the head has stopped moving. If the image velocity on the retina is greater than 2&#8211;3&#176;/s, then motion blur and degraded visual acuity result. Typical head motions and latencies result in scene motion greater than 3&#176;/s. For example, a sinusoidal head motion <a id="page_184"></a>of 0.5 Hz at &#177;20&#176; and 133 ms of latency results in a peak scene motion of &#177;8.5&#176;/s [<a href="reference.html#ref2"><span class="blue">Adelstein et al. 2005</span></a>]. It is not known if users&#8217; eyes tend to follow a lagging image of the scene, resulting in no retinal image slip, or if their eyes tend to stay stabilized in space, resulting in retinal image slip.</p>
<h4 class="h4"><a id="lev15.1.2"></a><span class="font1">15.1.2</span> Degraded Performance</h4>
<p class="noindent">The level of latency necessary to negatively impact performance may be different from the level of latency that can be perceived. <a href="reference.html#ref292"><span class="blue">So and Griffin</span></a> [<a href="reference.html#ref292"><span class="blue">1995</span></a>]) studied the relationship between latency and operator learning in an HMD. The task consisted of tracking a target with the head. Training did not improve performance when latency was &#8805; 120 ms&#8212;subjects were unable to learn to compensate for these latencies in the task.</p>
<h4 class="h4"><a id="lev15.1.3"></a><span class="font1">15.1.3</span> Breaks-in-Presence</h4>
<p class="noindent">Latency detracts from the sense of presence in HMDs [<a href="reference.html#ref210"><span class="blue">Meehan et al. 2003</span></a>]. Latency combined with head movement causes the scene to move in a way not consistent with the real world. This incorrect scene motion can distract the user, who might otherwise <a id="pg184lev1"></a>feel present in the virtual environment, and cause her to realize the illusion is only a simulation.</p>
<h4 class="h4"><a id="lev15.1.4"></a><span class="font1">15.1.4</span> Negative Training Effects</h4>
<p class="noindent">A negative training effect is an unintended decrease in performance that results from training for a task. Latency has been shown to result in negative training effects with desktop displays [<a href="reference.html#ref55"><span class="blue">Cunningham et al. 2001a</span></a>] and driving simulators utilizing large screens [<a href="reference.html#ref56"><span class="blue">Cunningham et al. 2001b</span></a>].</p>
<h3 class="h3"><a id="lev15.2"></a><span class="font">15.2</span> Latency Thresholds</h3>
<p class="noindent">Often engineers build HMD systems with a goal of &#8220;low latency&#8221; without specifically defining what that &#8220;low latency&#8221; is. There is little consensus among researchers what latency requirements should be. Ideally, latency should be low enough so that users are not able to perceive scene motion. Latency thresholds decrease as head motion increases&#8212;latency is much easier to detect when making quick head movements.</p>
<p class="indent">Various experiments conducted at NASA Ames Research Center [<a href="reference.html#ref3"><span class="blue">Adelstein et al. 2003</span></a>, <a href="reference.html#ref84"><span class="blue">Ellis et al. 1999</span></a>, <a href="reference.html#ref83"><span class="blue">2004</span></a>, <a href="reference.html#ref196"><span class="blue">Mania et al. 2004</span></a>] reported latency thresholds during quasi-sinusoidal head yaw. They found absolute thresholds to vary by 85 ms due to bias, type of head movement, individual differences, differences of experimental conditions, and other known and unknown factors. Surprisingly, they found no differences <a id="page_185"></a>in latency thresholds for different scene complexities, ranging from single, simple objects to detailed, photorealistic rendered environments. They found just-noticeable differences to be more consistent at ~4&#8211;40 ms and that users are just as sensitive to changes in latency with a low base latency as those with a higher base latency. Consistent latency is important even when average latency is high.</p>
<p class="indent">After working with NASA on measuring motion thresholds during head turns [<a href="reference.html#ref4"><span class="blue">Adelstein et al. 2006</span></a>], <a href="reference.html#ref143"><span class="blue">Jerald</span></a> [<a href="reference.html#ref143"><span class="blue">2009</span></a>] built a VR system with 7.4 ms of latency (i.e., system delay) and measured HMD latency thresholds for various conditions. He found his most sensitive subject to be able to discriminate between latency differences as small as 3.2 ms. Jerald also developed a mathematical model relating latency, scene motion, head motion, and latency thresholds and then verified that model through psychophysical measurements. The model demonstrates that even though our sensitivity to scene motion decreases as head motion increases (Section <a href="chapter09.html#lev9.3.4"><span class="blue">9.3.4</span></a>), our sensitivity to latency increases. This is because as head motion increases, latency-induced scene motion increases more quickly than scene motion sensitivity decreases.</p>
<p class="indent">Note the above results apply to fully immersive VR. Optical-see-through displays have much lower latency thresholds (under 1 ms) due to users being able to directly discriminate between real-world cues and the synthesized cues (i.e., judgments are <a id="pg185lev1"></a>object-relative instead of subject-relative&#8212;see Section <a href="chapter09.html#lev9.3.2"><span class="blue">9.3.2</span></a>).</p>
<h3 class="h3"><a id="lev15.3"></a><span class="font">15.3</span> Delayed Perception as a Function of Dark Adaptation</h3>
<p class="noindent">The <strong>Pulfrich pendulum effect</strong> is a depth illusion that occurs when one eye is dark adapted (Section <a href="chapter10.html#lev10.2.1"><span class="blue">10.2.1</span></a>) by a different amount than the other eye [<a href="reference.html#ref105"><span class="blue">Gregory 1973</span></a>] or when one eye is covered with a dark filter [<a href="reference.html#ref8"><span class="blue">Arditi 1986</span></a>]. A pendulum swinging in a plane orthogonal to the line of sight appears to swing in an ellipse (i.e., a greater or less distance from the observer at the bottom of its arc, when maximum velocity is reached) instead of a flat arc.</p>
<p class="indent">The &#8220;dark&#8221; eye trades its acuity in space and time for increased light sensitivity. The dark eye&#8217;s retina integrates the incoming light over a longer period of time resulting in a time delay. The dark eye perceives the pendulum to be further in the past, and thus further behind its true position than the light eye. As the pendulum speeds up in the middle of the arc, the dark eye sees its position further and further behind the position seen by the light eye. This difference of effective position creates the illusion of an ellipse lying in depth. This illusion of depth is shown in Figure <a href="chapter15.html#fig15.1"><span class="blue">15.1</span></a>.</p>
<p class="indent">Delayed perception for dark adaptation brings up the question of how/if we perceive latency in VR differently for different lighting conditions. The differences in delay between a light and dark environment can be up to 400 ms [<a href="reference.html#ref6"><span class="blue">Anstis 1986</span></a>]. This delay produces a lengthening of reaction time for automobile drivers in dim light [<a href="reference.html#ref105"><span class="blue">Gregory 1973</span></a>]. Given that visual delay varies for different amounts of dark adaptation or stimulus intensity, then why do people not perceive the world to be unstable or experience motion sickness when they experience greater delays in the dark or with sunglasses as they do in delayed HMDs? Two possible answers to this question are as follows [<a href="reference.html#ref143"><span class="blue">Jerald 2009</span></a>].</p>
<p class="image"><a id="page_186"></a><a id="fig15.1"></a><img src="../images/f0186-01.png" alt="image"/></p>
<p class="caption"><a id="pg186lev1"></a><strong><span class="blue">Figure 15.1</span> The Pulfrich pendulum effect: A pendulum swinging in a straight arc across the line of sight appears to swing in an ellipse when one eye is dark adapted due to the longer delay of the dark-adapted eye.</strong> (Based on <a href="reference.html#ref105"><span class="blue">Gregory</span></a> [<a href="reference.html#ref105"><span class="blue">1973</span></a>])</p>
<p class="indentbullett"><a id="page_187"></a>&#8226; The brain might recognize stimuli to be darker and calibrates for the delay appropriately. If this is true, this suggests users can adapt to latency in HMDs. Furthermore, once the brain understands the relationship between wearing an HMD and delay, position constancy could exist for both the delayed HMD and the real world; the brain would know to expect more delay when an HMD is being worn. However, the brain has had a lifetime of correlating dark adaptation and/or stimulus intensity to delay. The brain might take years to correlate the wearing of an HMD to delay.</p>
<p class="indentbullet">&#8226; The delay inherent in dark adaptation and/or low intensities is not a precise, single delay. Stimuli 1 ms in duration can be perceived to be as long as 400 ms in duration (Section <a href="chapter09.html#lev9.2.2"><span class="blue">9.2.2</span></a>). This imprecise delay results in motion smear (Section <a href="chapter09.html#lev9.3.8"><span class="blue">9.3.8</span></a>) and makes precise localization of moving objects more difficult. Perhaps observers are biased to perceive objects to be more stable in such dark situations, but not for the case of brighter HMDs. If this is true, perhaps darkening the scene or adding motion blur to the scene would cause it to appear to be more stable and reduce sickness (although presenting motion blur typically adds to average latency unless prediction is used).</p>
<h3 class="h3"><a id="pg187lev1"></a><a id="lev15.4"></a><span class="font">15.4</span> Sources of Delay</h3>
<p class="noindent"><a href="reference.html#ref218"><span class="blue">Min&#233;</span></a> [<a href="reference.html#ref218"><span class="blue">1993</span></a>] and <a href="reference.html#ref233"><span class="blue">Olano et al.</span></a> [<a href="reference.html#ref233"><span class="blue">1995</span></a>] characterize system delays in VR systems and discuss various methods of reducing latency. <strong>System delay</strong> is the sum of delays from tracking, application, rendering, display, and synchronization among components. Note the term system delay is used here because some consider latency to be the effective delay that can be less than system delay, accomplished by using delay compensation techniques (Section <a href="chapter18.html#lev18.7"><span class="blue">18.7</span></a>). Thus, system delay is equivalent to true latency, rather than effective latency. Figure <a href="chapter15.html#fig15.2"><span class="blue">15.2</span></a> shows how the various delays contribute to total system delay.</p>
<p class="indent">Note that system delay is greater than the inverse of the update rate; i.e., a pipelined system can have a frame rate of 60 Hz but have a delay of several frames.</p>
<h4 class="h4"><a id="lev15.4.1"></a><span class="font1">15.4.1</span> Tracking Delay</h4>
<p class="noindent"><strong>Tracking delay</strong> is the time from when the tracked part of the body moves until movement information from the tracker&#8217;s sensors resulting from that movement is input into the application or rendering component of the VR system. Tracking products can include techniques that complicate delay analysis. For example, many tracking systems incorporate filtering to smooth jitter. If filters are used, the resulting output pose is only partially determined by the most recent tracker reading, so that precise delay is not well defined. Some trackers use different filtering models that are selected depending on the current motion estimate for different situations&#8212;delay during some movements may differ from that during other movements. For example, the 3rdTech HiBall tracking system allows the option of using multi-modal filtering. A low-pass filter is used to reduce jitter if there is little movement, whereas a different model is used for larger velocities.</p>
<p class="image"><a id="page_188"></a><a id="fig15.2"></a><img src="../images/f0188-01.png" alt="image"/></p>
<p class="caption"><a id="pg188lev1"></a><strong><span class="blue">Figure 15.2</span> End-to-end system delay comes from the delay of the individual system components and from the synchronization of those components.</strong> (Adapted from <a href="reference.html#ref143"><span class="blue">Jerald</span></a> [<a href="reference.html#ref143"><span class="blue">2009</span></a>])</p>
<p class="indent">Tracking is sometimes processed on a different computer from the computer that executes the application and renders the scene. In that case network delay can be considered to be a part of tracking delay.</p>
<h4 class="h4"><a id="lev15.4.2"></a><span class="font1">15.4.2</span> Application Delay</h4>
<p class="noindent"><strong>Application delay</strong> is the time from when tracking data is received until the time data is passed onto the rendering stage. This includes updating the world model, computing the results of a user interaction, physics simulation, etc. This application delay can <a id="page_189"></a>vary greatly depending on the complexity of the task and the virtual world. Application processing can often be executed asynchronously from the rest of the system [<a href="reference.html#ref37"><span class="blue">Bryson and Johan 1996</span></a>]. For example, a weather simulation with input from remote sources could be delayed by several seconds and computed at a slow update rate, whereas rendering needs to be tightly coupled to head pose with minimal delay. Even if the simulation is slow, the user should be able to naturally look around and into the slow updating simulation.</p>
<h4 class="h4"><a id="lev15.4.3"></a><span class="font1">15.4.3</span> Rendering Delay</h4>
<p class="noindent">A <strong>frame</strong> is a full-resolution rendered image. <strong>Rendering delay</strong> is the time from when new data enters the graphics pipeline to the time a new frame resulting from that data is completely drawn. Rendering delay depends on the complexity of the virtual world, the desired quality of the resulting image, the number of rendering passes, and the performance of the graphics software/hardware. The <strong>frame rate</strong> is the number of times the system renders the entire scene per second. <strong>Rendering time</strong> is the inverse of the frame rate, and in non-pipelined rendering systems is equivalent to rendering delay.</p>
<p class="indent"><a id="pg189lev1"></a>Rendering is normally performed on graphics hardware in parallel with the application. For simple scenes, current graphics cards can achieve frame rates of several thousand hertz. Fortunately, rendering delay is what content creators and software developers have the most control over. If geometry is reasonable, the application is optimized, and high-end graphics cards are used, then rendering will be a small portion of end-to-end delay.</p>
<h4 class="h4"><a id="lev15.4.4"></a><span class="font1">15.4.4</span> Display Delay</h4>
<p class="noindent"><strong>Display delay</strong> is the time from when a signal leaves the graphics card to the time a pixel changes to some percentage of the intended intensity defined by the graphics card output. Various display technologies are used for HMDs. These include CRTs (cathode ray tubes), LCDs (liquid crystal displays), OLEDs (organic light-emitting diodes), DLP (digital light processing) projectors, and VRD (virtual retinal displays). Different display technologies also have different advantages and disadvantages. See <a href="reference.html#ref143"><span class="blue">Jerald</span></a> [<a href="reference.html#ref143"><span class="blue">2009</span></a>] for a summary of these display technologies as they relate to delay. The basics of general display delays are discussed below.</p>
<h5 class="h5">Refresh Rate</h5>
<p class="noindent">The <strong>refresh rate</strong> is the number of times per second (Hz) that the display hardware scans out a full image. Note this can be different from the frame rate described in Section <a href="chapter15.html#lev15.4.3"><span class="blue">15.4.3</span></a>. The <strong>refresh time</strong> (also known as stimulus onset asynchrony&#8212;see <a id="page_190"></a>Section <a href="chapter09.html#lev9.3.6"><span class="blue">9.3.6</span></a>) is the inverse of the refresh rate (seconds per refresh). A display with a refresh rate of 60 Hz has a refresh time of 16.7 ms. Typical displays have refresh rates from 60 to 120 Hz.</p>
<h5 class="h5">Double Buffering</h5>
<p class="noindent">In order to avoid memory access issues, the display should not read the frame at the same time it is being written to by the renderer. The frame should not scan out pixels until the rendering is complete, otherwise geometric primitives may not be occluded properly. Furthermore, rendering time can vary, depending on scene complexity, implementation, and hardware, whereas the refresh rate is set solely by the display hardware.</p>
<p class="indent">A solution to this problem of dual access to the frame can be solved by using a <strong>double-buffer</strong> scheme. The display processor renders to one buffer while the refresh controller feeds data to the display from an alternate buffer. The <strong>vertical sync</strong> signal occurs just before the refresh controller begins to scan an image out to the display. Most commonly, the system waits for this vertical sync to swap buffers. The previously rendered frame is then scanned out to the display while a newer frame is rendered. <a id="pg190lev1"></a>Unfortunately, waiting for vertical sync causes additional delay, since rendering must wait up to 16.7 ms (for a 60 Hz display) before starting to render a new frame.</p>
<h5 class="h5">Raster Displays</h5>
<p class="noindent">A <strong>raster display</strong> sweeps pixels out to the display, scanning out left to right in a series of horizontal scanlines from top to bottom [<a href="reference.html#ref331"><span class="blue">Whitton 1984</span></a>]. This pattern is called a raster. Timings are precisely controlled to draw pixels from memory to the correct locations on the screen.</p>
<p class="indent">Pixels on raster displays have inconsistent intra-frame delay (i.e., different parts of the frame have different delay), because pixels are rendered from a single time-sampled viewpoint but are presented at different times; if the system waits on vertical sync, then the bottommost pixels are presented nearly a full refresh time after the top-most pixels are presented.</p>
<p class="noindentt"><strong>Tearing.</strong>&#160;&#160;If the system does not wait for vertical sync to swap buffers, then the buffer swap occurs while the frame is being scanned out to the display hardware. In this case, <strong>tearing</strong> occurs during viewpoint or object motion and appears as a spatially discontinuous image, due to two or more frames (each rendered from a different sampled viewpoint) contributing to the same displayed image. When the system waits for vertical sync to swap buffers, no tearing is evident, because the displayed image comes from a single rendered frame.</p>
<p class="image"><a id="page_191"></a><a id="fig15.3"></a><img src="../images/f0191-01.png" alt="image"/></p>
<p class="caption"><a id="pg191lev1"></a><strong><span class="blue">Figure 15.3</span> A representation of a rectangular object as seen in an HMD as the user is looking from right to left with and without waiting for vertical sync to swap buffers. Not waiting for vertical sync to swap buffers causes image tearing.</strong> (Adapted from <a href="reference.html#ref143"><span class="blue">Jerald</span></a> [<a href="reference.html#ref143"><span class="blue">2009</span></a>])</p>
<p class="indent">Figure <a href="chapter15.html#fig15.3"><span class="blue">15.3</span></a> shows a simulated image that would occur with a system that does not wait for vertical sync superimposed over a simulated image that would occur with a system that does wait for vertical sync. The figure shows what a static virtual block would look like when a user is turning her head from right to left. The tearing is obvious when the swap does not wait for vertical sync, due to four renderings of the images with four different head poses. Thus, most current VR systems avoid tearing at the cost of additional and variable intra-frame delay.</p>
<p class="noindentt"><strong>Just-in-time pixels.</strong>&#160;&#160;Tearing decreases with decreasing differences of head pose. As the sampling rate of tracking increases and the frame rate increases, pose coherence increases and the tearing becomes less evident. If the system were to render each pixel with the correct up-to-date viewpoint, then the tearing would occur between pixels. The tearing would be small compared to the pixel sizes, resulting in a smooth image without perceptual tearing. <a href="reference.html#ref217"><span class="blue">Min&#233; and Bishop</span></a> [<a href="reference.html#ref217"><span class="blue">1993</span></a>] call this <strong>just-in-time pixels.</strong></p>
<p class="indent"><a id="page_192"></a>Rendering could conceivably occur at a rate fast enough that buffers would be swapped for every pixel. Although the entire image would be rendered, only a single pixel would be displayed for each rendered image. However, a 1280 &#215; 1024 image at 60 Hz would require a frame rate of over 78 MHz&#8212;clearly impossible for the foreseeable future using standard rendering algorithms and commodity hardware. If a new image were rendered for every scanline, then the rendering would need to occur at approximately 1/1000 of that or at about 78 kHz. In practice, today&#8217;s systems can render at rates up to 20 kHz for very simple scenes, which make it possible to show a new image every few scanlines.</p>
<p class="indent">With some VR systems, delay caused by waiting for vertical sync can be the largest source of system delay. Ignoring vertical sync greatly reduces overall delay at the cost of image tearing. Note some HMDs rely on vertical sync to perform delay compensation (Section <a href="chapter18.html#lev18.7"><span class="blue">18.7</span></a>), so not waiting on vertical sync is not appropriate for all hardware.</p>
<h5 class="h5">Response Time</h5>
<p class="noindent"><strong>Response time</strong> is the time it takes for a pixel to reach some percentage of its intended intensity. Each technology behaves differently with respect to pixel response time. For example, liquid crystals take time to turn resulting in slow response times&#8212;in some cases over 100 ms! Slow-response displays make it impossible to define a precise delay. Typically, a percentage of the intended intensity is defined, although even that is often not precise due to response time being a function of the starting and intended intensity on a per pixel basis.</p>
<h5 class="h5">Persistence</h5>
<p class="noindent">Display persistence is the amount of time a pixel remains on a display before going away. Many displays hold pixel values/intensities until the next refresh and some displays have a slow dissipation time, causing pixels to remain visible even after the system attempts to turn off those pixels. This results in delay not being well defined. Is the delay up to the time that the pixel first appears or up to the average time that the pixel is visible?</p>
<p class="indent">A slow response time and/or persistence on the display can appear as motion blur and/or &#8220;ghosting.&#8221; Some systems only flash the pixels for some portion of the refresh time (e.g., some OLED implementations) in order to reduce motion blur and judder (Section <a href="chapter09.html#lev9.3.6"><span class="blue">9.3.6</span></a>).</p>
<h4 class="h4"><a id="lev15.4.5"></a><span class="font1">15.4.5</span> Synchronization Delay</h4>
<p class="noindent">Total system delay is not simply a sum of component delays. <strong>Synchronization delay</strong> is the delay that occurs due to integration of pipelined components. Synchronization delay is equal to the sum of component delays subtracted from total system delay. <a id="page_193"></a>Synchronization delay can be due to components waiting for a signal to start new computations and/or asynchrony among components.</p>
<p class="indent">Pipelined components depend upon data from the previous component. When a component starts a new computation and the previous component has not updated data, then old data must be used. Alternatively, the component can in some cases wait for a signal or wait for the input component to finish.</p>
<p class="indent">Trackers provide a good example of a synchronization problem. Commercial tracker vendors report their delays as the tracker response time&#8212;the minimum delay incurred assumes the tracker data is read as soon as it becomes available. If the tracker is not synchronized with the application or rendering component, then the tracking update rate is also a crucial factor and affects both average delay and delay consistency.</p>
<h3 class="h3"><a id="lev15.5"></a><span class="font">15.5</span> Timing Analysis</h3>
<p class="noindent">This section presents an example timing analysis and discusses complexities encountered when analyzing system delay. Figure <a href="chapter15.html#fig15.4"><span class="blue">15.4</span></a> shows a timing diagram for a typical VR system where each colored rectangle represents a period of time that some piece of information is being created. In this example, the scanout to the display begins just after the time of vertical sync. The display stage is shown for discrete frames, even though typical displays present individual pixels at different times. The image delays are the time from the beginning of tracking until the start of scanout to the display. Response time of the display is not shown in this diagram.</p>
<p class="indent">As can be seen in the figure, asynchronous computations often produce repeated images when no new input is available. For example, the rendering component cannot start computing a new result until the application component provides new information. If new application data is not yet available, then the rendering stage repeats the same computation.</p>
<p class="indent">In the figure, the display component displays frame <em>n</em> with the results from the most up-to-date rendering. All the component timings happen to line up fairly well for frame <em>n</em>, and image <em>i</em> delay is not much more than the sum of the individual component delays. Frame <em>n</em> + 1 repeats the display of an entire frame because no new data is available when starting to display that frame. Frame <em>n</em> + 1 has a delay of an additional frame time more than the image <em>i</em> delay because a newly rendered frame was not yet available when display began. Frame <em>n</em> + 4 is delayed even further due to similar reasons. No duplicate data is computed for frame <em>n</em> + 5, but image <em>i</em> + 2 delay is quite high because the rendering and application components must complete their previous computations before starting new computations.</p>
<p class="image"><a id="page_194"></a><a id="fig15.4"></a><img src="../images/f0194-01.png" alt="image"/></p>
<p class="caption"><a id="pg194lev1"></a><strong><span class="blue">Figure 15.4</span> A timing diagram for a typical VR system. In this non-optimal example, the pipelined components execute asynchronously. Components are not able to compute new results until preceding components compute new results themselves.</strong> (Based on [<a href="reference.html#ref143"><span class="blue">Jerald 2009</span></a>])</p>
<h4 class="h4"><a id="lev15.5.1"></a><span class="font1">15.5.1</span> Measuring Delays</h4>
<p class="noindent">To better understand system delay, one can measure timings not only for the end-to-end system delay but also for sub-components of the system. Means and standard deviations can be derived from several such measurements. <strong>Latency meters</strong> can be used to measure system delay. For an example of an open-source, open-hardware latency meter, see <a href="reference.html#ref305"><span class="blue">Taylor</span></a> [<a href="reference.html#ref305"><span class="blue">2015</span></a>].</p>
<p class="indent">Timings can be further analyzed by sampling signals at various stages of the pipeline and measuring the time differences. The parallel port on PCs can be used to output timing signals. These signals are precise in time since there is no additional delay due to a protocol stack; writing to the parallel port is equivalent to writing to memory.</p>
<p class="indent">Synchronization delays between two adjacent components of the pipeline can also be measured indirectly. If the delays of individual components are known, then the sum of two adjacent components can be compared with the measured delay across both components. The difference is the synchronization delay between the two components.</p>
</body>
</html>
