<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" >
<head>
<title>The VR Book</title>
<link rel="stylesheet" type="text/css" href="../styles/9781970001143.css"/>
</head>
<body>
<h2 class="h2"><a id="page_322"></a><a id="page_323"></a><a id="ch28"></a><span class="blue1">28</span></h2>
<h2 class="h2b"><span class="blue">Interaction Patterns and Techniques</span></h2>
<p class="noindent">An <strong>interaction pattern</strong> is a generalized high-level interaction concept that can be used over and over again across different applications to achieve common user goals. The interaction patterns here are intended to describe common approaches to general VR interaction concepts at a high level. Note interaction patterns are different than software design patterns (Section <a href="chapter32.html#lev32.2.5"><span class="blue">32.2.5</span></a>) that many system architects are familiar with. Interaction patterns are described from the user&#8217;s point of view, are largely implementation independent, and state relationships/interactions between the user and the virtual world along with its perceived objects.</p>
<p class="indent"><a id="pg323lev1"></a>An <strong>interaction technique</strong> is more specific and more technology dependent than an interaction pattern. Different interaction techniques that are similar are grouped under the same interaction pattern. For example, the walking pattern (Section <a href="chapter28.html#lev28.3.1"><span class="blue">28.3.1</span></a>) covers several walking interaction techniques ranging from real walking to walking in place. The best interaction techniques consist of high-quality affordances, signifiers, feedback, and mappings (Section <a href="chapter25.html#lev25.2"><span class="blue">25.2</span></a>) that result in an immediate and useful mental model for users.</p>
<p class="indent">Distinguishing between interaction patterns and interaction techniques is important for multiple reasons [<a href="reference.html#ref278"><span class="blue">Sedig and Parsons 2013</span></a>].</p>
<p class="indentbullett">&#8226; There are too many existing interaction techniques with many names and characterizations to remember, and many more will be developed in the future.</p>
<p class="indentbullet">&#8226; Organizing interaction techniques under the umbrella of a broader interaction pattern makes it easier to consider appropriate design possibilities by focusing on conceptual utility and higher-level design decisions before worrying about more specific details.</p>
<p class="indentbullet">&#8226; Broader pattern names and concepts make it easier to communicate interaction concepts.</p>
<p class="indentbullet">&#8226; Higher-level groupings enable easier systematic analysis and comparison.</p>
<p class="indentbullet"><a id="page_324"></a>&#8226; When a specific technique fails, then other techniques within the same pattern can be more easily thought about and explored, resulting in better understanding of why that specific interaction technique did not work as intended.</p>
<p class="indentt">Both interaction patterns and interaction techniques provide conceptual models to experiment with, suggestions and warnings of use, and starting points for innovative new designs. Interaction designers should know and understand these patterns and techniques well so they have a library of options to choose from depending on their needs and a base to innovate upon. Do not fall into the trap that there is a single best interaction pattern or technique. Each pattern and technique has strengths and weaknesses depending on application goals and the type of user [<a href="reference.html#ref337"><span class="blue">Wingrave et al. 2005</span></a>]. Understanding distinctions and managing trade-offs of different techniques is essential to creating high-quality interactive experiences.</p>
<p class="indent">This chapter divides VR interaction patterns into selection, manipulation, viewpoint control, indirect control, and compound patterns. The interaction patterns and their interaction techniques as organized in this chapter are shown in Table <a href="chapter28.html#tab28.1"><span class="blue">28.1</span></a>. The first four patterns are often used sequentially (e.g., a user may travel toward a table, select a tool on the table, and then use that tool to manipulate other objects on the table) or can be integrated together into compound patterns. Interaction techniques that researchers and practitioners have found to be useful are then described within the context of the broader pattern. These techniques are only examples, as many other techniques exist, and such a list could never be exhaustive. The intent for describing these patterns and techniques is for readers to directly use them, to extend them, and to serve as inspiration for creating entirely new ways of interacting within VR.</p>
<p class="tabcaption"><a id="tab28.1"></a><strong><span class="blue">Table 28.1</span> Interaction patterns as organized in this chapter. More specific example interaction techniques are described for each interaction pattern.</strong></p>
<div class="bgcolor2">
<p class="boxbullet"><a id="pg324lev1"></a>&#8226; Selection Patterns (Section <a href="chapter28.html#lev28.1"><span class="blue">28.1</span></a>)</p>
</div>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Hand Selection Pattern</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Pointing Pattern</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Image-Plane Selection Pattern</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Volume-Based Selection Pattern</p>
<div class="bgcolor2">
<p class="boxbullet">&#8226; Manipulation Patterns (Section <a href="chapter28.html#lev28.2"><span class="blue">28.2</span></a>)</p>
</div>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Direct Hand Manipulation Pattern</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Proxy Pattern</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> 3D Tool Pattern</p>
<div class="bgcolor2">
<p class="boxbullet">&#8226; Viewpoint Control Patterns (Section <a href="chapter28.html#lev28.3"><span class="blue">28.3</span></a>)</p>
</div>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Walking Pattern</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Steering Pattern</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> 3D Multi-Touch Pattern</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Automated Pattern</p>
<div class="bgcolor2">
<p class="boxbullet">&#8226; Indirect Control Patterns (Section <a href="chapter28.html#lev28.4"><span class="blue">28.4</span></a>)</p>
</div>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Widgets and Panels Pattern</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Non-Spatial Control Pattern</p>
<div class="bgcolor2">
<p class="boxbullet">&#8226; Compound Patterns (Section <a href="chapter28.html#lev28.5"><span class="blue">28.5</span></a>)</p>
</div>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Pointing Hand Pattern</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> World-in-Miniature Pattern</p>
<p class="boxbulleta"><img src="../images/round.jpg" class="block" alt="image"/> Multimodal Pattern</p>
<h3 class="h3"><a id="page_325"></a><a id="lev28.1"></a><span class="font">28.1</span> <strong>Selection Patterns</strong></h3>
<p class="noindent"><strong>Selection</strong> is the specification of one or more objects from a set in order to specify an object to which a command will be applied, to denote the beginning of a manipulation task, or to specify a target to travel toward [<a href="reference.html#ref208"><span class="blue">McMahan et al. 2014</span></a>]. Selection of objects is not necessarily obvious in VR, especially when most objects are located at a distance from the user. <strong>Selection patterns</strong> include the Hand Selection Pattern, Pointing Pattern, Image-Plane Selection Pattern, and Volume-Based Selection Pattern. Each has advantages over the others depending on the application and task.</p>
<h4 class="h4"><a id="lev28.1.1"></a><span class="font1">28.1.1</span> Hand Selection Pattern</h4>
<h5 class="h5a"><a id="pg325lev1"></a><strong>Related Patterns</strong></h5>
<p class="noindent">Direct Hand Manipulation Pattern (Section <a href="chapter28.html#lev28.2.1"><span class="blue">28.2.1</span></a>) and 3D Tool Pattern (Section <a href="chapter28.html#lev28.2.3"><span class="blue">28.2.3</span></a>).</p>
<h5 class="h5"><strong>Description</strong></h5>
<p class="noindent">The <strong>Hand Selection Pattern</strong> is a direct object-touching pattern that mimics real-world interaction&#8212;the user directly reaches out the hand to touch some object and then triggers a grab (e.g., pushing a button on a controller, making a fist, or uttering a voice command).</p>
<h5 class="h5"><strong>When to Use</strong></h5>
<p class="noindent">Hand selection is ideal for realistic interactions.</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">Physiology limits a fully realistic implementation of hand selection (the arm can only be stretched so far and the wrist rotated so far) to those objects within reach (personal space), requiring the user to first travel to place himself close to the object to be selected. Different user heights and arm lengths can make it uncomfortable for some people to select objects that are at the edge of personal space. Virtual hands and <a id="page_326"></a>arms often occlude objects of interest and can be too large to select small items. Non-realistic hand selection techniques are not as limiting.</p>
<p class="image"><a id="fig28.1"></a><img src="../images/f0326-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 28.1</span> A realistic hand with arm (left), semi-realistic hands with no arms (center), and abstract hands (right).</strong> (Courtesy of Cloudhead Games (left), NextGen Interactions (center), and Digital ArtForms (right))</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>Realistic hands.&#160;&#160;&#160;Realistic hands</strong> are extremely compelling for providing an illusion of self-embodiment (Section <a href="chapter04.html#lev4.3"><span class="blue">4.3</span></a>). Although ideally the entire arm would be tracked, inverse kinematics can estimate the pose of the arm quite well where users typically don&#8217;t notice differences in arm pose if the head and/or torso is tracked along with the hand. Figure <a href="chapter28.html#fig28.1"><span class="blue">28.1</span></a> (left) shows an example of a view from a user who has grabbed a bottle.</p>
<p class="indent">Modeling users (e.g., measuring arm length) and placing objects within a comfortable range depending on the measured arm length is ideal. However, Digital ArtForms found no complaints from ages 10 to adult in a semi-immersive world after setting a single arm-length scale value that was reasonable for the entire range of body sizes.</p>
<p class="noindentt"><strong>Non-realistic hands.</strong>&#160;&#160;&#160;Hands do not need to necessarily look real, and trying to make hands and arms realistic can limit interaction. <strong>Non-realistic hands</strong> does not try to mimic reality but instead focuses on ease of interaction. Often hands are used without arms (Figure <a href="chapter28.html#fig28.1"><span class="blue">28.1</span></a>, center) so that reach can be scaled to make the design of interactions easier. Although the lack of arms can be disturbing at first, users quickly learn to accept having no arms. The hands also need not look like hands. For abstract applications, users are quite accepting of abstract 3D cursors (Figure <a href="chapter28.html#fig28.1"><span class="blue">28.1</span></a>, right) and still feel like they are directly selecting objects. Such hand cursors reduce problems of visual occlusion. Hand occlusion can also be mitigated by making the hand transparent (although proper transparency rendering of multiple objects can be technically challenging to do correctly).</p>
<p class="noindentt"><a id="page_327"></a><strong>Go-go technique.</strong>&#160;&#160;&#160;The <strong>go-go technique</strong> [<a href="reference.html#ref246"><span class="blue">Poupyrev et al. 1996</span></a>] expands upon the concept of a non-realistic hand by enabling one to reach far beyond personal space. The virtual hand is directly mapped to the physical hand when within 2/3 of the full arm&#8217;s reach and when extended further, the hand &#8220;grows&#8221; in a nonlinear manner enabling the user to reach further into the environment. This technique enables closer objects to be selected (and manipulated) with greater accuracy while allowing further objects to be easily reached. Physical aspects of arm length and height have been found to be important for the go-go technique, so measuring arm length should be considered when using this technique [<a href="reference.html#ref27"><span class="blue">Bowman 1999</span></a>]. Measuring arm length can be done by simply asking the user to hold out the hands in front of the body at the start of the application. <a href="reference.html#ref23"><span class="blue">Bowman and Hodges</span></a> [<a href="reference.html#ref23"><span class="blue">1997</span></a>] describes extensions to the go-go technique, such as providing rate control (i.e., velocity) options that enable infinite reach, and compares these to pointing techniques. Non-isomorphic hand rotations (Section <a href="chapter28.html#lev28.2.1"><span class="blue">28.2.1</span></a>) are similar but scale rotations instead of position for manipulation.</p>
<h4 class="h4"><a id="lev28.1.2"></a><span class="font1">28.1.2</span> <strong>Pointing Pattern</strong></h4>
<h5 class="h5a"><strong>Related Patterns</strong></h5>
<p class="noindent">Widgets and Panels Pattern (Section <a href="chapter28.html#lev28.4.1"><span class="blue">28.4.1</span></a>) and Pointing Hand Pattern (Section <a href="chapter28.html#lev28.5.1"><span class="blue">28.5.1</span></a>).</p>
<h5 class="h5"><a id="pg327lev1"></a><strong>Description</strong></h5>
<p class="noindent">The Pointing Pattern is one of the most fundamental and often-used patterns for selection. The <strong>Pointing Pattern</strong> extends a ray into the distance and the first object intersected can then be selected via a user-controlled trigger. Pointing is most typically done with the head (e.g., a crosshair in the center of the field of view) or a hand/finger.</p>
<h5 class="h5"><strong>When to Use</strong></h5>
<p class="noindent">The Pointing Pattern is typically better for selection than the Hand Selection Pattern unless realistic interaction is required. This is especially true for selection beyond personal space and when small hand motions are desired. Pointing is faster when speed of remote selection is important [<a href="reference.html#ref27"><span class="blue">Bowman 1999</span></a>], but is also often used for precisely selecting close objects, such as pointing with the dominant hand to select components on a panel (Section <a href="chapter28.html#lev28.4.1"><span class="blue">28.4.1</span></a>) held in the non-dominant hand.</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">Selection by pointing is usually not appropriate when realistic interaction is required (the exception might be if a laser pointer or remote control is being modeled).</p>
<p class="indent"><a id="page_328"></a>Straightforward implementations result in difficulty selecting small objects in the distance. Pointing with the hand can be imprecise due to natural hand tremor [<a href="reference.html#ref27"><span class="blue">Bowman 1999</span></a>]. Object snapping and precision mode pointing described below can mitigate this problem.</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>Hand pointing.</strong>&#160;&#160;&#160;When hand tracking is available, <strong>hand pointing</strong> with a ray extending from the hand or finger is the most common method of selection. The user then provides a signal to actually select the item of interest (e.g., a button press or gesture with the other hand).</p>
<p class="noindentt"><strong>Head pointing.</strong>&#160;&#160;&#160;When no hand tracking is available, selection via head pointing is the most common form of selection. <strong>Head pointing</strong> is typically implemented by drawing a small pointer or reticle at the center of the field of view so the user simply lines up the pointer with the object of interest and then provides a signal to select the object. The signal is most commonly a button press but when buttons are not available then the item is often selected by <strong>dwell selection</strong>, that is, by holding the pointer on the object for some defined period of time. Dwell selection is not ideal due to having to wait for objects to be selected and accidental selection when looking at an object of interest.</p>
<p class="noindentt"><a id="pg328lev1"></a><strong>Eye gaze selection.&#160;&#160;&#160;Eye gaze selection</strong> is a form of pointing implemented with eye tracking (Section <a href="chapter27.html#lev27.3.2"><span class="blue">27.3.2</span></a>). The user simply looks at an item of interest and then provides a signal to select the looked-at object. In general, eye gaze selection is typically not a good selection technique, primarily due to the Midas Touch problem discussed in Section <a href="chapter27.html#lev27.3.2"><span class="blue">27.3.2</span></a>.</p>
<p class="noindentt"><strong>Object snapping.&#160;&#160;&#160;Object snapping</strong> [<a href="reference.html#ref70"><span class="blue">Haan et al. 2005</span></a>] works by objects having scoring functions that cause the selection ray to snap/bend toward the object with the highest score. This technique works well when selectable objects are small and/or moving.</p>
<p class="noindentt"><strong>Precision mode pointing.&#160;&#160;&#160;Precision mode pointing</strong> [<a href="reference.html#ref165"><span class="blue">Kopper et al. 2010</span></a>] is a non-isomorphic rotation technique that scales down the rotational mapping of the hand to the pointer, as defined by the <strong>control/display (C/D) ratio</strong>. The result is a &#8220;slow motion cursor&#8221; that enables fine pointer control. A zoom lens can also be used that scales the area around the cursor to enable seeing smaller objects (but the zoom should not be affected by head pose unless the zoom area on the display is small; Section <a href="chapter23.html#lev23.2.5"><span class="blue">23.2.5</span></a>). The user can control the amount of zoom with a scroll wheel on a hand-held device.</p>
<p class="noindentt"><strong>Two-handed pointing.&#160;&#160;&#160;Two-handed pointing</strong> originates the selection at the near hand and extends the ray through the far hand [<a href="reference.html#ref219"><span class="blue">Min&#233; et al. 1997</span></a>]. This provides more <a id="page_329"></a>precision when the hands are further apart and fast rotations about a full 360&#176; range when the hands are closer together (whereas a full range of 360&#176; for a single hand pointer is difficult due to physical hand constraints). The distance between the hands can also be used to control the length of the pointer.</p>
<h4 class="h4"><a id="lev28.1.3"></a><span class="font1">28.1.3</span> Image-Plane Selection Pattern</h4>
<h5 class="h5a">Also Known As</h5>
<p class="noindent">Occlusion and Framing.</p>
<h5 class="h5"><strong>Related Pattern</strong></h5>
<p class="noindent">World-in-Miniature Pattern (Section <a href="chapter28.html#lev28.5.2"><span class="blue">28.5.2</span></a>).</p>
<h5 class="h5"><strong>Description</strong></h5>
<p class="noindent">The <strong>Image-Plane Selection Pattern</strong> uses a combination of eye position and hand position for selection [<a href="reference.html#ref240"><span class="blue">Pierce et al. 1997</span></a>]. This pattern can be thought about as the scene and hand being projected onto a 2D image plane in front of the user (or on the eye). The user simply holds one or two hands between the eye and the desired object and then provides a signal to select the object when the object lines up with the hand and eye.</p>
<h5 class="h5"><a id="pg329lev1"></a><strong>When to Use</strong></h5>
<p class="noindent">Image-plane techniques simulate direct touch at a distance, thus are easy to use [<a href="reference.html#ref24"><span class="blue">Bowman et al. 2004</span></a>]. These techniques work well at any distance as long as the object can be seen.</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">Image-plane selection works for a single eye, so users should close one eye while using these techniques (or use a monoscopic display). Image-plane selection results in fatigue when used often due to having to hold the hand up high in front of the eye. As in the Hand Selection Pattern, the hand often occludes objects if not transparent.</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>Head crusher technique.</strong>&#160;&#160;&#160;In the <strong>head crusher technique</strong> (Figure <a href="chapter28.html#fig28.2"><span class="blue">28.2</span></a>), the user positions his thumb and forefinger around the desired object in the 2D image plane.</p>
<p class="noindentt"><strong>Sticky finger technique.</strong>&#160;&#160;&#160;The <strong>sticky finger technique</strong> provides an easier gesture&#8212;the object underneath the user&#8217;s finger in the 2D image is selected.</p>
<p class="image"><a id="page_330"></a><a id="fig28.2"></a><img src="../images/f0330-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 28.2</span> The head crusher selection technique. The inset shows the user&#8217;s view of selecting the chair.</strong> (From <a href="reference.html#ref240"><span class="blue">Pierce et al.</span></a> [<a href="reference.html#ref240"><span class="blue">1997</span></a>])</p>
<p class="noindentt"><strong>Lifting palm technique.</strong>&#160;&#160;&#160;In the <strong>lifting palm technique,</strong> the user selects objects by flattening his outstretched hand and positions the palm so that it appears to lie below the desired object.</p>
<p class="noindentt"><strong>Framing hands technique.</strong>&#160;&#160;&#160;The <strong>framing hands technique</strong> is a two-handed technique where the hands are positioned to form the two corners of a frame in the 2D image surrounding an object.</p>
<h4 class="h4"><a id="lev28.1.4"></a><span class="font1">28.1.4</span> Volume-Based Selection Pattern</h4>
<h5 class="h5a"><a id="pg330lev1"></a><strong>Related Patterns</strong></h5>
<p class="noindent">3D Multi-Touch Pattern (Section <a href="chapter28.html#lev28.3.3"><span class="blue">28.3.3</span></a>) and World-in-Miniature Pattern (Section <a href="chapter28.html#lev28.5.2"><span class="blue">28.5.2</span></a>).</p>
<h5 class="h5"><strong>Description</strong></h5>
<p class="noindent">The <strong>Volume-Based Selection Pattern</strong> enables selection of a volume of space (e.g., a box, sphere, or cone) and is independent of the type of data being selected. Data to be selected can be volumetric (voxels), point clouds, geometric surfaces, or even space containing no data (e.g., to follow with filling that space with some new data). Figure <a href="chapter28.html#fig28.3"><span class="blue">28.3</span></a> shows how a selection box can be used to carve out a volume of space from a medical dataset.</p>
<h5 class="h5"><strong>When to Use</strong></h5>
<p class="noindent">The Volume-Based Selection Pattern is appropriate when the user needs to select a notyet-defined set of data in 3D space or to carve out space within an existing dataset. This pattern enables selection of data when there are no geometric surfaces (e.g., medical CT datasets), whereas geometric surfaces are required for implementing many other selection patterns/techniques (e.g., pointing requires intersecting a ray with object surfaces).</p>
<p class="image"><a id="page_331"></a><a id="fig28.3"></a><img src="../images/f0331-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 28.3</span> The blue user/avatar on the right has carved out a volume of space within a medical dataset via snapping, nudging, and reshaping a selection box (the gray box in front of the green avatar). The green avatar near the center is stepping inside the dataset to examine it from within.</strong> (Courtesy of Digital ArtForms)</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent"><a id="pg331lev1"></a>Selecting volumetric space can be more challenging than selecting a single object with the other more common selection patterns.</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>Cone-casting flashlight.</strong>&#160;&#160;&#160;The <strong>cone-casting flashlight technique</strong> uses pointing, but instead of using a ray, a cone is used. This results in easier selection of small objects than standard pointing via ray casting. If the intent is a single object, then the object closest to the cone&#8217;s center line or the object closest to the user can be selected [<a href="reference.html#ref182"><span class="blue">Liang and Green 1994</span></a>]. A modification of this technique is the aperture technique [<a href="reference.html#ref88"><span class="blue">Forsberg et al. 1996</span></a>], which enables the user to control the spread of the selection volume by bringing the hand closer or further away.</p>
<p class="noindentt"><strong>Two-handed box selection.&#160;&#160;&#160;Two-handed box selection</strong> uses both hands to position, orient, and shape a box via snapping and nudging. Snap and nudge are asymmetric techniques where one hand controls the position and orientation of the selection box, and the second hand controls the shape of the box [<a href="reference.html#ref340"><span class="blue">Yoganandan et al. 2014</span></a>]. Both snap and nudge mechanisms have two stages of interaction&#8212;grab and reshape. Grab influences the position and orientation of the box. Reshape changes the shape of the box.</p>
<p class="indent"><a id="page_332"></a><strong>Snap</strong> immediately brings the selection box to the hand and is designed to quickly access regions of interest that are within arm&#8217;s reach of the user. Snap is an absolute interaction technique, i.e., every time snap is initiated the box position/orientation is reassigned. Therefore, snap focuses on setting the initial pose of the box and placing it comfortably within arm&#8217;s reach.</p>
<p class="indent"><strong>Nudge</strong> enables incremental and precise adjustment and control of the selection box. Nudge works whether the selection box is near or far away from the user by maintaining the box&#8217;s current position, orientation, and scale for the initial grab, but subsequent motion of the box is locked to the hand. Once attached to the hand, the box is positioned and oriented relative to its initial state&#8212;because of this, nudge can be thought of as a relative change in box pose. The box can then be simultaneously reshaped with the other hand while holding down a button.</p>
<h3 class="h3"><a id="lev28.2"></a><span class="font">28.2</span> Manipulation Patterns</h3>
<p class="noindent"><strong>Manipulation</strong> is the modification of attributes for one or more objects such as position, orientation, scale, shape, color, and texture. Manipulation typically follows selection, such as the need to first pick up an object before throwing it. <strong>Manipulation Patterns</strong> include the Direct Hand Manipulation Pattern, Proxy Pattern, and 3D Tool Pattern.</p>
<h4 class="h4"><a id="pg332lev1"></a><a id="lev28.2.1"></a><span class="font1">28.2.1</span> Direct Hand Manipulation Pattern</h4>
<h5 class="h5a"><strong>Related Patterns</strong></h5>
<p class="noindent">Hand Selection Pattern (Section <a href="chapter28.html#lev28.1.1"><span class="blue">28.1.1</span></a>), Pointing Hand Pattern (Section <a href="chapter28.html#lev28.5.1"><span class="blue">28.5.1</span></a>), and 3D Tool Pattern (Section <a href="chapter28.html#lev28.2.3"><span class="blue">28.2.3</span></a>).</p>
<h5 class="h5"><strong>Description</strong></h5>
<p class="noindent">The <strong>Direct Hand Manipulation Pattern</strong> corresponds to the way we manipulate objects with our hands in the real world. After selecting the object, the object is attached to the hand moving along with it until released.</p>
<h5 class="h5"><strong>When to Use</strong></h5>
<p class="noindent">Direct positioning and orientation with the hand have been shown to be more efficient and result in greater user satisfaction than other manipulation patterns [<a href="reference.html#ref23"><span class="blue">Bowman and Hodges 1997</span></a>].</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">Like the Hand Selection Pattern (Section <a href="chapter28.html#lev28.1.1"><span class="blue">28.1.1</span></a>), a straightforward implementation is limited by the physical reach of the user.</p>
<h5 class="h5"><a id="page_333"></a><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>Non-isomorphic rotations.</strong>&#160;&#160;&#160;Some form of clutching is required to rotate beyond certain angles, and clutching can hinder performance due to wasted motion [<a href="reference.html#ref344"><span class="blue">Zhai et al. 1996</span></a>]. Clutching can be reduced by using non-isomorphic rotations [<a href="reference.html#ref247"><span class="blue">Poupyrev et al. 2000</span></a>] that allow one to control larger ranges of 3D rotation with smaller wrist rotation. Non-isomorphic rotations can also be used to provide more precision by mapping large physical rotations to smaller virtual rotations.</p>
<p class="noindentt"><strong>Go-go technique.</strong>&#160;&#160;&#160;The go-go technique (Section <a href="chapter28.html#lev28.1.1"><span class="blue">28.1.1</span></a>) can be used for manipulation as well as selection with no mode change.</p>
<h4 class="h4"><a id="lev28.2.2"></a><span class="font1">28.2.2</span> Proxy Pattern</h4>
<h5 class="h5a"><strong>Related Patterns</strong></h5>
<p class="noindent">Direct Hand Manipulation Pattern (Section <a href="chapter28.html#lev28.2.1"><span class="blue">28.2.1</span></a>) and World-in-Miniature Pattern (Section <a href="chapter28.html#lev28.5.1"><span class="blue">28.5.2</span></a>).</p>
<h5 class="h5"><strong>Description</strong></h5>
<p class="noindent">A <strong>proxy</strong> is a local object (physical or virtual) that represents and maps directly to a remote object. The <strong>Proxy Pattern</strong> uses a proxy to manipulate a remote object. As the user directly manipulates the local object(s), the remote object(s) is manipulated in <a id="pg333lev1"></a>the same way.</p>
<h5 class="h5"><strong>When to Use</strong></h5>
<p class="noindent">This pattern works well when a remote object needs to be intuitively manipulated as if it were in the user&#8217;s hands or when viewing and manipulating objects at multiple scales (e.g., the proxy object can stay the same size relative to the user even as the user scales himself relative to the world and remote object).</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">The proxy can be difficult to manipulate as intended when there is a lack of directional compliance (Section <a href="chapter25.html#lev25.2.5"><span class="blue">25.2.5</span></a>); that is, when there is an orientation offset between the proxy and the remote object.</p>
<h5 class="h5"><strong>Exemplar Interaction Technique</strong></h5>
<p class="noindent"><strong>Tracked physical props.&#160;&#160;&#160;Tracked physical props</strong> are objects directly manipulated by the user (a form of passive haptics; Section <a href="chapter03.html#lev3.2.3"><span class="blue">3.2.3</span></a>) that map to one or more virtual objects and are often used to specify spatial relationships between virtual objects. <a href="reference.html#ref121"><span class="blue">Hinckley et al.</span></a> [<a href="reference.html#ref121"><span class="blue">1998</span></a>] describe an asymmetric two-handed 3D neurosurgical visualization system where the non-dominant hand holds a doll&#8217;s head and the dominant hand holds a planar object or a pointing device (Figure <a href="chapter28.html#fig28.4"><span class="blue">28.4</span></a>). The doll&#8217;s head directly maps to a remotely viewed neurological dataset and the planar object controls a slicing plane <a id="pg334lev1"></a>to see inside the dataset. The pointing device controls a virtual probe. Such physical proxies provide direct action-task correspondence, facilitate natural two-handed interactions, provide tactile feedback to the user, and are extremely easy to use without requiring any training.</p>
<p class="image"><a id="page_334"></a><a id="fig28.4"></a><img src="../images/f0334-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 28.4</span> A physical proxy prop used to control the orientation of a neurological dataset.</strong> (From <a href="reference.html#ref120"><span class="blue">Hinckley et al.</span></a> [<a href="reference.html#ref120"><span class="blue">1994</span></a>])</p>
<h4 class="h4"><a id="lev28.2.3"></a><span class="font1">28.2.3</span> 3D Tool Pattern</h4>
<h5 class="h5a"><strong>Related Patterns</strong></h5>
<p class="noindent">Hand Selection Pattern (Section <a href="chapter28.html#lev28.1.1"><span class="blue">28.1.1</span></a>) and Direct Hand Manipulation Pattern (Section <a href="chapter28.html#lev28.2.1"><span class="blue">28.2.1</span></a>).</p>
<h5 class="h5"><strong>Description</strong></h5>
<p class="noindent">The <strong>3D Tool Pattern</strong> enables users to directly manipulate an intermediary 3D tool with their hands that in turn directly manipulates some object in the world. An example of a 3D tool is a stick to extend one&#8217;s reach or a handle on an object that enables the object to be reshaped.</p>
<h5 class="h5"><strong>When to Use</strong></h5>
<p class="noindent">Use the 3D Tool Pattern to enhance the capability of the hands to manipulate objects. For example, a screwdriver provides precise control of an object by mapping large rotations to small translations along a single axis.</p>
<h5 class="h5"><a id="page_335"></a><strong>Limitations</strong></h5>
<p class="noindent">3D tools can take more effort to use if the user must first travel and maneuver to an appropriate angle in order to apply the tool to an object.</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>Hand-held tools.&#160;&#160;&#160;Hand-held tools</strong> are virtual objects with geometry and behavior that are attached to/held with a hand. Such tools can be used to control objects from afar (like a TV remote control) or to work more directly on an object. A paintbrush used to draw on a surface of an object is an example of a hand-held tool. Hand-held tools are often easier to use and understand than widgets (Section <a href="chapter28.html#lev28.4.1"><span class="blue">28.4.1</span></a>) due to being more direct.</p>
<p class="noindentt"><strong>Object-attached tools.</strong>&#160;&#160;&#160;An <strong>object-attached tool</strong> is a manipulable tool that is attached/colocated with an object. Such a tool results in a more coupled signifier representing the affordance between the object, tool, and user. For example, a color icon might be located on an object, and the user simply selects the icon at which point a color cube appears so the user can choose the color of the object. Or if the shape of a box can be changed, then an adjustment tool can be made available on the corners of the box (e.g., dragging the vertex).</p>
<p class="noindentt"><strong>Jigs.</strong>&#160;&#160;&#160;Precisely aligning and modeling objects can be difficult with 6 DoF input devices due to having no physical constraints. One way to enable precision is to add virtual constraints with jigs. <strong>Jigs,</strong> similar to real-world physical guides used by carpenters and machinists, are grids, rulers, and other referenceable shapes that the user attaches to object vertices, edges, and faces. The user can adjust the jig parameters (e.g., grid spacing) and snap other objects into exact position and orientation relative to the object the jig is attached to. Figure <a href="chapter28.html#fig28.5"><span class="blue">28.5</span></a> shows some examples of jigs. Jig kits support the snapping together of multiple jigs (e.g., snapping a ruler to a grid) for more complex alignments.</p>
<h3 class="h3"><a id="lev28.3"></a><span class="font">28.3</span> Viewpoint Control Patterns</h3>
<p class="noindent"><strong>Viewpoint control</strong> is the task of manipulating one&#8217;s perspective and can include translation, orientation, and scale. Travel (Section <a href="chapter10.html#lev10.4.3"><span class="blue">10.4.3</span></a>) is a form of viewpoint control that does not allow scaling.</p>
<p class="indent">Controlling one&#8217;s viewpoint is equivalent to moving, rotating, or scaling the world. For example, moving the viewpoint to the left is equivalent to moving the world to the right, or scaling oneself to be smaller is equivalent to scaling the world to be larger. Thus, users perceive themselves either as moving through the world (self-motion) or as the world moving around them (world motion) as the viewpoint changes.</p>
<p class="image"><a id="page_336"></a><a id="fig28.5"></a><img src="../images/f0336-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 28.5</span> Jigs used for precision modeling. The blue 3D crosshairs in the left image represent the user&#8217;s hand. The user drags the lower left corner of the orange object to a grid point (left). The user cuts shapes out of a cylinder at 15&#176; angles (center). The user precisely snaps a wireframe-viewed object onto a grid (right).</strong> (Courtesy of Digital ArtForms and Sixense)</p>
<p class="indent"><strong>Viewpoint Control Patterns</strong> include the Walking Pattern, Steering Pattern, 3D Multi-Touch Pattern, and Automated Pattern. Warning: Some implementations of these patterns may induce motion sickness and are not appropriate for users new to VR or those who are sensitive to scene motion. In many cases, sickness can be reduced <a id="pg336lev1"></a>by integrating the techniques with the suggestions in Part <span class="blue">III</span>.</p>
<h4 class="h4"><a id="lev28.3.1"></a><span class="font1">28.3.1</span> Walking Pattern</h4>
<h5 class="h5a"><strong>Description</strong></h5>
<p class="noindent">The <strong>Walking Pattern</strong> leverages motion of the feet to control the viewpoint. Walking within VR [<a href="reference.html#ref297"><span class="blue">Steinicke et al. 2013</span></a>] includes everything from real walking to mimicking walking by moving the feet when seated.</p>
<h5 class="h5"><strong>When to Use</strong></h5>
<p class="noindent">This pattern matches or mimics real-world locomotion and therefore provides a high degree of interaction fidelity. Walking enhances presence and ease of navigation [<a href="reference.html#ref312"><span class="blue">Usoh et al. 1999</span></a>] as well as spatial orientation and movement understanding [<a href="reference.html#ref44"><span class="blue">Chance et al. 1998</span></a>]. Real walking is ideal for navigating small to medium-size spaces, and such travel results in no motion sickness if implemented well.</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">Walking is not appropriate when rapid or distant navigation is important. True walking across large distances requires a large tracked space, and for wired headsets cable tangling can pull on the headset and be a tripping hazard (Section <a href="chapter14.html#lev14.3"><span class="blue">14.3</span></a>). A human <a id="page_337"></a>spotter should closely watch walking users to help stabilize them if necessary. Fatigue can result with prolonged use, and walking distance is limited by the physical motion that users are willing to endure. Cable tangling is also an issue when using a wired system; often an assistant holds the wires and follows the user to prevent tangling and tripping.</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>Real walking.&#160;&#160;&#160;Real walking</strong> matches physical walking with motion in the virtual environment; the mapping from real to virtual is one-to-one. Because of high interaction fidelity, real walking is an ideal interface for many VR experiences. Real walking typically does not directly measure foot motion, but instead tracks the head. Real walking results in less motion sickness due to better matching visual and vestibular cues (although sickness can still result due to issues such as latency and system miscalibration). The motion of one&#8217;s feet in a self-embodied avatar can be estimated or the feet can be tracked to provide greater biomechanical symmetry. Unfortunately, real walking by itself limits travel in the virtual world to the physically tracked space.</p>
<p class="noindentt"><strong>Redirected walking.&#160;&#160;&#160;Redirected walking</strong> [<a href="reference.html#ref257"><span class="blue">Razzaque et al. 2001</span></a>] is a technique that allows users to walk in a VR space larger than the physically tracked space. This is <a id="pg337lev1"></a>accomplished by rotation and translation gains that are different than the true motion of the user, directing the user away from the edges of tracked space (or physical obstacles). Ideally, the gains are below perceptible thresholds so that the user does not consciously realize he is being redirected.</p>
<p class="noindentt"><strong>Walking in place.</strong>&#160;&#160;&#160;Various forms of <strong>walking in place</strong> exist [<a href="reference.html#ref330"><span class="blue">Wendt 2010</span></a>], but all consist of making physical walking motions (e.g., lifting the legs) while staying in the same physical spot but moving virtually. Walking in place works well when there is only a small tracked area and when safety is a primary concern. The safest form of walking in place is for the user to be seated. Theoretically users can walk in place for any distance. However, travel distances are limited by the physical effort the users are willing to make. Thus, walking in place works well for small and medium-size environments where only short durations of travel are required.</p>
<p class="noindentt"><strong>The human joystick.</strong>&#160;&#160;&#160;The <strong>human joystick</strong> [<a href="reference.html#ref207"><span class="blue">McMahan et al. 2012</span></a>] utilizes the user&#8217;s position relative to a central zone to create a 2D vector that defines the horizontal direction and velocity of virtual travel. The user simply steps forward to control speed. The human joystick has the advantage that only a small amount of tracked space is required (albeit more than walking in place).</p>
<p class="noindentt"><a id="page_338"></a><strong>Treadmill walking and running.</strong>&#160;&#160;&#160;Various types of treadmills exist for simulating the physical act of walking and running (Section <a href="chapter03.html#lev3.2.5"><span class="blue">3.2.5</span></a>). Although not as realistic as real walking, treadmills can be quite effective for controlling the viewpoint, providing a sense of self-motion, and walking an unlimited distance. Techniques should make sure foot direction movement is compliant with forward visual motion. Otherwise, treadmill techniques that lack directional and temporal compliance (Section <a href="chapter25.html#lev25.2.5"><span class="blue">25.2.5</span></a>) can be worse than no treadmill. Treadmills with safety harnesses are ideal, especially when physical running is required.</p>
<h4 class="h4"><a id="lev28.3.2"></a><span class="font1">28.3.2</span> Steering Pattern</h4>
<h5 class="h5a"><strong>Description</strong></h5>
<p class="noindent">The <strong>Steering Pattern</strong> is continuous control of viewpoint direction that does not involve movement of the feet. There is typically no need to control viewpoint pitch with controllers as is done with desktop systems since users can physically look up and down.</p>
<h5 class="h5"><strong>When to Use</strong></h5>
<p class="noindent">Steering is appropriate for traveling great distances without the need for physical exertion. When exploring, viewpoint control techniques should allow continuous control, or at least the ability to interrupt a movement after it has begun. Such techniques should also require minimum cognitive load so the user can focus on spatial knowledge acquisition and information gathering. Steering works best when travel is constrained to some height above a surface, acceleration/deceleration can be minimized (Section <a href="chapter18.html#lev18.5"><span class="blue">18.5</span></a>), and real-world stabilized cues can be provided (Sections <a href="chapter12.html#lev12.3.4"><span class="blue">12.3.4</span></a> and <a href="chapter18.html#lev18.2"><span class="blue">18.2</span></a>).</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">Steering provides less biomechanical symmetry than the walking pattern. Many users report symptoms of motion sickness with steering. Virtual turning is more disorienting than physical turning.</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>Navigation by leaning.&#160;&#160;&#160;Navigation by leaning</strong> moves the user in the direction of the lean. The amount of lean typically maps to velocity. One advantage of this technique is no requirement for hand tracking. Motion sickness can be significant as velocity varies (i.e., acceleration).</p>
<p class="noindentt"><strong>Gaze-directed steering.&#160;&#160;&#160;Gaze-directed steering</strong> moves the user in the direction she is looking. Typically, the starting and stopping motion in the gaze direction is controlled <a id="page_339"></a>by the user via a hand-held button or joystick. This is easy to understand and can work well for novices or for those accustomed to first-person video games where the forward direction is identical to the look direction. However, it can also be disorienting as any small head motion changes the direction of travel, and frustrating since users cannot look in one direction while traveling in a different direction.</p>
<p class="noindentt"><strong>Torso-directed steering.&#160;&#160;&#160;Torso-directed steering</strong> (also called chair-directed steering when the torso is not tracked), utilized when traveling over a terrain, separates the direction of travel from the way one is looking. This has more interaction fidelity than gaze-directed steering since in the real world one does not always walk in the direction the head is pointed. In the case when the torso or chair is not tracked, a general forward direction can be assumed. This technique can have more of a nauseating effect if the user does not have a mental model of what the forward direction is or when the entire body is turned when the torso or chair is not tracked. Visual cues can be provided to help users maintain a sense of the forward direction (Figure <a href="chapter18.html#fig18.2"><span class="blue">18.2</span></a>).</p>
<p class="noindentt"><strong>One-handed flying.&#160;&#160;&#160;One-handed flying</strong> works by moving the user in the direction the finger or hand is pointing. Velocity can be determined by the horizontal distance of the hand from the head.</p>
<p class="noindentt"><a id="pg339lev1"></a><strong>Two-handed flying.&#160;&#160;&#160;Two-handed flying</strong> works by moving the user in the direction determined by the vector between the two hands, and the speed is proportional to the distance between the hands [<a href="reference.html#ref219"><span class="blue">Min&#233; et al. 1997</span></a>]. A minimum hand separation is considered to be a &#8220;dead zone&#8221; where motion is stopped. This enables one to quickly stop motion by quickly bringing the hands together. Flying backward with two hands is more easily accomplished than one-handed flying (which requires an awkward hand or device rotation) by swapping the location of the hands.</p>
<p class="noindentt"><strong>Dual analog stick steering.&#160;&#160;&#160;Dual analog stick steering</strong> (also known as joysticks or analog pads) work surprisingly well for steering over a terrain (i.e., forward/back and left/right). In most cases, standard first-person game controls should be used where the left stick controls 2D translation (pushing up/down translates the body and viewpoint forward/backward, pushing left/right translates the body and viewpoint left/right) and the right stick controls left/right orientation (pushing left rotates the body and viewing direction to the left and pushing right rotates the body and viewing direction to the right). This mapping is surprisingly intuitive and is consistent with traditional first-person video games (i.e., gamers already understand how to use such controls so they have little learning curve).</p>
<p class="indent"><a id="page_340"></a>Virtual rotations can be disorienting and sickness inducing for some people. Because of this, the designer might design the experience to have the content consistently in the forward direction so that no virtual rotation is required. Alternatively, if the system is wireless and the torso or chair is tracked, then there is no need for virtual rotations since the user can physically rotate 360&#176;.</p>
<p class="noindentt"><strong>World-grounded steering devices.</strong>&#160;&#160;&#160;World-grounded input devices (Section <a href="chapter27.html#lev27.2.1"><span class="blue">27.2.1</span></a>) such as flight sticks or steering wheels are often used to steer through a world. Such devices can be quite effective for viewpoint control due to the sense of actively controlling a physical device.</p>
<p class="noindentt"><strong>Virtual steering devices.</strong>&#160;&#160;&#160;Instead of using physical steering devices, virtual steering devices can be used. <strong>Virtual steering devices</strong> are visual representations of real-world steering devices (although they do not actually physically exist in the experience) that are used to navigate through the environment. For example, a virtual steering wheel can be used to control a virtual vehicle the user sits in. Virtual devices are more flexible than physical devices as they can be easily changed in software. Unfortunately, virtual devices are difficult to control due to having no proprioceptive force feedback (although some haptic feedback can be provided when using a hand-held controller with haptic capability).</p>
<h4 class="h4"><a id="lev28.3.3"></a><span class="font1">28.3.3</span> 3D Multi-Touch Pattern</h4>
<h5 class="h5a"><strong>Related Patterns</strong></h5>
<p class="noindent">World-in-Miniature Pattern (Section <a href="chapter28.html#lev28.5.2"><span class="blue">28.5.2</span></a>) and Volume-Based Selection Pattern (Section <a href="chapter28.html#lev28.1.4"><span class="blue">28.1.4</span></a>).</p>
<h5 class="h5"><strong>Description</strong></h5>
<p class="noindent">The <strong>3D Multi-Touch Pattern</strong> enables simultaneous modification of the position, orientation, and scale of the world with the use of two hands. Similar to 2D multi-touch on a touch screen, translation via 3D multi-touch is obtained by grabbing and moving space with one hand (monomanual interaction) or with both hands (synchronous bimanual interaction). One difference from 2D multi-touch is that one of the most common ways of using 3D multi-touch is to &#8220;walk&#8221; with the hands by alternating the grabbing of space with each hand (like pulling on a rope but with hands typically wider apart). Scaling of the world is accomplished by grabbing space with both hands and moving the hands apart or bringing them closer together. Rotation of the world is accomplished by grabbing space with both hands and rotating about a point (typically either about one hand or the midpoint between the hands). Translation, rotation, and scale can all be performed simultaneously with a single two-handed gesture.</p>
<h5 class="h5"><a id="page_341"></a><strong>When to Use</strong></h5>
<p class="noindent">3D multi-touch works well for non-realistic interactions when creating assets, manipulating abstract data, viewing scientific datasets, or rapidly exploring large and small areas of interest from arbitrary viewpoints.</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">3D multi-touch is not appropriate when the user is confined to the ground. 3D multi-touch can be challenging to implement as small nuances can affect the usability of the system. If not implemented well, 3D multi-touch can be frustrating to use. Even if implemented well, there can be a learning curve on the order of minutes for some users. Constraints, such as those that keep the user upright, limit scale, and/or disable rotations, can be added for novice users. When scaling is enabled and the display is monoscopic (or there are few depth cues), it can be difficult to distinguish between a small nearby object and a larger object that is further away. Therefore, visual cues that help the user create and maintain a mental model of the world and one&#8217;s place in it can be helpful.</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><a id="pg341lev1"></a><strong>Digital ArtForms&#8217; Two-Handed Interface.</strong> Digital ArtForms has built a mature 3D multi-touch interface called THI (Two-Handed Interface) [<a href="reference.html#ref277"><span class="blue">Schultheis et al. 2012</span></a>] based off of the work of <a href="reference.html#ref197"><span class="blue">Mapes and Moshell</span></a> [<a href="reference.html#ref197"><span class="blue">1995</span></a>] and Multigen-Paradigm&#8217;s SmartScene interface [<a href="reference.html#ref127"><span class="blue">Homan 1996</span></a>] from the 1990s. Figure <a href="chapter28.html#fig28.6"><span class="blue">28.6</span></a> shows a schematic for manipulating the viewpoint. Scale and rotation occur about the middle point between the two hands. Rotation of the world is accomplished by grabbing space with both hands and orbiting the hands about the midpoint between the hands, similar to grabbing a globe on both sides and turning it. Note this is not the same as attaching the world to a single hand as rotating with a single hand can be quite nauseating. Translation, rotation, and scale can all be performed simultaneously with a single two-handed gesture. In multi-user environments, other users&#8217; avatars appear to grow or shrink as they scale themselves.</p>
<p class="image"><a id="fig28.6"></a><img src="../images/f0341-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 28.6</span> Translation, scale, and rotation using Digital ArtForms&#8217; Two-Handed Interface.</strong> (From <a href="reference.html#ref223"><span class="blue">Mlyniec et al.</span></a> [<a href="reference.html#ref223"><span class="blue">2011</span></a>])</p>
<p class="indent"><a id="page_342"></a>Once learned, this implementation of treating the world as an object works well when navigation and selection/manipulation tasks are frequent and interspersed, since viewpoint control and object control are similar other than pushing a different button (i.e., only a single interaction metaphor needs to be learned and cognitive load is reduced by not having to switch between techniques). This gives users the ability to place the world and objects of interest into personal space at the most comfortable working pose via position, rotation, and scaling operations. Digital ArtForms calls this &#8220;posture and approach&#8221; (similar to what <a href="reference.html#ref24"><span class="blue">Bowman et al.</span></a> [<a href="reference.html#ref24"><span class="blue">2004</span></a>] call maneuvering). Posture and approach reduces gorilla arm (Section <a href="chapter14.html#lev14.1"><span class="blue">14.1</span></a>) and users have worked for hours without reports of fatigue [<a href="reference.html#ref147"><span class="blue">Jerald et al. 2013</span></a>]. In addition, physical hand motions are non-repetitive by nature and are reported not to be subject to repetitive stress due to the lack of a physical planar constraint as is the case with a mouse.</p>
<p class="noindentt"><a id="pg342lev1"></a><strong>The spindle.</strong> Building a mental model of the point being scaled and rotated about along with visualizing intent can be difficult for new users, especially when depth cues are absent. A &#8220;spindle&#8221; (Figure <a href="chapter28.html#fig28.7"><span class="blue">28.7</span></a>) consisting of geometry connecting the two hands (what <a href="reference.html#ref14"><span class="blue">Balakrishnan and Hinckley</span></a> [<a href="reference.html#ref14"><span class="blue">2000</span></a>] call visual integration), along with a visual indication of the center of rotation/scale dramatically helps users plan their actions and speeds the training process. Users simply place the center point of the spindle at the point they want to scale and rotate about, push a button in each hand, and &#8220;pull/scale&#8221; themselves toward it (or equivalently &#8220;pull/scale&#8221; the point and the world toward the user) while also optionally rotating about that point. In addition to visualizing the center of rotation/scale, the connecting geometry provides depth-occlusion cues that provide information of where the hands are relative to the geometry.</p>
<h4 class="h4"><a id="lev28.3.4"></a><strong><span class="font1"><span class="blue">28.3.4</span></span> Automated Pattern</strong></h4>
<h5 class="h5a"><strong>Description</strong></h5>
<p class="noindent">The <strong>Automated Pattern</strong> passively changes the user&#8217;s viewpoint. Common methods of achieving this are by being seated on a moving vehicle controlled by the computer or by teleportation.</p>
<p class="image"><a id="page_343"></a><a id="fig28.7"></a><img src="../images/f0343-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 28.7</span> Two hand cursors and a spindle connecting those cursors. The yellow dot between the two cursors is the point that is rotated and scaled about.</strong> (From <a href="reference.html#ref277"><span class="blue">Schultheis et al.</span></a> [<a href="reference.html#ref277"><span class="blue">2012</span></a>])</p>
<h5 class="h5"><a id="pg343lev1"></a><strong>When to Use</strong></h5>
<p class="noindent">Use when the user is playing the role of a passive observer as a passenger controlled by some other entity or when free exploration of the environment is not important or not possible (e.g., due to limitations of today&#8217;s cameras designed for immersive film).</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">The passive nature of this technique can be disorienting and sometimes nauseating (depending on implementation). This pattern is not meant to be used to completely control the camera independent of user motion. VR applications should generally allow the look direction to be independent from the travel direction so the user can freely look around even if other viewpoint motion is occurring. Otherwise significant motion sickness will result.</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>Reducing motion sickness.</strong> Motion sickness can be significantly reduced with this technique by keeping travel speed and direction constant (i.e., keep velocity constant; Section <a href="chapter18.html#lev18.5"><span class="blue">18.5</span></a>), providing world-stabilized cues (Section <a href="chapter18.html#lev18.2"><span class="blue">18.2</span></a>), and creating a leading indicator (Section <a href="chapter18.html#lev18.4"><span class="blue">18.4</span></a>) so users know what motions to expect.</p>
<p class="noindentt"><a id="page_344"></a><strong>Passive vehicles. Passive vehicles</strong> are virtual objects users can enter or step onto that transport the user along some path not controlled by the user. Passenger trains, cars, airplanes, elevators, escalators, and moving sidewalks are examples of passive vehicles.</p>
<p class="noindentt"><strong>Target-based travel and route planning. Target-based travel</strong> [<a href="reference.html#ref29"><span class="blue">Bowman et al. 1998</span></a>] gives a user the ability to select a goal or location he wishes to travel to before being passively moved to that location. <strong>Route planning</strong> [<a href="reference.html#ref28"><span class="blue">Bowman et al. 1999</span></a>] is the active specification of a path between the current location and the goal before being passively moved. Route planning can consist of drawing a path on a map or placing markers that the system uses to create a smooth path.</p>
<p class="noindentt"><strong>Teleportation.</strong> Teleportation is relocation to a new location without any motion. Teleportation is most appropriate when traveling large distances, between worlds, and/or when reducing motion sickness is a primary concern. Fading out and then fading in a scene is less startling than an instantaneous change. Unfortunately, straightforward teleportation comes at the cost of decreasing spatial orientation&#8212;users find it difficult to get their bearings when transported to a new location [<a href="reference.html#ref23"><span class="blue">Bowman and Hodges 1997</span></a>].</p>
<h3 class="h3"><a id="lev28.4"></a><a id="pg344lev1"></a><strong><span class="blue"><span class="font">28.4</span></span> Indirect Control Patterns</strong></h3>
<p class="noindent"><strong>Indirect Control Patterns</strong> provide control through an intermediary to modify an object, the environment, or the system. Indirect control is more abstract than selection, manipulation, and viewpoint control. Indirect control is ideal when an obvious spatial mapping does not exist or it is difficult to directly manipulate an aspect of the environment. Example uses include controlling the overall system, issuing commands, changing modes, and modifying non-spatial parameters.</p>
<p class="indent">Whereas the previously described techniques primarily describe both what should be done and how it is done, indirect control typically specifies only what should be done and the system determines how to do it. Because indirect control is not directly linked to that which it is controlling, signifiers such as the shape and size of controls, their visual representation and labeling, and apparent affordances of their underlying control structure are extremely important.</p>
<p class="indent">Indirect Control Patterns include the Widgets and Panels Pattern and Non-Spatial Control Pattern.</p>
<h4 class="h4"><a id="page_345"></a><a id="lev28.4.1"></a><strong><span class="font1"><span class="blue">28.4.1</span></span> Widgets and Panels Pattern</strong></h4>
<h5 class="h5a"><strong>Related Patterns</strong></h5>
<p class="noindent">Hand Selection Pattern (Section <a href="chapter28.html#lev28.1.1"><span class="blue">28.1.1</span></a>), Pointing Pattern (Section <a href="chapter28.html#lev28.1.2"><span class="blue">28.1.2</span></a>), and Direct Hand Manipulation Pattern.</p>
<h5 class="h5"><strong>Description</strong></h5>
<p class="noindent">The <strong>Widgets and Panels Pattern</strong> is the most common form of VR indirect control, and typically follows 2D desktop widget and panel/window metaphors. A <strong>widget</strong> is a geometric user interface element. A widget might only provide information to the user or might be directly interacted with by the user. The simplest widget is a single label that only provides information. Such a label can also act as a signifier for another widget (e.g., a label on a button). Many system controls are a 1D task and thus can be implemented with widgets such as pull-down menus, radio buttons, sliders, dials, and linear/rotary menus. <strong>Panels</strong> are container structures that multiple widgets and other panels can be placed upon. Placement of panels is important for being able to easily access them. For example, panels can be placed on an object, floating in the world, inside a vehicle, on the display, on the hand or input device, or somewhere near the body (e.g., a semicircular menu always surrounding the waist).</p>
<h5 class="h5"><a id="pg345lev1"></a><strong>When to Use</strong></h5>
<p class="noindent">Widgets and panels are useful for complex tasks where it is difficult to directly interact with an object. Widgets are normally activated via a Pointing Pattern (Section <a href="chapter28.html#lev28.1.2"><span class="blue">28.1.2</span></a>) but can also be combined with other selection options (e.g., select a property for objects within some defined volume). Widgets can provide more accuracy than directly manipulating objects [<a href="reference.html#ref208"><span class="blue">McMahan et al. 2014</span></a>]. Use gestalt concepts of perceptual organization (Section <a href="chapter20.html#lev20.4"><span class="blue">20.4</span></a>) when designing a panel&#8212;use position, color, and shape to emphasize relationships between widgets. For example, put widgets with similar functions close together.</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">Widgets and panels are not as obvious or intuitive as direct mappings and may take longer to learn. The placement of panels and widgets have a big impact on usability. If panels are not within personal space or attached to an appropriate reference frame (Section <a href="chapter26.html#lev26.3"><span class="blue">26.3</span></a>), then the widgets can be difficult to use. If the widgets are too high, gorilla arm (Section <a href="chapter14.html#lev14.1"><span class="blue">14.1</span></a>) can result for actions that take longer than a few seconds. If the widgets are in front of the body or head, then they can be annoying due to occluding the view (although this can be reduced by making the panel translucent).</p>
<p class="noindentt"><a id="page_346"></a>If not oriented to face the user, information on the widget can be difficult to see. Large panels can also block the user&#8217;s view.</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>2D desktop integration.</strong> Many panels are straightforward adaptations of windows from 2D desktop systems (Figure <a href="chapter28.html#fig28.8"><span class="blue">28.8</span></a>, left). An advantage of using desktop metaphors is their familiar interaction style and thus users have an instant understanding of how to use them. <strong>2D desktop integration</strong> brings existing 2D desktop applications into the environment via texture maps and mouse control with pointing. The system shown in Figure <a href="chapter03.html#fig3.5"><span class="blue">3.5</span></a> also provides desktop metaphors such as double-clicking on a selected window header to minimize the window to a small cube (or double clicking the cube to make it large again). Bringing in such existing WIMP (Windows Icon Mouse Pointer) applications is typically not ideal from a 3D interaction perspective, but doing so does provide access to software that otherwise could only be accessed by exiting the virtual world. For example, an existing 2D calculator app could be available as a cube attached to the user&#8217;s &#8220;belt.&#8221; The user then simply selects and double clicks the cube to use the calculator tool without causing a break-in-presence.</p>
<p class="noindentt"><a id="pg346lev1"></a><strong>Ring menus. A ring menu</strong> is a rotary 1D menu where a number of options are displayed concentrically about a center point [<a href="reference.html#ref182"><span class="blue">Liang and Green 1994</span></a>, <a href="reference.html#ref281"><span class="blue">Shaw and Green 1994</span></a>]. Options are selected by rotating the wrist until the intended option rotates into the center position or a pointer rotates to the intended item (Figure <a href="chapter28.html#fig28.8"><span class="blue">28.8</span></a>, center). Ring menus can be useful but can cause wrist discomfort when large rotations are required. Non-isomorphic rotations (Section <a href="chapter28.html#lev28.2.1"><span class="blue">28.2.1</span></a>) can be used to make small wrist rotations map to a larger menu rotation.</p>
<p class="noindentt"><strong>Pie menus. Pie menus</strong> (also known as marking menus) are circular menus with slice-shaped menu entries, where selection is based on direction, not distance. A disadvantage of pie menus is they take up more space than traditional menus (although using icons instead of text can help). Advantages of pie menus compared to traditional menus are that they are faster, more reliable with less error, and have equal distance for each option [<a href="reference.html#ref42"><span class="blue">Callahan 1988</span></a>]. The most important advantage, however, may be that commonly used options are embedded into muscle memory as usage increases. That is, pie menus are self-revealing gestures by showing users what they can do and directing how to do it. This helps novice users become experts in making directional gestures. For example, if a desired pie menu option is in the lower-right quadrant, then the user learns to initiate the pie menu, and then move the hand to the lower right. After extended usage, users can perform the task without the need to look at the pie menu or even for it to be visible. Some pie menu systems only display the pie menu after a delay so the menu does not show up and occlude the scene for fast expert users&#8212;known as &#8220;mark ahead&#8221; since the user marks the pie menu element before it appears [<a href="reference.html#ref169"><span class="blue">Kurtenbach et al. 1993</span></a>].</p>
<p class="image"><a id="page_347"></a><a id="fig28.8"></a><img src="../images/f0347-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 28.8</span> Three examples of hand-held panels with various widgets. The left panel contains standard icons and radio buttons. The center panel contains buttons, a dial, and a rotary menu that can be used as both a ring menu and a pie menu. The right image contains a color cube that the user is selecting a color from.</strong> (Courtesy of Sixense and Digital ArtForms)</p>
<p class="indent"><a id="pg347lev1"></a>Hierarchical pie menus can be used to extend the number of options. For example, to change a color of an object to red, a user might (1) select the object to be modified, (2) press a button to bring up a property&#8217;s pie menu, (3) move the hand to the right to select &#8220;colors,&#8221; which brings up a &#8220;colors&#8221; pie menu, or (4) move the hand down to select the color red by releasing the button. If the user had to commonly change objects to the color red then she would quickly learn to simply move the hand to the right and then down after selecting the object and initiating the pie menu.</p>
<p class="indent"><a href="reference.html#ref94"><span class="blue">Gebhardt et al.</span></a> [<a href="reference.html#ref94"><span class="blue">2013</span></a>] compared different VR pie menu selection methods and found pointing to take less time and to be more preferred by users compared to hand projection (i.e., translation) or wrist roll (twist) rotation.</p>
<p class="noindentt"><strong>Color cube.</strong> A <strong>color cube</strong> is a 3D space that users can select colors from. Figure <a href="chapter28.html#fig28.8"><span class="blue">28.8</span></a> (right) shows a 3D color cube widget&#8212;the color selection puck can be moved with the hand in 2D about the planar surface while the planar surface can be moved in and out.</p>
<p class="noindentt"><strong>Finger menus. Finger menus</strong> consist of menu options attached to the fingers. A pinch gesture with the thumb touching a finger can be used to select different options. Once learned the user is not required to look at the menus; the thumb simply touches the appropriate finger. This prevents occlusion as well as decreases fatigue. The non-dominant hand can select a menu (up to four menus) and the dominant hand can then select one of four items within that menu.</p>
<p class="image"><a id="page_348"></a><a id="fig28.9"></a><img src="../images/f0348-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 28.9</span> TULIP menus on seven fingers and the right palm.</strong> (From <a href="reference.html#ref26"><span class="blue">Bowman and Wingrave</span></a> [<a href="reference.html#ref26"><span class="blue">2001</span></a>])</p>
<p class="indent">For complex applications where more options are required, a TULIP menu (Three-Up, Labels In Palm) [<a href="reference.html#ref26"><span class="blue">Bowman and Wingrave 2001</span></a>] can be used. The dominant hand contains three menu options at a time and the pinky contains a &#8220;more&#8221; option. When the &#8220;more&#8221; option is selected, then the other three finger options are replaced with new options. By placing the upcoming options on the palm of the hand, users know what options will become available if they select the &#8220;more&#8221; option (Figure <a href="chapter28.html#fig28.9"><span class="blue">28.9</span></a>).</p>
<p class="noindentt"><a id="pg348lev1"></a><strong>Above-the-head widgets and panels. Above-the-head widgets and panels</strong> are placed out of the way above the user, and accessed via reaching up and pulling down the widget or panel with the non-dominant hand. Once the panel is released, then it moves up to its former location out of view. The panel might be visible above, especially for new users, but after learning where the panels are located relative to the body, the panel might be made invisible since the user can use his sense of proprioception to know where the panels are without looking. <a href="reference.html#ref219"><span class="blue">Min&#233;et al.</span></a> [<a href="reference.html#ref219"><span class="blue">1997</span></a>] found users could easily select among three options above their field of view (up to the left, up in the middle, and up to the right).</p>
<p class="noindentt"><strong>Virtual hand-held panels.</strong> If a widget or panel is attached somewhere in the environment, then it can be difficult to find. If it is locked in screen space, then it can occlude the scene. One solution is to use <strong>virtual hand-held panels</strong> that have the advantage of always being available (as well as turned off) at the click of a button. Attaching the panel to the hand greatly diminishes many of the problems of world-spaced panels (e.g., panels that are difficult to read or get in the way of other objects can be reoriented <a id="page_349"></a>and moved in an intuitive way without any cognitive effort). The panel should be attached to the non-dominant hand, and the panel is typically interacted with by pointing with the dominant hand (Figure <a href="chapter28.html#fig28.8"><span class="blue">28.8</span></a>). Such an interface provides a &#8220;double dexterity&#8221; where the panel can be brought to the pointer and the pointer can be brought to the panel.</p>
<p class="noindentt"><strong>Physical panels.</strong> Virtual panels offer no physical feedback, which can make it difficult to make precise movements. A <strong>physical panel</strong> is a real-world tracked surface that the user carries and interacts with via a tracked finger, object, or stylus. Using a physical panel can provide fast and accurate manipulation of widgets [<a href="reference.html#ref298"><span class="blue">Stoakley et al. 1995</span></a>, <a href="reference.html#ref186"><span class="blue">Lindeman et al. 1999</span></a>] due to the surface acting as a physical constraint when touched. The disadvantage of a physical panel is that users can become fatigued from carrying it and it can be misplaced if set down. Providing a physical table or other location to set the panel on can help reduce this problem where the panel still travels with the user when virtually moving. Another option is to strap the panel to the forearm [<a href="reference.html#ref322"><span class="blue">Wang and Lindeman 2014</span></a>]. Alternatively, the surface of the arm and/or hand can be used in place of a carried panel.</p>
<h4 class="h4"><a id="lev28.4.2"></a><a id="pg349lev1"></a><strong><span class="font1"><span class="blue">28.4.2</span></span> Non-Spatial Control Pattern</strong></h4>
<h5 class="h5a"><strong>Related Patterns</strong></h5>
<p class="noindent">Multimodal Pattern (Section <a href="chapter28.html#lev28.5.3"><span class="blue">28.5.3</span></a>).</p>
<h5 class="h5"><strong>Description</strong></h5>
<p class="noindent">The <strong>Non-Spatial Control Pattern</strong> provides global action performed through description instead of a spatial relationship. This pattern is most commonly implemented through speech or gestures (Section <a href="chapter26.html#lev26.4"><span class="blue">26.4</span></a>).</p>
<h5 class="h5"><strong>When to Use</strong></h5>
<p class="noindent">Use when options can be visually presented (e.g., gesture icons or text to speak) and appropriate feedback can be provided. This pattern is best used when there are a small number of options to choose from and when a button is available to push-to-talk or push-to-gesture. Use voice when moving the hands or the head would interrupt a task.</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">Gestures and accents are highly variable from user to user and even for a single user. There is often a trade-off of accuracy and generality&#8212;the more gestures or words to be recognized then the less accurate the recognition rate (Section <a href="chapter26.html#lev26.4.2"><span class="blue">26.4.2</span></a>). Defining each gesture or word to be based on its invariant properties and to be independent from <a id="page_350"></a>others makes the task easier for both the system and the user. System recognition of voice can be problematic when many users are present or there is a lot of noise. For important commands, verification may be required and can be annoying to users.</p>
<p class="indent">Even for hypothetically perfectly working systems, providing too many options for users can be overwhelming and confusing. Typically, it is best to only recognize a few options that are simple and easy to remember.</p>
<p class="indent">Depending too heavily on gestures can cause fatigue, especially if the gestures must be made often and above the waist. Some locations are inappropriate for speaking (e.g., a library) and some users are uncomfortable speaking to a computer.</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>Voice menu hierarchies. Voice menu hierarchies</strong> [<a href="reference.html#ref63"><span class="blue">Darken 1994</span></a>] are similar to traditional desktop menus where submenus are brought up after higher-level menu options are selected. Menu options should be visually shown to users so users explicitly know what options are available. See Section <a href="chapter26.html#lev26.4.2"><span class="blue">26.4.2</span></a> for more information about speech recognition.</p>
<p class="noindentt"><strong>Gestures.</strong> Gestures (Section <a href="chapter26.html#lev26.4.1"><span class="blue">26.4.1</span></a>) can work well for non-spatial commands. Gestures should be intuitive and easy to remember. For example, a thumbs-up to confirm, raising the index finger to select &#8220;option 1,&#8221; or raising the index and middle finger to select &#8220;option 2.&#8221; Visual signifiers showing the gesture options available should be presented to the user, especially for users who are learning the gestures. The system should always provide feedback to the user when a gesture has been recognized.</p>
<h3 class="h3"><a id="lev28.5"></a><a id="pg350lev1"></a><strong><span class="font"><span class="blue">28.5</span></span> Compound Patterns</strong></h3>
<p class="noindent"><strong>Compound Patterns</strong> combines two or more patterns into more complicated patterns. Compound Patterns include the Pointing Hand Pattern, World-in-Miniature Pattern, and Multimodal Pattern.</p>
<h4 class="h4"><a id="lev28.5.1"></a><strong><span class="font1"><span class="blue">28.5.1</span></span> Pointing Hand Pattern</strong></h4>
<h5 class="h5a"><strong>Related Patterns</strong></h5>
<p class="noindent">Pointing Pattern (Section <a href="chapter28.html#lev28.1.2"><span class="blue">28.1.2</span></a>), Direct Hand Manipulation Pattern (Section <a href="chapter28.html#lev28.2.1"><span class="blue">28.2.1</span></a>), and Proxy Pattern (Section <a href="chapter28.html#lev28.2.2"><span class="blue">28.2.2</span></a>).</p>
<h5 class="h5"><strong>Description</strong></h5>
<p class="noindent">Hand selection (Section <a href="chapter28.html#lev28.1.1"><span class="blue">28.1.1</span></a>) has limited reach. Pointing (Section <a href="chapter28.html#lev28.1.2"><span class="blue">28.1.2</span></a>) can be used to select distant objects and does not require as much hand movement. However, pointing is often (depending on the task) not good for spatially manipulating objects <a id="page_351"></a>because of the radial nature of pointing (i.e., positioning is done primarily by rotation about an arc around the user) [<a href="reference.html#ref24"><span class="blue">Bowman et al. 2004</span></a>]. Thus, pointing is often better for selection and a virtual hand is better for manipulation. The <strong>Pointing Hand Pattern</strong> combines the Pointing and Direct Hand Manipulation Patterns together so that far objects are first selected via pointing and then manipulated as if held in the hand. The user&#8217;s real hand can also be thought of (and possibly rendered) as a proxy to the remote object.</p>
<h5 class="h5"><strong>When to Use</strong></h5>
<p class="noindent">Use the Pointing Hand Pattern when objects are beyond the user&#8217;s personal space.</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">This pattern is typically not appropriate for applications requiring high interaction fidelity.</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>HOMER.</strong> The <strong>HOMER technique</strong> (Hand-centered Object Manipulation Extending Ray-casting) [<a href="reference.html#ref23"><span class="blue">Bowman and Hodges 1997</span></a>] causes the hand to jump to the object after <a id="pg351lev1"></a>selection by pointing, enabling the user to directly position and rotate the object as if it were held in the hand. The scaled HOMER technique [<a href="reference.html#ref332"><span class="blue">Wilkes and Bowman 2008</span></a>] scales object movement based on how fast the hand is moving (i.e., object translation is based on hand velocity). Fast hand motions enable gross manipulation whereas slow hand motions enable more precise manipulation, providing flexibility of object placement.</p>
<p class="noindentt"><strong>Extender grab.</strong> The <strong>extender grab</strong> [<a href="reference.html#ref219"><span class="blue">Min&#233; et al. 1997</span></a>] maps the object orientation to the user&#8217;s hand orientation. Translations are scaled depending on the distance of the object from the user at the start of the grab (the further the object, the larger the scale factor).</p>
<p class="noindentt"><strong>Scaled world grab.</strong> A <strong>scaled world grab</strong> scales the user to be larger or the environment to be smaller so that the virtual hand, which was originally far from the selected object, can directly manipulate the object in personal space [<a href="reference.html#ref219"><span class="blue">Min&#233; et al. 1997</span></a>]. Because the scaling is about the midpoint between the eyes, the user often does not realize scaling has taken place. If the interpupillary distance is scaled in the same way, then stereoscopic cues will remain the same. Likewise, if the virtual hand is scaled appropriately, then the hand will not appear to change size. What is noticeable is head-motion parallax due to the same physical head movement mapping to a larger movement relative to the scaled-down environment.</p>
<h4 class="h4"><a id="page_352"></a><a id="lev28.5.2"></a><strong><span class="font1"><span class="blue">28.5.2</span></span> World-in-Miniature Pattern</strong></h4>
<h5 class="h5a"><strong>Related Patterns</strong></h5>
<p class="noindent">Image-Plane Selection Pattern (Section <a href="chapter28.html#lev28.1.4"><span class="blue">28.1.4</span></a>), Proxy Pattern (Section <a href="chapter28.html#lev28.2.2"><span class="blue">28.2.2</span></a>), 3D Multi-Touch Pattern (Section <a href="chapter28.html#lev28.3.3"><span class="blue">28.3.3</span></a>), and Automated Pattern (Section <a href="chapter28.html#lev28.3.4"><span class="blue">28.3.4</span></a>).</p>
<h5 class="h5"><strong>Description</strong></h5>
<p class="noindent">A <strong>world-in-miniature</strong> (WIM) is an interactive live 3D map&#8212;an exocentric miniature graphical representation of the virtual environment one is simultaneously immersed in [<a href="reference.html#ref298"><span class="blue">Stoakley et al. 1995</span></a>]. An avatar or &#8220;doll&#8221; representing the self matches the user&#8217;s movements giving an exocentric view of oneself in the world. A transparent viewing frustum extending out from the doll&#8217;s head showing the direction the user is looking along with the user&#8217;s field of view is also useful. When the user moves his smaller avatar, he also moves in the virtual environment. When the user moves a proxy object in the WIM, the object also moves in the surrounding virtual environment. Multiple WIMs can be created to view the world from different perspectives, the VR equivalent of 3D CAD Windowing System.</p>
<h5 class="h5"><strong>When to Use</strong></h5>
<p class="noindent"><a id="pg352lev1"></a>WIMs work well as a way to provide situational awareness via an exocentric view of oneself and the surrounding environment. WIMs are also useful to quickly define user-defined proxies and to quickly move oneself.</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">A straightforward implementation can cause confusion due to the focus being on the WIM, not on the full-scale virtual world.</p>
<p class="indent">A challenge with rotating the WIM is that it results in a lack of directional compliance (Section <a href="chapter25.html#lev25.2.5"><span class="blue">25.2.5</span></a>); translating and rotating a proxy within the WIM can be confusing when looking at the WIM and larger surrounding world from different angles. Because of this challenge, the orientation of the WIM might be directly linked to the larger world orientation to prevent confusion (a forward-up map; Section <a href="chapter22.html#lev22.1.1"><span class="blue">22.1.1</span></a>). However, this comes at the price of not being able to lock in a specific perspective (e.g., to keep an eye on a bridge as the user is performing a task at some other location) as the user reorients himself in the larger space. For advanced users, there should be an option to turn on/off the forward-up locked perspective.</p>
<h5 class="h5"><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>Voodoo doll technique.</strong> The voodoo doll technique uses image-plane selection techniques (Section <a href="chapter28.html#lev28.1.3"><span class="blue">28.1.3</span></a>) by having users temporarily create miniature hand-held proxies of distant objects called <strong>dolls</strong> [<a href="reference.html#ref241"><span class="blue">Pierce et al. 1999</span></a>]. Dolls are selected and used in <a id="page_353"></a>pairs (with each doll representing a different object or set of objects in the scene). The doll in the non-dominant hand (typically representing multiple objects as a partial WIM) acts as a frame of reference, and the doll in the dominant hand (typically representing a single object) defines the position and orientation of its corresponding world object relative to the frame-of-reference doll. This provides the capability for users to quickly and easily position and orient objects relative to each other in the larger world. For example, a user can position a lamp on a table by first selecting the table with the non-dominant hand and then selecting the lamp with the dominant hand. Now, the user simply places the lamp doll on top of the table doll. The table does not move in the larger virtual world and the lamp is placed relative to that table.</p>
<p class="noindentt"><strong>Moving into one&#8217;s own avatar.</strong> By moving one&#8217;s own avatar in the WIM, the user&#8217;s egocentric viewpoint can be changed (i.e., a proxy controlling one&#8217;s own viewpoint). However, directly mapping the doll&#8217;s orientation to the viewpoint can cause significant motion sickness and disorientation. To reduce sickness and confusion, the doll pose can be independent of the egocentric viewpoint; then when the doll icon is released or the user gives a command, the system automatically animates/navigates or teleports the user into the WIM via an automated viewpoint control technique (Section <a href="chapter28.html#lev28.3.4"><span class="blue">28.3.4</span></a>) <a id="pg353lev1"></a>where the user &#8220;becomes&#8221; the doll [<a href="reference.html#ref298"><span class="blue">Stoakley et al. 1995</span></a>]. This avoids the problem of shifting the user&#8217;s cognitive focus back and forth from the map to the full-scale environment. That is, users think in terms of either looking at their doll in an exocentric manner or looking at the larger surrounding world in an egocentric manner, but not both simultaneously. In practice, users do not perceive a change in scale of either themselves or the WIM; they express a sense of going to the new location. Using multiple WIMs allows each WIM to act as a portal to a different, distant space.</p>
<p class="noindentt"><strong>Viewbox.</strong> The <strong>viewbox</strong> [<a href="reference.html#ref223"><span class="blue">Mlyniec et al. 2011</span></a>] is a WIM that also uses the Volume-Based Selection Pattern (Section <a href="chapter28.html#lev28.1.4"><span class="blue">28.1.4</span></a>) and 3D Multi-Touch Pattern (Section <a href="chapter28.html#lev28.3.3"><span class="blue">28.3.3</span></a>). After capturing a portion of the virtual world via volume-based selection, the space within that box is referenced so that it acts as a real-time copy. The viewbox can then be selected and manipulated like any other object. The viewbox can be attached to the hand or body (e.g., near the torso acting as a belt tool). In addition, the user can reach inside the space and manipulate that space via 3D Multi-Touch (e.g., translate, rotate, or scale) in the same way that she can manipulate the larger surrounding space. Because the viewbox is a reference, any object manipulation that occurs in either space occurs in the other space (e.g., painting on a surface in one space results in painting in both spaces). Note due to the object being a reference, care must be taken to stop the recursive nature of the view, otherwise an infinite number of viewboxes within other viewboxes can result.</p>
<h4 class="h4"><a id="page_354"></a><a id="lev28.5.3"></a><strong><span class="font1"><span class="blue">28.5.3</span></span> Multimodal Pattern</strong></h4>
<h5 class="h5a"><strong>Description</strong></h5>
<p class="noindent">Multimodal Patterns integrate different sensory/motor input modalities together (e.g., speech with pointing). Section <a href="chapter26.html#lev26.6"><span class="blue">26.6</span></a> categorizes how different modalities can work together for interaction.</p>
<h5 class="h5"><strong>When to Use</strong></h5>
<p class="noindent">Multimodal Patterns are appropriate when multiple facets of a task are required, reduction in input error is needed, or no single input modality can convey what is needed.</p>
<h5 class="h5"><strong>Limitations</strong></h5>
<p class="noindent">Multimodal techniques can be difficult to implement due to the need to integrate multiple systems/technologies into a single coherent interface and the risk of multiple points of failure. Techniques can also be confusing to users if not implemented well. In order to keep interaction as simple as possible, a multimodal technique should not be used unless there is a good reason to do so.</p>
<h5 class="h5"><a id="pg354lev1"></a><strong>Exemplar Interaction Techniques</strong></h5>
<p class="noindent"><strong>Immersive &#8220;put-that-there.&#8221;</strong> The classic example of the Multimodal Pattern is a <strong>put-that-there</strong> interface [<a href="reference.html#ref21"><span class="blue">Bolt 1980</span></a>] that uses a combination of the Pointing Pattern (to select the &#8220;that&#8221; and &#8220;there&#8221;) and the Non-Spatial Control Pattern via voice (to select the verb &#8220;put&#8221;). <a href="reference.html#ref227"><span class="blue">Neely et al.</span></a> [<a href="reference.html#ref227"><span class="blue">2004</span></a>] implemented an immersive form of a &#8220;put-that-there&#8221; style of region definition. With this technique, the user names and defines the vertices of a polygonal region on a terrain via pointing and speaking. An example is &#8220;Make Target Zone Apple from here (pointing gesture) to here (pointing gesture)&#160;.&#160;.&#160;.&#160;and to here (pointing gesture).&#8221; Figure <a href="chapter32.html#fig32.2"><span class="blue">32.2</span></a> shows the architectural block diagram for this system.</p>
<p class="indent">For manipulating a single object, a &#8220;that-moves-there&#8221; interface (Section <a href="chapter26.html#lev26.6"><span class="blue">26.6</span></a>) where the object is selected before specifying the action is often more efficient (Section <a href="chapter26.html#lev26.5"><span class="blue">26.5</span></a>).</p>
<p class="noindentt"><strong>Automatic mode switching.</strong> One knows when the hand is within the center of vision and can effortlessly bring the hand into view when needed. Mode switching can take advantage of this. For example, an application might switch to an image-plane selection technique (Section <a href="chapter28.html#lev28.1.3"><span class="blue">28.1.3</span></a>) when the hand is moved to the center of vision and to a pointing selection technique (Section <a href="chapter28.html#lev28.1.2"><span class="blue">28.1.2</span></a>) when moved to the periphery (since the source of a ray pointer does not need to be seen).</p>
</body>
</html>
