<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" >
<head>
<title>The VR Book</title>
<link rel="stylesheet" type="text/css" href="../styles/9781970001143.css"/>
</head>
<body>
<h2 class="h2"><a id="page_277"></a><a id="ch25"></a><span class="blue1">25</span></h2>
<h2 class="h2b"><span class="blue">Human-Centered Interaction</span></h2>
<p class="noindent">VR has the potential to provide experiences and deliver results that cannot be otherwise achieved. However, VR interaction is not just about an interface for the user to reach their goals. It is also about users working in an intuitive manner that is a pleasurable experience and devoid of frustration. Although VR systems and applications are incredibly complex, it is up to designers to take on the challenge of having the VR application effectively communicate to users how the virtual world and its tools work so that those users can achieve their goals in an elegant manner.</p>
<p class="indent">Perhaps the most important part of VR interaction is the person doing the interacting. Human-centered interaction design focuses on the human side of communication between user and machine&#8212;the interface from the user&#8217;s point of view. Quality interactions enhance user understanding of what has just occurred, what is happening, what can be done, and how to do it. In the best case, not only will goals and needs be efficiently achieved, but the experiences will be engaging and enjoyable.</p>
<p class="indent">This section describes several general human-centered design concepts that are essential for interaction designers to consider when designing VR interactions.</p>
<h3 class="h3"><a id="lev25.1"></a><strong><span class="font">25.1</span> Intuitiveness</strong></h3>
<p class="noindent">Mental models (Section <a href="chapter07.html#lev7.8"><span class="blue">7.8</span></a>) of how a virtual world works are almost always a simplified version of some complexity. When interacting, there is no need for users to understand the underlying algorithms&#8212;just the high-level relationship between objects, actions, and outcomes. Whether a VR interface attempts to be realistic or not, it should be intuitive. An intuitive interface is an interface that can be quickly understood, accurately predicted, and easily used. Intuitiveness is in the mind of the user, but the designer can help form this intuitiveness by conveying through the world and interface itself concepts that support the creation of a mental model.</p>
<p class="indent"><a id="page_278"></a>An <strong>interaction metaphor</strong> is an interaction concept that exploits specific knowledge that users already have of other domains. Interaction metaphors help users quickly develop a mental model of how an interaction works. For example, VR users typically think of themselves as &#8220;walking&#8221; through an environment, even though in most implementations they are not physically moving their feet (e.g., they may be controlling the walking with a hand-held controller). For such an interface, users assume they are traveling at a set height above a surface, and if that height changes, then the interaction does not fit the mental model and the user will likely become confused.</p>
<p class="indent">Mental models can be made more consistent across users by providing manuals, communicating with other users, and from the virtual world itself. For fully immersive VR, users are cut off from the real world and there is no guarantee they will look at a manual or talk with others. Thus, the virtual world should be sufficient in projecting appropriate information needed to create a consistent conceptual model of how things work in the mind of each user, without requiring external explanation. Otherwise, problems will occur when an application does not fit with the user&#8217;s mental model. It should not be assumed that an expert will always be available to directly explain how an interface works, answer questions, or correct mistakes. Too often, a VR creator expects the user&#8217;s model to be identical to what she thinks she has designed, but unfortunately this is rarely the case since creators and users think differently (as <a id="pg278lev1"></a>they should). Tutorials that cannot be completed until the user has indicated a clear understanding and effective interaction are a great method of inducing mental models into the minds of users.</p>
<h3 class="h3"><a id="lev25.2"></a><strong><span class="font">25.2</span> Norman&#8217;s Principles of Interaction Design</strong></h3>
<p class="noindent">When users interact in VR, they need to figure out how to work the system. <strong>Discoverability</strong> is exploring what something does, how it works, and what operations are possible [<a href="reference.html#ref230"><span class="blue">Norman 2013</span></a>]. Discoverability is especially important for fully immersive VR because the user is blind and deaf to the real world, cut off from those real-world humans who want to help. Essential tools can lead the mind into discovering how an interface works through consistent affordances, unambiguous signifiers, constraints to guide actions and ease interpretation, immediate and useful feedback, and obvious and understandable mappings. These principles as defined by Norman are summarized below along with how they relate to VR and are a good starting point when designing and refining interactions.</p>
<h4 class="h4"><a id="lev25.2.1"></a><strong><span class="font1">25.2.1</span> Affordances</strong></h4>
<p class="noindent"><strong>Affordances</strong> define what actions are possible and how something can be interacted with by a user. We are used to thinking that properties are associated with objects, but <a id="page_279"></a>an affordance is not a property; an affordance is a <em>relationship</em> between the capabilities of a user and the properties of a thing. Interface elements afford interaction, such as a virtual hand affords selection. An affordance between an object and one user may be different between that object and another user. Light switches on a wall offer the ability to control the lighting in a room, but only for those who are able to reach the switches. Some objects in virtual environments afford selecting, moving, controlling, etc. Good interaction design focuses on creating appropriate affordances to make desired actions easily doable with the technology used (e.g., a tracking system that is able to track the hand near the point of the light switch) and by the intended user base (e.g., the designer may intentionally place a light switch to not be reachable by a certain class of users in order to encourage collaboration).</p>
<h4 class="h4"><a id="lev25.2.2"></a><strong><span class="font1">25.2.2</span> Signifiers</strong></h4>
<p class="noindent">Some affordances are perceivable, others are not. To be effective, an affordance should be perceivable. A <strong>signifier</strong> is any perceivable indicator (a signal) that communicates appropriate purpose, structure, operation, and behavior of an object to a user. A good signifier informs a user what is possible before she interacts with its corresponding affordance.</p>
<p class="indent">Examples of signifiers are signs, labels, or images placed in the environment indicating what is to be acted upon, which direction to gesture, or where to navigate toward. Other signifiers directly represent an affordance, such as the handle on a door or the visual and/or physical feel of a button on a controller. A misleading signifier can be ambiguous or not represent an affordance&#8212;something may look like a drawer to be opened when in fact it cannot be opened. Such a false signifier is usually accidental or not yet implemented. But a misleading signifier can also be purposeful&#8212;such as to motivate users to find a key in order to turn the non-accessible drawer into something that can be opened. In such cases, the content creator should be aware of such anti-signifiers and be careful not to frustrate users.</p>
<p class="indent">Signifiers are most often intentional, but, as mentioned above, they may also be accidental. An example of an intentional signifier is a sign giving directions. In the real world, an example of an accidental and unintentional (but useful) signifier is garbage on a beach representing unhealthy conditions. At first thought, we might think signifiers are only intentionally created in VR, for the VR creator created everything from that which does not actually exist. However, this is not always the case. An unintended VR signifier might be an object that looks like it is designed to be picked up to be placed into a puzzle, but it can also be perceived as an object that can be picked up and thrown (a common occurrence much to the frustration of content creators). Or an unintended signifier in a social VR experience might be a gathering of users at an <a id="page_280"></a>area of interest, signifying to others to navigate to that location to investigate what is happening and what affordance might be available there.</p>
<p class="indent">Signifiers might not be attached to a specific object. Signifiers can be general information. Conveying what interaction mode a user is currently using can help prevent confusion.</p>
<p class="indent">Regardless of how and why signifiers are created, signifiers are important for communicating to users whether or not action is possible, and what those actions are. Good VR design ensures signifiers are effectively discoverable through signifiers that are well communicated and intelligible.</p>
<h4 class="h4"><a id="lev25.2.3"></a><strong><span class="font1">25.2.3</span> Constraints</strong></h4>
<p class="noindent">Interaction constraints are limitations of actions and behaviors. Such constraints include logical, semantic, and cultural limitations to guide actions and ease interpretation. With that being said, this section focuses on the physical and mathematical constraints as such constraints most directly apply to VR interactions. For an overview of general project constraints, see Section <a href="chapter31.html#lev31.10"><span class="blue">31.10</span></a>.</p>
<p class="indent">Proper use of constraints can limit possible actions, which makes interaction design feasible and can simplify interaction while improving accuracy, precision, and user efficiency [<a href="reference.html#ref24"><span class="blue">Bowman et al. 2004</span></a>]. A commonly used method for simplifying VR <a id="pg280lev1"></a>interactions is to constrain interfaces to only work in a limited number of dimensions. The <strong>degrees of freedom</strong> (DoF) for an entity are the number of independent dimensions available for the motion of that entity (also see Section <a href="chapter27.html#lev27.1.2"><span class="blue">27.1.2</span></a>). The DoF of an interface can be constrained by the physical limitations of an input device (e.g., a physical dial has one DoF, a joystick has two DoF), the possible motion of a virtual object (e.g., a slider on a panel is constrained to move along a single axis to control a single value), or travel that is constrained to the ground, enabling one to more easily navigate through a scene.</p>
<p class="indent">Physical limitations constrain possible actions and these limitations can also be simulated in software. In addition to being useful, constraints can also add more realism. For example, a virtual hand can be stopped even when there is nothing physically stopping a user&#8217;s real hand from going through the object (although this results in visual-physical conflict; Section <a href="chapter26.html#lev26.8"><span class="blue">26.8</span></a>). However, physics should not always necessarily be used as physics can sometimes make interactions more difficult. For example, it can be useful to leave virtual tools hanging in the air when not using them.</p>
<p class="indent">Interaction constraints are more effective and useful if appropriate signifiers make them easy to perceive and interpret, so users can plan appropriately before taking action. Without effective signifiers, users might effectively be constrained because they are not able to determine what actions are possible. Consistency of constraints <a id="page_281"></a>can also be useful as learning can be transferred across tasks. People are highly resistant to change; if a newway of doing things is only slightly better than the old, then it is better to be consistent. For experts, providing the ability to remove constraints can be useful in some situations. For example, advanced flying techniques might be enabled after users have proved they can efficiently maneuver by being constrained to the ground.</p>
<h4 class="h4"><a id="lev25.2.4"></a><span class="font1">25.2.4</span> Feedback</h4>
<p class="noindent"><strong>Feedback</strong> communicates to the user the results of an action or the status of a task, helps to aid understanding of the state of the thing being interacted with, and helps to drive future action. In VR, timely feedback is essential. Something as simple as moving the head requires immediate visual feedback, otherwise the illusion of a stable world is broken and a break-in-presence results (and, even worse, motion sickness). Input devices capture users&#8217; physical motion that is then transformed into visual, auditory, and haptic feedback. At the same time, feedback internal to the body is generated from within&#8212;proprioceptive feedback that enables one to feel the position and motion of the limbs and body. Unfortunately, it is quite difficult for VR to provide all possible types of feedback. Haptic feedback is especially difficult to implement in a way similar to how real forces occur in the real world. Section <a href="chapter26.html#lev26.8"><span class="blue">26.8</span></a> discusses how sensory substitution can be used in place of strong haptic cues. For example, objects can make a sound, become highlighted, or vibrate a hand-held controller when a hand collides with them or they have been selected.</p>
<p class="indent">Feedback is essential for interaction, but not when it gets in the way of interaction. Too much feedback can overwhelm the senses, resulting in cluttered perception and understanding. Feedback should be prioritized so less important information is presented in an unobtrusive manner and essential information always captures attention. Instead of putting information on a heads-up display in the head reference frame, place it near the waist or toward the ground in the torso reference frame so the information is easily accessed when needed but not in the way otherwise. If information must always be visible with a heads-up display, then only provide the most essential minimal information as anything directly in front of a user reduces awareness of the virtual world. Too many audio beeps or, worse, overlapping audio announcements, can cause users to ignore all of them or even cause them to be indecipherable even if users wanted to listen to them. Such clutter not only can result in unusable applications, but can be annoying (think of backseat drivers!) and is inappropriate. In many cases, users should have the option to turn off or turn down feedback that is not important to them.</p>
<h4 class="h4"><a id="page_282"></a><a id="lev25.2.5"></a><span class="font1">25.2.5</span> Mappings</h4>
<p class="noindent">A <strong>mapping</strong> is a relationship between two or more things. The relationship between a control and its results is easiest to learn where there is an obvious understandable mapping between controls, action, and the intended result. Mappings are useful even when the person is not directly holding the thing being manipulated, for example, when using a screwdriver to lift a lever up that cannot be directly reached.</p>
<p class="indent">Mappings from hardware to interaction techniques defined by software are especially important for VR. Often a device will have a natural mapping to one technique but poor mapping to another technique. For example, hand-tracked devices work well for a virtual hand technique and for pointing (Section <a href="chapter28.html#lev28.1"><span class="blue">28.1</span></a>) but not as well for a driving simulator where a physical steering wheel would be more appropriate.</p>
<h5 class="h5">Compliance</h5>
<p class="noindent"><strong>Compliance</strong> is the matching of sensory feedback with input devices across time and space. Maintaining compliance improves user performance and satisfaction. Compliance results in perceptual binding (Section <a href="chapter07.html#lev7.2.1"><span class="blue">7.2.1</span></a>) so that interaction feels as if one is interacting with a single coherent object. Visual-vestibular compliance is especially important for reducing motion sickness (Section <a href="chapter12.html#lev12.3.1"><span class="blue">12.3.1</span></a>). Compliance can be divided <a id="pg282lev1"></a>into spatial compliance and temporal compliance [<a href="reference.html#ref24"><span class="blue">Bowman et al. 2004</span></a>] as described below.</p>
<p class="noindentt"><strong>Spatial compliance.</strong>&#160;&#160;&#160;Direct spatial mappings lead immediately to understanding. For example, we intuitively understand that once we have grabbed an object, to move the object up, we simply move the hand holding the object up. <strong>Spatial compliance</strong> consists of position compliance, directional compliance, and nulling compliance.</p>
<p class="indent"><strong>Position compliance</strong> is the co-location of sensory feedback with the input device position. An example of spatial compliance is when the proprioceptive sense of where the hand is matches the visual sense of where the hand is. Not all interaction techniques require position compliance, but position compliance results in direct intuitive interaction and should be used whenever appropriate. Labels placed at the location where physical controls are located on a hand-held device is an example of position compliance. Position compliance is important for tracked physical devices that the user can pick up and/or interact with. Consider users who have just placed an HMD on their head but have not yet picked up the hand controllers (or who have previously set the controller down); the user must be able to see the controller in the correct location in order to pick it up.</p>
<p class="indent">Directional compliance is the most important of the three spatial compliances. <strong>Directional compliance</strong> states virtual objects should move and rotate in the same <a id="page_283"></a>direction as the manipulated input device. This results in correspondence of what is seen and what is felt by the body, resulting in more direct interaction. This enables the user to effectively anticipate motion in response to physical input and therefore plan and execute appropriately.</p>
<p class="indent">An example of directional compliance is the mapping from a mouse to a cursor on the screen. Even though a mouse is spatially dislocated from the screen (i.e., it lacks position compliance), the hand/mouse movement results in an immediate and isomorphic movement of the cursor on the screen, which makes the user feel as if he is directly moving the cursor itself [<a href="reference.html#ref315"><span class="blue">van der Veer and del C. P. Melguizo 2002</span></a>]. The user does not think about the offset between the mouse and the cursor when the mouse is used as intended. Even though the mouse typically sits on a flat horizontal surface and the screen sits vertically (i.e., the screen is rotated 90&#176; from the mouse), intuitively we expect if we move the mouse forward/back then the cursor will move up/down in the same way because we think of both as up/down movements. However, for anyone who has attempted to use a mouse when it is rotated by 90&#176; to the right, they know simple manipulations can be extremely difficult due to up becoming left and right becoming up. The same is true for moving objects in VR; the direction of movement for a grabbed object, even if at a distance, should be mapped directly to that selected <a id="pg283lev1"></a>object whenever possible. Likewise, when a user rotates a virtual object with an input device, the virtual object should rotate in the same direction; that is, both should rotate around the same axis of rotation [<a href="reference.html#ref247"><span class="blue">Poupyrev et al. 2000</span></a>].</p>
<p class="indent"><strong>Nulling compliance</strong> states that when a device returns to its initial placement, the corresponding virtual object should also return to its initial placement [<a href="reference.html#ref41"><span class="blue">Buxton 1986</span></a>]. Nulling compliance can be accomplished with absolute devices (relative to some reference), but not relative devices (relative to itself)&#8212;see Section <a href="chapter27.html#lev27.1.3"><span class="blue">27.1.3</span></a>. For example, if a device is attached to the user&#8217;s belt, nulling compliance is important, as the user can use &#8220;muscle memory&#8221; to remember the initial, neutral placement of the device and corresponding virtual object (Section <a href="chapter26.html#lev26.2"><span class="blue">26.2</span></a>).</p>
<p class="noindentt"><strong>Temporal compliance.&#160;&#160;&#160;Temporal compliance</strong> states that different sensory feedback corresponding to the same action or event should be synced appropriately in time. Viewpoint feedback should be immediate to match vestibular cues, otherwise motion sickness may result (Chapter <a href="chapter15.html#ch15"><span class="blue">15</span></a>). But even for reasons unrelated to sickness, feedback should be immediate; otherwise users may become frustrated or give up before tasks are completed. Even if the entire action cannot be completed immediately, there should be some form of feedback implying the problem is being worked on. Without such information, users can become annoyed and computing resources can be wasted since the user may have forgotten about the task and moved on to something else. In <a id="page_284"></a>fact, slow or poor feedback may be worse than no feedback, as it can be distracting, irritating, and anxiety provoking. Anyone who has attempted to browse the Web on an extremely slow Internet connection can attest to this.</p>
<h5 class="h5"><strong>Non-spatial Mappings</strong></h5>
<p class="noindent"><strong>Non-spatial mappings</strong> are functions that transform a spatial input into a non-spatial output or a non-spatial input into a spatial output. Some indirect spatial to non-spatial mappings are universal, such as moving the hand up signifies more and moving the hand down signifies less. Other mappings are personal, cultural, or task dependent. For example, some people think of time moving from left to right whereas others think of time moving from behind the body to the front of it.</p>
<h3 class="h3"><a id="lev25.3"></a><span class="font">25.3</span> Direct vs. Indirect Interaction</h3>
<p class="noindent">Different types of interaction can be thought of as being on a continuum from indirect interaction to direct interaction. Both direct and indirect interactions are useful for VR depending upon the task. Use each where appropriate and do not use one where not appropriate, e.g., don&#8217;t try to force everything to be direct.</p>
<p class="indent"><a id="pg284lev1"></a><strong>Direct interaction</strong> is an impression of direct involvement with an object rather than of communicating with an intermediary [<a href="reference.html#ref135"><span class="blue">Hutchins et al. 1986</span></a>]. The most direct interaction occurs when the user directly interacts with a physical object in the hands. Well-designed hand-held tools (e.g., a knife) that directly affect an object are only slightly less direct, as once a user understands the tool, it seems to become one with the user as if the tool was an extension of the body instead of an intermediary object.</p>
<p class="indent">An example of direct interaction is when a user moves a virtual object on a touch screen with his finger. However, the virtual object does not follow the finger when the finger leaves the plane of the screen. VR enables more direct interactions than other digital technologies because virtual objects can be directly mapped to the hands in 3D space with full spatial (both directional and positional) and temporal compliance (Section <a href="chapter25.html#lev25.2.5"><span class="blue">25.2.5</span></a>). Directional compliance and temporal compliance are more important than position compliance; consider the feeling of directness with a mouse (albeit not quite as directly as moving the cursor with a finger on a touch screen), even though there is no position compliance. Similarly, manipulating an object at a distance can feel like one is directly manipulating the object.</p>
<p class="indent"><strong>Indirect interaction</strong> requires more cognition and conversion between input and output. Performing a search for an image through a typed or verbal query is an example of an indirect interaction. The user must put thought into what is being searched for, provide the query, wait for a response, and then interpret the result. Not only <a id="page_285"></a>must the query be thought about but the conversion from words to and from visual imagery must be considered. Even though such indirect interaction requires more cognition than direct interactions, that does not mean direct interaction is always better. Indirect interactions are more effective for their intended purposes.</p>
<p class="indent">Somewhere in the middle of the extremes of direct and indirect interactions are semi-direct interactions. An example is a slider on a panel that controls the intensity of a light. The user is directly controlling the intermediary slider but less directly controlling the light. However, once the slider is grabbed and moved back and forth a couple of times it feels more direct due to the mental mapping of up for lighter and down for darker.</p>
<h3 class="h3"><a id="lev25.4"></a><span class="font">25.4</span> The Cycle of Interaction</h3>
<p class="noindent">Interaction can be broken down into three parts: (1) forming the goal, (2) executing the action, and (3) evaluating the results [<a href="reference.html#ref230"><span class="blue">Norman 2013</span></a>].</p>
<p class="indent"><strong>Execution</strong> bridges the gap between the goal and result. This feedforward is accomplished through the appropriate use of signifiers, constraints, mappings, and the user&#8217;s mental model. There are three stages of execution that follow from the goal: <a id="pg285lev1"></a>plan, specify, and perform.</p>
<p class="indent"><strong>Evaluation</strong> enables judgment about achieving a goal, making adjustments, or the creation of new goals. This feedback is obtained by perceiving the impact of the action. Evaluation consists of three stages: perceive, interpret, and compare.</p>
<p class="indent">As can be seen in Figure <a href="chapter25.html#fig25.1"><span class="blue">25.1</span></a>, the seven stages of interaction consist of one stage for goals, three stages for execution (plan, specify, and perform), and three stages for evaluation (perceive, interpret, and compare). Quality interaction design considers requirements, intentions, and desires at each stage. An example interaction is as follows.</p>
<p class="indentnumbert">1. Form the <strong>goal.</strong> The question is &#8220;What do I want to accomplish?&#8221; Example: Move a boulder blocking an intended route of travel.</p>
<p class="indentnumber">2. <strong>Plan</strong> the action. Determine which of many possible plans of action to follow. Question: &#8220;What are the alternative set of action sequences and what do I choose?&#8221; Example: Navigate to the boulder or select from a distance to move it.</p>
<p class="indentnumber">3. <strong>Specify</strong> an action sequence. Even after the plan is determined, the specific sequence of actions must be determined. Question: &#8220;What are the specific actions of the sequence?&#8221; Example: Shoot a ray from the hand, intersect the boulder, push the grab button, move the hand to the new location, release the grab button.</p>
<p class="image"><a id="page_286"></a><a id="fig25.1"></a><img src="../images/f0286-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 25.1</span> The cycle of interaction.</strong> (Adapted from <a href="reference.html#ref230"><span class="blue">Norman</span></a> [<a href="reference.html#ref230"><span class="blue">2013</span></a>])</p>
<p class="indentnumber">4. <strong>Perform</strong> the action sequence. Question: &#8220;Can I take action now?&#8221; One must <a id="pg286lev1"></a>actually take action to achieve a result. Example: Move the boulder.</p>
<p class="indentnumber">5. <strong>Perceive</strong> the state of the world. Question: &#8220;What happened?&#8221; Example: The boulder is now in a new location.</p>
<p class="indentnumber">6. <strong>Interpret</strong> the perception. Question: &#8220;What does it mean?&#8221; Example: The boulder is no longer in the path of desired travel.</p>
<p class="indentnumber">7. <strong>Compare</strong> the outcome with the goal. Questions: &#8220;Is this okay? Have I accomplished my goal?&#8221; Example: The route has been cleared so navigation along the route is possible.</p>
<p class="indentt">The interaction cycle can be initiated by establishing a new goal (goal-driven behavior). The cycle can also initiate from some event in the world (data-driven or event-driven behavior). When initiated from the world, goals are opportunistic rather than planned. Opportunistic interactions are less precise and less certain than explicitly specified goals, but they result in less mental effort and more convenience.</p>
<p class="indent">Not all activities in these seven stages are conscious. Goals tend to be reflective (Section <a href="chapter07.html#lev7.7"><span class="blue">7.7</span></a>) but not always. Often, we are only vaguely aware of the execution and evaluation stages until we come across something new or run into an obstacle, at which point conscious attention is needed. At a high reflective level of goal setting and comparison, we assess results in terms of cause and effect. The middle levels of <a id="page_287"></a>specifying and interpreting are often only semi-conscious behavior. The visceral level of performing and perceiving is typically automatic and subconscious unless careful attention is drawn to the actual action.</p>
<p class="indent">Many times the goal is known, but it is not clear how to achieve it. This is known as the <strong>gulf of execution.</strong> Similarly, the <strong>gulf of evaluation</strong> occurs when there is a lack of understanding about the results of an action. Designers can help users bridge these gulfs by thinking about the seven stages of interaction, performing task analysis for creating interaction techniques (Section <a href="chapter32.html#lev32.1"><span class="blue">32.1</span></a>), and utilizing signifiers, constraints, and mappings to help users create effective mental models for interaction.</p>
<h3 class="h3"><a id="lev25.5"></a><span class="font">25.5</span> The Human Hands</h3>
<p class="noindent">The human hands are remarkable and complex input and output devices that naturally interact with other physical objects quickly, precisely, and with little conscious attention. Hand tools have been perfected over thousands of years to perform intended tasks effectively. Figure <a href="chapter08.html#fig8.8"><span class="blue">8.8</span></a> shows that a large portion of the sensory cortex is devoted to the hands. It is not surprising, then, that the best fully interactive VR applications make use of the hands. Using interaction techniques that the hands can <a id="pg287lev1"></a>intuitively work with can add significant value to VR users.</p>
<h4 class="h4"><a id="lev25.5.1"></a><span class="font1">25.5.1</span> Two-Handed Interaction</h4>
<p class="noindent">It is natural and intuitive to reach out with both hands to manipulate objects in the real world, so common sense says two-handed 3D interfaces should be appropriate and intuitive for VR [<a href="reference.html#ref277"><span class="blue">Schultheis et al. 2012</span></a>]. Whereas such intuition tells us that two hands are better than one, two hands can in fact be worse than one hand if the interaction is designed inappropriately [<a href="reference.html#ref153"><span class="blue">Kabbash et al. 1994</span></a>]. Perhaps this is a reason why, even though devices like the mouse have been around for decades, most computer interactions are one handed. Anyone who has developed 3D systems knows that 3D devices alone do not guarantee superior performance, and this is even more true for two hands, in part because the hands do not necessarily work in parallel [<a href="reference.html#ref121"><span class="blue">Hinckley et al. 1998</span></a>]. Iterative design with feedback from real users (Part <a href="part06.html#part6"><span class="blue">VI</span></a>) is essential for creating high-quality bimanual interfaces.</p>
<h5 class="h5"><strong>Bimanual Classifications</strong></h5>
<p class="noindent"><strong>Bimanual interaction</strong> (two-handed interaction) can be classified as <strong>symmetric</strong> (each hand performs identical actions) or <strong>asymmetric</strong> (each hand performs a different action), with asymmetric tasks being more common in the real world [<a href="reference.html#ref109"><span class="blue">Guiard 1987</span></a>].</p>
<p class="indent"><a id="page_288"></a><strong>Bimanual symmetric interactions</strong> can further be classified as synchronous (e.g., pushing on a large object with both hands) or asynchronous (e.g., climbing a ladder with one arm reaching up at a time). Scaling by grabbing two sides of an object and spreading the hands apart is an example of a bimanual symmetric interaction.</p>
<p class="indent"><strong>Bimanual asymmetric interactions</strong> occur when both hands work differently but in a coordinated way to accomplish a task. The <strong>dominant hand</strong> is the user&#8217;s preferred hand for performing fine motor skills. The <strong>non-dominant hand</strong> provides the reference frame, giving the ergonomic benefit of placing the object being worked upon in a way that is comfortable (often subconsciously) for the dominant hand to work with and that does not force the dominant hand to work in a single locked position. The non-dominant hand also typically initiates manipulation of the task and performs gross movements of the object being manipulated, in order to provide convenient, efficient, and precise manipulation by the dominant hand. The most commonly given example is writing; the non-dominant hand controls the orientation of the paper for writing by the dominant hand. Or consider peeling a potato&#8212;the potato is much easier to peel when held in the non-dominant hand than when the potato is sitting on a table (whether the potato is locked in place or not). In a similar manner, unimanual interactions in VR can be awkward when the non-dominant hand does not control the reference frame for the dominant hand.</p>
<p class="indent">By using two hands in a natural manner, the user can specify spatial relationships, not just absolute positions in space. Bimanual interactions ideally should be designed so the two hands work together in a fluid manner, switching between symmetric and asymmetric modes depending on the current task.</p>
</body>
</html>
