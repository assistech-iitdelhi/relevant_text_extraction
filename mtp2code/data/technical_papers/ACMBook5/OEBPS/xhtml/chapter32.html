<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" >
<head>
<title>The VR Book</title>
<link rel="stylesheet" type="text/css" href="../styles/9781970001143.css"/>
</head>
<body>
<h2 class="h2"><a id="page_401"></a><a id="ch32"></a><span class="blue1">32</span></h2>
<h2 class="h2b"><span class="blue">The Make Stage</span></h2>
<p class="noindent">The <strong>Make Stage</strong> is where the specific design and implementation occurs. This stage is arguably the most important part of creating VR experiences. The other stages would only be dreams and theory without the Make Stage whereas at least something, albeit perhaps not good, can be used and experienced with only the Make Stage. The Make Stage is also often where most of the work occurs. Fortunately, the implementation of VR experiences is in many ways similar to creating other software once the experience is properly defined and feedback is obtained. The same logic, algorithms, engineering practices, and design patterns are still used. One primary difference is that there is more emphasis on working with hardware. Makers should not develop VR applications without full access to the VR hardware the system is designed for because the VR experience is so tightly integrated with that specific hardware.</p>
<p class="indent">The Make Stage often consists of primarily using existing tools, hardware, and code where the focus is on gluing the different pieces together so that they work together in harmony. This is the case for most VR projects, as development tools such as Unity and the Unreal Engine have proven to be very effective for building a wide variety of VR worlds. In some cases, the project may require implementing from the ground up. Examples of when that might be the better option is when a true real-time system is required, in-house code with specialized functionality already exists, or specialized/optimized algorithms are required (e.g., volumetric rendering). However, the default should be to use existing frameworks and tools unless there is a good reason not to do so. Resistance to change is no excuse for not using more effective technology. Developers will find the learning curve of using modern tools quite small relative to the benefits.</p>
<p class="indent">This chapter discusses task analysis, design specification, system considerations, simulation, networked environments, prototyping, final production, and delivery.</p>
<h3 class="h3"><a id="page_402"></a><a id="lev32.1"></a><strong><span class="font">32.1</span> Task Analysis</strong></h3>
<p class="noindent"><strong>Task analysis</strong> is analysis of how users accomplish or will accomplish tasks, including both physical actions and cognitive processes. Task analysis is done via insight gained through understanding the user, learning how the user currently performs tasks, and determining what the user wants to do. Task analysis provides organization and structure for description of user activities, which make it easier to describe how the activities fit together.</p>
<p class="indent">Task analysis processes have become complex, fragmented, difficult to perform, and difficult to understand and use [<a href="reference.html#ref53"><span class="blue">Crystal and Ellington 2004</span></a>]. The goal of task analysis should be to organize a set of tasks so they can be <em>easily and clearly</em> communicated and thought about in a systematic manner. If the result of the task analysis complicates understanding through extensive documentation, then the analysis has failed. Task analysis should be kept simple without requiring learning specialized processes or symbols. Part of the reason to do a task analysis is to communicate how interaction works, and anyone should be able to understand the result without requiring specialized training in reading specific forms of task analysis diagrams.</p>
<h4 class="h4"><a id="lev32.1.1"></a><a id="pg402lev1"></a><strong><span class="font1">32.1.1</span> When to Do Task Analysis</strong></h4>
<p class="noindent">The first iteration of task analysis is done when trying to understand how a task is or will be performed. When the goal for the VR application is to replicate actions in the real world (e.g., training applications), task analysis should start with the actual tasks performed in the real world in order to understand what tasks must be carried out, to document proper descriptions of activities, and to look for how to improve the process. Applications built without doing a full task analysis may have low validity relative to the real world due to the task being simplified in the virtual environment [<a href="reference.html#ref17"><span class="blue">Bliss et al. 2014</span></a>]. In other cases, task analysis might be used in creating a new magic VR interaction technique that is very different than any real-world task.</p>
<p class="indent">Task analysis helps to define the design specification (Section <a href="chapter32.html#lev32.2"><span class="blue">32.2</span></a>) and to realize early the implications of redesign. Task analysis results in more than just straightforward understanding of simple tasks; it helps to create understanding of the relationship between different tasks, how information flows, and how users affect task sequence by their decisions. This understanding can help to prioritize tasks and to determine what can be automated without user intervention. Without task analysis, designers might be forced to guess or interpret desired functionality that often leads to poor design. Evaluation plans and validation criteria also depend on task analysis.</p>
<p class="indent">Task analysis can be used for many purposes. One use is to understand, modify, and create new interaction techniques. This can be done by decomposing the techniques <a id="page_403"></a>into subtask components, which enables comparison of subtask components rather than comparing holistic techniques. One can then replace subtask components with other components (whether already used elsewhere or new methods) from other techniques.</p>
<p class="indent">Task analysis is also used at later stages of development to determine how the current implementation deviates from the design/intended solution and what the consequences are. Like all aspects of VR development, task analysis should be flexible and iterative, allowing for modification during all but the final stages of production.</p>
<h4 class="h4"><a id="lev32.1.2"></a><strong><span class="font1">32.1.2</span> How to Do Task Analysis</strong></h4>
<p class="noindent">There are many ways to do task analysis. Here steps are generalized into finding representative users, task elicitation, organizing and structuring, reviewing with representative users, and iteration.</p>
<h5 class="h5"><strong>Find Representative Users</strong></h5>
<p class="noindent">Although in practice task analysis is initially done by the team, it is important to observe and interview representative users. Those matching the personas discussed <a id="pg403lev1"></a>in Section <a href="chapter31.html#lev31.11"><span class="blue">31.11</span></a> should be sought out to make sure the analysis represents activities of the user population. Use more than one person to ensure the analysis is representative of the entire user population. This can be a time-consuming process, so outsourcing someone to find such individuals allows the team to focus on what they are good at.</p>
<h5 class="h5"><strong>Task Elicitation</strong></h5>
<p class="noindent"><strong>Task elicitation</strong> is gathering information in the form of interviews, questionnaires, observation, and documentation [<a href="reference.html#ref91"><span class="blue">Gabbard 2014</span></a>].</p>
<p class="indent">Interviewing (Section <a href="chapter33.html#lev33.3.3"><span class="blue">33.3.3</span></a>) is verbally talking with users, domain experts, and visionary representatives&#8212;this provides insight into what users need and expect. Questionnaires (Section <a href="chapter33.html#lev33.4"><span class="blue">33.4</span></a>) are generally used to help evaluate interfaces that are already in use or have some operational component. Observation is watching an expert perform a real-world task or a VR user trying a prototype (this resembles formative usability evaluation&#8212;see Section <a href="chapter33.html#lev33.3.6"><span class="blue">33.3.6</span></a>). Documentation review identifies task characteristics as derived from technical specifications, existing components, or previous legacy systems.</p>
<p class="indent">To collect information efficiently, one should plan/structure the process. Focus on activities that are currently most relevant and start with an activity that begins with a rough description of what to do. Carefully thinking about the different stages of the cycle of interaction (Section <a href="chapter25.html#lev25.4"><span class="blue">25.4</span></a>) is a great place to start. Ask &#8220;how&#8221; questions to break <a id="page_404"></a>the task into subtasks for more detail (but don&#8217;t get bogged down with details&#8212;know when enough is enough). Ask &#8220;why&#8221; questions to get higher-level task descriptions and context. &#8220;Why&#8221; questions and asking what happens before and what happens after can also help to obtain sequential information. Use graphical depictions with those being talked with to collect higher-quality information.</p>
<h5 class="h5"><strong>Organize and Structure</strong></h5>
<p class="noindent">The most common form of task analysis is <strong>hierarchical task analysis</strong> where tasks are decomposed into smaller subtasks until a sufficient level of detail is reached. Each node below a task in the hierarchy addresses a single subtask. Each subtask can be thought of as a question that must be answered by the designer, and the set of subtasks is the set of possible answers for that question. This is a good place to start as it works well for understanding the details of how users perform actions. High-level task descriptions go on the top with those tasks broken down into lower-level task descriptions. The sequence of tasks is ordered from left to right. Don&#8217;t be overly concerned about getting this perfect or thinking there is a right way to do it&#8212;like everything in iterative design, you will iterate toward something better. The specifics may depend on the project and you can use your own style, such as user activities being represented by boxes and system activities represented by circles and relationships between boxes/circles represented by arrows.</p>
<p class="indent">Hierarchical task analysis has its limitations as the hierarchical pattern limits what can be described with tasks. In addition to hierarchical diagrams, tables, flow charts, hierarchical decomposition, state transition diagrams, and notes can be used where appropriate. Task analysis might also include defining affordances and the corresponding signifiers, constraints, feedback, and mappings (Section <a href="chapter25.html#lev25.2"><span class="blue">25.2</span></a>). For activities that are performed multiple times, generalize the activity to a pattern and give it a name. Then that type of activity can be represented by a single node in different locations. If the task deviates from the general pattern, give it a secondary name and/or a note of specific differences.</p>
<h5 class="h5"><strong>Review with Users</strong></h5>
<p class="noindent">Once data has been organized and structured, one should review with the users who information was elicited to verify understanding.</p>
<h5 class="h5"><strong>Iteration</strong></h5>
<p class="noindent">Task analysis provides the basis for design in terms of what users need to be able to do. This analysis feeds into other steps of the Make Stage, and these Make Stage steps along with lessons acquired from the Learn Stage will feed back into refining the <a id="page_405"></a>task analysis. Like everything else in iterative design, changes to the task analysis will occur. Document the latest changes so it is obvious what changed. Different colors convey such change well.</p>
<h3 class="h3"><a id="lev32.2"></a><strong><span class="font">32.2</span> Design Specification</strong></h3>
<p class="noindent"><strong>Design specification</strong> describes the details of how an application currently is or will be put together and how it works. Design specification is the between stage of defining the project and implementation, and is often closely intertwined or simultaneous with prototyping. In fact, in many cases the person doing the design is also doing much of the prototyping.</p>
<p class="indent">Specifying the design isn&#8217;t just conducted to satisfy that which was created in the Define Stage, but elicits previously unknown assumptions, constraints, requirements, and details of task analysis. <strong>Early wanderings</strong> are the exploration of radically different designs early in the process before converging toward and committing to the final solution [<a href="reference.html#ref33"><span class="blue">Brooks 2010</span></a>]. As new things are discovered, the corresponding defining documentation should be updated as appropriate.</p>
<p class="indent">Some of the most common tools for building VR applications are described <a id="pg405lev1"></a>in this section: sketches, block diagrams, use cases, classes, and software design patterns.</p>
<h4 class="h4"><a id="lev32.2.1"></a><strong><span class="font1">32.2.1</span> Sketches</strong></h4>
<p class="noindent">A <strong>sketch</strong> is a rapid freehand drawing that is not intended as a finished work but is a preliminary exploration of ideas. Sketching is an art and very different from computer-generated renderings. Bill Buxton discusses what characteristics make good sketches in his book <em>Sketching User Experiences</em> [<a href="reference.html#ref40"><span class="blue">Buxton 2007</span></a>]:</p>
<p class="hangt"><strong>Quick and timely.</strong> Too much effort should not be put into creating a sketch and it should be able to be quickly created on demand.</p>
<p class="hanga"><strong>Inexpensive and disposable.</strong> A sketch should be able to be made without concern of cost or care if it is selected for refinement.</p>
<p class="hanga"><strong>Plentiful.</strong> Sketches should not exist in isolation. Many sketches should be made of the same or similar concepts so that different ideas can be explored.</p>
<p class="hanga"><strong>Distinct gestural style.</strong> The style should convey a sense of openness and freedom so that those viewing it don&#8217;t feel like it is in a final form that would be difficult to change. An example is the way edges typically do not perfectly line up that distinguishes a sketch from a computer rendering that is tight and precise.</p>
<p class="hanga"><a id="page_406"></a><strong>Minimal detail.</strong> Detail should be kept to a minimum so that a viewer quickly gets the concept that is trying to be conveyed. The sketch should not imply answers to questions that are not being asked. Going beyond good enough is a negative for sketching, not a positive.</p>
<p class="hanga"><strong>Appropriate degree of refinement.</strong> Detail should match the level of certainty in the designer&#8217;s mind.</p>
<p class="hanga"><strong>Suggestive and explorative.</strong> Good sketches do not inform but rather suggest, and result in discussion more than presentation.</p>
<p class="hanga"><strong>Ambiguity.</strong> Sketches should not specify everything and should be able to be interpreted differently, with viewers seeing relationships in new ways, even by the one doing the sketching.</p>
<p class="indentt">Figure <a href="chapter32.html#fig32.1"><span class="blue">32.1</span></a> shows a sketch drawn by Andrew Robinson of CCP Games with all of these characteristics from the early phases of the VR game EVE Valkyrie.</p>
<p class="image"><a id="fig32.1"></a><img src="../images/f0406-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 32.1</span> Example of an early sketch from the VR game EVE Valkyrie.</strong> (Courtesy of Andrew Robinson of CCP Games)</p>
<p class="image"><a id="page_407"></a><a id="fig32.2"></a><img src="../images/f0407-01.png" alt="image"/></p>
<p class="caption"><a id="pglev407"></a><strong><span class="blue">Figure 32.2</span> A block diagram for a multimodal VR system developed at HRL Laboratories.</strong> (Based on <a href="reference.html#ref227"><span class="blue">Neely et al.</span></a> [<a href="reference.html#ref227"><span class="blue">2004</span></a>])</p>
<h4 class="h4"><a id="lev32.2.2"></a><strong><span class="font1">32.2.2</span> Block Diagrams</strong></h4>
<p class="noindent"><strong>Block diagrams</strong> are high-level diagrams showing interconnections between different system components. Boxes are used to denote components, and arrows connecting the components show the input and output between the components. Block diagrams are used to show the overall relationship of components without concern for the details. Figure <a href="chapter32.html#fig32.2"><span class="blue">32.2</span></a> shows a block diagram for a multimodal VR system with various software and hardware components.</p>
<h4 class="h4"><a id="lev32.2.3"></a><strong><span class="font1">32.2.3</span> Use Cases</strong></h4>
<p class="noindent"><strong>A use case</strong> is a set of steps that helps to define interactions between the user and the system in order to achieve a goal. Defining use cases helps developers identify, clarify, and organize interactions in a way that makes it easier to implement the interactions. As interaction complexity grows, the need for explicit use cases increases. Use cases <a id="page_408"></a>often start from user stories (Section <a href="chapter31.html#lev31.12"><span class="blue">31.12</span></a>) but are more formal and detailed, enabling developers to get closer to implementation. Use cases can also result from task analysis (Section <a href="chapter32.html#lev32.1"><span class="blue">32.1</span></a>).</p>
<p class="indent"><strong>Use case scenarios</strong> are specific examples of interactions along a single path of a use case. Use cases have multiple scenarios, i.e., a use case is a collection of possible scenarios related to a particular goal.</p>
<p class="indent">There is no standard way of writing a use case. Some prefer visual diagrams whereas others prefer written text. Some form of the following should be included.</p>
<p class="indentbullett">&#8226; Use case name</p>
<p class="indentbullet">&#8226; Intended result</p>
<p class="indentbullet">&#8226; Description</p>
<p class="indentbullet">&#8226; Preconditions</p>
<p class="indentbullet">&#8226; Primary scenario</p>
<p class="indentbullet">&#8226; Alternate scenarios</p>
<p class="indentt">Outlining the scenarios with hierarchical levels can be useful to provide both high-level and more detailed views. This also allows the filling in of details as more specific detail is needed. Colors or different font types can be used to see what needs to be implemented and what has already been implemented/tested, the differences between the primary path and alternative paths, or who is assigned to what steps.</p>
<p class="indent">Use cases do not need to exist in isolation. A use case might extend upon an already existing use. Or a large use case might be broken apart into multiple use cases if there are common steps that are used multiple times throughout the larger use case. These smaller use cases can also be included within other larger use cases where appropriate. This helps to make user interactions consistent across the entire experience. For example, a pointing selection technique might be a small use case that is part of a larger use case that requires many selections and manipulations. The same small pointing use case might also be used as part of an automated travel technique (select an object to travel toward and then the system somehow moves you to that location).</p>
<h4 class="h4"><a id="lev32.2.4"></a><strong><span class="font1">32.2.4</span> Classes</strong></h4>
<p class="noindent">As the design specification evolves, it is necessary to get clearer on how the design will actually be implemented in code. This can be done by taking information from the previous steps and organizing that information into common structures and functionality into themes that can be described and implemented as classes. A <strong>class</strong> is a template for a set of data and methods. Data corresponds to nouns and properties <a id="page_409"></a>from information collected in previous steps, and methods correspond to verbs and behaviors.</p>
<p class="indent">A program <strong>object</strong> is a specific instantiation of a class. A class exists by itself without having to be compiled into an executable program (e.g., classes exist even when the virtual environment does not yet exist) whereas an object is a representation of information organized in a manner defined by a class that is included in the executable program. An example of a program object is a perceivable virtual object in the virtual environment, such as a rock on the ground that can be picked up.</p>
<p class="indent">A real-world analogy of a class is a set of blueprints for a house. A blueprint is like a class that describes the house but is not a house itself. Construction workers instantiate the blueprints into a real house. The blueprints can be used to build an arbitrary number of houses. Likewise, classes define what can be instantiated into objects inside a virtual environment. An example of a class is a box class that can be instantiated as a specific box that exists in the virtual environment. The box class can be instantiated many times to create multiple box objects in the environment. The class might define what characteristics are possible, and the box object can take on any of those possible characteristics. For example, different box objects in the environment might have different colors.</p>
<p class="indent">Classes and objects do not have to represent physical things. They might also represent properties, behaviors, or interaction techniques. A simple example is a color class. Instantiated objects of the color class might be specific colors such as red, green, or blue. These color objects, along with other objects, could then be associated to each individual box to describe what the box looks like.</p>
<p class="indent">A <strong>class diagram</strong> describes classes, along with their properties and methods, and the relationship between classes. Class diagrams directly state what needs to be (or already is) implemented. Figure <a href="chapter32.html#fig32.3"><span class="blue">32.3</span></a> shows an example of such a diagram consisting of a single class.</p>
<h4 class="h4"><a id="lev32.2.5"></a><strong><span class="font1">32.2.5</span> Software Design Patterns</strong></h4>
<p class="noindent">A software <strong>design pattern</strong> is a general reusable conceptual solution that is used to solve commonly occurring software architecture problems. It is described from a system architect&#8217;s and programmer&#8217;s point of view, where implementation structure is described by relationships and interactions between classes and objects. Design patterns speed up development by using tested, proven concepts that have been useful for other developers, and provide a common language to communicate with other developers.</p>
<p class="indent">Some common patterns for VR are factories to instantiate objects (e.g., for the user to create many similar objects); adapters to connect different software libraries (e.g., <a id="pg410lev1"></a>to create a single way of supporting different HMDs); singletons to ensure only one object of a type exists (e.g., a single scene graph or hierarchical structure of the world); decorators to dynamically add behaviors to objects (e.g., adding a sound to an object that previously did not support sound); and observers so objects can be informed when some event occurs (e.g., callbacks so some part of the system knows when the user has pushed a button).</p>
<p class="image"><a id="page_410"></a><a id="fig32.3"></a><img src="../images/f0410-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 32.3</span> An example class diagram.</strong></p>
<h3 class="h3"><a id="lev32.3"></a><strong><span class="font">32.3</span> System Considerations</strong></h3>
<h4 class="h4"><a id="lev32.3.1"></a><strong><span class="font1">32.3.1</span> System Trade-Offs and Making Decisions</strong></h4>
<p class="noindent">Considering the different trade-offs of a project leads to a new understanding of the intricately interlocked interplay of factors [<a href="reference.html#ref33"><span class="blue">Brooks 2010</span></a>] and how they might be implemented. Clarity of understanding trade-offs helps the team to choose hardware, interaction techniques, software design, and where to focus development. Such decisions depend upon many factors, such as hardware affordability/availability, time available to implement, interaction fidelity, representative users&#8217; susceptibility to sickness, etc. Table <a href="chapter32.html#tab32.1"><span class="blue">32.1</span></a> shows some of the more common decisions that must be made when designing and implementing a VR experience. Most of these system decisions should be made early in the project. Even if it turns out the decision was wrong, then it will quickly become apparent and can be changed accordingly at an early iteration.</p>
<p class="caption"><a id="page_411"></a><a id="tab32.1"></a><strong><span class="blue">Table 32.1</span> Some common VR decisions and example choices.</strong></p>
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<col width="40%"/>
<col width="60%"/>
<tr>
<td valign="top"><p class="thead411">Decision</p></td>
<td valign="top"><p class="thead411a">Example Choices</p></td>
</tr>
<tr>
<td valign="top"><p class="table411">Hand input hardware</p></td>
<td valign="top"><p class="table411a">None, Leap Motion, Sixense STEM</p></td>
</tr>
<tr>
<td valign="top" class="bgcolor411"><p class="table411">Number of participants</p></td>
<td valign="top" class="bgcolor411"><p class="table411a">Single user, multiplayer, massively multiplayer</p></td>
</tr>
<tr>
<td valign="top"><p class="table411">Viewpoint control pattern(s)</p></td>
<td valign="top"><p class="table411a">Walking, steering, world-in-miniature, 3D multi-touch, automated</p></td>
</tr>
<tr>
<td valign="top" class="bgcolor411"><p class="table411">Selection pattern(s)</p></td>
<td valign="top" class="bgcolor411"><p class="table411a">Hand selection, pointing, image-plane, volume-based</p></td>
</tr>
<tr>
<td valign="top"><p class="table411">Manipulation pattern(s)</p></td>
<td valign="top"><p class="table411a">Direct hand manipulation, proxy</p></td>
</tr>
<tr>
<td valign="top" class="bgcolor411"><p class="table411">Realism</p></td>
<td valign="top" class="bgcolor411"><p class="table411a">Real-world capture, cartoon world</p></td>
</tr>
<tr>
<td valign="top"><p class="table411">Vection</p></td>
<td valign="top"><p class="table411a">None, short periods of self-motion, linear motion only, active, passive</p></td>
</tr>
<tr>
<td valign="top" class="bgcolor411"><p class="table411">Intensity</p></td>
<td valign="top" class="bgcolor411"><p class="table411a">Relaxing, heart-thumping</p></td>
</tr>
<tr>
<td valign="top"><p class="table411">Sensory cues</p></td>
<td valign="top"><p class="table411a">Visual, auditory, haptics, motion platform</p></td>
</tr>
<tr>
<td valign="top" class="bgcolor411"><p class="table411">Posture</p></td>
<td valign="top" class="bgcolor411"><p class="table411a">Sitting, standing, walking around</p></td>
</tr>
</table>
<p class="indentt">In some cases it is appropriate to support multiple options. Be careful of trying to support too many options as supporting everything by implementing for the &#8220;average&#8221; option results in no single optimal experience. Instead, implement different metaphors and interactions that most appropriately fit each option. It is generally better to start by choosing a single option and optimize for it before adding secondary options, features, and support.</p>
<h4 class="h4"><a id="lev32.3.2"></a><strong><span class="font1">32.3.2</span> Support of Different Hardware</strong></h4>
<p class="noindent">Although some hardware has similar characteristics, other hardware can have very different characteristics (Chapter <a href="chapter27.html#ch27"><span class="blue">27</span></a>). Supporting different hardware that is in the same class can enable a wider audience since users may not all have the same access to the same hardware. Fortunately, in some cases the specific hardware only slightly changes the experience. For example, the Sony PlayStation Move is the same class of input device as the Sixense STEM Controller (both are 6 DoF tracked hand-held controllers). Supporting both will require additional code but the core interaction techniques can remain the same. Just because hardware is similar, do not assume <a id="page_412"></a>testing on one piece of hardware will apply to all hardware in that same class; test across the different hardware and optimize as needed.</p>
<p class="indent">It can be dangerous to support different hardware classes for a single experience. Supporting different input device classes as being interchangeable is difficult and most often results in the experience being optimized for none of them. Those experiences that take full advantage of and are dependent upon unique characteristics of some input device class should not support other input device classes. For example, the Sony Playstation Move and Sixense STEM are very different from Microsoft Kinect or Leap Motion. The exception is for special cases when the experience is designed to take advantage of multiple input device classes in a hybrid system where all users are expected to have access to and use all hardware.</p>
<p class="indent">If different hardware classes must be supported, then the core interaction techniques should be independently optimized for each in order to take advantage of their unique characteristics. A tracked hand-held controller will have different interactions and experiences than a bare-hands system.</p>
<h4 class="h4"><a id="lev32.3.3"></a><strong><span class="font1">32.3.3</span> Frame Rate and Latency</strong></h4>
<p class="noindent">A frame rate that maintains at least the refresh rate of the HMD should be obtained from the beginning and maintained throughout the project (Section <a href="chapter31.html#lev31.15.3"><span class="blue">31.15.3</span></a>). Otherwise it will be more difficult to optimize at a later time (e.g., lower polygon-count assets may need to be re-created). Fortunately, assuming scene complexity is reasonable, today&#8217;s hardware makes this relatively easy to achieve. Frame rate should be carefully observed as new assets are added and code complexity increases. Even occasional drops in frame rate can be uncomfortable to users. Consistent latency is as important as low latency (Section <a href="chapter15.html#lev15.2"><span class="blue">15.2</span></a>).</p>
<p class="indent">Some rendering algorithms/game engines perform multiple passes to add special effects. In some cases this can add one or more additional frames of latency even while a high frame rate is achieved. VR creators should not rely only on frame rate and should measure end-to-end latency with a latency meter (Section <a href="chapter15.html#lev15.5.1"><span class="blue">15.5.1</span></a>). As discussed in Section <a href="chapter18.html#lev18.7"><span class="blue">18.7</span></a>, prediction and warping can be used to reduce some of the negative effects of latency, but this only works well for latencies within a 30 ms range.</p>
<h4 class="h4"><a id="lev32.3.4"></a><strong><span class="font1">32.3.4</span> Sickness Guidelines</strong></h4>
<p class="noindent">Developers should respect the adverse effects of VR (Part <a href="part03.html#part3"><span class="blue">III</span></a>) even if they are not prone to VR sickness themselves. Waiting for feedback from the Learn Stage can dramatically slow down the iterative process and might require complete redesign/reimplementation if problems are not taken care of while implementing. It is as important for <a id="page_413"></a>programmers to understand adverse health effects and how to mitigate them as it is for anyone on the team.</p>
<h4 class="h4"><a id="lev32.3.5"></a><strong><span class="font1">32.3.5</span> Calibration</strong></h4>
<p class="noindent">Proper calibration of the system is essential and tools should be created for both developers and users to easily and quickly calibrate (if not automated). Examples of calibration include interpupillary distance, lens distortion-correction parameters, and tracker-to-eye offset. If these settings are not correct, scene motion will occur while rotating the head, which will result in motion sickness. See <a href="reference.html#ref125"><span class="blue">Holloway</span></a> [<a href="reference.html#ref125"><span class="blue">1997</span></a>] for a detailed discussion of various sources of HMD calibration error and their effects.</p>
<h3 class="h3"><a id="lev32.4"></a><strong><span class="font">32.4</span> Simulation</strong></h3>
<p class="noindent">Simulation can be more challenging for VR than it is in more traditional applications. This section briefly discusses some of these challenges and some ways to solve them.</p>
<h4 class="h4"><a id="lev32.4.1"></a><strong><span class="font1">32.4.1</span> Separate Simulation from Rendering</strong></h4>
<p class="noindent">Simulations should be executed asynchronously from rendering [<a href="reference.html#ref37"><span class="blue">Bryson and Johan 1996</span></a>, <a href="reference.html#ref308"><span class="blue">Taylor et al. 2010</span></a>]. Slow updates of parts of the scene are okay as long as <a id="pg413lev1"></a>rendering resulting from head motion occurs at the HMD refresh rate and under 30 ms of latency. For example, a scientific simulation of colliding galaxies might execute remotely on a supercomputer with updates coming in only once per second. This slow dynamic updating of the data will be obvious to the user, but the user will still be able to view the static aspects of the data (e.g., the user can move his head into the non-moving data) in real time.</p>
<p class="indent">For realistic physics simulations, it is usually required that the simulation be calculated at a fast update rate. This is especially true if the physics simulation is being used to render haptic forces (Section <a href="chapter03.html#lev3.2.3"><span class="blue">3.2.3</span></a>) as haptics updated at a rate less than 1,000 Hz can result in objects not feeling solid. Higher rates are even better at making objects feel even more solid [<a href="reference.html#ref272"><span class="blue">Salisbury et al. 2004</span></a>].</p>
<h4 class="h4"><a id="lev32.4.2"></a><strong><span class="font1">32.4.2</span> Fighting Physics</strong></h4>
<p class="noindent">Mixing human interaction with physics simulation can be extremely challenging in VR because the simulation of objects &#8220;fights&#8221; with where the physical hand is. This is due to the results of the physics simulation contradicting where the hand actually is. This can result in the object appearing to <strong>jitter</strong> (moving quickly back and forth) because the human input and the simulation &#8220;fight&#8221; or go back and forth between the calculated <a id="page_414"></a>pose and the actual hand pose. The simple solution is to not fight. That is do not try to have two separate policies simultaneously determine where an object is.</p>
<p class="indent">The most common solution when an object is picked up by a user is to stop applying physics simulation to that object. The object may still apply forces to other non-static objects (e.g., hitting a ball with a baseball bat). This lack of physics being applied to the hand-held object results in the object moving through static objects (e.g., a wall or large desk) since those static objects don&#8217;t move and don&#8217;t apply forces back to the hand-held object (although sensory substitution such as a vibrating controller can be applied as discussed in Section <a href="chapter26.html#lev26.8"><span class="blue">26.8</span></a>). Although not realistic, this approach is often less presence-breaking than the alternative of jitter and unrealistic physics.</p>
<p class="indent">The other option is to ignore hand position when a hand-held object penetrates another object. This works well when penetration is shallow but not for deeper penetrations (Section <a href="chapter26.html#lev26.8"><span class="blue">26.8</span></a>). Unfortunately, penetration depth typically cannot be controlled (i.e., most VR systems do not provide physical constraints) so this is typically not a good option.</p>
<h4 class="h4"><a id="lev32.4.3"></a><strong><span class="font1"><span class="blue">32.4.3</span></span> Jittering Objects</strong></h4>
<p class="noindent">Physics simulation even without user intervention can cause objects to jitter. For example, an object will sometimes appear to jitter on the ground even when the user is not touching it. This can occur due to different reasons such as round-off error/numerical approximation or using linear estimations for non-linear behavior. For example, the simulation might overestimate the distance an object has fallen resulting in the object penetrating the ground. The simulation then applies a force to move the ball back up just above the ground. But then gravity takes over and the object falls just under the surface and the cycle continues. A simple solution can be implemented by clamping object motion to zero when that motion reaches some minimum value. This unfortunately causes objects to sometimes appear to suddenly stop when the minimum value is reached. Allowing a small amount of penetration without causing a collision response can also help. This, however, can result in objects appearing to penetrate other objects if the value is too large. More elegant solutions can be implemented, such as applying a stronger damping force when the motion falls below some threshold.</p>
<p class="indent">Solving for rag-doll character motions with purely physically based simulation can also result in instability that appears as jitter. Thus, if a physically-based rag doll is used, the simulation might not control the character&#8217;s bones directly but be a hybrid solution of the physics simulation feeding into code that simplifies the simulation results, appearing as smoother believable motions.</p>
<h4 class="h4"><a id="page_415"></a><a id="lev32.4.4"></a><strong><span class="font1"><span class="blue">32.4.4</span></span> Flying Objects</strong></h4>
<p class="noindent">It is also often the case that mixing human input and simulation will result in extremely large forces causing objects to fly quickly off into the distance. This can happen when the physical hand pushes an object past the surface of some other object and then releases the object. The physics simulation then takes over and overcompensates by applying a large force to move the object instantaneously toward the surface. If the hand has penetrated the object too far, then the force will be large resulting in the object flying off into the distance in an unrealistic manner. Specialized code might be required to detect such occurrences and then handle the situation differently than using the standard physics formulas. For example, if the object is released inside an object then the object might first be moved toward the closest surface before letting the physics engine take over.</p>
<p class="indent">Similarly, simulation instability can result when multiple physically simulated objects are bound closely together. The instability can exacerbate quickly and cause objects to fly off into space. Such scenarios can be quite chaotic, and difficult to automatically detect and correct (David Collodi, personal communication, May 4, 2015). Thus, the designer should avoid having multiple physically simulated objects interacting whenever possible, especially if the objects are in an enclosed space or are tightly <a id="pg415lev1"></a>constrained against one another (e.g., a stack of blocks in a small pit or multiple boxes joined together in a circular fashion).</p>
<h3 class="h3"><a id="lev32.5"></a><strong><span class="font1"><span class="blue">32.5</span></span> Networked Environments</strong></h3>
<p class="noindent">A networked VR system where multiple users share the same virtual space has enormous challenges beyond the challenges of a single-user system. Essentially, a collaborative VR system is a distributed real-time database with multiple users modifying it in real time with the expectation that the database is always the same for all users [<a href="reference.html#ref71"><span class="blue">Delaney et al. 2006a</span></a>].</p>
<h4 class="h4"><a id="lev32.5.1"></a><strong><span class="font1"><span class="blue">32.5.1</span></span> Ideals of Networked Environments</strong></h4>
<p class="noindent"><strong>Network consistency</strong> is the ideal goal that, at any point in time, all users should perceive the same shared information at the same time [<a href="reference.html#ref93"><span class="blue">Gautier et al. 1999</span></a>]. Unfortunately, perfect consistency is impossible to achieve due to technical challenges such as network latency, packet loss, and difficulty in maintaining true quality of service. However, by understanding the challenges and violations of network consistency, we can work toward the ideal of creating higher-quality shared experiences.</p>
<p class="indent">Network consistency can be broken down into synchronization, causality, and concurrency [<a href="reference.html#ref71"><span class="blue">Delaney et al. 2006a</span></a>].</p>
<p class="indentbullett"><a id="page_416"></a>&#8226; <strong>Synchronization</strong> is the maintenance of consistent entity state and timing of events for all users. Ideally, all clocks are synchronized.</p>
<p class="indentbullet">&#8226; <strong>Causality</strong> (also known as ordering) maintains consistent ordering of events for all users. Ideally, all events are executed on each computer in the true order they occur no matter which computer they occurred on.</p>
<p class="indentbullet">&#8226; <strong>Concurrency</strong> is the simultaneous execution of events by different users on the same entities. Object ownership/control must be resolved. Ideally, there are no conflicts over shared objects.</p>
<p class="indentt">Violations of any of the three challenges listed above can lead to inconsistent states of the world between users and breaks-in-presence. Problems specific to networked environments include the following.</p>
<p class="indentbullett">&#8226; <strong>Divergence</strong> is the temporal-spatial state of an entity being different for different users. Divergent objects result in users interacting with those objects inconsistently, sometimes leading to inconsistent behavior causing object states to diverge even further. An example is simulation divergence that occurs when physics simulations are executed on different computers.</p>
<p class="indentbullet">&#8226; <strong>Causality violations</strong> are events that are out of order so that effects appear to occur before cause. A ball appearing to bounce before a remote user appears to dislodge the ball is a causality violation.</p>
<p class="indentbullet">&#8226; <strong>Expectation violations</strong> (also known as intention violations) are resulting effects from concurrent events that are different than the expected or intended effects. An expectation violation occurs when a remote user changes the color of an object at approximately the same time as the local users change the color of the same object. The local user expects it to turn one color but it changes to some other color.</p>
<p class="indentt">In addition to being consistent, networked VR systems should also be easily usable and believable. Responsiveness and perceived continuity are two primary contributors of the user experience as it relates to building networked systems.</p>
<p class="indentbullett">&#8226; <strong>Responsiveness</strong> is the time taken for the system to register and respond to a user action. Synchronization, causality, and concurrency could more easily be achieved by adding a delay for all users but would result in low responsiveness. Ideally, users can interact with all networked objects as if they were local without having to wait for confirmation from other computers.</p>
<p class="indentbullet">&#8226; <strong>Perceived continuity</strong> is the perception that all entities behave in a believable manner without visible jitter or jumps in position and that audio sounds smooth.</p>
<h4 class="h4"><a id="page_417"></a><strong><span class="blue"><a id="lev32.5.2"></a><span class="font1">32.5.2</span></span> Message Protocols</strong></h4>
<p class="noindent">Communication between computers occurs through packets. A packet is a formatted piece of data that travels over a computer network. The type of packets used to convey information can have a big impact on performance and synchronization.</p>
<p class="indent"><strong>UDP</strong> (user datagram protocol) is a minimal connectionless transmission model that operates on a best-effort basis. Packets are simply sent out to a destination without any guarantee of delivery, ordering, or duplication. Packets might not be received, for example, if the receiving computer is overloaded with too many incoming packets. UDP should be used when low latency is a priority, updates occur often, and each update is not essential (e.g., when the state of the world will get updated soon enough if a recent packet was not received). Examples of when to use UDP are for continually updated character position and audio.</p>
<p class="indent"><strong>TCP</strong> (transport control protocol) is a bidirectional reliable ordered byte-stream model that comes at the cost of additional latency. TCP works by the receiving computer acknowledging with the sending computer that the packet was successfully received. Information is also included in each packet that guarantees the order that packets were received will match the order in which they were sent. TCP should be used when state information is only sent once (or occasionally) to ensure the receiving computer can update its state of the world to match the sender&#8217;s. An example of when to use TCP is when some one-time event has occurred and the receiving computer needs to update its state of the world based on that one-time event.</p>
<p class="indent"><strong>Multicast</strong> is a one-to-many or many-to-many distribution of group communication where information is simultaneously sent to a group of addresses instead of a single address at a time. Clients simply subscribe to the multicast channel and then receive updates until they unsubscribe. Multicast works well in theory, but unfortunately there are technical challenges that often keep multicast from working well, and it can cause network overload depending on how it is implemented on network hardware (that is often out of control of the developer). True network multicasting (also known as IP multicasting) is ideal, but not all commercial routers support it (or it is disabled) so an application layer on top of the network is often built to simulate true multicasting. Unfortunately, the application layering approach is not as efficient. When possible, network multicasting should be used over application layering multicasting.</p>
<h4 class="h4"><strong><span class="blue"><a id="lev32.5.3"></a><span class="font1">32.5.3</span></span> Network Architectures</strong></h4>
<p class="noindent">There are many ways of connecting virtual worlds. However, <strong>network architectures</strong> are generally described as peer-to-peer, client-server, or hybrid architectures.</p>
<p class="indent"><strong>Peer-to-peer architectures</strong> transmit information directly between individual computers. Each computer maintains its own state of the world and ideally matches all <a id="page_418"></a>other computer states of the world. In a true peer-to-peer architecture, all peers have equal roles and responsibilities. The greatest advantage of peer-to-peer architectures for VR is fast responsiveness. One of the challenges of peer-to-peer architectures is discovering virtually local peers so that users can see each other. Thus, hybrid architectures are often used for networked virtual environments where some centralized server with global knowledge connects users.</p>
<p class="indent"><strong>Client-server architectures</strong> consist of each client first communicating with a server and then the server distributing information out to the clients. Fully <strong>authoritative servers</strong> control all the world state, simulation, and processing of input from clients. This makes all the users&#8217; worlds consistent at the cost of responsiveness and continuity (although this can be improved by the local system giving the illusion of responsiveness and continuity&#8212;see Section <a href="chapter32.html#lev32.5.4"><span class="blue">32.5.4</span></a>). As demonstrated by Valve and others, authoritative client-server models can work well for a limited number of users [<a href="reference.html#ref313"><span class="blue">Valve 2015</span></a>]. To scale to larger persistent worlds, multiple servers should be used to reduce computational and bandwidth requirements, as well as to provide redundancy if the server crashes.</p>
<p class="indent">Some network architectures do not clearly fall within either of the above architectures. <strong>Hybrid architectures</strong> use elements of peer-to-peer and client-server architectures.</p>
<p class="indent"><a id="pg418lev1"></a><strong>Non-authoritative servers</strong> technically follow the client-server model but behave in many ways like a peer-to-peer model because the server only relays messages between clients (i.e., the server does not modify the messages or perform any extra processing of those messages). Primary advantages are ease of implementation and knowledge of all other users. Disadvantages are that systems can become out of sync and clients can cheat as they are not governed by an authoritative system.</p>
<p class="indent"><strong>Super-peers</strong> are clients that also act as servers for regions of virtual space and can be changed between users (e.g., when a user logs off). The greatest advantage of super-peers over pure client-server architectures is increased heterogeneity of resources (e.g., bandwidth, processing power) across peers. However, super-peer networks can be challenging to implement well.</p>
<p class="indent">Other hybrid architectures use peer-to-peer for transferring some data (e.g., voice) while also using a server to secure important data and to maintain a persistent world. Figure <a href="chapter32.html#fig32.4"><span class="blue">32.4</span></a> shows an example of a VR system for distributed design review that uses a mix of a server (HIVE Server; <a href="reference.html#ref132"><span class="blue">Howard et al. 1998</span></a>), audio multicast, and a peer-to-peer distributed persistent memory system (CAVERNSoft; <a href="reference.html#ref179"><span class="blue">Leigh et al. 1997</span></a>).</p>
<h4 class="h4"><strong><span class="blue"><a id="lev32.5.4"></a><span class="font1">32.5.4</span></span> Determinism and Local Estimation</strong></h4>
<p class="noindent">Since bandwidth limits the rate at which packets can be sent and received, a straightforward implementation can result in remotely controlled entities appearing to update in discrete steps&#8212;i.e., they will appear to be frozen for moments of time with frequent teleportation to new locations as new packets are received. Such discontinuities can be reduced depending if the actions are deterministic, nondeterministic, or partially deterministic. Nondeterministic actions occur through user interaction, such as when a user grabs an object, and remote nondeterministic actions can only be known by receiving a packet from the computer on which the action took place. Deterministic actions should be computed locally, and there is no need to send out or receive packets in such cases.</p>
<p class="image"><a id="page_419"></a><a id="fig32.4"></a><img src="../images/f0419-01.png" alt="image"/></p>
<p class="caption"><a id="pg419lev1"></a><strong><span class="blue">Figure 32.4</span> Example of a network architecture with a centralized server and audio separated from other data.</strong> (Based on <a href="reference.html#ref59"><span class="blue">Daily et al</span></a>. [<a href="reference.html#ref59"><span class="blue">2000</span></a>])</p>
<p class="indent">Partially deterministic actions of remote entities (e.g., a user is steering in a general direction) can be estimated every rendered frame in order to create perceived continuity. <strong>Dead reckoning</strong> or extrapolation estimates where a dynamic entity is located based on its state defined by information from its most recently received packet. As a new packet is received the state is updated with the new true information. If the estimated state and updated state diverge by too much, then the entity will appear to instantaneously teleport to the new location. Such discontinuities can be reduced by interpolating between the estimated position and the new position defined by the <a id="page_420"></a>new packet. Valve&#8217;s Source game engine uses both extrapolation and interpolation to provide a fast and smooth gaming experience [<a href="reference.html#ref313"><span class="blue">Valve 2015</span></a>]. Although there is a tradeoff of smoothness versus entity latency, this form of latency does not cause sickness as the local user&#8217;s head tracking and frame rate are independent of other users.</p>
<h4 class="h4"><strong><span class="blue"><a id="lev32.5.5"></a><span class="font1">32.5.5</span></span> Reducing Network Traffic</strong></h4>
<p class="noindent">Reducing network traffic is often a necessity to optimize networked environments no matter what architecture or communication protocol is used. This is especially true when scaling up to a large number of users. In addition to general techniques such as data compression, sending out packets only when information is needed can dramatically reduce network traffic [<a href="reference.html#ref71"><span class="blue">Delaney et al. 2006b</span></a>].</p>
<p class="indent">One way to reduce network traffic is to compute the divergence between the local true state and the remote estimated state (e.g., entity position estimated by dead reckoning). For example, divergence will occur when a user starts/stops moving or steers in a different direction. <strong>Divergence filtering</strong> only sends out packets when the divergence of an entity reaches some threshold.</p>
<p class="indent"><strong>Relevance filtering</strong> only sends out a subset of information to each individual computer as a function of some criterion. A common way to do this is for individual servers <a id="pg420lev1"></a>to control some virtual space (e.g., a grid structure consisting of cells with an individual server controlling each cell) so that packets are only sent to nearby users. Server overloading caused by a large number of users in a small area can be a problem. This can be solved by using a dynamic grid/cell size that varies cell size depending on the number of users in an area (e.g., <a href="reference.html#ref133"><span class="blue">Hu et al. 2014</span></a>).</p>
<p class="indent">Animations can also be used to reduce bandwidth requirements. For example, animating a user&#8217;s legs is often more appropriate when a user moves instead of updating the actual joints of the avatar every frame based on incoming network packets (and legs are often animated anyway even in a single-user system). Gesture recognition can be used to first interpret a user&#8217;s signal (e.g., a hand wave) so a high-level signal can be sent out and then reproduced on each client.</p>
<p class="indent">Audio often takes an order of magnitude or more bandwidth than other updates [<a href="reference.html#ref59"><span class="blue">Daily et al. 2000</span></a>] and is especially a problem when many users attempt to speak simultaneously (Jesse Joudrey, personal communication, April 20, 2015). Stress tests can quickly be conducted by getting many users to speak simultaneously through the same server. Supporting spatialized audio requires each user&#8217;s audio stream to remain independent, so combining all audio into a single stream on a server, as is often done with non-immersive communication, is not an option. If spatialized audio is used, it is better to send the audio using peer-to-peer rather than all audio going through a server. If feasible, multicasting should be used so that users automatically subscribe <a id="page_421"></a>and unsubscribe from the audio streams of virtual neighbors. Relevance filtering can be used to only send audio to nearby users. Less bandwidth, and thus lower-quality audio, can also be allocated to users who are further away or not in the general forward-looking direction [<a href="reference.html#ref85"><span class="blue">Fann et al. 2011</span></a>]. Depending on the level of realism intended, the application might also provide the option to subscribe to teammates or important announcements regardless of distance.</p>
<h4 class="h4"><a id="lev32.5.6"></a><strong><span class="font1">32.5.6</span> Simultaneous Interactions</strong></h4>
<p class="noindent">Two or more users simultaneously interacting with the same object is extremely difficult to do in networked environments and should be avoided if at all possible. Only one user at a time should own an object so that no other users can interact with it. This can be done through a token. A <strong>token</strong> is a shared data structure that can only be owned by a single user at a time. Before a user interacts with an object he must first request the token to make sure another user does not own it. This can make interaction difficult as there will be a delayed response when attempting to interact with an object. One way to reduce this latency is to request the key when the user approaches an object but before reaching for or selecting the object [<a href="reference.html#ref268"><span class="blue">Roberts and Sharkey 1997</span></a>].</p>
<h4 class="h4"><a id="lev32.5.7"></a><a id="pg421lev1"></a><strong><span class="font1">32.5.7</span> Networking Physics</strong></h4>
<p class="noindent">Networked physics simulations are even a greater challenge than physics simulation on a single computer (Section <a href="chapter32.html#lev32.3"><span class="blue">32.3</span></a>) because multiple representations of the same action occur at different physical locations in time and space. Theoretically, the same results should occur on different computers using the same equations and code. However, slight differences between clients (e.g., round-off error or differences in timing) cause the physics simulation to diverge over time. If a ball rolls in one direction on one computer it may roll in a different direction on a different computer, and eventually the ball may be in an entirely different part of the environment. Because of this, an authoritative server or single computer should own the simulation. Other computers can estimate the simulation, but the owning computer should send out regular updates to correct the current state, as described in Section <a href="chapter32.html#lev32.5.4"><span class="blue">32.5.4</span></a>.</p>
<h3 class="h3"><a id="lev32.6"></a><strong><span class="font">32.6</span> Prototypes</strong></h3>
<p class="noindent">A <strong>prototype</strong> is a simplistic implementation of what is trying to be accomplished without being overly concerned with aesthetics or perfection. Prototypes are informing because they enable the team to observe and measure what users do, not just what they say. Prototypes win arguments and measurement using these prototypes trumps opinion.</p>
<p class="indent"><a id="page_422"></a>A <strong>minimal prototype</strong> is built with the least amount of work necessary to receive meaningful feedback. Temporary tunnel vision is okay when trying to find an answer to a question about a specific task or small portion of an experience. At early stages, the team shouldn&#8217;t bother debating the details. Instead focus on getting <em>something</em> working as quickly as possible and then modify and add additional functionality/features on top of that or start from scratch if necessary after learning from the basic prototype.</p>
<p class="indent">Have a clear goal for each prototype. Only build primitive functionality that is essential to achieving the goal and reject elements not essential to that goal. Non-critical details can be added and refined later. Give up on trying to look good and feeling that a prototype is ugly, unfinished, or not ready. Expect that it won&#8217;t be done well the first time or the fifth time and to be confronted by many unforeseen difficulties for what at first may have seemed to be a relatively straightforward goal. Discovering such difficulties is what prototypes are for, and it is best to find such problems as soon as possible. The sooner the bad ideas can be rejected and the best basic concepts and interaction techniques found, the sooner limited resources can be utilized for creating quality experiences.</p>
<p class="indent">Building prototypes quickly with minimal effort also adds the additional psychological benefit of not caring if the prototype is thrown away or not. Fast iteration and many prototypes result in an attitude of not being committed to one thing so developers don&#8217;t feel like they are killing their babies.</p>
<p class="indent">Prototypes can vary significantly depending on what is trying to be accomplished. For example, a question about whether a certain aspect of a new interaction technique will work or not is very different than determining if the general market likes an overall concept.</p>
<h4 class="h4"><a id="lev32.6.1"></a><strong><span class="font1">32.6.1</span> Forms of Prototypes</strong></h4>
<p class="noindent">A <strong>real-world prototype</strong> does not use any digital technology whatsoever. Instead of using a VR system, team members or users act out roles. This might include physical props or real-world tools such as laser pointers. Advantages are that such prototypes can often be created and tested spontaneously. Some disadvantages are lack of controlling conditions, difficulty in capturing quantitative data, mismatch of simulated actions to VR actions, and limitation of feedback to high-level structure/logic.</p>
<p class="indent">A <strong>Wizard of Oz prototype</strong> is a basic working VR application, but a human &#8220;wizard&#8221; behind a curtain (or on the other side of the HMD) controls the response of the system in place of software. The wizard typically enters commands on a keyboard or controller after the user verbally states his intention (e.g., to state where to travel to or to simulate a voice recognition system). When done well, this sort of prototype can be surprisingly compelling and the user may not even realize a human is controlling the system. <a id="page_423"></a>Although collecting data for a specific implementation cannot be done with this type of prototype, high-level feedback can be obtained.</p>
<p class="indent"><strong>Programmer prototypes</strong> are prototypes created and evaluated by the programmer or team of programmers. Programmers continually immerse themselves in their own systems to quickly check modifications to their code. This is how most programmers naturally operate, sometimes conducting hundreds of mini experiments (e.g., change a variable value or logic statement to see if it works as expected) in a day.</p>
<p class="indent"><strong>Team prototypes</strong> are built for those on the team not directly implementing the application. Feedback from others is extremely valuable as it is often difficult to evaluate the overall experience when working closely on a problem. This can also reduce groupthink and having the belief that your VR experience is the best in the world when it is not. Team prototypes are used for quality assurance testing. Note team members providing feedback on the prototype might include those external to the core team such as when performing expert evaluations (Section <a href="chapter33.html#lev33.3.6"><span class="blue">33.3.6</span></a>) via consultants. Other teams from other projects that understand the concepts of prototypes also work well.</p>
<p class="indent"><strong>Stakeholder prototypes</strong> are semi-polished prototypes that are taken more seriously and typically focus on the overall experience. Often stakeholders are less familiar with VR than they like to admit. They expect a higher level of fidelity that is closer to the real product, which helps them to more fully understand what the final product will be like. Most stakeholders, however, want to see progress, so don&#8217;t wait until the product is finalized. Their feedback about market demand, understanding business needs, competition, etc. can be extremely valuable.</p>
<p class="indent"><strong>Representative user prototypes</strong> are prototypes designed for feedback that should be built and shown to users as soon as possible. Users that have never experienced VR before are ideal for testing for VR sickness since they have not had the chance to adapt. These prototypes are primarily used for collecting data, as described in Chapter <a href="chapter33.html#ch33"><span class="blue">33</span></a>. The focus should be on building a prototype that best collects data for what is being targeted.</p>
<p class="indent"><strong>Marketing prototypes</strong> are built to attract positive attention to the company/project and are most often shown at meetups, conferences, trade shows, or made publicly available via download. They may not convey the entire experience but should be polished. A secondary advantage of these prototypes outside of marketing is that a lot of users can provide feedback in a short amount of time.</p>
<h3 class="h3"><a id="lev32.7"></a><strong><span class="font">32.7</span> Final Production</strong></h3>
<p class="noindent"><strong>Final production</strong> starts once the design and features have been finalized so that the team can focus on polishing the deliverable. When final production begins, it is time <a id="page_424"></a>to stop exploring possibilities and stop adding more features. Save those ideas for a later deliverable. One of the great challenges of developing VR applications is that when stakeholders experience a solid demo, they get excited and start imagining possibilities and requesting new features. Whereas the team should always be open to input, they should also be aware and make it clear that there is a limit of what is possible within the time and budget constraints. Some suggestions might be easy to add (e.g., changing the color of an object), but others add risk as modifying a seemingly simple part of the system can affect other parts. Adding features also takes away from other aspects of the project. The team should make it clear that if stakeholders want additional features, there are trade-offs involved and the new features may come at the cost of delaying other features.</p>
<h3 class="h3"><a id="lev32.8"></a><strong><span class="font">32.8</span> Delivery</strong></h3>
<p class="noindent">Deployment ranges from the extremes of making the application continuously available online with weekly updates to multiple team members traveling to a site (or multiple sites) for a full system installation.</p>
<h4 class="h4"><a id="lev32.8.1"></a><strong><span class="font1">32.8.1</span> Demos</strong></h4>
<p class="noindent"><strong>Demos</strong> (the showing of a prototype or a more fully produced experience) are the lifeblood of VR. After all, most people are not interested in a conglomeration of documentation, plans, and reports&#8212;they want to experience the results. They are also not interested in excuses of how it was working at the last conference or how something is broken. It is the VR experience that gives confidence to others that progress is being made and the team is creating something of value.</p>
<p class="indent">Conferences, meetups, and other events give the team their time to show off their creation. Scheduling demos is also a great way to create real accountability to get things done and get them done well. In the worst case, the team will be significantly motivated to do better next time after a demo turns out to be a disaster.</p>
<p class="indent">Before traveling, set the demo up in a room different from where equipment is normally located to make sure no equipment is missed when packing. If the equipment is beyond the basics, then take the packed material to yet a different room and set up again to ensure everything that is needed was indeed packed. Then pack two backups of everything. If at all possible, set up in the space where the demo will take place at least one day in advance to make sure there will be no surprises. On the day of the demo, arrive early and confirm everything is working.</p>
<p class="indent">Being prepared for in-house demos is also essential. With all the excitement of what is possible with VR, you never know when a stakeholder or celebrity is going to stop by <a id="page_425"></a>wanting to see what you have. Note this does not mean anyone wanting a demo should be able to walk in any time of day to test the system&#8212;it simply means to be prepared when the important people show up. The team&#8217;s time is valuable and demos can be distracting, so demos should be given sparingly unless of direct value to the project. A single individual should be responsible for maintaining and updating onsite demos and equipment so that when someone important shows up the system is more than barely working. The same individual might be responsible for keeping a schedule of demo events, or that might be some other individual.</p>
<h4 class="h4"><a id="lev32.8.2"></a><strong><span class="font1">32.8.2</span> Onsite Installation</strong></h4>
<p class="noindent">Full project delivery often includes <strong>onsite installation</strong>, where one or more members of the team travel to the customer&#8217;s site to set up the system and application. In such cases, do not assume installation will go smoothly. Debugging, calibration, development, and extra electrical/gaffer tape should be expected for complicated installation with multiple pieces of hardware. Issues might include electromagnetic interference and incompatible connections/cables with a client&#8217;s system. In addition, training of staff may be required to use and maintain the system. What is obvious to you may not be obvious to them. Support and updates might also be included as part of a contract.</p>
<h4 class="h4"><a id="lev32.8.3"></a><strong><span class="font1">32.8.3</span> Continuous Delivery</strong></h4>
<p class="noindent">The opposite of onsite installation is <strong>continuous online delivery</strong>, where online updates are provided often. Agile methods recommend delivering a new executable that goes out to customers on a weekly basis. This has value in forcing the team to focus on what&#8217;s important and to be accountable, and it provides plenty of opportunity for data collection (A/B testing of changing one thing for a subset of customers is most common in this situation&#8212;see Section <a href="chapter33.html#lev33.4.1"><span class="blue">33.4.1</span></a>).</p>
</body>
</html>
