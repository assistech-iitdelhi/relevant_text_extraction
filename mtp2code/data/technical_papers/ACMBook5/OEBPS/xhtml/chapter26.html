<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" >
<head>
<title>The VR Book</title>
<link rel="stylesheet" type="text/css" href="../styles/9781970001143.css"/>
</head>
<body>
<h2 class="h2"><a id="page_289"></a><a id="ch26"></a><span class="blue1">26</span></h2>
<h2 class="h2b"><span class="blue">VR Interaction Concepts</span></h2>
<p class="noindent">VR interactions are not without their challenges. Trade-offs must be considered that may result in interactions being different than in the real world. However, there are enormous advantages VR has over the real world as well. This chapter focuses upon interaction concepts, challenges, and benefits specific to VR.</p>
<h3 class="h3"><a id="lev26.1"></a><span class="font">26.1</span> Interaction Fidelity</h3>
<p class="noindent">VR interactions are designed on a continuum ranging from an attempt to imitate reality as closely as possible to in no way resembling the real world. Which goal to <a id="pg289lev1"></a>strive toward depends on the application goals, and most interactions fall somewhere in the middle. <strong>Interaction fidelity</strong> is the degree to which physical actions used for a virtual task correspond to the physical actions used in the equivalent real-world task [<a href="reference.html#ref25"><span class="blue">Bowman et al. 2012</span></a>].</p>
<p class="indent">On the high end of the interaction fidelity spectrum are <strong>realistic interactions</strong>&#8212;VR interactions that work as closely as possible to the way we interact in the real world. Realistic interactions strive to provide the highest level of interaction fidelity possible given the hardware being used. Holding one hand above the other as if holding a bat and swinging them together to hit a virtual baseball has high interaction fidelity. Realistic interactions are often important for training applications, so that which is learned in VR can be transferred to the real-world task. Realistic interactions can also be important for simulations, surgical applications, therapy, and human-factors evaluations. If interactions are not realistic in such applications, problems such as adaptation (Section <a href="chapter10.html#lev10.2"><span class="blue">10.2</span></a>) may occur, which can lead to negative training effects for the real-world task being trained for. An advantage of using realistic interactions is that there is little learning required of users since they already know how to perform the actions.</p>
<p class="indent">On the other end of the interaction fidelity spectrum are <strong>non-realistic interactions</strong> that in no way relate to reality. Pushing a button on a non-tracked controller to shoot a laser from the eyes is an example of an interaction technique that has low interaction <a id="page_290"></a>fidelity. Low interaction fidelity is not necessarily a disadvantage as it can increase performance, cause less fatigue, and increase enjoyment.</p>
<p class="indent">Somewhere in the middle of the interaction fidelity spectrum are <strong>magical interactions</strong> where users make natural physical movements but the technique makes users more powerful by giving them new and enhanced abilities or intelligent guidance [<a href="reference.html#ref25"><span class="blue">Bowman et al. 2012</span></a>]. Such magical hyper-natural interactions attempt to create &#8220;better&#8221; ways of interacting by enhancing usability and performance through superhuman capabilities and unrealistic interactions [<a href="reference.html#ref291"><span class="blue">Smith 1987</span></a>]. Although not realistic, magical often uses interaction metaphors (Section <a href="chapter25.html#lev25.1"><span class="blue">25.1</span></a>) to help users quickly develop a mental model of how an interaction works. Consider interaction metaphors as a source of inspiration for creating new magical interaction techniques. Grabbing an object at a distance, pointing to fly through a scene, or shooting fireballs from the hand are examples of magical interactions. Magic interactions strive to enhance the user experience by reducing interaction fidelity and circumventing the limitations of the real world. Magic works well for games and teaching of abstract concepts.</p>
<p class="indent">Interaction fidelity is a multi-dimensional continuum of components. The Framework for Interaction Fidelity Analysis [<a href="reference.html#ref209"><span class="blue">McMahan et al. 2015</span></a>] categorizes interaction fidelity into three concepts: biomechanical symmetry, input veracity, and control <a id="pg290lev1"></a>symmetry.</p>
<p class="indent"><strong>Biomechanical symmetry</strong> is the degree to which physical body movements for a virtual interaction correspond to the body movements of the equivalent real-world task. Biomechanical symmetry makes heavy use of posture and gestures that replicate how one positions and moves their body in the real world. This provides a high sense of proprioception resulting in a high sense of presence since the user feels his body physically acting in the environment as if he were performing the task in the real world. Real walking for VR navigation purposely has a biomechanical symmetry to how we walk in the real world. Walking in place has a lower biomechanical symmetry due to its less realistic movements. Pressing a button or joystick to walk forward has no biomechanical symmetry.</p>
<p class="indent"><strong>Input veracity</strong> is the degree to which an input device captures and measures users&#8217; actions. Three aspects that dictate the quality of input veracity are accuracy, precision, and latency. A system with low input veracity can significantly affect performance due to difficulty capturing quality input.</p>
<p class="indent"><strong>Control symmetry</strong> is the degree of control a user has for an interaction as compared to the equivalent real-world task. High-fidelity techniques provide the same control as the real world without the need for different modes of interaction. Low control symmetry can result in frustration due to the need to switch between techniques to obtain full control. For example, directly manipulating object position and rotations (6 DoF) with a tracked hand controller has greater control symmetry than indirectly manipulating <a id="page_291"></a>the same object with gamepad controls because the gamepad controls (less than 6 DoF) require using multiple translation and rotational modes. However, low control symmetry can also have superior performance if implemented well. For example, non-isomorphic rotations (Section <a href="chapter28.html#lev28.2.1"><span class="blue">28.2.1</span></a>) can be used to increase performance by amplifying hand rotations.</p>
<h3 class="h3"><a id="lev26.2"></a><span class="font">26.2</span> Proprioceptive and Egocentric Interaction</h3>
<p class="noindent">As described in Section <a href="chapter08.html#lev8.4"><span class="blue">8.4</span></a>, proprioception is the physical sense of the pose and motion of the body and limbs. Because most VR systems do not provide a sense of touch outside of hand-held devices, proprioception can be especially important for exploiting the one real object every user has&#8212;the human body [<a href="reference.html#ref219"><span class="blue">Min&#233; et al. 1997</span></a>]. The body provides an egocentric frame of reference (Section <a href="chapter26.html#lev26.3.3"><span class="blue">26.3.3</span></a>) in which to work, and interactions relative to the body&#8217;s reference frame are more effective than techniques relying solely on visual information. In fact, eyes-off interactions can be performed in peripheral vision or even outside the field of view of the display, which reduces visual clutter. The user also has a more direct sense of control within personal space&#8212;it is easier to place an object directly with the hand rather than through less direct means.</p>
<h4 class="h4"><a id="lev26.2.1"></a><a id="pg291lev1"></a><span class="font1">26.2.1</span> Mixing Egocentric and Exocentric Interactions</h4>
<p class="noindent"><strong>Exocentric interactions</strong> consist of viewing and manipulating a virtual model of the environment from outside of it. With egocentric interaction, the user has a first-person view of the world and typically interacts from within the environment. Don&#8217;t assume one or the other must be chosen. These egocentric and exocentric interactions can be mixed together so that the user can view himself on a smaller map (Sections <a href="chapter22.html#lev22.1.1"><span class="blue">22.1.1</span></a> and <a href="chapter28.html#lev28.5.2"><span class="blue">28.5.2</span></a>), and/or manipulate the world in an exocentric manner but from an egocentric perspective (Figure <a href="chapter26.html#fig26.1"><span class="blue">26.1</span></a>).</p>
<h3 class="h3"><a id="lev26.3"></a><span class="font">26.3</span> Reference Frames</h3>
<p class="noindent">A <strong>reference frame</strong> is a coordinate system that serves as a basis to locate and orient objects. Understanding reference frames is essential to creating usable VR interactions. This section describes the most important reference frames as they relate specifically to VR interaction. The virtual-world reference frame, real-world reference frame, and torso reference frame are all consistent with one other when there is no capability to rotate, move, or scale the body or the world (e.g., no virtual body motion, no torso tracking, and no moving the world). The reference frames diverge when that is not the case.</p>
<p class="indent">Although thinking abstractly about reference frames and how they relate can be difficult, reference frames are naturally perceived and more intuitively understood <a id="page_292"></a>when one is immersed and interacting with the different reference frames. The reference frames discussed in this section will be more intuitively understood by actually experiencing them.</p>
<p class="image"><a id="fig26.1"></a><img src="../images/f0292-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 26.1</span> An exocentric map view from an egocentric perspective.</strong> (Courtesy of Digital ArtForms)</p>
<h4 class="h4"><a id="lev26.3.1"></a><span class="font1">26.3.1</span> The Virtual-World Reference Frame</h4>
<p class="noindent">The <strong>virtual-world reference frame</strong> matches the layout of the virtual environment and includes geographic directions (e.g., north) and global distances (e.g., meters) independent of how the user is oriented, positioned, or scaled. When creating content over a wide area, forming a cognitive map, determining global position, or planning travel on a large scale (Section <a href="chapter10.html#lev10.4.3"><span class="blue">10.4.3</span></a>), it is typically best to think in terms of the exocentric virtual-world reference frames. Care should be taken when placing direct hand interfaces relative to the virtual-world reference frame as reaching specific locations can be difficult and awkward unless the user is able to easily and precisely navigate and turn through the environment.</p>
<h4 class="h4"><a id="lev26.3.2"></a><span class="font1">26.3.2</span> The Real-World Reference Frame</h4>
<p class="noindent">The <strong>real-world reference frame</strong> is defined by real-world physical space and is independent of any user motion (virtual or physical). For example, as a user virtually flies forward the user&#8217;s physical body is still located in the real-world reference frame. A physical desk, computer screen, or keyboard sitting in front of the user is in the real-world reference frame. A consistent physical location should be provided to set <a id="page_293"></a>any tracked or non-tracked hand-held controller when not being used. For tracked controllers or other tracked objects, make sure to match the virtual model with the physical controller in form and position/orientation in the real-world reference frame (i.e., full spatial compliance) so users can see it correctly and more easily pick it up. In order for virtual objects, interfaces, or rest frames to be solidly locked into the real-world reference frame, the VR system must be well calibrated and have low latency. Such interfaces often, but not always, provide output cues only to help provide a rest frame (Section <a href="chapter12.html#lev12.3.4"><span class="blue">12.3.4</span></a>) in order for users to feel stabilized in physical space and to reduce motion sickness. Automobile interiors, cockpits (Figure <a href="chapter18.html#fig18.1"><span class="blue">18.1</span></a>), or non-realistic stabilizing cues (Figure <a href="chapter18.html#fig18.2"><span class="blue">18.2</span></a>) are examples of cues in the real-world reference frame. In some cases it makes sense to add the capability to input information through real-world reference-framed elements (e.g., buttons located on a virtual cockpit). A big advantage of real-world reference frames is that passive haptics (Section <a href="chapter03.html#lev3.2.3"><span class="blue">3.2.3</span></a>) can be added to provide a sense of touch that matches visually rendered elements.</p>
<h4 class="h4"><a id="lev26.3.3"></a><span class="font1">26.3.3</span> The Torso Reference Frame</h4>
<p class="noindent">The <strong>torso reference frame</strong> is defined by the body&#8217;s spinal axis and the forward direction perpendicular to the torso. The torso reference frames are especially useful for interaction because of proprioception (Sections <a href="chapter08.html#lev8.4"><span class="blue">8.4</span></a> and <a href="chapter26.html#lev26.2"><span class="blue">26.2</span></a>)&#8212;the sense of where one&#8217;s arms and hands are felt relative to the body. The torso reference frame can also be useful for steering in the direction the body is facing (Section <a href="chapter28.html#lev28.3.2"><span class="blue">28.3.2</span></a>).</p>
<p class="indent">The torso reference frame is similar to the real-world reference frame in the sense that both frames move with the user through the virtual world as the user virtually translates or scales. The difference is that virtual objects in the torso reference frame, virtual objects rotate with the body (both virtual and physical body turns) and move with physical translation whereas objects in the real-world reference frame do not. The chair a user is seated in can be tracked instead of the torso if it can be assumed the torso is stable relative to the chair. Systems with head tracking but not torso or chair tracking can assume the body is always facing forward (i.e., the torso reference frame and real-world reference frame are consistent). However, physical turning of the body can cause problems due to the system not knowing if only the head turned or the entire body turned.</p>
<p class="indent">If no hand tracking is available, the hand reference frame can be assumed to be consistent with the torso reference frame. For example, a visual representation of a non-tracked hand-held controller should move and rotate with the body (hand-held controllers are often assumed to be held in the lap). For VR, information displays often work better in torso reference frames rather than head reference frames as commonly done with heads-up displays in traditional first-person video games. Figure <a href="chapter26.html#fig26.2"><span class="blue">26.2</span></a> shows <a id="page_294"></a>an example of a visual representation of a non-tracked hand-held controller and other information at waist level in the torso reference frame.</p>
<p class="image"><a id="fig26.2"></a><img src="../images/f0294-01.jpg" alt="image"/></p>
<p class="caption"><a id="pg294lev1"></a><strong><span class="blue">Figure 26.2</span> Information and a visual representation of a non-tracked hand-held controller in the torso reference frame.</strong> (Courtesy of NextGen Interactions)</p>
<h5 class="h5"><strong>Body-Relative Tools</strong></h5>
<p class="noindent">Just like in the real world, tools in VR can be attached to the body so that they are always within reach no matter where the user goes. This is done in VR by simply placing the tool in the torso reference frame. This not only provides convenience of the tool always being available but also takes advantage of the user&#8217;s body acting as a physical mnemonic, which helps in recall and acquisition of frequently used control [<a href="reference.html#ref219"><span class="blue">Min&#233; et al. 1997</span></a>].</p>
<p class="indent">Items should be placed outside the forward direction so that they do not get in the way of viewing the scene (e.g., the user simply looks down to see the options and then can select via a point or grab). Advanced users should be able to turn off items or make them invisible. Examples of physical mnemonics are pull-down menus located above the head (Section <a href="chapter28.html#lev28.4.1"><span class="blue">28.4.1</span></a>), tools surrounding the waste as a utility belt, audio options <a id="page_295"></a>at the ear, navigation options at the user&#8217;s feet, and deletion by throwing an object behind the shoulder (and/or object retrieval by reaching behind the shoulder).</p>
<p class="image"><a id="fig26.3"></a><img src="../images/f0295-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 26.3</span> A simple transparent texture in the hand can convey the physical interface.</strong> (Courtesy of NextGen Interactions)</p>
<h4 class="h4"><a id="lev26.3.4"></a><span class="font1">26.3.4</span> The Hand Reference Frames</h4>
<p class="noindent">The <strong>hand reference frames</strong> are defined by the position and orientation of the user&#8217;s hands, and hand-centric judgments occur when holding an object in the hand. Hand-centric thinking is especially important when using a phone, tablet, or VR controller. Placing a visual representation of a tracked hand-held controller (Section <a href="chapter27.html#lev27.2.3"><span class="blue">27.2.3</span></a>) in the hand(s) can help add a sense of presence due to the sense of touch matching the visuals. Placing labels or icons/signifiers in the hand reference frame that point to buttons, analog sticks, or fingers is extremely helpful (Figure <a href="chapter26.html#fig26.3"><span class="blue">26.3</span></a>), especially for new users. The option to turn on/off such visuals should be provided so as to not occlude/clutter the scene when not using the interface or after the interface has been memorized. Although both the left and right hands can be thought of as separate reference frames, the non-dominant hand is useful for serving as a reference frame for the dominant hand to work in (Section <a href="chapter25.html#lev25.5.1"><span class="blue">25.5.1</span></a>), especially for hand-held panels (Section <a href="chapter28.html#lev28.4.1"><span class="blue">28.4.1</span></a>).</p>
<h4 class="h4"><a id="page_296"></a><a id="lev26.3.5"></a><span class="font1">26.3.5</span> The Head Reference Frame</h4>
<p class="noindent">The <strong>head reference</strong> frame is based on the point between the two eyes and a reference direction perpendicular to the forehead. In psychological literature, this reference frame is known as the <strong>cyclopean eye,</strong> which is a hypothetical position in the head that serves as our reference point for the determination of a head-centric straight-ahead [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. People generally tend to think of this straight-ahead as a direction in front of themselves, oriented around the midline of the head, regardless of where the eyes are actually looking. From an implementation point of view, the head reference frame is equivalent to the head-mounted-display reference frame, but from the user&#8217;s point of view (assuming a wide field of view), the display is not visually perceived. A world-fixed secondary display that shows what the user is seeing matches the head reference frame.</p>
<p class="indent">Heads-up displays (HUDs) are often located in the head reference frame. Such heads-up display information should be minimized, if used at all, other than a selection pointer for gaze-directed selection (Section <a href="chapter28.html#lev28.1.2"><span class="blue">28.1.2</span></a>). If used, it is important to make cues small (but large enough to be easily perceived/readable), minimize the number of visual cues to not be annoying or distracting, not make the cues too far in the periphery, give the cues depth so they are occluded properly by other objects <a id="pg296lev1"></a>(Section <a href="chapter13.html#lev13.2"><span class="blue">13.2</span></a>), and place the cues at a far enough distance so that there is not an extreme accommodation-vergence conflict (Section <a href="chapter13.html#lev13.1"><span class="blue">13.1</span></a>). It can also be useful to make the cues transparent. Figure <a href="chapter26.html#fig26.4"><span class="blue">26.4</span></a> shows an example HUD in the head reference frame that serves as a virtual helmet that helps the user to target objects.</p>
<h4 class="h4"><a id="lev26.3.6"></a><span class="font1">26.3.6</span> The Eye Reference Frames</h4>
<p class="noindent">The <strong>eye reference frames</strong> are defined by the position and orientation of the eyeballs. Very few VR systems support the orientation portion of eye reference frames due to the requirement for eye tracking (Section <a href="chapter27.html#lev27.3.2"><span class="blue">27.3.2</span></a>). However, the left/right horizontal offset of the eyes is easily determined by the interpupillary distance of the specific user measured in advance of usage.</p>
<p class="indent">When looking at close objects (for example, when sighting down the barrel of a gun) and assuming a binocular display, eye reference frames are important to consider because they are differentiated from the head reference frame due to the offset from the forehead to the eye. The offset between the left and right eyes results in double images for close objects when looking at an object further in the distance (or double images of the further object when looking at the close object). Thus, when users are attempting to line up close objects with further objects (e.g., a targeting task), users should be advised to close the non-dominant eye and to sight with the dominant eye (Sections <a href="chapter09.html#lev9.1.1"><span class="blue">9.1.1</span></a> and <a href="chapter23.html#lev23.2.2"><span class="blue">23.2.2</span></a>).</p>
<p class="image"><a id="page_297"></a><a id="fig26.4"></a><img src="../images/f0297-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 26.4</span> A heads-up display in the head reference frame. No matter where the user looks with the head, the cues are always visible.</strong> (Courtesy of NextGen Interactions)</p>
<h3 class="h3"><a id="lev26.4"></a><span class="font">26.4</span> Speech and Gestures</h3>
<p class="noindent">The usability of speech and gestures depends upon the number and complexity of the commands. More commands require more learning&#8212;the number of voice commands and gestures should be limited to keep interaction simple and learnable.</p>
<p class="indent">Voice interfaces and gesture recognition systems are normally invisible to the user. Use explicit signifiers, such as a list of possible commands or icons of gestures, in the users&#8217; view so they know and remember what is possible. Neither speech nor gesture recognition is perfect. In many cases it is appropriate to have users verify commands to confirm the system understands correctly before taking action. Feedback should also be provided to let the user know a command has been understood (e.g., highlight the signifier if the corresponding command has been activated).</p>
<p class="indent"><a id="page_298"></a>Use a set of well-defined, natural, easy-to-understand, and easy-to-recognize gestures/words. Pushing a button to signal to the computer that a word or gesture is intended to start (i.e., push-to-talk or push-to-gesture) can keep the system from recognizing unintended commands. This is especially true when the user is also communicating with other humans, rather than just the system itself (for both voice and gestures as humans subconsciously make gestures as they talk).</p>
<h4 class="h4"><a id="lev26.4.1"></a><span class="font1">26.4.1</span> Gestures</h4>
<p class="noindent">A <strong>gesture</strong> is a movement of the body or body part whereas a <strong>posture</strong> is a single static configuration. Each conveys some meaning whether intentional or not. Postures can be considered a subset of gestures (i.e., a gesture over a very short period of time or a gesture with imperceptible movement). Dynamic gestures consist of one or more tracked points (consider making a gesture with a controller) whereas a posture requires multiple tracked points (e.g., a hand posture).</p>
<p class="indent">Gestures can communicate four types of information [<a href="reference.html#ref134"><span class="blue">Hummels and Stappers 1998</span></a>].</p>
<p class="hangt"><strong>Spatial information</strong> is the spatial relationship that a gesture refers to. Such gestures <a id="pg298lev1"></a>can manipulate (e.g., push/pull), indicate (e.g., point or draw a path), describe form (e.g., convey size), describe functionality (e.g., twisting motion to describe twisting a screw), or use objects. Such direct interaction is a form of structural communication (Section <a href="chapter01.html#lev1.2.1"><span class="blue">1.2.1</span></a>) and can be quite effective for VR interaction due to its direct and immediate effect on objects.</p>
<p class="hang"><strong>Symbolic information</strong> is the sign that a gesture refers to. Such gestures can be concepts like forming a V shape with the fingers, waving to say hello or goodbye, and explicit rudeness with a finger. The formation of such gestures is structural communication (Section <a href="chapter01.html#lev1.2.1"><span class="blue">1.2.1</span></a>) whereas the interpretation of the gesture is indirect communication (Section <a href="chapter01.html#lev1.2.2"><span class="blue">1.2.2</span></a>). Symbolic information can be useful for both human-computer interaction and human-human interaction.</p>
<p class="hang"><strong>Pathic information</strong> is the process of thinking and doing that a gesture is used with (e.g., subconsciously talking with one&#8217;s hands). Pathic information is most commonly visceral communication (Section <a href="chapter01.html#lev1.2.1"><span class="blue">1.2.1</span></a>) added on to indirect communication (Section <a href="chapter01.html#lev1.2.1"><span class="blue">1.2.1</span></a>) that is useful for human-human interaction.</p>
<p class="hang"><strong>Affective information</strong> is the emotion a gesture refers to. Such gestures are more typically body gestures that convey mood such as distressed, relaxed, or enthusiastic. Affective information is a form of visceral communication (Section <a href="chapter01.html#lev1.2.1"><span class="blue">1.2.1</span></a>) most often used for human-human interaction, although pathic information is less commonly recognized with computer vision as discussed in Section <a href="chapter35.html#lev35.3.3"><span class="blue">35.3.3</span></a>.</p>
<p class="indentt"><a id="page_299"></a>In the real world, hand gestures often augment communication with gestures such as okay, stop, size, silence, kill, goodbye, pointing, etc. Many early VR systems used gloves as input and gestures to indicate similar commands. Advantages of gestures include flexibility, the number of degrees of freedom of the human hand, the lack of having to hold a device in the hand, and not necessarily having to see (or at least look directly at) the hand. Gestures, like voice, can also be challenging due to having to remember them and most current systems have low recognition rates for more than a few gestures. Although gloves are not as comfortable, they are more consistent than camera-based systems due to not having line-of-sight issues. Push-to-gesture systems can drastically reduce false positives. This is especially true when the user is communicating with other humans, rather than just the system itself.</p>
<h5 class="h5"><strong>Direct vs. Indirect Gestures</strong></h5>
<p class="noindent"><strong>Direct gestures</strong> are immediate and structural (Section <a href="chapter01.html#lev1.2.1"><span class="blue">1.2.1</span></a>) in nature and convey spatial information; they can be interpreted and responded to by the system as soon as the gesture starts. Direct manipulation, such as pushing an object, and selection via hand pointing are examples of direct gestures. <strong>Indirect gestures</strong> indicate more complex semantic meaning over a period of time so the start of the gesture is not sufficient&#8212;the application interprets over a range of movement so there is a delay from the start of the <a id="pg299lev1"></a>gesture. Indirect gestures convey symbolic, pathic, and affective information. A single posture command is somewhere between direct and indirect because the system response is immediate but not structural (the posture is interpreted as a command).</p>
<h4 class="h4"><a id="lev26.4.2"></a><span class="font1">26.4.2</span> Speech Recognition</h4>
<p class="noindent"><strong>Speech recognition</strong> translates spoken words into textual and semantic form. If implemented well, voice commands have many advantages including keeping the head and hands free to interact while giving commands to the system. Voice recognition does have significant challenges including limited recognition capability, not always obvious command options, difficulty in selecting from a continuous scale, background noise, variability between speakers, and distraction to other individuals [<a href="reference.html#ref208"><span class="blue">McMahan et al. 2014</span></a>]. Regardless, speech can work well for multimodal interactions (Section <a href="chapter26.html#lev26.6"><span class="blue">26.6</span></a>). Speech recognition categories, strategies, and errors are discussed below as described by <a href="reference.html#ref112"><span class="blue">Hannema</span></a> [<a href="reference.html#ref112"><span class="blue">2001</span></a>].</p>
<h5 class="h5"><strong>Speech Recognition Categories</strong></h5>
<p class="noindent">Speech recognition is often categorized into the following groups.</p>
<p class="hangt"><strong>Speaker-independent speech recognition</strong> has the flexibility to recognize a small number of words from a wide range of users. This type of speech recognition is used with telephone navigation systems and is best used with VR when there are <a id="page_300"></a>only a small number of options provided to the user (a VR system should visually show available commands so the user knows what the options are).</p>
<p class="hang"><strong>Speaker-dependent speech recognition</strong> recognizes a large number of words from a single user where the system has been extensively trained to recognize words from that specific user. This type of speech recognition can work well with VR when the user has a personal system that she uses often.</p>
<p class="hang"><strong>Adaptive recognition</strong> is a mix of speaker-independent and speaker-dependent speech recognition. The system does not need explicit training but learns the characteristics of the specific user as he speaks. This often requires that the user corrects the system when words are misinterpreted. Use adaptive recognition when users have their own system but don&#8217;t want to bother with explicitly training the voice recognizer.</p>
<h5 class="h5"><strong>Speech Recognition Strategies</strong></h5>
<p class="noindent">Each of the speech recognition categories listed above can use one or more of the following strategies to recognize words.</p>
<p class="hangt"><strong>Discrete/isolated</strong> strategies recognize one word at a time from a predefined vocabulary. This strategy works well when only one word is used or there is a silence between consecutive words. Examples include commands such as &#8220;save,&#8221; &#8220;undo,&#8221; &#8220;restart,&#8221; or &#8220;freeze.&#8221;</p>
<p class="hang"><strong>Continuous/connected</strong> strategies recognize consecutive words from a predefined vocabulary. This is more challenging to implement than a discrete/isolated strategy.</p>
<p class="hang"><strong>Phonetic strategies</strong> recognize individual phonemes (small, perceptually distinct sounds; Section <a href="chapter08.html#lev8.2.3"><span class="blue">8.2.3</span></a>), diphones (combinations of two adjacent phonemes), or triphones (combinations of three adjacent phonemes). Triphones are computationally expensive, and the system may be slow to respond due to the number of combinations that must be recognized so are rarely used.</p>
<p class="hang"><strong>Spontaneous/conversational strategies</strong> attempt to determine the context of the words in sentences in a way similar to what humans do. This results in a natural spoken dialogue with the computer. This strategy can be difficult to implement well.</p>
<h5 class="h5"><strong>Speech Recognition Errors</strong></h5>
<p class="noindent">There are several reasons why speech recognition is difficult. By being aware of the common types of errors listed below, the system can be better designed to minimize <a id="page_301"></a>such errors. Errors can also be reduced by using a microphone designed for speech recognition (Section <a href="chapter27.html#lev27.3.3"><span class="blue">27.3.3</span></a>).</p>
<p class="hangt"><strong>Deletion/rejection</strong> occurs when the system fails to match or recognize a word from the predetermined vocabulary. The advantage of this type of error is that the system recognizes the failure and can request the user to repeat the word.</p>
<p class="hanga"><strong>Substitution</strong> occurs when the system misrecognizes a word for a different word than that which was intended. If the error is not caught, the system might execute a wrong command. This error is difficult to detect, but statistical measures can be used to calculate confidence.</p>
<p class="hanga"><strong>Insertion</strong> occurs when an unintended word is recognized. This most often occurs when the user is not intentionally speaking to the system, such as when thinking out loud or when speaking to another human. Similar to a substitution error, this can execute an unintended command. Requiring the user to push a button (e.g., a push-to-talk interface) can drastically reduce this type of error.</p>
<h5 class="h5"><strong>Context</strong></h5>
<p class="noindent">The specific context that the user is engaged in at any particular time can help improve accuracy and better match the user&#8217;s intention. This is important as words can be homonyms (the same word with multiple meanings, e.g., volume can be a volume of space or audio volume) or homophones (different words with the same sound, e.g., die and dye). Context-sensitive systems with a large vocabulary can be implemented by having the system only able to recognize a subset of that vocabulary at any particular time.</p>
<h3 class="h3"><a id="lev26.5"></a><span class="font">26.5</span> <strong>Modes and Flow</strong></h3>
<p class="noindent">Although ideally the same metaphors should be applied across all interactions for a single application, this is often not possible. Complex applications with different types of tasks may require different interaction techniques. In such a case, different techniques might be combined together. The mechanism to choose a technique may be as simple as pressing a different button or a mode selection from a hand-held panel, or the technique may be order dependent (e.g., a specific manipulation technique only occurs after a specific selection technique). Whatever the mode, that mode should be made clear to the user.</p>
<p class="indent">All interactions should also integrate and flow together well. The overall usability of a system depends on the seamless integration of various tasks and techniques provided by the application. One way to think about flow is to consider the sequence of <a id="page_302"></a>basic actions. People may more often verbally state commands with the action coming before the object to be acted upon, but they tend to think about the object first. Objects are more concrete in nature so they are easier to first think about, whereas verbs are more abstract and are more easily thought about when being applied to something. For example, someone might think &#8220;pick up the book,&#8221; but before thinking about picking up the book the person must first perceive and think about the book. Users prefer object-action sequences over action-object sequences as it requires less mental effort [<a href="reference.html#ref206"><span class="blue">McMahan and Bowman 2007</span></a>]. Thus, when designing interaction techniques, the selection of the object to be acted upon should be performed (at least in most cases) before taking action upon that object. The interaction technique should also enable easy and smooth transition between selecting an object and manipulating or using that object.</p>
<p class="indent">At a higher level, the flow of longer interactions should occur without distractions so the user can give full attention to the primary task. Ideally, users should not have to physically (whether the eyes, head, or hands) or cognitively move between tasks. Lightweight mode switching, physical props, and multimodal techniques can help to <a id="pg302lev1"></a>maintain the flow of interaction.</p>
<h3 class="h3"><a id="lev26.6"></a><span class="font">26.6</span> <strong>Multimodal Interaction</strong></h3>
<p class="noindent">No single sensory input or output is appropriate for all situations. <strong>Multimodal interactions</strong> combine multiple input and output sensory modalities to provide the user with a richer set of interactions. The &#8220;put-that-there&#8221; interface is known as the first human-computer interface to effectively and naturally mix voice and gesture [<a href="reference.html#ref21"><span class="blue">Bolt 1980</span></a>]. Note although &#8220;put-that-there&#8221; is an action-object sequence, as discussed above in Section <a href="chapter26.html#lev26.5"><span class="blue">26.5</span></a>, better flow often occurs by first selecting the object to be moved. The better implementation might be called a &#8220;that-moves-there&#8221; interface.</p>
<p class="indent">When choosing or designing multimodal interactions, it can be helpful to consider different ways of integrating the modalities together. Input can be categorized into six types of combinations: specialized, equivalence, redundancy, concurrency, complementarity, and transfer [<a href="reference.html#ref174"><span class="blue">Laviola 1999</span></a>, <a href="reference.html#ref200"><span class="blue">Martin 1998</span></a>]. All of the input modality types are multimodal other than specialized.</p>
<p class="hangt"><strong>Specialized input</strong> limits input options to a single modality for a specific application. Specialization is ideal when there is clearly a single best modality for the task. For example, for some environments, selecting an object might only be performed by pointing.</p>
<p class="hanga"><a id="page_303"></a><strong>Equivalent input modalities</strong> provide the user a choice of which input to use, even though the result would be the same across modalities. Equivalence can be thought of as the system being indifferent to user preferences. For example, a user might be able to create the same objects either by voice or through a panel.</p>
<p class="hanga"><strong>Redundant input modalities</strong> take advantage of two or more simultaneous types of input that convey the same information to perform a single command. Redundancy can reduce noise and ambiguous signals, resulting in increased recognition rates. For example, a user might select a red cube with the hand while saying &#8220;select the red cube&#8221; or physically move an object with the hand while saying &#8220;move.&#8221;</p>
<p class="hanga"><strong>Concurrent input modalities</strong> enable users to issue different commands simultaneously, and thus enable users to be more efficient. For example, a user might be pointing to fly while verbally requesting information about an object in the distance.</p>
<p class="hanga"><strong>Complementarity input modalities</strong> merge different types of input together into a <a id="pg303lev1"></a>single command. Complementarity often results in faster interactions as the different modalities are typically close in time or even concurrent. For example, to delete an object, the application might require the user to move the object behind the shoulder while saying &#8220;delete.&#8221; Another example is a &#8220;put-that-there&#8221; interface [<a href="reference.html#ref21"><span class="blue">Bolt 1980</span></a>] that merges voice and gesture to place an object.</p>
<p class="hanga"><strong>Transfer</strong> occurs when information from one input modality is transferred to another input modality. Transfer can improve recognition and enable faster interactions. A user may achieve part of a task by one modality but then determine a different modality would be more appropriate to complete the task. In such a case, transfer would prevent the user from needing to start over. An example is verbally requesting a specific menu to appear, which can be interacted with by speaking or pointing. Another example is a &#8220;push-to-talk&#8221; interface. Transfer is most appropriate to use when hardware is unreliable or does not work well in some situations.</p>
<h3 class="h3"><a id="lev26.7"></a><span class="font">26.7</span> <strong>Beware of Sickness and Fatigue</strong></h3>
<p class="noindent">Some interaction techniques, especially those controlling the viewpoint, can cause motion sickness. When choosing or creating navigation techniques, designers should carefully understand and consider scene motion and motion sickness as discussed in Part <a href="part03.html#part3"><span class="blue">III</span></a>. If motion sickness is a primary concern, then changing the viewpoint <a id="page_304"></a>should only occur through one-to-one mapping of real head motion or teleportation (Section <a href="chapter28.html#lev28.3.4"><span class="blue">28.3.4</span></a>).</p>
<p class="indent">Some users are not comfortable looking at interfaces close to the face for extended periods of time due to the accommodation-vergence conflict (Section <a href="chapter13.html#lev13.1"><span class="blue">13.1</span></a>) that occurs in most of today&#8217;s HMDs. Visual interfaces close to the face should be minimized.</p>
<p class="indent">As mentioned in Section <a href="chapter14.html#lev14.1"><span class="blue">14.1</span></a>, gorilla arm can be a problem for interactions that require the user to hold their hands up high and out in front of themselves for more than a few seconds at a time. This occurs even with bare-hand systems (Section <a href="chapter27.html#lev27.2.5"><span class="blue">27.2.5</span></a>) where the user is not carrying any additional weight. Interactions should be designed to minimize holding the hands above the waist for more than a few seconds at a time. For example, shooting a ray from the hand held at the hip is quite comfortable.</p>
<h3 class="h3"><a id="lev26.8"></a><span class="font">26.8</span> <strong>Visual-Physical Conflict and Sensory Substitution</strong></h3>
<p class="noindent">Most VR experiences offer little haptic feedback, and when they do the feedback is quite limited compared to the sense of touch in the real world. Not having full haptic <a id="pg304lev1"></a>feedback is more of a problem than just not feeling objects. The hand or other body part (or physical device) continues to move through the object since there is no (or limited) physical force stopping it from doing so. As a result, the physical location of the hand may no longer match the visual location.</p>
<p class="indent">Enforcing simulated physics so the hand does not visually pass through visual geometry is often preferred by users when the penetrations are only slight (shallow penetration). When deeper penetration occurs, then users prefer the visual hands to match the physical hands even though that breaks the intuition that hands do not pass through objects [<a href="reference.html#ref185"><span class="blue">Lindeman 1999</span></a>]. Stopping the visual hand for deep penetration can be especially confusing when the visual hand pops out of a different part of the penetrated object than where the visual hand has previously been visually stopped. A compromise solution for non-realistic interactions is to draw two hands when the physical hand and physically simulated hand diverge (see ghosting below).</p>
<p class="indent">In some cases, the virtual hand can be considerably offset from the physical hand without the user noticing as visual representation tends to dominate proprioception [<a href="reference.html#ref38"><span class="blue">Burns et al. 2006</span></a>]. However, this is not always the case. Vision is generally stronger than proprioception when moving the hand in a left/right and/or up/down direction, but proprioception can be stronger when moving the hand in depth (forward/back) [<a href="reference.html#ref314"><span class="blue">Van Beers et al. 2002</span></a>].</p>
<p class="indent"><strong>Sensory substitution</strong> is the replacement of an ideal sensory cue that is not available with one or more other sensory cues. Examples of sensory substitution that work well with VR are described below.</p>
<p class="image"><a id="page_305"></a><a id="fig26.5"></a><img src="../images/f0305-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 26.5</span> In the game &#8220;The Gallery: Six Elements,&#8221; the bottle is highlighted to show the object can be grabbed.</strong> (Courtesy of Cloudhead Games)</p>
<p class="hang1"><strong>Ghosting</strong> is a second simultaneous rendering of an object in a different pose than the actual object. In some cases it is appropriate to render the hand twice&#8212;both where the physical hand is located and where the physics simulation states the hand is located. Ghosting is also often used to provide a clue of where a virtual object will be snapped into place if released. Be careful of using ghosting for training applications as users can depend on ghosting as a crutch that will not be available for the real-world task.</p>
<p class="hanga"><strong>Highlighting</strong> is visually outlining or changing the color of an object. Highlighting is most often used to show the hand has intersected with an object so that it can be selected or picked up. Highlighting is also used to convey that an object is able to be selected or grabbed when the hand is close even though a collision has not yet occurred. Figure <a href="chapter26.html#fig26.5"><span class="blue">26.5</span></a> shows an example of highlighting.</p>
<p class="hanga"><strong>Audio cues</strong> are very effective in conveying to a user that one of his hands has collided with some geometry. Audio might be as simple as a tone sound or real-world recorded audio track. In some cases, providing multiple audio files with variations (e.g., random grunt sounds when colliding with a virtual wall or when shot by an enemy) can help with a sense of realism and reduce annoyance. Continuous contact sounds can also be used to convey sliding along surfaces. Sound properties such as pitch or amplitude might also change depending on penetration depth.</p>
<p class="hanga"><strong>Passive haptics</strong> (static physical objects that can be touched; Section <a href="chapter03.html#lev3.2.3"><span class="blue">3.2.3</span></a>) are effective when the virtual world is limited to the physical space where no virtual <a id="page_306"></a>navigation can occur (i.e., when the real-world reference frames and virtual-world reference frames are consistent; Section <a href="chapter26.html#lev26.3"><span class="blue">26.3</span></a>) or when tracked physical tools travel with the user (i.e., the physical and virtual objects are spatially compliant; Section <a href="chapter25.html#lev25.2.5"><span class="blue">25.2.5</span></a>). Because vision often dominates proprioception, perfect spatial compliance is not always required [<a href="reference.html#ref38"><span class="blue">Burns et al. 2006</span></a>]. Redirected touching warps virtual space to map many differently shaped virtual objects onto a single real object (i.e., hand or finger tracking is not one-to-one) in a way that the discrepancy between virtual and physical is below the user&#8217;s perceptual threshold [<a href="reference.html#ref163"><span class="blue">Kohli 2013</span></a>]. For example, when one&#8217;s real hand traces a physical object, the virtual hand can trace a slightly differently shaped virtual object.</p>
<p class="hanga"><a id="pg306lev1"></a><strong>Rumble</strong> causes an input device to vibrate. Although not the same haptic force that would occur in the real world, rumble feedback can be quite an effective cue for informing the user she has collided with an object.</p>
</body>
</html>
