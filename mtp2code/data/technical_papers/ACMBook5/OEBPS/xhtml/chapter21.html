<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" >
<head>
<title>The VR Book</title>
<link rel="stylesheet" type="text/css" href="../styles/9781970001143.css"/>
</head>
<body>
<h2 class="h2"><a id="page_237"></a><a id="ch21"></a><span class="blue1">21</span></h2>
<h2 class="h2b"><span class="blue">Environmental Design</span></h2>
<p class="noindent">The environment users find themselves in defines the context of everything that occurs in a VR experience. This chapter focuses on the virtual scene and its different aspects that environmental designers should keep in mind when creating virtual worlds.</p>
<h3 class="h3"><a id="lev21.1"></a><span class="font">21.1</span> <strong>The Scene</strong></h3>
<p class="noindent">The <strong>scene</strong> is the entire current environment that extends into space and is acted within.</p>
<p class="indent"><a id="pg237lev1"></a>The scene can be divided into the background, contextual geometry, fundamental geometry, and interactive objects (similar to that defined by <a href="reference.html#ref79"><span class="blue">Eastgate et al.</span></a> [<a href="reference.html#ref79"><span class="blue">2014</span></a>]). Background, contextual geometry, and fundamental geometry correspond approximately to vista space, action space, and personal space as described in Section <a href="chapter09.html#lev9.1.2"><span class="blue">9.1.2</span></a>.</p>
<p class="hangt"><strong>The background</strong> is scenery in the periphery of the scene located in far vista space. Examples are the sky, mountains, and the sun. The background can simply be a textured box since non-pictorial depth cues (Section <a href="chapter09.html#lev9.1.3"><span class="blue">9.1.3</span></a>) at such a distance are non-existent.</p>
<p class="hanga"><strong>Contextual geometry</strong> helps to define the environment one is in. Contextual geometry includes far landmarks (Section <a href="chapter21.html#lev21.5"><span class="blue">21.5</span></a>) that aid in wayfinding and are typically located in action space. Contextual geometry has no affordances (i.e., can&#8217;t be picked up; Section <a href="chapter25.html#lev25.2.1"><span class="blue">25.2.1</span></a>). Contextual geometry is often far enough away that it can consist of simple faked geometry (e.g., 2D billboards/textures often used to portray more complex geometry such as trees).</p>
<p class="hanga"><strong>Fundamental geometry</strong> consists of nearby static components that add to the fundamental experience. Fundamental geometry includes items such as tables, instructions, and doorways. Fundamental geometry often has some affordances, such as preventing users from walking through walls or providing the ability to set an object upon something. This geometry is most often located in personal space and action space. Because of its nearness, artists should focus on fundamental <a id="page_238"></a>geometry. For VR, 3D details are especially important at such distances (Section <a href="chapter23.html#lev23.2.1"><span class="blue">23.2.1</span></a>).</p>
<p class="hanga"><strong>Interactive objects</strong> are dynamic items that can be interacted with (discussed extensively in Part <a href="part05.html#part5"><span class="blue">V</span></a>). These typically small objects are in personal space when directly interacted with and most commonly in action space when indirectly interacted with.</p>
<p class="indentt">All objects and geometry scaling should be consistent relative to each other and the user. For example, a truck in the distance should be scaled appropriately relative to a closer car in 3D space; the truck should not be scaled smaller because it is in the distance. A proper VR rendering library should handle the projection transformation from 3D space to the eye; artists need not be concerned with such detail. For realistic experiences, include familiar objects with standard sizes that are easily seen by the user (see relative/familiar size in Section <a href="chapter09.html#lev9.1.3"><span class="blue">9.1.3</span></a>). Most countries have standardized dimensions for sheets of paper, cans of soda, money, stair height, and exterior doors.</p>
<h3 class="h3"><a id="lev21.2"></a><span class="font">21.2</span> <strong>Color and Lighting</strong></h3>
<p class="noindent">Color is so pervasive in the world that we often take it for granted. However, we constantly perceive and interact with colors throughout the day whether getting dressed or driving. We associate color to emotional reactions (purple with rage, green with envy, feeling blue) and special meanings (red signifies danger, purple signifies royalty, greens signifies ecology).</p>
<p class="indent">Other factors influence our perception of color. For example, the surrounding context/background can influence our perception of an object&#8217;s color. Exaggerated coloring of the entire scene or extreme lighting conditions can result in loss of lightness and color constancy (Section <a href="chapter10.html#lev10.1.1"><span class="blue">10.1.1</span></a>) and objects seen as being different than they would be under normal conditions. Unless intentional for special situations, content creators should use multiple colors and only slight variations of white light so users perceive the intended color of objects and maintain color constancy.</p>
<p class="indent">Color enables us to better distinguish between objects. For example, bright colors can capture users&#8217; attention through salience (salience refers to physical properties of a scene that reflexively grabs our attention; Section <a href="chapter10.html#lev10.3.2"><span class="blue">10.3.2</span></a>). Figure <a href="chapter21.html#fig21.1"><span class="blue">21.1</span></a> (left) shows an example of how non-relevant background can be made a dull gray to draw attention to more relevant objects. Figure <a href="chapter21.html#fig21.1"><span class="blue">21.1</span></a> (right) shows how color is turned on, signifying that brain lobes can be grabbed after the system provides audio instructions.</p>
<p class="indent">Consider that real-world painters choose from only a little over 1,000 colors (the Pantone Matching System has about 1,200 color choices). This does not mean we only need 1,200 pixel options for VR. Much of the subtle changes in color come from gradual changes over surfaces&#8212;for example, change in lighting intensity across a surface. However, ~1,000 colors is fine for creating content before lighting is added to the scene (unless one is attempting to perfectly match a color from outside the 1,000-color set).</p>
<p class="image"><a id="page_239"></a><a id="fig21.1"></a><img src="../images/f0239-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 21.1</span> Color is used as salience to direct attention. The user&#8217;s eyes are drawn toward the colored items in the scene (left). After the system provides audio instructions, the brain lobes and table are colored to draw attention and to signify they can be interacted with (right).</strong> (Courtesy of Digital ArtForms)</p>
<h3 class="h3"><a id="lev21.3"></a><span class="font">21.3</span> <strong>Audio</strong></h3>
<p class="noindent">As discussed in Section <a href="chapter08.html#lev8.2"><span class="blue">8.2</span></a>, sound is quite complex with many factors affecting how we perceive it. Auditory cues play a crucial role in everyday life as well as VR, including adding awareness of surroundings, adding emotional impact, cuing visual attention, conveying a variety of complex information without taxing the visual system, and providing unique cues that cannot be perceived through other sensory systems. Although deaf people have learned to function quite well, they have a lifetime of experience learning techniques to interact with a soundless world. VR without sound is equivalent to making someone deaf without the benefit of having years of experience learning to cope without hearing.</p>
<p class="indent">The entertainment industry understands audio well. For example, George Lucas, known for his stunning visual effects, has stated sound is 50% of the motion picture experience. Like a great movie, music is especially good at evoking emotion. Subtle <strong>ambient sound effects</strong> such as birds chirping along with rustling of trees by the wind, children playing in the distance, or the clanking of an industrial setting can provide a surprisingly strong effect on a sense of realism and presence.</p>
<p class="indent">When sounds are presented in an intelligent manner, they can be informing and extremely useful. Sounds work well for creating situational awareness and can obtain <a id="page_240"></a>attention independent of visual properties and where one is looking. Although sound is important, sound can be overwhelming and annoying if not presented well. It is also difficult to ignore sounds like what can be done with vision (e.g., closing the eyes). Aggressive warning sounds that are infrequent with short durations are designed to be unpleasant and attention-getting, and thus are appropriate if that is the intention. However, if overused such sounds can quickly become annoying rather than useful.</p>
<p class="indent">Vital information can be conveyed through spoken language (Section <a href="chapter08.html#lev8.2.3"><span class="blue">8.2.3</span></a>) that is either synthesized, prerecorded by a human, or live from a remote user. The spoken information might be something as simple as an interface responding with &#8220;confirmed&#8221; or could be as complex as a spoken interface where the system both provides information and responds to the user&#8217;s speech. Other verbal examples include clues to help users reach their goals, warnings of upcoming challenges, annotations describing objects, help as requested from the user, or adding personality to computer-controlled characters.</p>
<p class="indent">At a minimum, VR should include audio that conveys basic information about the environment and user interface. Sound is a powerful feedback cue for 3D interfaces when haptic feedback is not available, as discussed in Section <a href="chapter26.html#lev26.8"><span class="blue">26.8</span></a>.</p>
<p class="indent">For more realistic audio and where auditory cues are especially important, <strong>auralization</strong>&#8212;the rendering of sound to simulate reflections and binaural differences between the ears&#8212;can be used. The result of auralization is <strong>spatialized audio</strong>&#8212;sound that is perceived to come from some location in 3D space (Section <a href="chapter08.html#lev8.2.2"><span class="blue">8.2.2</span></a>). Spatialized audio can be useful to serve as a wayfinding aid (Section <a href="chapter21.html#lev21.5"><span class="blue">21.5</span></a>), to provide cues of where other characters are located, and to give feedback of where the element of a user interface is located. A <strong>head-related transfer function</strong> (HRTF) is a spatial filter that describes how sound waves interact with the listener&#8217;s body, most notably the outer ear, from a specific location. Ideally, HRTFs model a specific user&#8217;s ears, but in practice a generic ear is modeled. Multiple HRTFs from different directions can be interpolated to create an HRTF from any direction relative to the ear. The HRTF can then modify a sound wave from a sound source, resulting in a realistic spatialized audio cue to the user.</p>
<h3 class="h3"><a id="lev21.4"></a><span class="font">21.4</span> <strong>Sampling and Aliasing</strong></h3>
<p class="noindent"><strong>Aliasing</strong> is an artifact that occurs due to approximating data with discrete sampling. With computer graphics, aliasing occurs due to approximating edges of geometry and textures with discretely sampled/rendered pixels. This occurs because the geometry or texture representing the point sample either does or does not project onto the pixel, i.e., a pixel either represents a piece of geometry or color of a texture or it does not. <a id="page_241"></a>Edges in the environment that are projected onto the display cause discontinuities called &#8220;jaggies&#8221; or &#8220;staircasing,&#8221; as shown in Figure <a href="chapter21.html#fig21.2"><span class="blue">21.2</span></a>. Other artifacts also occur, such as moire patterns, as seen in Figure <a href="chapter21.html#fig21.3"><span class="blue">21.3</span></a>. Such artifacts are worse in VR due to the artifact patterns continuously fluctuating as a result of viewpoint movement, each eye being rendered from different views, and the pixels being spread out over a larger field of view. Even subconscious small head motion that is normally imperceptible can be distracting due to the eye being drawn toward the continuous motion of the jaggies and other artifact patterns.</p>
<p class="image"><a id="fig21.2"></a><img src="../images/f0241-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 21.2</span> Jaggies/staircasing as seen on a display that represents the edge of an object. Such artifacts result due to discrete sampling of the object.</strong></p>
<p class="image"><a id="fig21.3"></a><img src="../images/f0241-02.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 21.3</span> Aliasing artifacts can clearly be seen at the left-center area of the chain-link fence. Such artifacts are even worse in VR due to the artifact patterns continuously moving as a result of viewpoint movement as small as normally imperceptible head motion.</strong> (Courtesy of NextGen Interactions)</p>
<p class="indent">Such artifacts can be reduced through various anti-aliasing techniques, such as mipmapping, filtering, rendering, and jittered multi-sampling. Although such techniques can reduce aliasing artifacts, they unfortunately cannot completely obviate all <a id="page_242"></a>artifacts from every possible viewpoint. Some of these techniques also add significantly to rendering time, which can add latency. In addition to using anti-aliasing techniques, content creators can help to reduce such artifacts by not including high-frequency repeating components in the scene. Unfortunately, it is not possible to remove all aliasing artifacts. For example, linear perspective causes far geometry to have high spatial frequency when projected/rendered onto the display (although artifacts can be somewhat reduced by implementing fog or atmospheric attenuation). Swapping in/out models with different levels of detail (e.g., removing geometry with higher-frequency components when it becomes further from the viewpoint) can help but often occurs at the cost of geometry &#8220;popping&#8221; into and out of view, which can cause a break-in-presence. The ideal solution is to have ultra-high resolution displays, but until then content creators should do what they can to minimize such artifacts by not creating or using high-frequency components.</p>
<h3 class="h3"><a id="lev21.5"></a><strong><span class="font">21.5</span> Environmental Wayfinding Aids</strong></h3>
<p class="noindent"><strong>Wayfinding aids</strong> [<a href="reference.html#ref67"><span class="blue">Darken and Sibert 1996</span></a>] help people form cognitive maps and find their way in the world (Section <a href="chapter10.html#lev10.4.3"><span class="blue">10.4.3</span></a>). Wayfinding aids help to maintain a sense of <a id="pg242lev1"></a>position and direction of travel, to know where goals are located, and to plan in the mind how to get to those goals. Examples of wayfinding aids include architectural structures, markings, signposts, paths, compasses, etc. Wayfinding aids are especially important for VR because it is very easy to get disoriented with many VR navigation techniques. Virtual turns can be especially disorienting (e.g., turning with a hand controller but not physically turning the body) due to the lack of vestibular cues and other physical sensations of turning. The lack of physically stepping/walking can also cause incorrect judgments of distance.</p>
<p class="indent">Fortunately, there is much opportunity for creating wayfinding aids in VR that can&#8217;t be done in the real world. Wayfinding aids are most often visual but do not need to be. Floating arrows, spatialized audio, and haptic belts conveying direction are examples of VR wayfinding aids that would be difficult to implement in the real world. The aids may or may not be consciously noticed by users but are often useful in either case. Like the real world, there is far too much sensory information in VR for users to consciously take in information from every single element in the environment. Understanding application and user goals can be useful for designers to know what to put where to help users find their way.</p>
<p class="indent">The remaining portion of this section focuses on environmental wayfinding aids. Section <a href="chapter22.html#lev22.1"><span class="blue">22.1</span></a> focuses on personal wayfinding aids.</p>
<p class="indent"><a id="page_243"></a>Environmental wayfinding aids are cues within the virtual world that are independent of the user. These aids are primarily focused on the organization of the scene itself. They can be overt such as signs along a road, church bells ringing, or maps placed in the environment. They can also be subtle such as buildings, characters traveling in some direction, and shadows from the sun. The sound of vehicles on a highway can be a useful cue when occlusions, such as trees, are common to the environment. Although not typical for VR, smell can also be a strong cue that a user is in a general area. Well-constructed environments do not happen by accident and good level designers will consciously include subtle environmental wayfinding aids even though many users may not consciously notice. There is much that can be done in designing VR spaces that enhances spatial understanding of the environment so users can comprehend and operate effectively.</p>
<p class="indent">Although VR designers can certainly create more than what can be done within the limitations of physical reality, we can learn much from non-VR designers. Just because more is possible with VR does not mean we should not understand real spaces and construct space in a meaningful way. Architectural designers and urban planners have been dealing with wayfinding aids for centuries through the relationship between people and the environment. <a href="reference.html#ref192"><span class="blue">Lynch</span></a> [<a href="reference.html#ref192"><span class="blue">1960</span></a>] found there are similarities across cities <a id="pg243lev1"></a>as described below.</p>
<p class="indent"><strong>Landmarks</strong> are disconnected static cues in the environment that are unmistakable in form from the rest of the scene. They are easy to recognize and help users with spatial understanding. Global landmarks can be seen from about anywhere in the environment (such as a tower), whereas local landmarks provide spatial information closer to the user.</p>
<p class="indent">The strongest landmarks are strategically placed (e.g., on a street corner) and often have the most salient properties (highly distinguishable characteristics), such as a brightly colored light or pulsing glowing arrows showing a path. However, landmarks do not need to be explicit. They might be more subtle like colors or lighting on the floor. In fact, landmarks that are overdominating may cause users to move from one location to another without paying attention to other parts of the scene, discouraging acquisition of spatial knowledge [<a href="reference.html#ref67"><span class="blue">Darken and Sibert 1996</span></a>]. The structure and form of a well-designed natural environment provides strong spatial cues without clutter.</p>
<p class="indent">Landmarks are sometimes directional, meaning they might be perceived from one side but not another. Landmarks are especially important when a user enters a new space, because they are the first things users pay attention to when getting oriented. Landmarks are important for all environments ranging from walking on the ground <a id="page_244"></a>to sea travel to space travel. Familiar landmarks, such as a recognizable building, aid in distance estimation due to their familiarity and relation to surrounding objects.</p>
<p class="indent"><strong>Regions</strong> (also known as districts and neighborhoods) are areas of the environment that are implicitly or explicitly separated from each other. They are best differentiated perceptually by having different visual characteristics (e.g., lighting, building style, and color). <strong>Routes</strong> (also known as paths) are one or more travelable segments that connect two locations. Routes include roads, connected lines on a map, and textual directions. Directing users can be done via channels. <strong>Channels</strong> are constricted routes, often used in VR and video games (e.g., car-racing games) that give users the feeling of a relatively open environment when it is not very open at all. <strong>Nodes</strong> are the interchanges between routes or entrances to regions. Nodes include freeway exits, intersections, and rooms with multiple doorways. Signs giving directions are extremely useful at nodes. <strong>Edges</strong> are boundaries between regions that prevent or deter travel. Examples of edges are rivers, lakes, and fences. Being in a largely void environment is uncomfortable for many users, and most people like to have regular reassurance they are not lost [<a href="reference.html#ref65"><span class="blue">Darken and Peterson 2014</span></a>]. A visual <strong>handrail</strong> is a linear feature of an environment, such as a side of a building or a fence, which is used to guide navigation. Such handrails are used to psychologically constrain movement, keeping the user to <a id="pg244lev1"></a>one side and traveling along it for some distance.</p>
<p class="indent">How these parts of the scene are classified often depends on user capabilities. For walkers, paths are routes and highways tend to be edges, whereas for drivers, highways are routes. For flying users, paths and routes might only serve as landmarks. Often these distinctions are also in the mind of the user, and geometry and cues can be perceived differently by different users or even the same user under different conditions (e.g., when one starts driving a vehicle/aircraft after walking).</p>
<p class="indent">Constructing geometric structure is not enough. The user&#8217;s understanding of overarching themes and structural organization are important for wayfinding. Users should know the structure of the environment for best results, and making such structure explicit in the minds of users is important as it can help give cues meaning, affect what strategies users employ, and improve navigation performance. Street numbers or names in alphabetical order make more sense after understanding streets follow a grid pattern and are named in numerical and alphabetical order.</p>
<p class="indent">Once the person builds a mental model of the meta-structure of the world, violations of the metaphor used to explain the model can be very confusing and it should be made explicit when such violations do occur. A person who lived his entire life in Manhattan (a grid-like structure of city blocks) traveling to Washington, DC, will become very confused until the person understands the hub-and-spoke structure of many of <a id="page_245"></a>the city&#8217;s streets, at which point wayfinding becomes easier. However, due to having many more violations of the metaphor, Washington, DC, is much more confusing than New York for most people.</p>
<p class="indent">Adding concepts of city-like structures to abstract data such as information or scientific visualization can help users better understand and navigate that space [<a href="reference.html#ref136"><span class="blue">Ingram and Benford 1995</span></a>]. Adding cues such as a simple rectangular grid or radial grid can improve performance. Adding paths suggests leading to somewhere useful and interesting. Sectioning data into regions can emphasize different parts of the dataset. However, developers should be careful of imposing inappropriate structure onto abstract or scientific data as users will grasp onto anything they perceive as structure, and that may result in perceiving structure in the data that does not exist in the data itself. Subject-matter experts familiar with the data should be consulted before adding such cues.</p>
<h4 class="h4"><a id="lev21.5.1"></a><strong><span class="font1">21.5.1</span> Markers, Trails, and Measurement</strong></h4>
<p class="noindent"><strong>Markers</strong> are user-placed cues. If using a map (Section <a href="chapter22.html#lev22.1.1"><span class="blue">22.1.1</span></a>), markers can be placed on the map (e.g., as colored pushpins) as well as in the environment. This helps users remember which preexisting landmarks are important. <strong>Breadcrumbs</strong> are markers <a id="pg245lev1"></a>that are dropped often by the user when traveling through an environment [<a href="reference.html#ref66"><span class="blue">Darken and Sibert 1993</span></a>]. A <strong>trail</strong> is evidence of a path traveled by a user and informs the person who left the path as well as other users that they have already been to that place and how they traveled to and from there. Trails consisting of individual directional cues (such as footprints) are generally better than non-directional cues. Multiple trails inform of well-traveled paths. Unfortunately, many trails can result in visual clutter. Footprints can fade away after some time to reduce clutter. Sometimes it is better to identify areas searched instead of paths followed.</p>
<p class="indent">Data understanding and spatial comprehension via interactive analysis often requires markup and quantification of dataset features within the environment. Different types of markup tools such as paintbrushes and skewers that contain symbolic, textual, or numerical information can be provided to interactively mark, count, and measure features of the environment. Users may want to simply indicate areas of interest for later investigation, or to count the number of vessels exiting a mass (e.g., a tumor) by having the system automatically increment a counter as markers are placed. Linear segments, surface area, and angles can also be measured and the resulting values placed in the environment with a line attached to the markups. Figure <a href="chapter21.html#fig21.4"><span class="blue">21.4</span></a> shows examples of drawing on the world and placing skewers. Figure <a href="chapter21.html#fig21.5"><span class="blue">21.5</span></a> shows an example of measuring the circumference of an opening within a medical dataset.</p>
<p class="image"><a id="fig21.4"></a><a id="page_246"></a><img src="../images/f0246-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 21.4</span> A user observes a colleague marking up a terrain.</strong> (Courtesy of Digital ArtForms)</p>
<p class="image"><a id="fig21.5"></a><img src="../images/f0246-02.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 21.5</span> A user measures the circumference of an opening in a CT medical dataset.</strong> (Courtesy of Digital ArtForms)</p>
<h3 class="h3"><a id="lev21.6"></a><strong><span class="font">21.6</span> Real-World Content</strong></h3>
<p class="noindent">Content does not necessarily need to be created by artists. One way of creating VR environments is not to build content but to reuse what the real world already provides. Real-world data capture can result in quite compelling experiences.</p>
<p class="image"><a id="page_247"></a><a id="fig21.6"></a><img src="../images/f0247-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 21.6</span> A 360&#176; image from the VR experience <em>Strangers with Patrick Watson</em>.</strong> (&#169; Strangers with Patrick Watson / F&#233;lix &#38; Paul Studios)</p>
<h4 class="h4"><a id="lev21.6.1"></a><strong><span class="font1">21.6.1</span> 360&#176; Cameras</strong></h4>
<p class="noindent">Specialized panoramic cameras can be used to capture the world in 360&#176; from one or more specific viewpoints (Figure <a href="chapter21.html#fig21.6"><span class="blue">21.6</span></a>). Capturing the world in this way is forcing filmmakers to rethink what it means to experience content. Instead of watching a movie through a &#8220;window,&#8221; viewers of immersive film are in and part of the scene. An example of capturing 360&#176; data that is very different from traditional filming is having to carefully place equipment in a way so that it is not seen by the camera in all directions. In a similar way, all individuals that are not part of the story must leave the set or hide behind objects so they do not unintentionally become part of the story. Those capturing 360&#176; should do so from static poses unless they are extremely careful in controlling the motion of the camera (Section <a href="chapter18.html#lev18.5"><span class="blue">18.5</span></a>).</p>
<h5 class="h5"><strong>Stereoscopic Capture</strong></h5>
<p class="noindent">Creating stereoscopic 360&#176; content is technically challenging due to the cameras capturing content from only a limited number of views. Such content can work reasonably well with VR when the viewer is seated in a fixed position and primarily only makes yaw (left-to-right or right-to-left) type of head motions. However, if the viewer pitches the head (looks down) or rolls the head (twists the head while still looking straight ahead), then the assumptions for presenting the stereoscopic cues no longer hold and the viewer will perceive strange results. A common method of dealing with the pitch problem is to have the disparity between the left and right eyes in the upper and lower parts of the scene converge. This increases comfort at the cost of everything above the viewer and below the viewer appearing far away. Another method is to smooth out the ground beneath the viewer to a single color such that it contains no depth cues. Or a virtual object can be placed beneath the viewer to prevent seeing the real-world data <a id="page_248"></a>at that location. These methods all work well enough when the content of interest is not above or below the viewer.</p>
<p class="indent">Another solution for the stereo problem is simply not to show the captured content in stereo. This makes content capture a lot easier and reduces artifacts. Computer-generated content that has depth information can be added to the scene but will always appear in front of the captured content.</p>
<h4 class="h4"><a id="lev21.6.2"></a><strong><span class="font1">21.6.2</span> Light Fields and Image-Based Capture/Rendering</strong></h4>
<p class="noindent">Capture from a single location cannot properly convey a fully immersive VR experience when users move their heads&#8212;for example, when leaning left or right. A <strong>light field</strong> describes light that flows in multiple directions through multiple points in space. An array of cameras can be coupled with light field capture and rendering techniques to portray multiple viewpoints from different locations, including those not originally captured [<a href="reference.html#ref101"><span class="blue">Gortler et al. 1996</span></a>, <a href="reference.html#ref180"><span class="blue">Levoy and Hanrahan 1996</span></a>]. Examples include a system that captures a static scene with an outward-looking spherical capture device [<a href="reference.html#ref69"><span class="blue">Debevec et al. 2015</span></a>] and a system for portraying animated stop motion puppets by capturing sequences of a 360&#176; ring of images around each puppet [<a href="reference.html#ref18"><span class="blue">Bolas et al. 2015</span></a>].</p>
<h4 class="h4"><a id="lev21.6.3"></a><a id="pg248lev1"></a><strong><span class="font1">21.6.3</span> True 3D Capture</strong></h4>
<p class="noindent">Depth cameras or laser scanners capture true 3D data and can result in more presence-inducing experiences at the cost of the scene not seeming quite as real due to artifacts such as gaps or skins (interpolating color where the camera can&#8217;t see). Data used from multiple depth cameras can be used to help reduce some of these artifacts. Data acquired from 3rdTech laser scanners is used for crime scenes where investigators can go back and take measurements after data has been collected (Figure <a href="chapter21.html#fig21.7"><span class="blue">21.7</span></a>). When 3rdTech CEO Nick England (personal communication, June 4, 2015) viewed himself in an HMD lying dead on the floor in a mock murder scene, he witnessed quite a jarring out-of-body experience.</p>
<h4 class="h4"><a id="lev21.6.4"></a><strong><span class="font1">21.6.4</span> Medical and Scientific Data</strong></h4>
<p class="noindent">Figure <a href="chapter21.html#fig21.8"><span class="blue">21.8</span></a> shows an image from iMedic&#8212;immersive medical environment for distributed interactive consultation [<a href="reference.html#ref223"><span class="blue">Mlyniec et al. 2011</span></a>, <a href="reference.html#ref149"><span class="blue">Jerald 2011</span></a>]. iMedic enables real-time interactive exploration of volumetric datasets by &#8220;crawling&#8221; through the data with a 3D multi-touch interface (Section <a href="chapter28.html#lev28.3.3"><span class="blue">28.3.3</span></a>). The mapping of voxel (3D volumetric pixels) source density values to visual transparency can also be controlled via a virtual panel held in the non-dominant hand, enabling real-time changes of what structures can be seen in the dataset.</p>
<p class="image"><a id="page_249"></a><a id="fig21.7"></a><img src="../images/f0249-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 21.7</span> Mock crime scenes captured with a 3rdTech laser scanner. Even though artifacts such as cracks and shadows can be seen in parts of the scene that the laser scanner(s) could not see (e.g., behind the victim&#8217;s legs), the results can be quite compelling.</strong> (Courtesy of 3rdTech)</p>
<p class="image"><a id="fig21.8"></a><img src="../images/f0249-02.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 21.8</span> A real-time voxel-based visualization of a medical dataset. The system enables the user to explore the dataset by &#8220;crawling&#8221; through it with a 3D multi-touch interface.</strong> (Courtesy of Digital ArtForms)</p>
<p class="indent">Visualizing data from within VR can provide significant insight for scientists to understand their data. Multiple depth cues such as head-motion parallax and real walking that are not available with traditional displays enable scientists to gain insight by better seeing shape, protrusions, and relationships they previously did not know existed even when they were, or so they thought, intimately knowledgeable about their data.</p>
<p class="image"><a id="page_250"></a><a id="fig21.9"></a><img src="../images/f0250-01.jpg" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 21.9</span> Whereas this dataset of a fibrin network (green) grown over platelets (blue) may seem like random polygon soup to those not familiar with the subject, scientists intimately familiar with the dataset gained significant insight after physically walking through the dataset with an HMD. Even with such simple rendering, the wide range of depth cues and interactivity provides understanding not possible with traditional tools.</strong> (Courtesy of UNC CISMM NIH Resource 5-P41-EB002025 from data collected in Alisa S. Wolberg&#8217;s laboratory under NIH award HL094740.)</p>
<p class="indent"><a id="pg250lev1"></a>From March 2008 to March 2010, scientists from various departments and organizations used the UNC-Chapel Hill Department of Computer Science VR Lab to physically walk through their own datasets [<a href="reference.html#ref304"><span class="blue">Taylor 2010</span></a>] (Figure <a href="chapter21.html#fig21.9"><span class="blue">21.9</span></a>). In addition to clarifying understanding of their datasets, several scientists were able to gain insight that was not possible with traditional displays they typically used. They understood the complexity of structural protrusions better; saw concentrated material next to small pieces of yeast that led to ideas for new experiments; saw branching protrusions and more complex structures that were completely unexpected; more easily tracked and navigated branching structures of lungs, nasal passages, and fibers; and more easily saw clot heterogeneity to get an idea of overall branch density. In one case, a scientist stated, &#8220;We didn&#8217;t do the experiment right to answer the question about clumping&#160;.&#160;.&#160;.&#160;we didn&#8217;t know that these tools existed to view the data this way.&#8221; If they were able to view the data in an HMD before the experiment, then the experiment design could have been corrected.</p>
</body>
</html>
