<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" >
<head>
<title>The VR Book</title>
<link rel="stylesheet" type="text/css" href="../styles/9781970001143.css"/>
</head>
<body>
<h2 class="h2"><a id="page_85"></a><a id="ch8"></a><span class="blue1">8</span></h2>
<h2 class="h2b"><span class="blue">Perceptual Modalities</span></h2>
<p class="chblock"><em>Mind-machine collaboration demands a two-way channel. The broadband path into the mind is via the eyes. It is not, however, the only path. The ears are especially good for situational awareness, monitoring alerts, sensing environmental changes, and speech. The haptic (feeling) and the olfactory systems seem to access deeper levels of consciousness. Our language is rich in metaphors suggesting this depth. We have &#8220;feelings&#8221; about complex cognitive situations on which we need to &#8220;get a handle&#8221; because we &#8220;smell&#8221; a rat</em>.</p>
<p class="chblock">&#8212;Frederick P. Brooks, Jr. [<a href="reference.html#ref33"><span class="blue">2010</span></a>]</p>
<p class="noindentt"><a id="pg85lev1"></a>We interact with the world through sight, hearing, touch, proprioception, balance/motion, smell, and taste. This chapter discusses all of these with a focus on aspects of each that most relate to VR.</p>
<h3 class="h3"><a id="lev8.1"></a><strong><span class="font">8.1</span> Sight</strong></h3>
<h4 class="h4a"><a id="lev8.1.1"></a><strong><span class="font1">8.1.1</span> The Visual System</strong></h4>
<p class="noindent">Light falls onto photoreceptors in the retina of the eye. These photoreceptors transduce photons into electrochemical signals that travel through different pathways in the brain.</p>
<h5 class="h5"><strong>The Photoreceptors: Cones and Rods</strong></h5>
<p class="noindent">The <strong>retina</strong> is a multilayered network of neurons covering the inside back of the eye that processes photon input. The first retinal layer includes two types of photoreceptors&#8212;cones and rods. <strong>Cones</strong> are primarily responsible for vision during high levels of illumination, color vision, and detailed vision. The <strong>fovea</strong> is a small area in the center of the retina that contains only cones packed densely together. The fovea is located on the line of sight, so that when a person looks at something, its image falls on the fovea. <strong>Rods</strong> are primarily responsible for vision at low levels of illumination and are located across the retina everywhere except the fovea and blind spot. Rods are extremely sensitive in the dark but cannot resolve fine details. Figure <a href="chapter08.html#fig8.1"><span class="blue">8.1</span></a> shows the distribution of rods and cones across the retina.</p>
<p class="image"><a id="page_86"></a><a id="fig8.1"></a><img src="../images/f0086-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 8.1</span> Distribution of rods and cones in the eye.</strong> (Based on <a href="reference.html#ref49"><span class="blue">Coren et al</span></a>. [<a href="reference.html#ref49"><span class="blue">1999</span></a>])</p>
<p class="indent">Electrochemical signals from multiple photoreceptors converge to single neurons in the retina. The number of converging signals per neuron increases toward the periphery, resulting in higher sensitivity to light but decreased visual acuity. In the fovea, some cones have a &#8220;private line&#8221; to structures deeper within the brain [<a href="reference.html#ref98"><span class="blue">Goldstein 2007</span></a>].</p>
<h5 class="h5"><strong>Beyond Cones and Rods: Parvo and Magno Cells</strong></h5>
<p class="noindent">Beyond the first layer of the retina, neurons relating to vision come in many shapes and sizes. Scientists categorize these neurons into parvo cells (those with smaller bodies) and magno cells (those with larger bodies). There are more parvo cells than magno cells with magno cells increasing in distribution toward peripheral vision whereas parvo cells decrease in distribution toward peripheral vision [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>].</p>
<p class="indent"><strong>Parvo cells</strong> have a slower conduction rate (20 m/s) compared to magno cells, have a sustained response (continual neural activity as long as the stimulus remains), have a small receptive field (the area that influences the firing rate of the neuron), and are <a id="pg86lev1"></a>color sensitive. These qualities result in parvo cells being optimized for local shape, spatial analysis, color vision, and texture.</p>
<p class="indent"><a id="page_87"></a><strong>Magno cells</strong> are color blind, have a large receptive field, have a rapid conduction rate (40 m/s), and have a transient response (the neurons briefly fire when a change occurs and then stop responding). Magno cells are optimized for motion detection, time keeping operations/temporal analysis, and depth perception. Magno neurons enable observers to quickly perceive large visuals before perceiving small details.</p>
<h5 class="h5"><strong>Multiple Visual Pathways</strong></h5>
<p class="noindent">Signals from the retina travel along the optic nerve before diverging along different paths.</p>
<p class="noindentt"><strong>The primitive visual pathway.</strong>&#160;&#160;&#160;The <strong>primitive visual pathway</strong> (also known as the tectopulvinar system) starts to branch off at the end of the optic nerve (about 10% of retinal signals, mostly magno cells) toward the <strong>superior colliculus,</strong> a part of the brain just above the brain stem that is much older in evolutionary terms and a more primitive visual center than the cortex. The superior colliculus is highly sensitive to motion and is a primary cause of VR sickness (Part <a href="part03.html#part3"><span class="blue">III</span></a>) as it alters the response to the vestibular system, plays an important role in reflexive eye/neck movement and fixation, changes the curvature of the lens to bring objects into focus (eye accommodation), mediates postural adjustments to visual stimuli, induces nausea, and coordinates the diaphragm and abdominal muscles that evoke vomiting [<a href="reference.html#ref100"><span class="blue">Goldstein 2014</span></a>, <a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>, <a href="reference.html#ref284"><span class="blue">Siegel and Sapru 2014</span></a>, <a href="reference.html#ref170"><span class="blue">Lackner 2014</span></a>].</p>
<p class="indent">The superior colliculus also receives input from the auditory and somatic (e.g., touch and proprioception) sensory systems as well as the visual cortex. Signals from the visual cortex are called <strong>back projections</strong>&#8212;a form of feedback from higher-level areas of the brain based on previous information that has already been processed (i.e., top-down processing).</p>
<p class="noindentt"><strong>The primary visual pathway.</strong>&#160;&#160;&#160;The <strong>primary visual pathway</strong> (also known as the geniculostriate system) goes through the <strong>lateral geniculate nucleus</strong> (LGN) in the thalamus. The LGN acts as a relay center to send visual signals to different parts of the visual cortex. The LGN is represented by central vision more than peripheral vision. In addition to receiving information from the eye, the LGN receives a large amount of information (as much as 80&#8211;90%) from higher visual centers in the cortex (back projections) [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. The LGN also receives information from the reticular activating system, the part of the brain that helps to mediate increased alertness and attention (Section <a href="chapter10.html#lev10.3.1"><span class="blue">10.3.1</span></a>). Thus, LGN processes are not solely a function of the eyes&#8212;visual processing <a id="pg87lev1"></a>is influenced by both bottom-up (from the retina) and top-down processing (from the reticular activating system and cortex) as described in Section <a href="chapter07.html#lev7.3"><span class="blue">7.3</span></a>. In fact, <a id="page_88"></a>the LGN receives more information from the cortex than it sends out to the cortex. What we see is largely based on our experience and how we think.</p>
<p class="indent">Signals from the parvo and magno cells in the LGN feed into the visual cortex (and conversely back projections from the visual cortex feed into the LGN). After the visual cortex, signals diverge into two different pathways toward the temporal and parietal lobes. The path to the temporal lobe is known as the ventral or &#8220;what&#8221; pathway. The path to the parietal lobe is known as the dorsal or &#8220;where/how/action&#8221; pathway. Note the pathways are not totally independent and signals flow in both directions along each pathway.</p>
<p class="noindentt"><strong>The what pathway.</strong>&#160;&#160;&#160;The <strong>ventral pathway</strong> leads to the temporal lobe, which is responsible for recognizing and determining an objects&#8217; identity. Those with damaged temporal lobes can see but have difficulty in counting dots in an array, recognizing new faces, and placing pictures in a sequence that relates a meaningful story or pattern [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. Thus, the ventral pathway is commonly called the &#8220;what&#8221; pathway.</p>
<p class="noindentt"><strong>The where/how/action pathway.</strong>&#160;&#160;&#160;The <strong>dorsal pathway</strong> leads to the parietal lobe, which is responsible for determining an object&#8217;s location (as well as other responsibilities). Picking up an object requires knowing not only where the object is located but also where the hand is located and how it is moving. The parietal reach region at the end of this pathway is the area of the brain that controls both reaching and grasping [<a href="reference.html#ref100"><span class="blue">Goldstein 2014</span></a>]. Thus, the dorsal pathway is commonly called the &#8220;where,&#8221; &#8220;how,&#8221; or &#8220;action&#8221; pathway. Section <a href="chapter10.html#lev10.4"><span class="blue">10.4</span></a> discusses action, which is heavily influenced by the dorsal pathway.</p>
<h5 class="h5"><strong>Central vs. Peripheral Vision</strong></h5>
<p class="noindent">Central and peripheral vision have different properties, due to not only the retina but also the different visual pathways described above. Central vision</p>
<p class="indentbullett">&#8226; has high visual acuity,</p>
<p class="indentbullet">&#8226; is optimized for bright daytime conditions, and</p>
<p class="indentbullet">&#8226; is color-sensitive.</p>
<p class="indentt">Peripheral vision</p>
<p class="indentbullett">&#8226; is color insensitive,</p>
<p class="indentbullet">&#8226; is more sensitive to light than central vision in dark conditions,</p>
<p class="indentbullet">&#8226; is less sensitive to longer wavelengths (i.e., red),</p>
<p class="indentbullet"><a id="pg88lev1"></a>&#8226; has fast response and is more sensitive to fast motion and flicker, and</p>
<p class="indentbullet">&#8226; is less sensitive to slow motions.</p>
<p class="indentt"><a id="page_89"></a>Sensitivity to motion in central and peripheral vision is described further in Section <a href="chapter09.html#lev9.3.4"><span class="blue">9.3.4</span></a>.</p>
<h5 class="h5"><strong>Field of View and Field of Regard</strong></h5>
<p class="noindent">Although we use central vision to see detail, peripheral vision is extremely important to function in real or virtual worlds. In fact, peripheral vision is so important that the US government defines those who cannot see more than 20&#176; in the better eye as legally blind.</p>
<p class="indent">The <strong>field of view</strong> is the angular measure of what can be seen at a single point in time. As can be seen in Figure <a href="chapter08.html#fig8.2"><span class="blue">8.2</span></a>, a human eye has about a 160&#176; horizontal field of view and both eyes can see the same area over an angle of about 120&#176; when looking straight ahead [<a href="reference.html#ref12"><span class="blue">Badcock et al. 2014</span></a>]. The total horizontal field of view when looking straight ahead is about 200&#176;&#8212;thus we can see &#8220;behind&#8221; us by 10&#176; on each side of the head! If we rotate our eyes to one side or another, we can see an additional 50&#176; on each side&#8212;thus for an HMD to cover our entire visual potential range, we would need an HMD with 300&#176; horizontal field of view! Of course we can also turn our bodies and head to see 360&#176; in all directions. The measure for what can be seen by physically rotating the eyes, head, and body is known as <strong>field of regard.</strong> Fully immersive VR has the capability to provide a 360&#176; horizontal and vertical field of regard.</p>
<p class="image"><a id="fig8.2"></a><img src="../images/f0089-01.png" alt="image"/></p>
<p class="caption"><a id="pg89lev1"></a><strong><span class="blue">Figure 8.2</span> Horizontal field of view of the right eye with straight-ahead fixation (looking toward the top of diagram), maximum lateral eye rotation, and maximum lateral head rotation.</strong> (Based on <a href="reference.html#ref12"><span class="blue">Badcock et al</span></a>. [<a href="reference.html#ref12"><span class="blue">2014</span></a>])</p>
<p class="indent"><a id="page_90"></a>Our eyes cannot see nearly as much in the vertical direction due to our foreheads, torso, and the eyes not being vertically additive. We only see about 60&#176; above due to the forehead getting in the way whereas we can see 75&#176; below, for a total of 135&#176; vertical field of view [<a href="reference.html#ref293"><span class="blue">Spector 1990</span></a>].</p>
<h4 class="h4"><a id="lev8.1.2"></a><strong><span class="font1">8.1.2</span> Brightness and Lightness</strong></h4>
<p class="noindent"><strong>Brightness</strong> is the apparent intensity of light that illuminates a region of the visual field. Under ideal conditions, a single photon can stimulate a rod and we can perceive as few as six photons hitting six rods [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. Even at higher levels of illumination, fluctuations of only a few photons can affect brightness perception.</p>
<p class="indent">Brightness is not simply explained by the amount of light reaching the eye, but is also dependent on several other factors. Different wavelengths appear as different brightnesses (e.g., yellow light appears to be brighter than blue light). Peripheral vision <a id="pg90lev1"></a>is more sensitive than foveal vision (looking directly at a dim star can cause it to disappear) for all but longer wavelengths that appear as red in foveal vision. Dark-adapted vision provides six orders of magnitude more sensitivity than light-adapted vision (Section <a href="chapter10.html#lev10.2.1"><span class="blue">10.2.1</span></a>). Longer bursts of light (up to about 100 ms) are more easily detected than shorter bursts, and increasing stimulus sizes are more easily detected up to about 24&#176;. Surrounding stimuli also affect brightness perception. Figure <a href="chapter08.html#fig8.3"><span class="blue">8.3</span></a> shows how a light square can be made to appear darker by darkening the surrounding background.</p>
<p class="indent"><strong>Lightness</strong> (sometimes referred to as whiteness) is the apparent reflectance of a surface, with objects reflecting a small proportion of light appearing dark and objects reflecting a larger proportion of light appearing light/white. The lightness of an object is a function of much more than just the amount of light reaching the eyes. The perception that objects appear to maintain the same lightness even as the amount of light reaching the eyes changes is known as lightness constancy and is discussed in Section <a href="chapter10.html#lev10.1.1"><span class="blue">10.1.1</span></a>.</p>
<h4 class="h4"><a id="lev8.1.3"></a><strong><span class="font1">8.1.3</span> Color</strong></h4>
<p class="noindent">Colors do not exist in the world outside of ourselves, but are created by our perceptual system. What exists in objective reality are different wavelengths of electromagnetic radiation. Although what we call colors is systematically related to light wavelengths, there is nothing intrinsically &#8220;blue&#8221; about short wavelengths or &#8220;red&#8221; about longer wavelengths.</p>
<p class="image"><a id="page_91"></a><a id="fig8.3"></a><img src="../images/f0091-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 8.3</span> The luminance of surrounding stimuli affects our perception of brightness. All four squares have the same luminance, but (d) is perceived to be darkest.</strong> (Based on <a href="reference.html#ref49"><span class="blue">Coren et al.</span></a> [<a href="reference.html#ref49"><span class="blue">1999</span></a>])</p>
<p class="indent">We perceive colors in wavelengths from about 360 nm (violet) to 830 nm (red) [<a href="reference.html#ref243"><span class="blue">Pokorny and Smith 1986</span></a>], and bands of wavelengths within this range are associated with different colors. Colors of objects are largely determined by wavelengths of light that are reflected from the objects into our eyes, which contain three different cone visual pigments with different absorption spectra. Figure <a href="chapter08.html#fig8.4"><span class="blue">8.4</span></a> shows plots of reflected light as a function of wavelength for some common objects. Black paper and white paper both reflect all wavelengths to some extent. When some objects reflect more of some wavelengths than other wavelengths, we call these chromatic colors or hues.</p>
<p class="indent">We can add more variation to hues by changing the intensity and by adding white to change the saturation (e.g., changing red to pink). <strong>Color vision</strong> is the ability to discriminate between stimuli of equal luminance on the basis of wavelength alone. Given equal intensity and saturation, people can discriminate between about 200 colors, but by changing the wavelength, intensity, and saturation, people can differentiate between about one million colors [<a href="reference.html#ref100"><span class="blue">Goldstein 2014</span></a>]. Color discrimination ability decreases with luminance, especially at lower wavelengths, but remains fairly constant at higher luminance. Chromatic discrimination is poor at eccentricities beyond 8&#176;.</p>
<p class="indent">Colors are more than just a direct effect of the physical variation of wavelengths&#8212;colors can subconsciously evoke our emotions and affect our decisions [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. Colors can delight and impress. The color of detergent changes how consumers <a id="pg91lev1"></a>rate the strength of the detergent. The color of pills may affect whether patients take prescribed medications&#8212;black, gray, tan, or brown pills are rejected whereas blue, red, and yellow pills are preferred. Blue colors are described as cool whereas yellows tend to be described as warm. In fact, people turn a heat control to a higher setting in a blue room than in a yellow room. VR creators should be very cognizant of what colors they choose as arbitrary colors may result in unintended experiences.</p>
<p class="image"><a id="page_92"></a><a id="fig8.4"></a><img src="../images/f0092-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 8.4</span> Reflectance curves for different colored surfaces.</strong> (Adapted from <a href="reference.html#ref46"><span class="blue">Clulow</span></a> [<a href="reference.html#ref46"><span class="blue">1972</span></a>])</p>
<h4 class="h4"><a id="lev8.1.4"></a><strong><span class="font1">8.1.4</span> Visual Acuity</strong></h4>
<p class="noindent"><strong>Visual acuity</strong> is the ability to resolve details and is often measured in visual angle. A &#8220;rule of thumb&#8221; is that the thumb or a quarter viewed at arm&#8217;s length subtends an angle of about 2&#176; on the retina. A person with normal eyesight can see a quarter at 81 meters (nearly the length of a football field), which corresponds to 1 arc min (1/60th of a degree) [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>]. Under ideal conditions, we can see a line as thin as 0.5 arc sec (1/7200th of a degree) [<a href="reference.html#ref12"><span class="blue">Badcock et al. 2014</span></a>]!</p>
<h5 class="h5"><strong>Factors</strong></h5>
<p class="noindent">Several factors affect visual acuity. As can be seen in Figure <a href="chapter08.html#fig8.5"><span class="blue">8.5</span></a>, acuity dramatically <a id="pg92lev1"></a>falls off with eye eccentricity. Note how visual acuity matches quite well with the distribution of cones shown in Figure <a href="chapter08.html#fig8.1"><span class="blue">8.1</span></a>. We also have better visual acuity with illumination, high contrast, and long line segments. Visual acuity also depends upon the type of visual acuity that is being measured.</p>
<p class="image"><a id="page_93"></a><a id="fig8.5"></a><img src="../images/f0093-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 8.5</span> Visual acuity is greatest at the fovea.</strong> (Based on <a href="reference.html#ref49"><span class="blue">Coren et al.</span></a> [<a href="reference.html#ref49"><span class="blue">1999</span></a>])</p>
<h5 class="h5"><strong>Types of Visual Acuity</strong></h5>
<p class="noindent">There are different types of visual acuity: detection acuity, separation acuity, grating acuity, vernier acuity, recognition acuity, and stereoscopic acuity. Figure <a href="chapter08.html#fig8.6"><span class="blue">8.6</span></a> shows some acuity targets used for measuring some of these visual acuities.</p>
<p class="indent"><strong>Detection acuity</strong> (also known as visible acuity) is the smallest stimulus that one can detect in an otherwise empty field and represents the absolute threshold of vision. Under ideal conditions a person can see a line as thin as 0.5 arc sec (1/7200&#176; or 0.00014&#176;) [<a href="reference.html#ref12"><span class="blue">Badcock et al. 2014</span></a>]. Increasing the target size up to a point is equivalent to increasing its relative intensity. This is because the mechanism of detection is contrast, and for small objects, minimum visible acuity does not actually depend on <a id="pg93lev1"></a>the width of the line that cannot be discerned. As the line gets thinner, it appears to get fainter but not thinner.</p>
<p class="image"><a id="page_94"></a><a id="fig8.6"></a><img src="../images/f0094-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 8.6</span> Typical acuity targets for different methods of measuring visual acuity.</strong> (Adapted from <a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>)</p>
<p class="indent"><strong>Separation acuity</strong> (also known as resolvable acuity or resolution acuity) is the smallest angular separation between neighboring stimuli that can be resolved, i.e., the two stimuli are perceived as two. More than 5,000 years ago, Egyptians measured separation acuity by the ability to resolve double stars [<a href="reference.html#ref99"><span class="blue">Goldstein 2010</span></a>]. Today, minimum separation acuity is measured by the ability to resolve two black stripes separated by white space. Using this method, observers are able to see the separation of two lines with a cycle of 1 arc min.</p>
<p class="indent"><strong>Grating acuity</strong> is the ability to distinguish the elements of a fine grating composed of alternating dark and light stripes or squares. Grating acuity is similar to separation acuity, but thresholds are lower. Under ideal conditions, grating acuity is 30 arc sec [<a href="reference.html#ref263"><span class="blue">Reichelt et al. 2010</span></a>].</p>
<p class="indent"><strong>Vernier acuity</strong> is the ability to perceive the misalignment of two line segments. Under optimal conditions vernier acuity is very good at 1&#8211;2 arc sec [<a href="reference.html#ref12"><span class="blue">Badcock et al. 2014</span></a>].</p>
<p class="indent"><strong>Recognition acuity</strong> is the ability to recognize simple shapes or symbols such as letters. The most common method of measuring recognition acuity is to use the Snellen eye chart. The observer views the chart from a distance and is asked to identify letters on the chart, and the smallest recognizable letters determine acuity. The smallest letter on the chart is about 5 arc min at 6 m (20 ft); this is also the size of the average newsprint at a normal viewing distance. For the Snellen eye chart, acuity is measured <a id="pg94lev1"></a>relative to the performance of a normal observer. An acuity of 6/6 (20/20) means the observer is able to recognize letters at a distance of 6 m (20 ft) that a normal observer <a id="page_95"></a>is able to recognize. An acuity of 6/9 (20/30) means the observer is able recognize letters at 6 m (20 ft) that a normal observer can recognize at 9 m (30 ft), i.e., the observer cannot see as well.</p>
<p class="indent"><strong>Stereoscopic acuity</strong> is the ability to detect small differences in depth due to the binocular disparity between the two eyes (Section <a href="chapter09.html#lev9.1.3"><span class="blue">9.1.3</span></a>). For complex stimuli, stereoscopic acuity is similar to monocular visual acuity. Under well-lit conditions, stereo acuity of 10 arc sec is assumed to be a reasonable value [<a href="reference.html#ref263"><span class="blue">Reichelt et al. 2010</span></a>]. For simpler targets such as vertical rods, stereoscopic acuity can be as good as 2 arc sec. Factors that affect stereoscopic acuity include spatial frequency, location on the retina (e.g., at an eccentricity of 2&#176;, stereoscopic acuity decreases to 30 arc sec), and observation time (fast-changing scenes result in reduced acuity).</p>
<p class="indent">Stereopsis can result in better visual acuity than monocular vision. In one recent experiment, disparity-based depth perception was determined to occur beyond 40 meters when no other cues were present [<a href="reference.html#ref12"><span class="blue">Badcock et al. 2014</span></a>]. Interestingly, the contribution of stereopsis to depth was considerably greater than that of motion parallax that occurred by moving the head.</p>
<h4 class="h4"><a id="lev8.1.5"></a><strong><span class="font1">8.1.5</span> Eye Movements</strong></h4>
<p class="noindent">Six extraocular muscles control rotations of each eye around three axes. Eye movements can be classifed in several different ways. They are categorized here as gaze-shifting, fixational, and gaze-stabilizing eye movements.</p>
<h5 class="h5"><strong>Gaze-Shifting Eye Movements</strong></h5>
<p class="noindent"><strong>Gaze-shifting eye movements</strong> enable people to track moving objects to look at different objects.</p>
<p class="indent"><strong>Pursuit</strong> is the voluntary tracking with the eye of a visual target. The purpose of pursuit is to stabilize a target on the fovea, in order to maintain maximum resolution, and to prevent motion blur, as the brain is too slow to completely process all details of foveal images moving faster than a few degrees per second.</p>
<p class="indent"><strong>Saccades</strong> are fast voluntary or involuntary movements of the eye that allow different parts of the scene to fall on the fovea and are important for visual scanning (Section <a href="chapter10.html#lev10.3.2"><span class="blue">10.3.2</span></a>). Saccades are the fastest moving external part of the body with speeds up to 1,000&#176;/s and are ballistic where once started the destination cannot be changed [<a href="reference.html#ref31"><span class="blue">Bridgeman et al. 1994</span></a>]. Durations are about 20&#8211;100 ms, with most being about 50 ms <a id="pg95lev1"></a>in duration [<a href="reference.html#ref110"><span class="blue">Hallett 1986</span></a>], and amplitudes are up to 70&#176;. Saccades typically occur at about three times per second.</p>
<p class="indent"><a id="page_96"></a><strong>Saccadic suppression</strong> greatly reduces vision during and just before saccades, effectively blinding observers. Although observers do not consciously notice this loss of vision, events can occur during these saccades and observers will not notice. A scene can rotate by 8&#8211;20% of eye rotation during these saccades without observers noticing [<a href="reference.html#ref319"><span class="blue">Wallach 1987</span></a>]. If eye tracking were available in VR, then the system could perhaps (for the purposes of redirected walking; Section <a href="chapter28.html#lev28.3.1"><span class="blue">28.3.1</span></a>) move the scene during saccades without users perceiving motion.</p>
<p class="indent"><strong>Vergence</strong> is the simultaneous rotation of both eyes in opposite directions in order to obtain or maintain binocular vision for objects at different depths. <strong>Convergence</strong> (the root con means &#8220;toward&#8221;) rotates the eyes toward each other. <strong>Divergence</strong> (the root di means &#8220;apart&#8221;) rotates the eyes away from each other in order to look further into the distance.</p>
<h5 class="h5"><strong>Fixational Eye Movements</strong></h5>
<p class="noindent"><strong>Fixational eye movements</strong> enable people to maintain vision when holding the head still and looking in a single direction. These small movements keep rods and cones from becoming bleached. Humans do not consciously notice these small and involuntary eye movements, but without such eye movements the visual scene would fade into nothingness.</p>
<p class="indent">Small and quick movements of the eyes can be classified as microtremors (less than 1 arc min at 30&#8211;100 Hz) and microsaccades (about 5 arc min at variable rates) [<a href="reference.html#ref110"><span class="blue">Hallett 1986</span></a>].</p>
<p class="indent"><strong>Ocular drift</strong> is slow movement of the eye, and the eye may drift as much as a degree without the observer noticing [<a href="reference.html#ref203"><span class="blue">May and Badcock 2002</span></a>]. Involuntary drifts during attempts at steady fixation have a median extent of 2.5 arc min and have a speed of about 4 arc min per second [<a href="reference.html#ref110"><span class="blue">Hallett 1986</span></a>]. In the dark, the drift rate is faster. Ocular drift plays an important part in the autokinetic illusion described in Section <a href="chapter06.html#lev6.2.7"><span class="blue">6.2.7</span></a>. As discussed there, this may play an important role in judgments of position constancy (Section <a href="chapter10.html#lev10.1.3"><span class="blue">10.1.3</span></a>).</p>
<h5 class="h5"><strong>Gaze-Stabilizing Eye Movements</strong></h5>
<p class="noindent"><strong>Gaze-stabilizing eye movements</strong> enable people to see objects clearly even as their heads move. Gaze-stabilizing eye movements play a role in position constancy adaptation (Section <a href="chapter10.html#lev10.2.2"><span class="blue">10.2.2</span></a>), and the eye movement theory of motion sickness (Section <a href="chapter12.html#lev12.3.5"><span class="blue">12.3.5</span></a>) states problems with gaze-stabilizing eye movements can cause motion sickness.</p>
<p class="indent"><strong>Retinal image slip</strong> is movement of the retina relative to a visual stimulus being <a id="pg96lev1"></a>viewed [<a href="reference.html#ref299"><span class="blue">Stoffregen et al. 2002</span></a>]. The greatest potential source of retinal image slip is due to rotation of the head [<a href="reference.html#ref269"><span class="blue">Robinson 1981</span></a>]. Two mechanisms work together to stabilize <a id="page_97"></a>gaze direction as the head moves&#8212;the vestibulo-ocular reflex and the optokinetic reflex.</p>
<p class="noindentt"><strong>Vestibulo-ocular reflex.</strong>&#160;&#160;&#160;The <strong>vestibulo-ocular reflex</strong> (VOR) rotates the eyes as a function of vestibular input and occurs even in the dark with no visual stimuli. Eye rotations due to the VOR can reach smooth speeds up to 500&#176;/s [<a href="reference.html#ref110"><span class="blue">Hallett 1986</span></a>]. This fast reflex (4&#8211;14 ms from the onset of head motion) serves to keep retinal image slip in the low-frequency range to which the optokinetic reflex (described below) is sensitive; both work together to enable stable vision during head rotation [<a href="reference.html#ref256"><span class="blue">Razzaque 2005</span></a>]. There are also proprioceptive eye-motion reflexes similar to VOR that use the neck, trunk, and even leg motion to help stabilize the eyes.</p>
<p class="noindentt"><strong>Optokinetic reflex.</strong>&#160;&#160;&#160;The <strong>optokinetic reflex</strong> (OKR) stabilizes retinal gaze direction as a function of visual input from the entire retina. If uniform motion of the visual scene occurs on the retina, then the eyes reflexively rotate to compensate. Eye rotations due to OKR can reach smooth speeds up to 80&#176;/s [<a href="reference.html#ref110"><span class="blue">Hallett 1986</span></a>].</p>
<p class="noindentt"><strong>Eye rotation gain. Eye rotation gain</strong>&#160;&#160;&#160;is the ratio of eye rotation velocity divided by head rotation velocity [<a href="reference.html#ref110"><span class="blue">Hallett 1986</span></a>, <a href="reference.html#ref77"><span class="blue">Draper 1998</span></a>]. A gain of 1.0 means that as the head rotates to the right the eyes rotate by an equal amount to the left so that the eyes are looking in the same direction as they were at the start of the head turn. Gain due to the VOR alone (i.e., in the dark) is approximately 0.7. If the observer imagines a stable target in the world, gain increases to 0.95. If the observer imagines a target that turns with the head, then gain is suppressed to 0.2 or lower. Thus, VOR is not perfect, and OKR corrects for residual error. VOR is most effective at 1&#8211;7 Hz (e.g., VOR helps maintain fixation on objects while walking or running) and progressively less effective at lower frequencies, particularly below 0.1 Hz. OKR is most effective at frequencies below 0.1 Hz and has decreasing effectiveness at 0.1&#8211;1 Hz. Thus, the VOR and OKR complement each other for typical head motions&#8212;both VOR and OKR working together result in a gain close to 1 over a wide range of head motions.</p>
<p class="indent">Gain also depends on the distance to the visual target being viewed. For an object at an infinite distance, the eyes are looking straight ahead in parallel. In this case, gain is ideally equal to 1.0 so that the target image remains on the fovea. For a closer target, eye rotation must be greater than head rotation for the image to remain on the fovea. These differences in gains are because rotational axis for the head is different than the rotational axis of the eyes. The differences in gain can quickly be demonstrated <a id="pg97lev1"></a>by comparing eye movements while looking at a finger held in front of the eyes versus an object further in the distance.</p>
<p class="noindentt"><a id="page_98"></a><strong>Active vs. passive head and eye motion.</strong>&#160;&#160;&#160;Active head rotations by the observer result in more robust and more consistent VOR gains and phase lag than passive rotations by an external force (e.g., being automatically rotated in a chair) [<a href="reference.html#ref77"><span class="blue">Draper 1998</span></a>]. Similarily, sensitivity to motion of a visual scene while the head moves depends upon whether the head is actively moved or passively moved [<a href="reference.html#ref130"><span class="blue">Howard 1986a</span></a>, <a href="reference.html#ref77"><span class="blue">Draper 1998</span></a>]. Afference and efference help explain motion perception by taking into account active eye movements. See Section <a href="chapter07.html#lev7.4"><span class="blue">7.4</span></a> for an explanation of afference and efference, and an explanation of how the world appears stable even as the eyes move.</p>
<p class="noindentt"><strong>Nystagmus. Nystagmus</strong>&#160;&#160;&#160;is a rhythmic and involuntary rotation of the eyes [<a href="reference.html#ref130"><span class="blue">Howard 1986a</span></a>] caused by both VOR and OKR [<a href="reference.html#ref256"><span class="blue">Razzaque 2005</span></a>] that can help stabilize gaze. Researchers typically discuss nystagmus caused by rotating the observer continuously at a constant angular velocity. The slow phase of nystagmus is the rotation of the eyes to keep the observer looking straight ahead in the world as the chair rotates. As the eyes reach their maximum amount of rotation in the eye sockets, a saccade snaps the eyes back to looking straight ahead relative to the head. This rotation is called the fast phase of nystagmus. This pattern repeats, resulting in a rhythmic rotation of the eyes. Nystagmus can also be demonstrated by looking at someone&#8217;s eyes after they have become dizzy from spinning that stimulates their vestibular system [<a href="reference.html#ref49"><span class="blue">Coren et al. 1999</span></a>].</p>
<p class="indent"><strong>Pendular nystagmus</strong> occurs when one rotates her head back and forth at a fixed frequency. This results in an always-changing slow phase with no fast phase. Pendular nystagmus occurs when walking and running. Those looking for latency in HMD often quickly rotate their head back and forth to try to estimate latency by seeing how the scene moves as they rotate their head. Little is known about how nystagmus works when perceiving such latency-induced scene motion in an HMD (see Chapter <a href="chapter15.html#ch15"><span class="blue">15</span></a>). A user&#8217;s eye gaze might remain stationary in space (because of the VOR) when looking for scene motion, resulting in retinal image slip, or might follow the scene (because of OKR), resulting in no retinal image slip. This likely varies by the amount of head motion, the type of task, the individual, etc. Eye tracking would allow an investigator to study this in detail. Subjects in one latency experiment [<a href="reference.html#ref83"><span class="blue">Ellis et al. 2004</span></a>] claimed to concentrate on a single feature in the moving visual field. However, this was anecdotal evidence and was not verified.</p>
<h4 class="h4"><a id="lev8.1.6"></a><strong><span class="font1">8.1.6</span> Visual Displays</strong></h4>
<p class="noindent">Given that each eye can see about 210&#176; by rotating the eye and assuming vernier or <a id="pg98lev1"></a>stereoscopic acuity of 2 arc sec, then a display would require horizontal resolution of 378,000 pixels for each eye to match what we can see in reality! This number is a simple <a id="page_99"></a>extreme analysis as this is for perfectly ideal situations and we can design around limitations for different circumstances. For example, since we do not perceive such high resolution outside of central vision, eye tracking could be used to only render at high resolution where the user is looking. This would require new algorithms, nonuniform display hardware, and fast eye tracking. Clearly, there is plenty of work to be done to get to the point of truly simulating visual reality. Latency will be even more of a concern in this case, as the system will be racing the eye to display at high resolution by the time the user is looking at a specific location.</p>
<h3 class="h3"><a id="lev8.2"></a><strong><span class="font">8.2</span> Hearing</strong></h3>
<p class="noindent">Auditory perception is quite complex and is affected by head pose, physiology, expectation, and its relationship to other sensory modality cues. We can deduce qualities of the environment from sound (e.g., large rooms sound different than small rooms), and we can determine where an object is located by its sound alone. Below are discussed general concepts of sound, and Section <a href="chapter21.html#lev21.3"><span class="blue">21.3</span></a> discusses audio as it applies specifically to VR.</p>
<h4 class="h4"><a id="lev8.2.1"></a><strong><span class="font1">8.2.1</span> Properties of Sound</strong></h4>
<p class="noindent">Sound can be broken down into two distinct parts: the physical aspects and the perceptual aspects.</p>
<h5 class="h5"><strong>Physical Aspects</strong></h5>
<p class="noindent">A person need not be present for a tree falling in the woods to make a sound. Physical sound is a pressure wave in a material medium (i.e., air) created when an object vibrates rapidly back and forth. <strong>Sound frequency</strong> is the number of cycles per second (hertz (Hz)) or vibrations that a change in pressure repeats. <strong>Sound amplitude</strong> is the difference in pressure between the high and low peaks of the sound wave. The decibel (dB) is a logarithmic transformation of sound amplitude where doubling the sound amplitude results in an increase of 3 dB.</p>
<h5 class="h5"><strong>Perceptual Aspects</strong></h5>
<p class="noindent">Physical sound enters the ear, the eardrum is stimulated, and then receptor cells transduce those sound vibrations into electrical signals. The brain then processes these electrical signals into qualities of sound such as loudness, pitch, and timbre.</p>
<p class="indent">Loudness is the perceptual quality most closely related to the amplitude of a sound, although frequency can affect loudness. A 10 dB increase (a bit over three doublings of amplitude) results in approximately twice the subjective loudness.</p>
<p class="indent"><a id="page_100"></a>Pitch is most closely related to the physical property of fundamental frequency. Low frequencies are associated with low pitches and high frequencies are associated with high pitches. Most real-world sounds are not strictly periodic, as they do not have repeated temporal patterns but have fluctuations from one cycle to the next. The perceived pitch of such imperfect sound patterns is the average period of the cyclical variations. Other sounds such as noise are non-periodic and do not have a perceived pitch.</p>
<p class="indent">Timbre is closely related to the harmonic structure (both strength of the harmonics and number of harmonics) of a sound and is the quality that distinguishes between two tones that have the same loudness, pitch, and duration, but sound different. For example, a guitar has more high-frequency harmonics than either the bassoon or alto saxophone [<a href="reference.html#ref100"><span class="blue">Goldstein 2014</span></a>]. Timbre also depends on the attack (the buildup of sound at the beginning of the tone) and the tone&#8217;s decay (the decrease in sound at the end of the tone). Playing a recording of a piano backwards sounds more like an organ because the original decay has become the attack and the attack has become the decay.</p>
<h5 class="h5"><strong>Auditory Thresholds</strong></h5>
<p class="noindent">Humans can hear frequencies from about 20 to 22,000 Hz [<a href="reference.html#ref317"><span class="blue">Vorlander and Shinn-Cunningham 2014</span></a>] with the most sensitivity occurring at about 2,000&#8211;4,000 Hz, which is the range of frequencies that is most important for understanding speech [<a href="reference.html#ref100"><span class="blue">Goldstein 2014</span></a>]. We hear sound in a wide range of about a factor of a trillion (120 dB) before the onset of pain begins. Most sounds encountered in everyday experience span a dynamic intensity range of 80&#8211;90 dB. Figure <a href="chapter08.html#fig8.7"><span class="blue">8.7</span></a> shows what amplitudes and frequencies we can hear and how perceived volume is significantly dependent upon frequency&#8212;lower- and higher-frequency sounds require greater amplitude than midrange frequencies for us to perceive equal loudness. This is largely due to the outer ear&#8217;s auditory canal reinforcing/amplifying midrange frequencies. Interestingly, we only perceive musical melodies for pitches below 5,000 Hz [<a href="reference.html#ref9"><span class="blue">Attneave and Olson 1971</span></a>)].</p>
<p class="indent">The auditory channel is much more sensitive to temporal variations than both vision and proprioception. For example, amplitude fluctuations can be detected at 50 Hz [<a href="reference.html#ref341"><span class="blue">Yost 2006</span></a>] and temporal fluctuations can be detected at up to 1,000 Hz. Not only can listeners detect rapid fluctuations but they can react quickly. Reaction times to auditory stimuli are faster than visual reaction times by 30&#8211;40 ms [<a href="reference.html#ref329"><span class="blue">Welch and Warren 1986</span></a>].</p>
<h4 class="h4"><a id="lev8.2.2"></a><strong><span class="font1">8.2.2</span> Binaural Cues</strong></h4>
<p class="noindent"><a id="pg100lev1"></a><strong>Binaural cues</strong> (also known as stereophonic cues) are two different audio cues, one for each ear, that help to determine the position of sounds. Each ear hears a slightly different sound&#8212;different in time and different in level. Interaural time differences provide an effective cue for localizing low-frequency sounds. Interaural time differences as small as ~10 ms can be discerned [<a href="reference.html#ref162"><span class="blue">Klumpp 1956</span></a>]. Interaural level differences (called acoustic shadows) occur due to acoustic energy being reflected and diffracted by the head, providing an effective cue for localizing sounds above 2 kHz [<a href="reference.html#ref24"><span class="blue">Bowman et al. 2004</span></a>].</p>
<p class="image"><a id="page_101"></a><a id="fig8.7"></a><img src="../images/f0101-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 8.7</span> The audibility curve and auditory response area. The area above the audibility curve <a id="pg101lev1"></a>represents volume and frequencies that we can hear. The area above the threshold of feeling can result in pain.</strong> (Adapted from <a href="reference.html#ref100"><span class="blue">Goldstein</span></a> [<a href="reference.html#ref100"><span class="blue">2014</span></a>])</p>
<p class="indent">Monaural cues use differences in the distribution (or spectrum) of frequencies entering the ear (due to the shape of the ear) to help determine the position of sounds. This monaural cue is helpful for determining the elevation (up/down) direction of a sound where binaural cues are not helpful. Head motion also helps determine where a sound is coming from due to changes in interaural time differences, interaural level differences, and monaural spectral cues. See Section <a href="chapter21.html#lev21.3"><span class="blue">21.3</span></a> for a discussion of head-related transfer functions.</p>
<p class="indent">Spatial acuity of the auditory system is not nearly as good as vision. We can detect differences of sound at about 1&#176; in front of or behind us, but our sensitivity decreases to 10&#176; when the sound is to the far left/right side of us and 15&#176; when the sound is above/below us. Vision can play a role in locating sounds&#8212;see Section <a href="chapter08.html#lev8.7"><span class="blue">8.7</span></a>.</p>
<h4 class="h4"><a id="page_102"></a><a id="lev8.2.3"></a><strong><span class="blue"><span class="font1">8.2.3</span></span> Speech Perception</strong></h4>
<p class="noindent">A <strong>phoneme</strong> is the smallest perceptually distinct unit of sound in a language that helps to distinguish between similar-sounding words. Phonemes combine to form morphemes, words, and sentences. Different languages use different sounds, so the number of phonemes varies across languages. For example, American English has 47 phonemes, Hawaiian has 11 phonemes, and some African languages have up to 60 phonemes [<a href="reference.html#ref99"><span class="blue">Goldstein 2014</span></a>].</p>
<p class="indent">A <strong>morpheme</strong> is a minimal grammatical unit of a language, with each morpheme constituting a word or meaningful part of a word that cannot be divided into smaller independent grammatical parts. A morpheme can be, but is not necessarily, a subset of a word (i.e., every word is comprised of one or more morphemes). When a morpheme stands by itself, it is considered a root because it has meaning on its own. When a morpheme depends on another morpheme to express an idea, it is an affix because it has a grammatical function (e.g., adding an <em>s</em> to the end of a word to make it plural). The more ways morphemes can be combined with other morphemes, the more productive that morpheme is. Phoneme and morpheme awareness is the ability to identify speech sounds, the vocal gestures from which words are constructed, when <a id="pg102lev1"></a>they are found in their natural context&#8212;spoken words.</p>
<p class="indent"><strong>Speech segmentation</strong> is the perception of individual words in a conversation even when the acoustic signal is continuous. Our perception of words is not solely based upon energy stimulating our receptors, but is heavily influenced by our experience with those sounds and relationships between those sounds. To someone listening to an unfamiliar foreign language, words seem to speed by in a single stream of sound.</p>
<p class="indent">It is easier to perceive phonemes in a meaningful context. <a href="reference.html#ref323"><span class="blue">Warren</span></a> [<a href="reference.html#ref323"><span class="blue">1970</span></a>] had test subjects listen to the sentence &#8220;The state governors met with their respective legislatures convening in the capital city.&#8221; When the first <em>s</em> in &#8220;legislatures&#8221; was replaced with a cough sound, none of the subjects were able to state where in the sentence the cough occurred or that the <em>s</em> in &#8220;legislatures&#8221; was missing. This is called the <strong>phonemic restoration effect.</strong></p>
<p class="indent"><a href="reference.html#ref273"><span class="blue">Samuel</span></a> [<a href="reference.html#ref273"><span class="blue">1981</span></a>] used the phonemic restoration effect to show that speech perception is determined both by the acoustic signal (bottom-up processing) and by context that produces expectations in the listener (top-down processing). Samuel also found longer words increase the likelihood of the phonemic restoration effect. A similar effect occurs for meaningfulness of spoken words in a sentence and knowledge of the rules of grammar; it is easier to perceive spoken words when heard in the context of familiar grammatical sentences [<a href="reference.html#ref215"><span class="blue">Miller and Isard 1963</span></a>].</p>
<h3 class="h3"><a id="page_103"></a><a id="lev8.3"></a><strong><span class="blue"><span class="font">8.3</span></span> Touch</strong></h3>
<p class="noindent">When we touch something or we are touched, receptors in the skin provide information about what is happening to our skin and about the object contacting the skin. These receptors enable us to perceive information about small details, vibration, textures, shapes, and potentially damaging stimuli.</p>
<p class="indent">Although those who are deaf or blind can get along surprisingly well, those with a rare condition that results in losing the sensation of touch often suffer constant bruising, burns, and broken bones due to the absence of warnings provided by touch and pain. Losing the sense of touch also makes it difficult to interact with the environment. Without the feedback of touch, actions as simple as picking objects up or typing on a keyboard can be difficult. Unfortunately, touch is extremely challenging to implement in VR but by understanding how we perceive touch, we can at least take advantage of providing some simple cues to users.</p>
<p class="indent">Adults have 1.3&#8211;1.7 square m (14&#8211;18 sq ft) of skin. However, the brain does not consider all skin to be created equally. Humans are very dependent on speech and manipulation of objects through the use of the hands, thus we have large amounts of our brain devoted to the hands. The right side of Figure <a href="chapter08.html#fig8.8"><span class="blue">8.8</span></a> shows the sensory <a id="pg103lev1"></a>homunculus (&#8220;little man&#8221; in Latin), which represents the location and proportion of sensory cortex devoted to different body parts. The proportion of sensory cortex devoted to each body part also correlates with the density of tactile receptors on that body part. The left side shows the motor homunculus that helps to plan and execute movements. Some areas of the body are represented by a disproportionately large area of the sensory and motor cortexes, notably the lips, tongue, and hands.</p>
<p class="image"><a id="fig8.8"></a><img src="../images/f0103-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 8.8</span> This motor and sensory homunculus represents &#8220;the body within the brain.&#8221; The size of a body part in the diagram represents the amount of cerebral cortex devoted to that body part.</strong> (Based on <a href="reference.html#ref39"><span class="blue">Burton</span></a> [<a href="reference.html#ref39"><span class="blue">2012</span></a>])</p>
<h4 class="h4"><a id="page_104"></a><a id="lev8.3.1"></a><strong><span class="blue"><span class="font1">8.3.1</span></span> Vibration</strong></h4>
<p class="noindent">The skin is capable of detecting not only spatial details but vibrations as well. This is due to mechanoreceptors called Pacinian corpuscles. Nerve fibers located within corpuscles respond slowly to slow or constant pushing, but respond well to high vibrational frequencies.</p>
<h4 class="h4"><a id="lev8.3.2"></a><strong><span class="blue"><span class="font1">8.3.2</span></span> Texture</strong></h4>
<p class="noindent">Depending on vision for perceiving texture is not always sufficient, because seeing that texture is dependent on lighting. We can also perceive texture through touch. Spatial cues are provided by surface elements such as bumps and grooves, resulting in feelings of shape, size, and distribution of surface elements.</p>
<p class="indent">Temporal cues occur when the skin moves across a textured surface, and occur as a form of vibration. Fine textures are often only felt with a finger moving across the surface. Temporal cues are also important for feeling surfaces indirectly through the use of tools (e.g., dragging a stick across a rough surface); this is typically felt not as vibration of the tool but texture of the surface even though the finger is not touching the texture.</p>
<p class="indent">Most VR creators do not consider physical textures. Where they are considered is with passive haptics (Section <a href="chapter03.html#lev3.2.3"><span class="blue">3.2.3</span></a>), such as when building physical devices (e.g., hand-held controllers or a steering system) and real-world surfaces that users interact with while immersed.</p>
<h4 class="h4"><a id="lev8.3.3"></a><strong><span class="blue"><span class="font1">8.3.3</span></span> Passive Touch vs. Active Touch</strong></h4>
<p class="noindent"><strong>Passive touch</strong> occurs when stimuli are applied to the skin. Passive touch can be quite compelling in VR when combined with visuals. For example, rubbing a real feather on the skin of users when they see a virtual feather rubbing their physical body is used quite effectively to embody users into their avatars (Section <a href="chapter04.html#lev4.3"><span class="blue">4.3</span></a>).</p>
<p class="indent"><strong>Active touch</strong> occurs when a person actively explores an object, usually with the fingers and hands. Note passive and active touch is not to be confused with passive and active haptics as discussed in Section <a href="chapter03.html#lev3.2.3"><span class="blue">3.2.3</span></a>. Humans use three distinct systems together when using active touch.</p>
<p class="indentbullett">&#8226; The sensory system, used in detecting cutaneous sensations such as touch, temperature, textures, and positions/movements of the fingers.</p>
<p class="indentbullet"><a id="page_105"></a>&#8226; The motor system, used in moving the fingers and hands.</p>
<p class="indentbullet">&#8226; The cognitive system, used in thinking about the information provided by the sensory and motor systems.</p>
<p class="indentt">These systems work together to create an experience that is quite different from passive touch. For passive touch, the sensation is typically experienced on the skin, whereas for active touch, we perceive the object being touched.</p>
<h4 class="h4"><a id="lev8.3.4"></a><strong><span class="blue"><span class="font1">8.3.4</span></span> Pain</strong></h4>
<p class="noindent"><strong>Pain</strong> functions to warn us of dangerous situations. Three types of pain are</p>
<p class="indentbullett">&#8226; neuropathic pain caused by lesions, repetitive tasks (e.g., carpal tunnel syndrome), or damage to the nervous system (e.g., spinal cord injury or stroke);</p>
<p class="indentbullet">&#8226; nociceptive pain caused by activation of receptors in the skin called nociceptors, which are specialized to respond to tissue damage or potential damage from heat, chemicals, pressure, and cold; and</p>
<p class="indentbullet">&#8226; inflammatory pain caused by previous damage to tissue, inflammation of joints, <a id="pg105lev1"></a>or tumor cells.</p>
<p class="indentt">The perception of pain is strongly affected by factors other than just stimulation of skin, such as expectation, attention, distracting stimuli, and hypnotic suggestion. One specific example is phantom limb pain, where individuals who have had a limb amputated continue to experience the limb, and in some cases continue to experience pain in a limb that does not exist [<a href="reference.html#ref253"><span class="blue">Ramachandran and Hirstein 1998</span></a>]. An example of reducing pain using VR is Snow World, a distracting VR game set in a cold-conveying environment that doctors use when they have burn victims&#8217; bandages removed [<a href="reference.html#ref123"><span class="blue">Hoffman 2004</span></a>].</p>
<h3 class="h3"><a id="lev8.4"></a><strong><span class="blue"><span class="font">8.4</span></span> Proprioception</strong></h3>
<p class="noindent"><strong>Proprioception</strong> is the sensation of limb and whole body pose and motion derived from the receptors of muscles, tendons, and joint capsules. Proprioception enables us to touch our nose with a hand even when the eyes are closed. Proprioception includes both conscious and subconscious components, not only enabling us to sense the position and motion of our limbs, but also providing us the sensation of force generation enabling us to regulate force output.</p>
<p class="indent">Whereas touch seems straightforward because we are often aware of the sensations that result, proprioception is more mysterious because we are largely unaware of it and take it for granted during daily living. However, as VR creators, becoming familiar <a id="page_106"></a>with the sense of proprioception is important for understanding how users physically move to interact with a virtual environment (at least until directly connected neural interfaces become common). This includes moving the head, eyes, limbs, and/or whole body. Without the senses of touch and proprioception, we would crush brittle objects when picking them up. Proprioception can be very useful for designing VR interactions as discussed in Section <a href="chapter26.html#lev26.2"><span class="blue">26.2</span></a>.</p>
<h3 class="h3"><a id="lev8.5"></a><strong><span class="blue"><span class="font">8.5</span></span> Balance and Physical Motion</strong></h3>
<p class="indent">The <strong>vestibular system</strong> consists of labyrinths in the inner ears that act as mechanical motion detectors (Figure <a href="chapter08.html#fig8.9"><span class="blue">8.9</span></a>), which provide input for balance and sensing physical motion. The vestibular organs are composed of the otolith organs and the semicircular canals.</p>
<p class="indent">Each set (right and left) of two <strong>otolith organs</strong> acts as a three-axis accelerometer, measuring linear acceleration. Cessation of linear motion is sensed almost immediately by the otolith organs [<a href="reference.html#ref131"><span class="blue">Howard 1986b</span></a>]. The nervous system&#8217;s interpretation of signals from the otolith organs relies almost entirely on the direction and not the magnitude of acceleration [<a href="reference.html#ref256"><span class="blue">Razzaque 2005</span></a>].</p>
<p class="image"><a id="pg106lev1"></a><a id="fig8.9"></a><img src="../images/f0106-01.png" alt="image"/></p>
<p class="caption"><strong><span class="blue">Figure 8.9</span> A cutaway illustration of the outer, middle, and inner ear, revealing the vestibular system.</strong> (Based on <a href="reference.html#ref143"><span class="blue">Jerald</span></a> [<a href="reference.html#ref143"><span class="blue">2009</span></a>], adapted from <a href="reference.html#ref200"><span class="blue">Martini</span></a> [<a href="reference.html#ref200"><span class="blue">1998</span></a>])</p>
<p class="indent"><a id="page_107"></a>Each set of the three nearly orthogonal <strong>semicircular canals</strong> (SCCs) acts as a three-axis gyroscope. The SCCs act primarily as gyroscopes measuring angular velocity, but only for a limited amount of time if velocity is constant. After 3&#8211;30 seconds the SCCs cannot disambiguate between no velocity and some constant velocity [<a href="reference.html#ref256"><span class="blue">Razzaque 2005</span></a>]. In real and virtual worlds, head angular velocity is almost never constant other than zero velocity. The SCCs are most sensitive between 0.1 Hz and 5.0 Hz. Below 0.1 Hz, SCC output is roughly equal to angular acceleration, 0.1&#8211;5.0 Hz roughly equal to angular velocity, and above 5.0 Hz roughly equal to angular displacement [<a href="reference.html#ref131"><span class="blue">Howard 1986b</span></a>]. The 0.1&#8211;5.0 Hz range fits well within typical head motions&#8212;head motions while walking (at least one foot always touching the ground) are in the 1&#8211;2 Hz range and 3&#8211;6 Hz while running (moments when neither foot is touching the ground) [<a href="reference.html#ref77"><span class="blue">Draper 1998</span></a>, <a href="reference.html#ref256"><span class="blue">Razzaque 2005</span></a>].</p>
<p class="indent">Although we are not typically aware of the vestibular system in normal situations, we can become very aware of this sense in atypical situations when things go wrong. Understanding the vestibular system is extremely important for creating VR content <a id="pg107lev1"></a>as motion sickness can result when vestibular stimuli does not match stimuli from the other senses. The vestibular system and its relationship to motion sickness are discussed in more detail in Chapter <a href="chapter12.html#ch12"><span class="blue">12</span></a>.</p>
<h3 class="h3"><a id="lev8.6"></a><strong><span class="blue"><span class="font">8.6</span></span> Smell and Taste</strong></h3>
<p class="noindent">Smell and taste both work through chemoreceptors, where the chemoreceptors detect chemical stimuli in the environment.</p>
<p class="indent"><strong>Smell</strong> (also known as olfactory perception) is the ability to perceive odors when odorant airborne molecules bind to specific sites on the olfactory receptors high in the nose. Very low concentrations of material can be detected by the olfactory system. The number of smells the olfactory system can distinguish between is highly debated with different researchers claiming a wide range from hundreds to trillions. Different people smell different odors due to genetic differences.</p>
<p class="indent"><strong>Taste</strong> (also known as gustatory perception) is the chemosensory sensation of substances on the tongue. The human tongue by itself can distinguish among five universal tastes: sweet, sour, bitter, salty, and umami.</p>
<p class="indent">Smell, taste, temperature, and texture all combine together to provide the perception of <strong>flavor</strong>. Combining these senses provides a much wider range of flavor than if taste alone determined flavor. In fact, a pseudo-gustatory VR system has been shown <a id="page_108"></a>to fool users into thinking a plain cookie tasted differently by providing different visual and smell cues [<a href="reference.html#ref221"><span class="blue">Miyaura et al. 2011</span></a>].</p>
<h3 class="h3"><a id="lev8.7"></a><strong><span class="blue"><span class="font">8.7</span></span> Multimodal Perceptions</strong></h3>
<p class="noindent">Integration of our different senses occurs automatically and rarely does perception occur as a function of a single modality. Examining each of the senses as if it were independent of the others leads to only partial understanding of everyday perceptual experience [<a href="reference.html#ref329"><span class="blue">Welch and Warren 1986</span></a>]. <a href="reference.html#ref283"><span class="blue">Sherrington</span></a> [<a href="reference.html#ref283"><span class="blue">1920</span></a>] states, &#8220;All parts of the nervous system are connected together and no part of it is probably ever capable of action without affecting and being affected by various other parts.&#8221;</p>
<p class="indent">Perception of a single modality can influence other modalities. A surprising example is if audio cues are not precisely synchronized with visual cues, then visual perception may be affected. For example, auditory stimulation can influence the visual flicker-fusion frequency (the frequency at which subjects start to notice the flashing of a visual stimulus; see Section <a href="chapter09.html#lev9.2.4"><span class="blue">9.2.4</span></a>) [<a href="reference.html#ref329"><span class="blue">Welch and Warren 1986</span></a>].</p>
<p class="indent"><a href="reference.html#ref279"><span class="blue">Sekuler et al.</span></a> [<a href="reference.html#ref279"><span class="blue">1997</span></a>] found that when two moving shapes cross on a display, most subjects perceived these shapes as moving past each other and continuing their <a id="pg108lev1"></a>straight-line motion. When a &#8220;click&#8221; sounded just when the shapes appeared adjacent to each other, a majority of people perceived the shapes as colliding and bouncing off in opposite directions. This is congruent with what typically happens in the real world&#8212;when a sound occurs just as two moving objects become close to each other, a collision has likely occurred that caused that sound.</p>
<p class="indent">Perceiving speech is often a multisensory experience involving both audition and vision. <strong>Lip sync</strong> refers to the synchronization between the visual movement of the speaker&#8217;s lips and the spoken voice. Thresholds for perceiving speech synchronization vary depending on the complexity, congruency, and predictability of the audiovisual event as well as the context and applied experimental methodology [<a href="reference.html#ref81"><span class="blue">Eg and Behne 2013</span></a>]. In general, we are more tolerant of visuals leading sound and thresholds increase with sound complexity (e.g., we notice asynchronies less for sentences than for syllables). One study found that when audio precedes video by 5 video fields (about 80 ms), viewers evaluated speakers more negatively (e.g., less interesting, more unpleasant, less influential, more agitated, and less successful) even when they did not consciously identify the asynchrony [<a href="reference.html#ref259"><span class="blue">Reeves and Voelker 1993</span></a>].</p>
<p class="indent">The McGurk effect [<a href="reference.html#ref205"><span class="blue">McGurk and MacDonald 1976</span></a>] illustrates how visual information can exert a strong influence on what we hear. For example, when a listener hears the sound /ba-ba/ but is viewing a person making lip movements for the sound /ga-ga/, the listener hears the sound /da-da/.</p>
<p class="indent"><a id="page_109"></a>Vision tends to dominate other sensory modalities [<a href="reference.html#ref245"><span class="blue">Posner et al. 1976</span></a>]. For example, vision dominates spatialized audio. <strong>Visual capture</strong> or the ventriloquism effect occurs when sounds coming from one location (e.g., the speakers in a movie theater) are mislocalized to seem to come from a place of visual motion (e.g., an actor&#8217;s mouth on the screen). This occurs due to the tendency to try to identify visual events or objects that could be causing the sound.</p>
<p class="indent">In many cases, when vision and proprioception disagree, we tend to perceive hand position to be where vision tells us it is [<a href="reference.html#ref95"><span class="blue">Gibson 1933</span></a>]. Under certain conditions, VR users can be made to believe that their hand is touching a different seen shape than a felt shape [<a href="reference.html#ref163"><span class="blue">Kohli 2013</span></a>]. Under other conditions, VR users are more sensitive to their virtual hand visually penetrating a virtual object than they are to the proprioceptive sense of their hand not being colocated with their visual hand in space [<a href="reference.html#ref38"><span class="blue">Burns et al. 2006</span></a>]. Sensory substitution can be used to partially make up for the lack of physical touch in VR systems as discussed in Section <a href="chapter26.html#lev26.8"><span class="blue">26.8</span></a>.</p>
<p class="indent">Mismatch of visual and vestibular cues is a major problem of VR as it can cause motion sickness. Chapter <a href="chapter12.html#ch12"><span class="blue">12</span></a> discusses such cue conflicts in detail.</p>
<h4 class="h4"><a id="lev8.7.1"></a><strong><span class="blue"><span class="font1">8.7.1</span></span> Visual and Vestibular Cues Complement Each Other</strong></h4>
<p class="noindent"><a id="pg109lev1"></a>The vestibular system provides primarily first-order approximations of angular velocity and linear acceleration, and positional drift occurs over time [<a href="reference.html#ref131"><span class="blue">Howard 1986b</span></a>]. Thus, absolute position or orientation cannot be determined from vestibular cues alone. Visual and vestibular cues combine to enable people to disambiguate between moving stimuli and self-motion.</p>
<p class="indent">The visual system is good at capturing lower-frequency motions whereas the vestibular system is better at detecting high-frequency motions. The vestibular system is a mechanical system and has a faster response (as fast as 3&#8211;5 ms!) than the slower electrochemical visual response of the eyes. Thus, the vestibular system is initially more responsive to a sudden onset of physical motion. After some time of sustained constant velocity, vestibular cues subside and visual cues take over. Midrange frequencies use both vestibular and visual cues.</p>
<p class="indent">Missing or misleading vestibular cues can lead to life-threatening motion illusions in pilots [<a href="reference.html#ref256"><span class="blue">Razzaque 2005</span></a>]. For example, the vestibular otolith organs cannot distinguish between linear acceleration and tilt. This ambiguity can be resolved from the semicircular canals when within the canals&#8217; sensitive range. However, when not within the sensitive range, the ambiguity is resolved from visual cues. Aircraft accidents, and loss of many lives, have occurred due to the ambiguity when visual cues are missing (e.g., low visibility conditions). This ambiguity can be used to our advantage in VR when motion platforms are available by tilting the platform to give a sense of forward acceleration that can be extremely compelling.</p>
</body>
</html>
